03-Mar-22 09:01:51 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar100', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar100//checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:01:51 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:05:31 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar100', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar100/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:05:31 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:09:00 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar100', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar100/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:09:00 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:09:00 - match all modules defined in bit_config: False
03-Mar-22 09:09:00 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:09:05 - Epoch: [0][  0/352]	Time  0.435 ( 0.435)	Data  0.236 ( 0.236)	Loss 1.7540e+00 (1.7540e+00)	Acc@1  61.72 ( 61.72)	Acc@5  85.94 ( 85.94)
03-Mar-22 09:09:06 - Epoch: [0][ 10/352]	Time  0.155 ( 0.158)	Data  0.002 ( 0.023)	Loss 1.4186e+00 (1.4944e+00)	Acc@1  61.72 ( 65.70)	Acc@5  87.50 ( 89.91)
03-Mar-22 09:09:08 - Epoch: [0][ 20/352]	Time  0.130 ( 0.146)	Data  0.002 ( 0.013)	Loss 1.3235e+00 (1.4372e+00)	Acc@1  71.09 ( 66.37)	Acc@5  93.75 ( 90.81)
03-Mar-22 09:09:09 - Epoch: [0][ 30/352]	Time  0.159 ( 0.142)	Data  0.002 ( 0.010)	Loss 1.2741e+00 (1.4074e+00)	Acc@1  66.41 ( 66.76)	Acc@5  91.41 ( 90.57)
03-Mar-22 09:09:10 - Epoch: [0][ 40/352]	Time  0.134 ( 0.140)	Data  0.002 ( 0.008)	Loss 1.3852e+00 (1.3743e+00)	Acc@1  61.72 ( 67.04)	Acc@5  90.62 ( 90.66)
03-Mar-22 09:09:12 - Epoch: [0][ 50/352]	Time  0.141 ( 0.141)	Data  0.003 ( 0.007)	Loss 1.1733e+00 (1.3412e+00)	Acc@1  74.22 ( 67.86)	Acc@5  89.84 ( 90.98)
03-Mar-22 09:09:13 - Epoch: [0][ 60/352]	Time  0.136 ( 0.141)	Data  0.003 ( 0.006)	Loss 1.1195e+00 (1.3258e+00)	Acc@1  68.75 ( 68.03)	Acc@5  95.31 ( 91.16)
03-Mar-22 09:09:14 - Epoch: [0][ 70/352]	Time  0.162 ( 0.140)	Data  0.002 ( 0.006)	Loss 1.1554e+00 (1.3083e+00)	Acc@1  67.19 ( 68.32)	Acc@5  92.19 ( 91.26)
03-Mar-22 09:09:16 - Epoch: [0][ 80/352]	Time  0.127 ( 0.139)	Data  0.003 ( 0.005)	Loss 1.2360e+00 (1.2939e+00)	Acc@1  70.31 ( 68.65)	Acc@5  92.19 ( 91.39)
03-Mar-22 09:09:17 - Epoch: [0][ 90/352]	Time  0.132 ( 0.138)	Data  0.002 ( 0.005)	Loss 1.0652e+00 (1.2731e+00)	Acc@1  72.66 ( 69.08)	Acc@5  94.53 ( 91.60)
03-Mar-22 09:09:18 - Epoch: [0][100/352]	Time  0.129 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.0455e+00 (1.2594e+00)	Acc@1  75.78 ( 69.30)	Acc@5  95.31 ( 91.71)
03-Mar-22 09:09:20 - Epoch: [0][110/352]	Time  0.132 ( 0.137)	Data  0.002 ( 0.005)	Loss 9.3381e-01 (1.2398e+00)	Acc@1  78.91 ( 69.67)	Acc@5  96.88 ( 91.93)
03-Mar-22 09:09:21 - Epoch: [0][120/352]	Time  0.132 ( 0.137)	Data  0.003 ( 0.004)	Loss 1.1611e+00 (1.2289e+00)	Acc@1  72.66 ( 69.89)	Acc@5  90.62 ( 91.97)
03-Mar-22 09:09:22 - Epoch: [0][130/352]	Time  0.155 ( 0.136)	Data  0.002 ( 0.004)	Loss 1.0086e+00 (1.2181e+00)	Acc@1  74.22 ( 70.12)	Acc@5  96.09 ( 92.08)
03-Mar-22 09:09:24 - Epoch: [0][140/352]	Time  0.122 ( 0.136)	Data  0.002 ( 0.004)	Loss 1.0565e+00 (1.2113e+00)	Acc@1  76.56 ( 70.19)	Acc@5  92.97 ( 92.08)
03-Mar-22 09:09:25 - Epoch: [0][150/352]	Time  0.158 ( 0.136)	Data  0.002 ( 0.004)	Loss 1.1923e+00 (1.1999e+00)	Acc@1  67.97 ( 70.36)	Acc@5  94.53 ( 92.23)
03-Mar-22 09:09:26 - Epoch: [0][160/352]	Time  0.113 ( 0.136)	Data  0.002 ( 0.004)	Loss 1.0746e+00 (1.1932e+00)	Acc@1  71.09 ( 70.54)	Acc@5  92.97 ( 92.28)
03-Mar-22 09:09:28 - Epoch: [0][170/352]	Time  0.155 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.2766e+00 (1.1869e+00)	Acc@1  64.06 ( 70.60)	Acc@5  90.62 ( 92.32)
03-Mar-22 09:09:29 - Epoch: [0][180/352]	Time  0.131 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.2568e+00 (1.1800e+00)	Acc@1  68.75 ( 70.74)	Acc@5  90.62 ( 92.40)
03-Mar-22 09:09:30 - Epoch: [0][190/352]	Time  0.152 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.0162e+00 (1.1707e+00)	Acc@1  73.44 ( 70.88)	Acc@5  92.97 ( 92.50)
03-Mar-22 09:09:32 - Epoch: [0][200/352]	Time  0.130 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.0725e+00 (1.1673e+00)	Acc@1  73.44 ( 70.92)	Acc@5  91.41 ( 92.49)
03-Mar-22 09:09:33 - Epoch: [0][210/352]	Time  0.130 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.0683e+00 (1.1617e+00)	Acc@1  69.53 ( 70.94)	Acc@5  92.97 ( 92.57)
03-Mar-22 09:09:34 - Epoch: [0][220/352]	Time  0.128 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.1946e+00 (1.1590e+00)	Acc@1  64.06 ( 70.87)	Acc@5  92.19 ( 92.60)
03-Mar-22 09:09:36 - Epoch: [0][230/352]	Time  0.152 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.1550e+00 (1.1562e+00)	Acc@1  67.97 ( 70.87)	Acc@5  93.75 ( 92.60)
03-Mar-22 09:09:37 - Epoch: [0][240/352]	Time  0.128 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.0745e+00 (1.1531e+00)	Acc@1  71.88 ( 70.94)	Acc@5  95.31 ( 92.63)
03-Mar-22 09:09:38 - Epoch: [0][250/352]	Time  0.158 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.2385e+00 (1.1503e+00)	Acc@1  67.97 ( 70.97)	Acc@5  87.50 ( 92.65)
03-Mar-22 09:09:40 - Epoch: [0][260/352]	Time  0.112 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.0706e+00 (1.1458e+00)	Acc@1  67.97 ( 71.06)	Acc@5  95.31 ( 92.68)
03-Mar-22 09:09:41 - Epoch: [0][270/352]	Time  0.134 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.2071e+00 (1.1440e+00)	Acc@1  68.75 ( 71.07)	Acc@5  92.97 ( 92.69)
03-Mar-22 09:09:42 - Epoch: [0][280/352]	Time  0.124 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.1118e+00 (1.1416e+00)	Acc@1  69.53 ( 71.12)	Acc@5  93.75 ( 92.72)
03-Mar-22 09:09:43 - Epoch: [0][290/352]	Time  0.154 ( 0.134)	Data  0.002 ( 0.003)	Loss 9.0021e-01 (1.1371e+00)	Acc@1  71.88 ( 71.17)	Acc@5  96.09 ( 92.79)
03-Mar-22 09:09:45 - Epoch: [0][300/352]	Time  0.128 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.2259e+00 (1.1343e+00)	Acc@1  69.53 ( 71.18)	Acc@5  92.19 ( 92.82)
03-Mar-22 09:09:46 - Epoch: [0][310/352]	Time  0.157 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.0633e+00 (1.1325e+00)	Acc@1  70.31 ( 71.20)	Acc@5  92.97 ( 92.80)
03-Mar-22 09:09:47 - Epoch: [0][320/352]	Time  0.131 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.1271e+00 (1.1304e+00)	Acc@1  70.31 ( 71.20)	Acc@5  91.41 ( 92.83)
03-Mar-22 09:09:49 - Epoch: [0][330/352]	Time  0.152 ( 0.134)	Data  0.002 ( 0.003)	Loss 8.6877e-01 (1.1266e+00)	Acc@1  78.91 ( 71.29)	Acc@5  96.09 ( 92.86)
03-Mar-22 09:09:50 - Epoch: [0][340/352]	Time  0.123 ( 0.134)	Data  0.002 ( 0.003)	Loss 9.9527e-01 (1.1267e+00)	Acc@1  73.44 ( 71.27)	Acc@5  93.75 ( 92.82)
03-Mar-22 09:09:51 - Epoch: [0][350/352]	Time  0.121 ( 0.133)	Data  0.002 ( 0.003)	Loss 9.5544e-01 (1.1223e+00)	Acc@1  74.22 ( 71.37)	Acc@5  94.53 ( 92.86)
03-Mar-22 09:09:52 - Test: [ 0/20]	Time  0.364 ( 0.364)	Loss 1.0615e+00 (1.0615e+00)	Acc@1  69.53 ( 69.53)	Acc@5  94.14 ( 94.14)
03-Mar-22 09:09:53 - Test: [10/20]	Time  0.069 ( 0.100)	Loss 8.7278e-01 (1.0054e+00)	Acc@1  75.78 ( 73.37)	Acc@5  95.31 ( 94.00)
03-Mar-22 09:09:54 -  * Acc@1 73.220 Acc@5 93.860
03-Mar-22 09:09:54 - Best acc at epoch 0: 73.22000122070312
03-Mar-22 09:09:54 - Epoch: [1][  0/352]	Time  0.333 ( 0.333)	Data  0.232 ( 0.232)	Loss 9.8704e-01 (9.8704e-01)	Acc@1  75.78 ( 75.78)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:09:55 - Epoch: [1][ 10/352]	Time  0.106 ( 0.141)	Data  0.002 ( 0.023)	Loss 1.0265e+00 (1.0525e+00)	Acc@1  77.34 ( 73.51)	Acc@5  92.97 ( 92.83)
03-Mar-22 09:09:56 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar100', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar100/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:09:56 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:09:56 - match all modules defined in bit_config: False
03-Mar-22 09:09:56 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:09:56 - Epoch: [1][ 20/352]	Time  0.103 ( 0.132)	Data  0.001 ( 0.013)	Loss 1.0799e+00 (1.0789e+00)	Acc@1  71.09 ( 71.32)	Acc@5  95.31 ( 92.56)
03-Mar-22 09:09:58 - Epoch: [1][ 30/352]	Time  0.129 ( 0.132)	Data  0.002 ( 0.010)	Loss 7.6985e-01 (1.0340e+00)	Acc@1  82.81 ( 72.73)	Acc@5  97.66 ( 93.07)
03-Mar-22 09:09:59 - Epoch: [1][ 40/352]	Time  0.127 ( 0.132)	Data  0.002 ( 0.008)	Loss 9.8064e-01 (1.0289e+00)	Acc@1  70.31 ( 72.56)	Acc@5  94.53 ( 93.50)
03-Mar-22 09:10:00 - Epoch: [1][ 50/352]	Time  0.117 ( 0.133)	Data  0.002 ( 0.007)	Loss 9.9767e-01 (1.0340e+00)	Acc@1  72.66 ( 72.50)	Acc@5  94.53 ( 93.41)
03-Mar-22 09:10:01 - Epoch: [0][  0/352]	Time  0.488 ( 0.488)	Data  0.252 ( 0.252)	Loss 1.5176e+00 (1.5176e+00)	Acc@1  69.53 ( 69.53)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:10:02 - Epoch: [1][ 60/352]	Time  0.157 ( 0.134)	Data  0.002 ( 0.006)	Loss 8.9517e-01 (1.0337e+00)	Acc@1  76.56 ( 72.59)	Acc@5  94.53 ( 93.40)
03-Mar-22 09:10:02 - Epoch: [0][ 10/352]	Time  0.155 ( 0.183)	Data  0.002 ( 0.025)	Loss 1.5948e+00 (1.4596e+00)	Acc@1  64.06 ( 67.05)	Acc@5  87.50 ( 90.20)
03-Mar-22 09:10:03 - Epoch: [1][ 70/352]	Time  0.139 ( 0.134)	Data  0.003 ( 0.006)	Loss 1.2048e+00 (1.0416e+00)	Acc@1  71.88 ( 72.57)	Acc@5  91.41 ( 93.30)
03-Mar-22 09:10:04 - Epoch: [0][ 20/352]	Time  0.159 ( 0.167)	Data  0.002 ( 0.014)	Loss 1.2542e+00 (1.4121e+00)	Acc@1  71.09 ( 67.26)	Acc@5  94.53 ( 90.77)
03-Mar-22 09:10:04 - Epoch: [1][ 80/352]	Time  0.132 ( 0.134)	Data  0.002 ( 0.005)	Loss 9.3981e-01 (1.0415e+00)	Acc@1  73.44 ( 72.36)	Acc@5  92.97 ( 93.40)
03-Mar-22 09:10:05 - Epoch: [0][ 30/352]	Time  0.154 ( 0.164)	Data  0.002 ( 0.010)	Loss 1.1674e+00 (1.3601e+00)	Acc@1  72.66 ( 68.17)	Acc@5  94.53 ( 91.08)
03-Mar-22 09:10:06 - Epoch: [1][ 90/352]	Time  0.152 ( 0.135)	Data  0.002 ( 0.005)	Loss 1.1145e+00 (1.0421e+00)	Acc@1  70.31 ( 72.28)	Acc@5  90.62 ( 93.34)
03-Mar-22 09:10:07 - Epoch: [0][ 40/352]	Time  0.156 ( 0.160)	Data  0.002 ( 0.008)	Loss 1.1699e+00 (1.3326e+00)	Acc@1  70.31 ( 68.16)	Acc@5  94.53 ( 91.58)
03-Mar-22 09:10:07 - Epoch: [1][100/352]	Time  0.161 ( 0.136)	Data  0.003 ( 0.005)	Loss 9.3933e-01 (1.0387e+00)	Acc@1  74.22 ( 72.25)	Acc@5  93.75 ( 93.37)
03-Mar-22 09:10:08 - Epoch: [0][ 50/352]	Time  0.157 ( 0.158)	Data  0.002 ( 0.007)	Loss 9.7523e-01 (1.2992e+00)	Acc@1  75.78 ( 68.72)	Acc@5  97.66 ( 91.82)
03-Mar-22 09:10:09 - Epoch: [1][110/352]	Time  0.150 ( 0.138)	Data  0.002 ( 0.004)	Loss 1.2442e+00 (1.0394e+00)	Acc@1  65.62 ( 72.23)	Acc@5  92.97 ( 93.43)
03-Mar-22 09:10:10 - Epoch: [0][ 60/352]	Time  0.139 ( 0.156)	Data  0.002 ( 0.006)	Loss 1.0778e+00 (1.2902e+00)	Acc@1  73.44 ( 69.11)	Acc@5  94.53 ( 91.78)
03-Mar-22 09:10:10 - Epoch: [1][120/352]	Time  0.164 ( 0.139)	Data  0.002 ( 0.004)	Loss 1.1310e+00 (1.0405e+00)	Acc@1  65.62 ( 72.22)	Acc@5  91.41 ( 93.45)
03-Mar-22 09:10:11 - Epoch: [0][ 70/352]	Time  0.148 ( 0.155)	Data  0.003 ( 0.006)	Loss 1.1248e+00 (1.2714e+00)	Acc@1  72.66 ( 69.44)	Acc@5  91.41 ( 91.90)
03-Mar-22 09:10:12 - Epoch: [1][130/352]	Time  0.185 ( 0.140)	Data  0.003 ( 0.004)	Loss 1.0913e+00 (1.0385e+00)	Acc@1  66.41 ( 72.24)	Acc@5  93.75 ( 93.53)
03-Mar-22 09:10:13 - Epoch: [0][ 80/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.005)	Loss 1.0439e+00 (1.2512e+00)	Acc@1  75.78 ( 69.82)	Acc@5  94.53 ( 92.15)
03-Mar-22 09:10:14 - Epoch: [1][140/352]	Time  0.138 ( 0.141)	Data  0.002 ( 0.004)	Loss 9.4310e-01 (1.0354e+00)	Acc@1  75.78 ( 72.29)	Acc@5  94.53 ( 93.59)
03-Mar-22 09:10:14 - Epoch: [0][ 90/352]	Time  0.149 ( 0.154)	Data  0.003 ( 0.005)	Loss 1.0903e+00 (1.2383e+00)	Acc@1  75.00 ( 70.05)	Acc@5  91.41 ( 92.18)
03-Mar-22 09:10:15 - Epoch: [1][150/352]	Time  0.146 ( 0.141)	Data  0.002 ( 0.004)	Loss 1.1577e+00 (1.0389e+00)	Acc@1  66.41 ( 72.23)	Acc@5  92.19 ( 93.52)
03-Mar-22 09:10:16 - Epoch: [0][100/352]	Time  0.151 ( 0.153)	Data  0.002 ( 0.005)	Loss 1.1581e+00 (1.2264e+00)	Acc@1  71.09 ( 70.11)	Acc@5  92.19 ( 92.30)
03-Mar-22 09:10:16 - Epoch: [1][160/352]	Time  0.148 ( 0.141)	Data  0.003 ( 0.004)	Loss 8.7117e-01 (1.0388e+00)	Acc@1  79.69 ( 72.20)	Acc@5  97.66 ( 93.57)
03-Mar-22 09:10:17 - Epoch: [0][110/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.005)	Loss 1.1687e+00 (1.2170e+00)	Acc@1  70.31 ( 70.24)	Acc@5  90.62 ( 92.34)
03-Mar-22 09:10:18 - Epoch: [1][170/352]	Time  0.151 ( 0.142)	Data  0.002 ( 0.004)	Loss 8.6067e-01 (1.0330e+00)	Acc@1  79.69 ( 72.35)	Acc@5  96.09 ( 93.64)
03-Mar-22 09:10:19 - Epoch: [0][120/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1405e+00 (1.2073e+00)	Acc@1  68.75 ( 70.25)	Acc@5  93.75 ( 92.43)
03-Mar-22 09:10:19 - Epoch: [1][180/352]	Time  0.152 ( 0.142)	Data  0.002 ( 0.004)	Loss 8.7439e-01 (1.0306e+00)	Acc@1  77.34 ( 72.38)	Acc@5  93.75 ( 93.65)
03-Mar-22 09:10:20 - Epoch: [0][130/352]	Time  0.157 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1549e+00 (1.1987e+00)	Acc@1  72.66 ( 70.43)	Acc@5  90.62 ( 92.52)
03-Mar-22 09:10:21 - Epoch: [1][190/352]	Time  0.131 ( 0.143)	Data  0.002 ( 0.004)	Loss 1.0112e+00 (1.0297e+00)	Acc@1  72.66 ( 72.43)	Acc@5  94.53 ( 93.62)
03-Mar-22 09:10:22 - Epoch: [0][140/352]	Time  0.149 ( 0.152)	Data  0.003 ( 0.004)	Loss 8.6043e-01 (1.1898e+00)	Acc@1  80.47 ( 70.55)	Acc@5  94.53 ( 92.61)
03-Mar-22 09:10:22 - Epoch: [1][200/352]	Time  0.153 ( 0.143)	Data  0.002 ( 0.004)	Loss 1.2096e+00 (1.0312e+00)	Acc@1  64.06 ( 72.38)	Acc@5  90.62 ( 93.56)
03-Mar-22 09:10:23 - Epoch: [0][150/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1209e+00 (1.1796e+00)	Acc@1  72.66 ( 70.73)	Acc@5  92.97 ( 92.68)
03-Mar-22 09:10:24 - Epoch: [1][210/352]	Time  0.141 ( 0.143)	Data  0.002 ( 0.003)	Loss 9.9856e-01 (1.0326e+00)	Acc@1  72.66 ( 72.36)	Acc@5  97.66 ( 93.55)
03-Mar-22 09:10:24 - Epoch: [0][160/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1752e+00 (1.1757e+00)	Acc@1  67.97 ( 70.76)	Acc@5  92.97 ( 92.74)
03-Mar-22 09:10:25 - Epoch: [1][220/352]	Time  0.179 ( 0.144)	Data  0.002 ( 0.003)	Loss 8.7209e-01 (1.0310e+00)	Acc@1  77.34 ( 72.44)	Acc@5  95.31 ( 93.57)
03-Mar-22 09:10:26 - Epoch: [0][170/352]	Time  0.156 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.2048e+00 (1.1726e+00)	Acc@1  72.66 ( 70.82)	Acc@5  90.62 ( 92.71)
03-Mar-22 09:10:27 - Epoch: [1][230/352]	Time  0.146 ( 0.144)	Data  0.002 ( 0.003)	Loss 9.4343e-01 (1.0286e+00)	Acc@1  75.78 ( 72.50)	Acc@5  92.97 ( 93.61)
03-Mar-22 09:10:27 - Epoch: [0][180/352]	Time  0.150 ( 0.151)	Data  0.003 ( 0.004)	Loss 1.0019e+00 (1.1670e+00)	Acc@1  69.53 ( 70.90)	Acc@5  96.09 ( 92.77)
03-Mar-22 09:10:28 - Epoch: [1][240/352]	Time  0.125 ( 0.144)	Data  0.002 ( 0.003)	Loss 9.9233e-01 (1.0293e+00)	Acc@1  71.88 ( 72.41)	Acc@5  93.75 ( 93.60)
03-Mar-22 09:10:29 - Epoch: [0][190/352]	Time  0.146 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.4084e-01 (1.1611e+00)	Acc@1  75.00 ( 70.94)	Acc@5  94.53 ( 92.82)
03-Mar-22 09:10:30 - Epoch: [1][250/352]	Time  0.149 ( 0.144)	Data  0.002 ( 0.003)	Loss 1.1268e+00 (1.0277e+00)	Acc@1  70.31 ( 72.49)	Acc@5  91.41 ( 93.61)
03-Mar-22 09:10:30 - Epoch: [0][200/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.0456e+00 (1.1586e+00)	Acc@1  74.22 ( 70.97)	Acc@5  95.31 ( 92.80)
03-Mar-22 09:10:31 - Epoch: [1][260/352]	Time  0.150 ( 0.144)	Data  0.002 ( 0.003)	Loss 9.9108e-01 (1.0250e+00)	Acc@1  72.66 ( 72.58)	Acc@5  93.75 ( 93.64)
03-Mar-22 09:10:32 - Epoch: [0][210/352]	Time  0.156 ( 0.151)	Data  0.003 ( 0.004)	Loss 1.0136e+00 (1.1531e+00)	Acc@1  73.44 ( 71.02)	Acc@5  93.75 ( 92.86)
03-Mar-22 09:10:33 - Epoch: [1][270/352]	Time  0.148 ( 0.144)	Data  0.002 ( 0.003)	Loss 1.1788e+00 (1.0234e+00)	Acc@1  69.53 ( 72.62)	Acc@5  91.41 ( 93.66)
03-Mar-22 09:10:33 - Epoch: [0][220/352]	Time  0.155 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.1635e-01 (1.1470e+00)	Acc@1  80.47 ( 71.11)	Acc@5  96.88 ( 92.91)
03-Mar-22 09:10:34 - Epoch: [1][280/352]	Time  0.135 ( 0.144)	Data  0.002 ( 0.003)	Loss 1.0616e+00 (1.0228e+00)	Acc@1  65.62 ( 72.59)	Acc@5  96.88 ( 93.68)
03-Mar-22 09:10:35 - Epoch: [0][230/352]	Time  0.154 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.1147e+00 (1.1440e+00)	Acc@1  71.88 ( 71.15)	Acc@5  89.84 ( 92.94)
03-Mar-22 09:10:36 - Epoch: [1][290/352]	Time  0.152 ( 0.144)	Data  0.002 ( 0.003)	Loss 9.1484e-01 (1.0216e+00)	Acc@1  75.78 ( 72.58)	Acc@5  96.88 ( 93.69)
03-Mar-22 09:10:36 - Epoch: [0][240/352]	Time  0.157 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.6322e-01 (1.1401e+00)	Acc@1  74.22 ( 71.20)	Acc@5  96.09 ( 92.98)
03-Mar-22 09:10:37 - Epoch: [1][300/352]	Time  0.143 ( 0.144)	Data  0.002 ( 0.003)	Loss 1.0086e+00 (1.0208e+00)	Acc@1  74.22 ( 72.61)	Acc@5  92.97 ( 93.70)
03-Mar-22 09:10:38 - Epoch: [0][250/352]	Time  0.179 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.1529e+00 (1.1354e+00)	Acc@1  73.44 ( 71.30)	Acc@5  92.19 ( 93.02)
03-Mar-22 09:10:39 - Epoch: [1][310/352]	Time  0.147 ( 0.144)	Data  0.002 ( 0.003)	Loss 9.6277e-01 (1.0216e+00)	Acc@1  76.56 ( 72.60)	Acc@5  91.41 ( 93.65)
03-Mar-22 09:10:39 - Epoch: [0][260/352]	Time  0.155 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0709e+00 (1.1336e+00)	Acc@1  69.53 ( 71.28)	Acc@5  92.19 ( 93.04)
03-Mar-22 09:10:40 - Epoch: [1][320/352]	Time  0.165 ( 0.145)	Data  0.003 ( 0.003)	Loss 1.0568e+00 (1.0210e+00)	Acc@1  66.41 ( 72.59)	Acc@5  94.53 ( 93.67)
03-Mar-22 09:10:41 - Epoch: [0][270/352]	Time  0.146 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0964e+00 (1.1297e+00)	Acc@1  75.00 ( 71.33)	Acc@5  92.97 ( 93.11)
03-Mar-22 09:10:42 - Epoch: [1][330/352]	Time  0.155 ( 0.145)	Data  0.002 ( 0.003)	Loss 9.4207e-01 (1.0203e+00)	Acc@1  73.44 ( 72.63)	Acc@5  94.53 ( 93.68)
03-Mar-22 09:10:43 - Epoch: [0][280/352]	Time  0.164 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.2807e-01 (1.1267e+00)	Acc@1  73.44 ( 71.37)	Acc@5  96.88 ( 93.12)
03-Mar-22 09:10:43 - Epoch: [1][340/352]	Time  0.149 ( 0.145)	Data  0.003 ( 0.003)	Loss 1.1209e+00 (1.0199e+00)	Acc@1  69.53 ( 72.62)	Acc@5  91.41 ( 93.67)
03-Mar-22 09:10:44 - Epoch: [0][290/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.2027e+00 (1.1268e+00)	Acc@1  67.97 ( 71.33)	Acc@5  92.97 ( 93.12)
03-Mar-22 09:10:45 - Epoch: [1][350/352]	Time  0.149 ( 0.145)	Data  0.002 ( 0.003)	Loss 1.0919e+00 (1.0205e+00)	Acc@1  68.75 ( 72.63)	Acc@5  92.97 ( 93.67)
03-Mar-22 09:10:45 - Test: [ 0/20]	Time  0.366 ( 0.366)	Loss 1.0610e+00 (1.0610e+00)	Acc@1  69.14 ( 69.14)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:10:46 - Epoch: [0][300/352]	Time  0.159 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0888e+00 (1.1238e+00)	Acc@1  64.84 ( 71.33)	Acc@5  94.53 ( 93.14)
03-Mar-22 09:10:46 - Test: [10/20]	Time  0.097 ( 0.124)	Loss 8.5148e-01 (9.7810e-01)	Acc@1  77.73 ( 72.98)	Acc@5  93.36 ( 93.71)
03-Mar-22 09:10:47 -  * Acc@1 73.480 Acc@5 93.600
03-Mar-22 09:10:47 - Epoch: [0][310/352]	Time  0.109 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0904e+00 (1.1217e+00)	Acc@1  75.78 ( 71.34)	Acc@5  92.97 ( 93.13)
03-Mar-22 09:10:47 - Best acc at epoch 1: 73.47999572753906
03-Mar-22 09:10:48 - Epoch: [2][  0/352]	Time  0.382 ( 0.382)	Data  0.231 ( 0.231)	Loss 8.2319e-01 (8.2319e-01)	Acc@1  79.69 ( 79.69)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:10:48 - Epoch: [0][320/352]	Time  0.141 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.3252e-01 (1.1202e+00)	Acc@1  80.47 ( 71.33)	Acc@5  97.66 ( 93.14)
03-Mar-22 09:10:49 - Epoch: [2][ 10/352]	Time  0.133 ( 0.166)	Data  0.002 ( 0.024)	Loss 9.8632e-01 (9.9702e-01)	Acc@1  70.31 ( 72.30)	Acc@5  95.31 ( 94.25)
03-Mar-22 09:10:50 - Epoch: [0][330/352]	Time  0.136 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0716e+00 (1.1186e+00)	Acc@1  67.19 ( 71.34)	Acc@5  94.53 ( 93.15)
03-Mar-22 09:10:50 - Epoch: [2][ 20/352]	Time  0.144 ( 0.156)	Data  0.002 ( 0.013)	Loss 1.1357e+00 (9.9967e-01)	Acc@1  69.53 ( 72.84)	Acc@5  92.97 ( 94.05)
03-Mar-22 09:10:51 - Epoch: [0][340/352]	Time  0.152 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.4007e-01 (1.1142e+00)	Acc@1  77.34 ( 71.48)	Acc@5  92.97 ( 93.19)
03-Mar-22 09:10:52 - Epoch: [2][ 30/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.010)	Loss 8.5674e-01 (1.0027e+00)	Acc@1  76.56 ( 72.91)	Acc@5  94.53 ( 93.80)
03-Mar-22 09:10:53 - Epoch: [0][350/352]	Time  0.145 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0712e+00 (1.1120e+00)	Acc@1  69.53 ( 71.49)	Acc@5  92.97 ( 93.22)
03-Mar-22 09:10:53 - Epoch: [2][ 40/352]	Time  0.109 ( 0.150)	Data  0.002 ( 0.008)	Loss 1.1215e+00 (1.0105e+00)	Acc@1  64.84 ( 72.54)	Acc@5  92.19 ( 93.50)
03-Mar-22 09:10:53 - Test: [ 0/20]	Time  0.386 ( 0.386)	Loss 1.1059e+00 (1.1059e+00)	Acc@1  69.14 ( 69.14)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:10:55 - Test: [10/20]	Time  0.098 ( 0.129)	Loss 8.7308e-01 (1.0151e+00)	Acc@1  77.34 ( 72.55)	Acc@5  94.14 ( 93.61)
03-Mar-22 09:10:55 - Epoch: [2][ 50/352]	Time  0.165 ( 0.150)	Data  0.002 ( 0.007)	Loss 8.7777e-01 (9.9835e-01)	Acc@1  78.12 ( 72.82)	Acc@5  96.09 ( 93.73)
03-Mar-22 09:10:55 -  * Acc@1 73.100 Acc@5 93.800
03-Mar-22 09:10:56 - Best acc at epoch 0: 73.0999984741211
03-Mar-22 09:10:56 - Epoch: [1][  0/352]	Time  0.400 ( 0.400)	Data  0.248 ( 0.248)	Loss 8.5433e-01 (8.5433e-01)	Acc@1  78.91 ( 78.91)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:10:56 - Epoch: [2][ 60/352]	Time  0.150 ( 0.149)	Data  0.002 ( 0.006)	Loss 9.8278e-01 (9.8712e-01)	Acc@1  75.00 ( 73.45)	Acc@5  95.31 ( 93.89)
03-Mar-22 09:10:57 - Epoch: [1][ 10/352]	Time  0.132 ( 0.170)	Data  0.002 ( 0.024)	Loss 9.1422e-01 (1.0115e+00)	Acc@1  80.47 ( 74.01)	Acc@5  95.31 ( 93.96)
03-Mar-22 09:10:58 - Epoch: [2][ 70/352]	Time  0.159 ( 0.149)	Data  0.002 ( 0.006)	Loss 8.6314e-01 (9.8590e-01)	Acc@1  75.78 ( 73.35)	Acc@5  97.66 ( 93.90)
03-Mar-22 09:10:59 - Epoch: [1][ 20/352]	Time  0.143 ( 0.160)	Data  0.002 ( 0.014)	Loss 1.1363e+00 (1.0175e+00)	Acc@1  67.97 ( 73.66)	Acc@5  90.62 ( 93.64)
03-Mar-22 09:10:59 - Epoch: [2][ 80/352]	Time  0.136 ( 0.149)	Data  0.002 ( 0.005)	Loss 8.4967e-01 (9.8482e-01)	Acc@1  74.22 ( 73.40)	Acc@5  95.31 ( 93.79)
03-Mar-22 09:11:00 - Epoch: [1][ 30/352]	Time  0.141 ( 0.156)	Data  0.002 ( 0.010)	Loss 1.2426e+00 (1.0268e+00)	Acc@1  70.31 ( 73.14)	Acc@5  89.84 ( 93.80)
03-Mar-22 09:11:01 - Epoch: [2][ 90/352]	Time  0.156 ( 0.149)	Data  0.003 ( 0.005)	Loss 9.6408e-01 (9.8376e-01)	Acc@1  69.53 ( 73.36)	Acc@5  96.88 ( 93.84)
03-Mar-22 09:11:02 - Epoch: [1][ 40/352]	Time  0.131 ( 0.153)	Data  0.002 ( 0.008)	Loss 9.2495e-01 (1.0384e+00)	Acc@1  72.66 ( 72.62)	Acc@5  95.31 ( 93.60)
03-Mar-22 09:11:02 - Epoch: [2][100/352]	Time  0.154 ( 0.148)	Data  0.002 ( 0.005)	Loss 9.3084e-01 (9.7920e-01)	Acc@1  73.44 ( 73.46)	Acc@5  96.09 ( 93.93)
03-Mar-22 09:11:03 - Epoch: [1][ 50/352]	Time  0.169 ( 0.153)	Data  0.002 ( 0.007)	Loss 1.0605e+00 (1.0381e+00)	Acc@1  72.66 ( 72.58)	Acc@5  92.97 ( 93.64)
03-Mar-22 09:11:04 - Epoch: [2][110/352]	Time  0.146 ( 0.148)	Data  0.003 ( 0.004)	Loss 9.4590e-01 (9.8234e-01)	Acc@1  74.22 ( 73.25)	Acc@5  93.75 ( 93.92)
03-Mar-22 09:11:05 - Epoch: [1][ 60/352]	Time  0.164 ( 0.155)	Data  0.003 ( 0.006)	Loss 1.0721e+00 (1.0363e+00)	Acc@1  70.31 ( 72.78)	Acc@5  91.41 ( 93.51)
03-Mar-22 09:11:05 - Epoch: [2][120/352]	Time  0.152 ( 0.148)	Data  0.003 ( 0.004)	Loss 9.4644e-01 (9.8334e-01)	Acc@1  74.22 ( 73.26)	Acc@5  95.31 ( 93.89)
03-Mar-22 09:11:07 - Epoch: [1][ 70/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.006)	Loss 9.4744e-01 (1.0284e+00)	Acc@1  75.78 ( 72.99)	Acc@5  96.88 ( 93.73)
03-Mar-22 09:11:07 - Epoch: [2][130/352]	Time  0.146 ( 0.148)	Data  0.003 ( 0.004)	Loss 1.1243e+00 (9.8150e-01)	Acc@1  70.31 ( 73.25)	Acc@5  90.62 ( 93.92)
03-Mar-22 09:11:08 - Epoch: [2][140/352]	Time  0.144 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.6927e-01 (9.8334e-01)	Acc@1  75.78 ( 73.30)	Acc@5  94.53 ( 93.89)
03-Mar-22 09:11:08 - Epoch: [1][ 80/352]	Time  0.156 ( 0.154)	Data  0.002 ( 0.006)	Loss 8.8518e-01 (1.0297e+00)	Acc@1  78.12 ( 72.84)	Acc@5  95.31 ( 93.78)
03-Mar-22 09:11:09 - Epoch: [2][150/352]	Time  0.135 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.1614e-01 (9.8540e-01)	Acc@1  75.78 ( 73.20)	Acc@5  93.75 ( 93.85)
03-Mar-22 09:11:09 - Epoch: [1][ 90/352]	Time  0.138 ( 0.153)	Data  0.003 ( 0.005)	Loss 1.0440e+00 (1.0348e+00)	Acc@1  76.56 ( 72.69)	Acc@5  93.75 ( 93.64)
03-Mar-22 09:11:11 - Epoch: [1][100/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.6214e-01 (1.0283e+00)	Acc@1  71.09 ( 72.71)	Acc@5  91.41 ( 93.71)
03-Mar-22 09:11:11 - Epoch: [2][160/352]	Time  0.145 ( 0.148)	Data  0.002 ( 0.004)	Loss 1.1909e+00 (9.8712e-01)	Acc@1  66.41 ( 73.13)	Acc@5  89.84 ( 93.85)
03-Mar-22 09:11:12 - Epoch: [2][170/352]	Time  0.149 ( 0.148)	Data  0.003 ( 0.004)	Loss 9.5403e-01 (9.8677e-01)	Acc@1  75.78 ( 73.09)	Acc@5  92.19 ( 93.86)
03-Mar-22 09:11:12 - Epoch: [1][110/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.005)	Loss 9.7364e-01 (1.0318e+00)	Acc@1  75.00 ( 72.71)	Acc@5  94.53 ( 93.65)
03-Mar-22 09:11:14 - Epoch: [2][180/352]	Time  0.145 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.7942e-01 (9.8457e-01)	Acc@1  69.53 ( 73.12)	Acc@5  91.41 ( 93.88)
03-Mar-22 09:11:14 - Epoch: [1][120/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.005)	Loss 1.0069e+00 (1.0292e+00)	Acc@1  74.22 ( 72.84)	Acc@5  92.97 ( 93.71)
03-Mar-22 09:11:15 - Epoch: [2][190/352]	Time  0.136 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.8932e-01 (9.8536e-01)	Acc@1  74.22 ( 73.15)	Acc@5  92.97 ( 93.86)
03-Mar-22 09:11:15 - Epoch: [1][130/352]	Time  0.151 ( 0.152)	Data  0.003 ( 0.004)	Loss 8.6262e-01 (1.0252e+00)	Acc@1  77.34 ( 73.00)	Acc@5  96.09 ( 93.79)
03-Mar-22 09:11:17 - Epoch: [2][200/352]	Time  0.150 ( 0.148)	Data  0.002 ( 0.004)	Loss 1.1758e+00 (9.8824e-01)	Acc@1  71.88 ( 73.04)	Acc@5  90.62 ( 93.83)
03-Mar-22 09:11:17 - Epoch: [1][140/352]	Time  0.160 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1734e+00 (1.0248e+00)	Acc@1  67.97 ( 72.98)	Acc@5  91.41 ( 93.81)
03-Mar-22 09:11:18 - Epoch: [2][210/352]	Time  0.154 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.8031e-01 (9.8740e-01)	Acc@1  73.44 ( 73.06)	Acc@5  93.75 ( 93.82)
03-Mar-22 09:11:19 - Epoch: [1][150/352]	Time  0.155 ( 0.152)	Data  0.003 ( 0.004)	Loss 1.0782e+00 (1.0256e+00)	Acc@1  64.06 ( 72.88)	Acc@5  93.75 ( 93.79)
03-Mar-22 09:11:20 - Epoch: [2][220/352]	Time  0.149 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.1863e+00 (9.8904e-01)	Acc@1  68.75 ( 73.11)	Acc@5  93.75 ( 93.78)
03-Mar-22 09:11:20 - Epoch: [1][160/352]	Time  0.171 ( 0.153)	Data  0.004 ( 0.004)	Loss 1.1892e+00 (1.0270e+00)	Acc@1  64.06 ( 72.83)	Acc@5  94.53 ( 93.75)
03-Mar-22 09:11:21 - Epoch: [2][230/352]	Time  0.148 ( 0.148)	Data  0.003 ( 0.003)	Loss 1.1644e+00 (9.8804e-01)	Acc@1  66.41 ( 73.14)	Acc@5  91.41 ( 93.78)
03-Mar-22 09:11:22 - Epoch: [1][170/352]	Time  0.132 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.3626e-01 (1.0258e+00)	Acc@1  76.56 ( 72.93)	Acc@5  92.97 ( 93.67)
03-Mar-22 09:11:23 - Epoch: [2][240/352]	Time  0.150 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.0207e+00 (9.8690e-01)	Acc@1  69.53 ( 73.12)	Acc@5  95.31 ( 93.79)
03-Mar-22 09:11:23 - Epoch: [1][180/352]	Time  0.177 ( 0.154)	Data  0.003 ( 0.004)	Loss 1.0472e+00 (1.0260e+00)	Acc@1  64.06 ( 72.81)	Acc@5  96.09 ( 93.69)
03-Mar-22 09:11:24 - Epoch: [2][250/352]	Time  0.129 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.9330e-01 (9.8762e-01)	Acc@1  73.44 ( 73.07)	Acc@5  93.75 ( 93.78)
03-Mar-22 09:11:25 - Epoch: [1][190/352]	Time  0.148 ( 0.154)	Data  0.003 ( 0.004)	Loss 1.0190e+00 (1.0259e+00)	Acc@1  73.44 ( 72.75)	Acc@5  95.31 ( 93.69)
03-Mar-22 09:11:26 - Epoch: [2][260/352]	Time  0.156 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.0719e+00 (9.8678e-01)	Acc@1  67.19 ( 73.11)	Acc@5  90.62 ( 93.81)
03-Mar-22 09:11:27 - Epoch: [1][200/352]	Time  0.155 ( 0.154)	Data  0.003 ( 0.004)	Loss 8.1975e-01 (1.0238e+00)	Acc@1  76.56 ( 72.73)	Acc@5  95.31 ( 93.73)
03-Mar-22 09:11:27 - Epoch: [2][270/352]	Time  0.151 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.1675e-01 (9.8544e-01)	Acc@1  75.78 ( 73.15)	Acc@5  92.19 ( 93.80)
03-Mar-22 09:11:28 - Epoch: [1][210/352]	Time  0.158 ( 0.154)	Data  0.003 ( 0.004)	Loss 1.0087e+00 (1.0254e+00)	Acc@1  72.66 ( 72.65)	Acc@5  90.62 ( 93.70)
03-Mar-22 09:11:29 - Epoch: [2][280/352]	Time  0.152 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.0960e+00 (9.8446e-01)	Acc@1  70.31 ( 73.20)	Acc@5  93.75 ( 93.82)
03-Mar-22 09:11:30 - Epoch: [1][220/352]	Time  0.143 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.1567e-01 (1.0258e+00)	Acc@1  78.91 ( 72.64)	Acc@5  96.88 ( 93.68)
03-Mar-22 09:11:30 - Epoch: [2][290/352]	Time  0.160 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.5220e-01 (9.8321e-01)	Acc@1  75.00 ( 73.18)	Acc@5  96.09 ( 93.84)
03-Mar-22 09:11:31 - Epoch: [1][230/352]	Time  0.151 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0864e+00 (1.0257e+00)	Acc@1  75.00 ( 72.68)	Acc@5  92.97 ( 93.70)
03-Mar-22 09:11:32 - Epoch: [2][300/352]	Time  0.128 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.1841e-01 (9.8426e-01)	Acc@1  73.44 ( 73.12)	Acc@5  96.88 ( 93.85)
03-Mar-22 09:11:33 - Epoch: [1][240/352]	Time  0.142 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.1505e+00 (1.0271e+00)	Acc@1  69.53 ( 72.65)	Acc@5  90.62 ( 93.68)
03-Mar-22 09:11:33 - Epoch: [2][310/352]	Time  0.159 ( 0.148)	Data  0.003 ( 0.003)	Loss 7.1542e-01 (9.8206e-01)	Acc@1  80.47 ( 73.16)	Acc@5  96.88 ( 93.88)
03-Mar-22 09:11:34 - Epoch: [1][250/352]	Time  0.154 ( 0.154)	Data  0.003 ( 0.003)	Loss 1.0373e+00 (1.0262e+00)	Acc@1  67.97 ( 72.62)	Acc@5  92.19 ( 93.67)
03-Mar-22 09:11:35 - Epoch: [2][320/352]	Time  0.142 ( 0.148)	Data  0.003 ( 0.003)	Loss 1.0400e+00 (9.8373e-01)	Acc@1  67.97 ( 73.10)	Acc@5  94.53 ( 93.87)
03-Mar-22 09:11:36 - Epoch: [1][260/352]	Time  0.147 ( 0.154)	Data  0.003 ( 0.003)	Loss 1.0045e+00 (1.0247e+00)	Acc@1  71.88 ( 72.65)	Acc@5  95.31 ( 93.68)
03-Mar-22 09:11:36 - Epoch: [2][330/352]	Time  0.155 ( 0.148)	Data  0.003 ( 0.003)	Loss 9.3201e-01 (9.8306e-01)	Acc@1  80.47 ( 73.13)	Acc@5  92.19 ( 93.88)
03-Mar-22 09:11:37 - Epoch: [1][270/352]	Time  0.152 ( 0.154)	Data  0.003 ( 0.003)	Loss 9.6567e-01 (1.0257e+00)	Acc@1  72.66 ( 72.56)	Acc@5  92.97 ( 93.65)
03-Mar-22 09:11:38 - Epoch: [2][340/352]	Time  0.149 ( 0.148)	Data  0.003 ( 0.003)	Loss 9.1257e-01 (9.8214e-01)	Acc@1  75.78 ( 73.18)	Acc@5  93.75 ( 93.88)
03-Mar-22 09:11:39 - Epoch: [1][280/352]	Time  0.148 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.2215e-01 (1.0248e+00)	Acc@1  72.66 ( 72.60)	Acc@5  96.09 ( 93.61)
03-Mar-22 09:11:39 - Epoch: [2][350/352]	Time  0.151 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.3341e-01 (9.8245e-01)	Acc@1  73.44 ( 73.18)	Acc@5  97.66 ( 93.85)
03-Mar-22 09:11:40 - Test: [ 0/20]	Time  0.367 ( 0.367)	Loss 9.8246e-01 (9.8246e-01)	Acc@1  71.48 ( 71.48)	Acc@5  95.70 ( 95.70)
03-Mar-22 09:11:40 - Epoch: [1][290/352]	Time  0.178 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.2467e-01 (1.0239e+00)	Acc@1  71.88 ( 72.63)	Acc@5  96.88 ( 93.63)
03-Mar-22 09:11:41 - Test: [10/20]	Time  0.104 ( 0.129)	Loss 8.8707e-01 (9.7785e-01)	Acc@1  76.17 ( 73.33)	Acc@5  94.14 ( 94.00)
03-Mar-22 09:11:42 - Epoch: [1][300/352]	Time  0.180 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0666e+00 (1.0220e+00)	Acc@1  70.31 ( 72.62)	Acc@5  94.53 ( 93.65)
03-Mar-22 09:11:42 -  * Acc@1 73.280 Acc@5 93.920
03-Mar-22 09:11:42 - Best acc at epoch 2: 73.47999572753906
03-Mar-22 09:11:42 - Epoch: [3][  0/352]	Time  0.380 ( 0.380)	Data  0.237 ( 0.237)	Loss 1.0581e+00 (1.0581e+00)	Acc@1  71.09 ( 71.09)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:11:43 - Epoch: [1][310/352]	Time  0.162 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.1977e+00 (1.0237e+00)	Acc@1  69.53 ( 72.57)	Acc@5  91.41 ( 93.62)
03-Mar-22 09:11:44 - Epoch: [3][ 10/352]	Time  0.158 ( 0.169)	Data  0.002 ( 0.023)	Loss 1.0670e+00 (9.5376e-01)	Acc@1  68.75 ( 73.22)	Acc@5  94.53 ( 94.82)
03-Mar-22 09:11:45 - Epoch: [1][320/352]	Time  0.152 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.3438e-01 (1.0219e+00)	Acc@1  74.22 ( 72.63)	Acc@5  92.97 ( 93.61)
03-Mar-22 09:11:45 - Epoch: [3][ 20/352]	Time  0.149 ( 0.162)	Data  0.002 ( 0.014)	Loss 8.5733e-01 (9.3160e-01)	Acc@1  75.00 ( 73.96)	Acc@5 100.00 ( 94.90)
03-Mar-22 09:11:46 - Epoch: [1][330/352]	Time  0.135 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.1270e+00 (1.0202e+00)	Acc@1  70.31 ( 72.64)	Acc@5  92.97 ( 93.62)
03-Mar-22 09:11:47 - Epoch: [3][ 30/352]	Time  0.158 ( 0.160)	Data  0.002 ( 0.010)	Loss 9.5946e-01 (9.6085e-01)	Acc@1  74.22 ( 72.91)	Acc@5  92.19 ( 94.48)
03-Mar-22 09:11:48 - Epoch: [1][340/352]	Time  0.167 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0065e+00 (1.0191e+00)	Acc@1  75.78 ( 72.67)	Acc@5  93.75 ( 93.66)
03-Mar-22 09:11:49 - Epoch: [3][ 40/352]	Time  0.147 ( 0.158)	Data  0.002 ( 0.008)	Loss 8.8130e-01 (9.6299e-01)	Acc@1  75.00 ( 72.85)	Acc@5  96.88 ( 94.38)
03-Mar-22 09:11:49 - Epoch: [1][350/352]	Time  0.149 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.7916e-01 (1.0182e+00)	Acc@1  73.44 ( 72.68)	Acc@5  93.75 ( 93.67)
03-Mar-22 09:11:50 - Test: [ 0/20]	Time  0.351 ( 0.351)	Loss 1.0769e+00 (1.0769e+00)	Acc@1  71.48 ( 71.48)	Acc@5  91.02 ( 91.02)
03-Mar-22 09:11:50 - Epoch: [3][ 50/352]	Time  0.170 ( 0.155)	Data  0.002 ( 0.007)	Loss 9.7010e-01 (9.6136e-01)	Acc@1  75.78 ( 72.96)	Acc@5  94.53 ( 94.24)
03-Mar-22 09:11:51 - Test: [10/20]	Time  0.100 ( 0.120)	Loss 8.5466e-01 (9.8098e-01)	Acc@1  76.56 ( 73.83)	Acc@5  96.48 ( 93.89)
03-Mar-22 09:11:52 - Epoch: [3][ 60/352]	Time  0.142 ( 0.156)	Data  0.002 ( 0.006)	Loss 9.9789e-01 (9.6877e-01)	Acc@1  72.66 ( 72.67)	Acc@5  93.75 ( 94.13)
03-Mar-22 09:11:52 -  * Acc@1 73.540 Acc@5 93.920
03-Mar-22 09:11:52 - Best acc at epoch 1: 73.54000091552734
03-Mar-22 09:11:52 - Epoch: [2][  0/352]	Time  0.396 ( 0.396)	Data  0.227 ( 0.227)	Loss 8.4828e-01 (8.4828e-01)	Acc@1  77.34 ( 77.34)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:11:53 - Epoch: [3][ 70/352]	Time  0.142 ( 0.154)	Data  0.002 ( 0.006)	Loss 1.1829e+00 (9.6623e-01)	Acc@1  69.53 ( 72.72)	Acc@5  89.84 ( 94.12)
03-Mar-22 09:11:54 - Epoch: [2][ 10/352]	Time  0.142 ( 0.170)	Data  0.002 ( 0.023)	Loss 9.2640e-01 (9.6868e-01)	Acc@1  75.00 ( 73.86)	Acc@5  92.19 ( 93.89)
03-Mar-22 09:11:55 - Epoch: [3][ 80/352]	Time  0.168 ( 0.156)	Data  0.003 ( 0.005)	Loss 9.7158e-01 (9.6435e-01)	Acc@1  70.31 ( 72.96)	Acc@5  96.09 ( 94.16)
03-Mar-22 09:11:55 - Epoch: [2][ 20/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.013)	Loss 1.0835e+00 (9.8252e-01)	Acc@1  65.62 ( 73.07)	Acc@5  92.97 ( 93.71)
03-Mar-22 09:11:56 - Epoch: [3][ 90/352]	Time  0.157 ( 0.156)	Data  0.002 ( 0.005)	Loss 8.1304e-01 (9.5706e-01)	Acc@1  79.69 ( 73.20)	Acc@5  96.09 ( 94.26)
03-Mar-22 09:11:57 - Epoch: [2][ 30/352]	Time  0.150 ( 0.154)	Data  0.002 ( 0.009)	Loss 9.7038e-01 (9.8382e-01)	Acc@1  77.34 ( 73.41)	Acc@5  90.62 ( 93.55)
03-Mar-22 09:11:58 - Epoch: [3][100/352]	Time  0.182 ( 0.155)	Data  0.003 ( 0.005)	Loss 1.0897e+00 (9.5782e-01)	Acc@1  71.09 ( 73.19)	Acc@5  89.84 ( 94.19)
03-Mar-22 09:11:58 - Epoch: [2][ 40/352]	Time  0.159 ( 0.153)	Data  0.002 ( 0.008)	Loss 1.0371e+00 (1.0075e+00)	Acc@1  71.88 ( 72.52)	Acc@5  90.62 ( 93.35)
03-Mar-22 09:11:59 - Epoch: [3][110/352]	Time  0.146 ( 0.155)	Data  0.002 ( 0.005)	Loss 9.9441e-01 (9.6083e-01)	Acc@1  76.56 ( 73.18)	Acc@5  94.53 ( 94.12)
03-Mar-22 09:12:00 - Epoch: [2][ 50/352]	Time  0.166 ( 0.152)	Data  0.002 ( 0.007)	Loss 8.6704e-01 (1.0062e+00)	Acc@1  75.78 ( 72.52)	Acc@5  95.31 ( 93.34)
03-Mar-22 09:12:01 - Epoch: [3][120/352]	Time  0.151 ( 0.155)	Data  0.003 ( 0.005)	Loss 8.5311e-01 (9.5494e-01)	Acc@1  76.56 ( 73.34)	Acc@5  94.53 ( 94.21)
03-Mar-22 09:12:01 - Epoch: [2][ 60/352]	Time  0.164 ( 0.152)	Data  0.002 ( 0.006)	Loss 9.0798e-01 (1.0043e+00)	Acc@1  76.56 ( 72.80)	Acc@5  94.53 ( 93.40)
03-Mar-22 09:12:02 - Epoch: [3][130/352]	Time  0.149 ( 0.155)	Data  0.004 ( 0.004)	Loss 9.5596e-01 (9.5813e-01)	Acc@1  69.53 ( 73.26)	Acc@5  95.31 ( 94.17)
03-Mar-22 09:12:02 - Epoch: [2][ 70/352]	Time  0.155 ( 0.151)	Data  0.002 ( 0.006)	Loss 1.1666e+00 (1.0124e+00)	Acc@1  67.19 ( 72.46)	Acc@5  90.62 ( 93.44)
03-Mar-22 09:12:04 - Epoch: [3][140/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.7628e-01 (9.5565e-01)	Acc@1  75.78 ( 73.33)	Acc@5  88.28 ( 94.19)
03-Mar-22 09:12:04 - Epoch: [2][ 80/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.005)	Loss 8.9730e-01 (1.0133e+00)	Acc@1  73.44 ( 72.37)	Acc@5  93.75 ( 93.39)
03-Mar-22 09:12:05 - Epoch: [3][150/352]	Time  0.161 ( 0.154)	Data  0.003 ( 0.004)	Loss 1.0884e+00 (9.5921e-01)	Acc@1  67.97 ( 73.20)	Acc@5  94.53 ( 94.15)
03-Mar-22 09:12:05 - Epoch: [2][ 90/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.005)	Loss 1.0123e+00 (1.0115e+00)	Acc@1  72.66 ( 72.33)	Acc@5  93.75 ( 93.52)
03-Mar-22 09:12:07 - Epoch: [3][160/352]	Time  0.148 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.6627e-01 (9.5958e-01)	Acc@1  74.22 ( 73.23)	Acc@5  94.53 ( 94.14)
03-Mar-22 09:12:07 - Epoch: [2][100/352]	Time  0.150 ( 0.151)	Data  0.003 ( 0.005)	Loss 1.2190e+00 (1.0101e+00)	Acc@1  61.72 ( 72.39)	Acc@5  93.75 ( 93.64)
03-Mar-22 09:12:08 - Epoch: [3][170/352]	Time  0.152 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.5612e-01 (9.5856e-01)	Acc@1  72.66 ( 73.28)	Acc@5  96.09 ( 94.14)
03-Mar-22 09:12:08 - Epoch: [2][110/352]	Time  0.145 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.1233e+00 (1.0069e+00)	Acc@1  67.19 ( 72.52)	Acc@5  90.62 ( 93.69)
03-Mar-22 09:12:10 - Epoch: [3][180/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0380e+00 (9.5591e-01)	Acc@1  72.66 ( 73.41)	Acc@5  91.41 ( 94.11)
03-Mar-22 09:12:10 - Epoch: [2][120/352]	Time  0.148 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.8975e-01 (9.9932e-01)	Acc@1  78.12 ( 72.84)	Acc@5  92.97 ( 93.73)
03-Mar-22 09:12:11 - Epoch: [3][190/352]	Time  0.146 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.8277e-01 (9.5816e-01)	Acc@1  75.00 ( 73.40)	Acc@5  95.31 ( 94.09)
03-Mar-22 09:12:11 - Epoch: [2][130/352]	Time  0.150 ( 0.150)	Data  0.003 ( 0.004)	Loss 9.0538e-01 (9.9746e-01)	Acc@1  76.56 ( 72.95)	Acc@5  96.09 ( 93.77)
03-Mar-22 09:12:13 - Epoch: [3][200/352]	Time  0.160 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.2353e-01 (9.5815e-01)	Acc@1  77.34 ( 73.41)	Acc@5  94.53 ( 94.12)
03-Mar-22 09:12:13 - Epoch: [2][140/352]	Time  0.155 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0365e+00 (9.9830e-01)	Acc@1  70.31 ( 72.93)	Acc@5  92.19 ( 93.77)
03-Mar-22 09:12:14 - Epoch: [3][210/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0753e+00 (9.6032e-01)	Acc@1  70.31 ( 73.39)	Acc@5  92.97 ( 94.12)
03-Mar-22 09:12:14 - Epoch: [2][150/352]	Time  0.136 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0241e+00 (9.9716e-01)	Acc@1  75.00 ( 73.02)	Acc@5  94.53 ( 93.82)
03-Mar-22 09:12:16 - Epoch: [3][220/352]	Time  0.131 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.8683e-01 (9.6021e-01)	Acc@1  74.22 ( 73.36)	Acc@5  97.66 ( 94.14)
03-Mar-22 09:12:16 - Epoch: [2][160/352]	Time  0.156 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.4970e-01 (9.9992e-01)	Acc@1  72.66 ( 72.97)	Acc@5  95.31 ( 93.79)
03-Mar-22 09:12:17 - Epoch: [3][230/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0459e+00 (9.6256e-01)	Acc@1  69.53 ( 73.32)	Acc@5  95.31 ( 94.07)
03-Mar-22 09:12:17 - Epoch: [2][170/352]	Time  0.134 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.5907e-01 (9.9771e-01)	Acc@1  74.22 ( 73.01)	Acc@5  96.88 ( 93.85)
03-Mar-22 09:12:19 - Epoch: [3][240/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0360e+00 (9.6440e-01)	Acc@1  69.53 ( 73.23)	Acc@5  92.19 ( 94.04)
03-Mar-22 09:12:19 - Epoch: [2][180/352]	Time  0.161 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.1565e+00 (1.0002e+00)	Acc@1  64.84 ( 72.83)	Acc@5  92.19 ( 93.80)
03-Mar-22 09:12:20 - Epoch: [3][250/352]	Time  0.146 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.9937e-01 (9.6472e-01)	Acc@1  74.22 ( 73.23)	Acc@5  97.66 ( 94.05)
03-Mar-22 09:12:20 - Epoch: [2][190/352]	Time  0.163 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.6995e-01 (1.0021e+00)	Acc@1  79.69 ( 72.77)	Acc@5  93.75 ( 93.78)
03-Mar-22 09:12:22 - Epoch: [3][260/352]	Time  0.159 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.8531e-01 (9.6652e-01)	Acc@1  77.34 ( 73.20)	Acc@5  97.66 ( 94.01)
03-Mar-22 09:12:22 - Epoch: [2][200/352]	Time  0.163 ( 0.151)	Data  0.003 ( 0.004)	Loss 9.7392e-01 (1.0011e+00)	Acc@1  73.44 ( 72.75)	Acc@5  94.53 ( 93.82)
03-Mar-22 09:12:23 - Epoch: [3][270/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.2292e-01 (9.6682e-01)	Acc@1  77.34 ( 73.21)	Acc@5  96.09 ( 94.00)
03-Mar-22 09:12:24 - Epoch: [2][210/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.8533e-01 (9.9914e-01)	Acc@1  72.66 ( 72.80)	Acc@5  92.97 ( 93.83)
03-Mar-22 09:12:25 - Epoch: [3][280/352]	Time  0.143 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.8017e-01 (9.6853e-01)	Acc@1  76.56 ( 73.19)	Acc@5  94.53 ( 93.96)
03-Mar-22 09:12:25 - Epoch: [2][220/352]	Time  0.157 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.5441e-01 (9.9904e-01)	Acc@1  75.78 ( 72.78)	Acc@5  90.62 ( 93.81)
03-Mar-22 09:12:26 - Epoch: [3][290/352]	Time  0.150 ( 0.152)	Data  0.003 ( 0.003)	Loss 8.0160e-01 (9.6709e-01)	Acc@1  76.56 ( 73.25)	Acc@5  96.88 ( 93.99)
03-Mar-22 09:12:27 - Epoch: [2][230/352]	Time  0.156 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.9417e-01 (9.9650e-01)	Acc@1  76.56 ( 72.89)	Acc@5  93.75 ( 93.83)
03-Mar-22 09:12:28 - Epoch: [3][300/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.9926e-01 (9.6462e-01)	Acc@1  75.78 ( 73.30)	Acc@5  93.75 ( 94.03)
03-Mar-22 09:12:28 - Epoch: [2][240/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0640e+00 (9.9457e-01)	Acc@1  71.88 ( 72.96)	Acc@5  92.97 ( 93.86)
03-Mar-22 09:12:29 - Epoch: [3][310/352]	Time  0.135 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.3223e-01 (9.6573e-01)	Acc@1  67.97 ( 73.27)	Acc@5  94.53 ( 94.00)
03-Mar-22 09:12:30 - Epoch: [2][250/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0506e+00 (9.9491e-01)	Acc@1  67.97 ( 72.92)	Acc@5  94.53 ( 93.85)
03-Mar-22 09:12:31 - Epoch: [3][320/352]	Time  0.169 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.8546e-01 (9.6639e-01)	Acc@1  68.75 ( 73.27)	Acc@5  92.19 ( 94.00)
03-Mar-22 09:12:31 - Epoch: [2][260/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.2020e+00 (9.9539e-01)	Acc@1  67.97 ( 72.89)	Acc@5  91.41 ( 93.83)
03-Mar-22 09:12:32 - Epoch: [3][330/352]	Time  0.124 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.1038e+00 (9.6639e-01)	Acc@1  69.53 ( 73.25)	Acc@5  92.97 ( 94.02)
03-Mar-22 09:12:33 - Epoch: [2][270/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.9105e-01 (9.9344e-01)	Acc@1  72.66 ( 72.95)	Acc@5  94.53 ( 93.84)
03-Mar-22 09:12:34 - Epoch: [3][340/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.9288e-01 (9.6618e-01)	Acc@1  72.66 ( 73.27)	Acc@5  97.66 ( 94.00)
03-Mar-22 09:12:34 - Epoch: [2][280/352]	Time  0.145 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.1498e+00 (9.9265e-01)	Acc@1  64.84 ( 72.98)	Acc@5  90.62 ( 93.82)
03-Mar-22 09:12:35 - Epoch: [3][350/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.9907e-01 (9.6521e-01)	Acc@1  75.00 ( 73.31)	Acc@5  96.09 ( 94.02)
03-Mar-22 09:12:36 - Epoch: [2][290/352]	Time  0.137 ( 0.151)	Data  0.003 ( 0.003)	Loss 8.9197e-01 (9.9158e-01)	Acc@1  77.34 ( 73.01)	Acc@5  93.75 ( 93.82)
03-Mar-22 09:12:36 - Test: [ 0/20]	Time  0.387 ( 0.387)	Loss 9.8055e-01 (9.8055e-01)	Acc@1  71.88 ( 71.88)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:12:37 - Test: [10/20]	Time  0.095 ( 0.128)	Loss 8.5108e-01 (9.5467e-01)	Acc@1  77.34 ( 73.37)	Acc@5  94.14 ( 93.82)
03-Mar-22 09:12:37 - Epoch: [2][300/352]	Time  0.169 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.7314e-01 (9.9024e-01)	Acc@1  74.22 ( 73.04)	Acc@5  97.66 ( 93.84)
03-Mar-22 09:12:38 -  * Acc@1 73.040 Acc@5 93.920
03-Mar-22 09:12:38 - Best acc at epoch 3: 73.47999572753906
03-Mar-22 09:12:39 - Epoch: [4][  0/352]	Time  0.370 ( 0.370)	Data  0.238 ( 0.238)	Loss 8.9065e-01 (8.9065e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:12:39 - Epoch: [2][310/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0972e+00 (9.9058e-01)	Acc@1  71.88 ( 73.00)	Acc@5  93.75 ( 93.86)
03-Mar-22 09:12:40 - Epoch: [4][ 10/352]	Time  0.145 ( 0.168)	Data  0.002 ( 0.024)	Loss 9.2148e-01 (9.0829e-01)	Acc@1  75.78 ( 75.14)	Acc@5  94.53 ( 95.17)
03-Mar-22 09:12:40 - Epoch: [2][320/352]	Time  0.149 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0091e+00 (9.9010e-01)	Acc@1  71.88 ( 73.02)	Acc@5  92.19 ( 93.83)
03-Mar-22 09:12:42 - Epoch: [4][ 20/352]	Time  0.149 ( 0.159)	Data  0.002 ( 0.014)	Loss 1.0547e+00 (9.3479e-01)	Acc@1  70.31 ( 74.44)	Acc@5  91.41 ( 94.68)
03-Mar-22 09:12:42 - Epoch: [2][330/352]	Time  0.172 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.4555e-01 (9.9053e-01)	Acc@1  79.69 ( 73.05)	Acc@5  94.53 ( 93.81)
03-Mar-22 09:12:43 - Epoch: [4][ 30/352]	Time  0.138 ( 0.155)	Data  0.002 ( 0.010)	Loss 9.5017e-01 (9.3960e-01)	Acc@1  74.22 ( 74.14)	Acc@5  90.62 ( 94.46)
03-Mar-22 09:12:43 - Epoch: [2][340/352]	Time  0.151 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.3237e-01 (9.8998e-01)	Acc@1  78.12 ( 73.07)	Acc@5  92.19 ( 93.81)
03-Mar-22 09:12:44 - Epoch: [4][ 40/352]	Time  0.139 ( 0.152)	Data  0.002 ( 0.008)	Loss 8.9455e-01 (9.3765e-01)	Acc@1  79.69 ( 74.14)	Acc@5  92.19 ( 94.17)
03-Mar-22 09:12:45 - Epoch: [2][350/352]	Time  0.132 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0722e+00 (9.8957e-01)	Acc@1  67.19 ( 73.01)	Acc@5  95.31 ( 93.83)
03-Mar-22 09:12:45 - Test: [ 0/20]	Time  0.363 ( 0.363)	Loss 1.0392e+00 (1.0392e+00)	Acc@1  71.88 ( 71.88)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:12:46 - Epoch: [4][ 50/352]	Time  0.172 ( 0.150)	Data  0.002 ( 0.007)	Loss 9.6912e-01 (9.3839e-01)	Acc@1  73.44 ( 74.02)	Acc@5  92.97 ( 94.18)
03-Mar-22 09:12:46 - Test: [10/20]	Time  0.101 ( 0.125)	Loss 8.8246e-01 (9.9372e-01)	Acc@1  77.34 ( 72.73)	Acc@5  92.97 ( 93.25)
03-Mar-22 09:12:47 -  * Acc@1 72.920 Acc@5 93.440
03-Mar-22 09:12:47 - Best acc at epoch 2: 73.54000091552734
03-Mar-22 09:12:47 - Epoch: [4][ 60/352]	Time  0.105 ( 0.150)	Data  0.002 ( 0.006)	Loss 1.0240e+00 (9.3283e-01)	Acc@1  71.88 ( 74.37)	Acc@5  91.41 ( 94.17)
03-Mar-22 09:12:48 - Epoch: [3][  0/352]	Time  0.369 ( 0.369)	Data  0.229 ( 0.229)	Loss 9.0636e-01 (9.0636e-01)	Acc@1  75.78 ( 75.78)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:12:49 - Epoch: [4][ 70/352]	Time  0.145 ( 0.148)	Data  0.003 ( 0.006)	Loss 8.9461e-01 (9.3883e-01)	Acc@1  72.66 ( 74.20)	Acc@5  95.31 ( 94.11)
03-Mar-22 09:12:49 - Epoch: [3][ 10/352]	Time  0.134 ( 0.165)	Data  0.002 ( 0.023)	Loss 7.1194e-01 (9.4063e-01)	Acc@1  81.25 ( 72.94)	Acc@5  98.44 ( 94.74)
03-Mar-22 09:12:50 - Epoch: [4][ 80/352]	Time  0.151 ( 0.148)	Data  0.002 ( 0.005)	Loss 7.1759e-01 (9.4156e-01)	Acc@1  80.47 ( 74.03)	Acc@5  96.09 ( 94.07)
03-Mar-22 09:12:51 - Epoch: [3][ 20/352]	Time  0.133 ( 0.158)	Data  0.002 ( 0.013)	Loss 1.0971e+00 (9.4455e-01)	Acc@1  69.53 ( 74.22)	Acc@5  89.84 ( 94.05)
03-Mar-22 09:12:52 - Epoch: [4][ 90/352]	Time  0.156 ( 0.149)	Data  0.002 ( 0.005)	Loss 1.0950e+00 (9.4379e-01)	Acc@1  69.53 ( 73.92)	Acc@5  92.19 ( 94.03)
03-Mar-22 09:12:52 - Epoch: [3][ 30/352]	Time  0.150 ( 0.158)	Data  0.002 ( 0.010)	Loss 1.1593e+00 (9.6011e-01)	Acc@1  68.75 ( 74.14)	Acc@5  90.62 ( 93.65)
03-Mar-22 09:12:53 - Epoch: [4][100/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.005)	Loss 1.2731e+00 (9.4333e-01)	Acc@1  64.84 ( 74.00)	Acc@5  92.97 ( 94.05)
03-Mar-22 09:12:54 - Epoch: [3][ 40/352]	Time  0.153 ( 0.157)	Data  0.002 ( 0.008)	Loss 9.2052e-01 (9.6201e-01)	Acc@1  74.22 ( 73.74)	Acc@5  92.19 ( 93.65)
03-Mar-22 09:12:55 - Epoch: [4][110/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.005)	Loss 8.8654e-01 (9.4346e-01)	Acc@1  77.34 ( 73.78)	Acc@5  92.97 ( 94.07)
03-Mar-22 09:12:55 - Epoch: [3][ 50/352]	Time  0.152 ( 0.155)	Data  0.002 ( 0.007)	Loss 8.9899e-01 (9.6298e-01)	Acc@1  73.44 ( 73.70)	Acc@5  97.66 ( 93.70)
03-Mar-22 09:12:56 - Epoch: [4][120/352]	Time  0.149 ( 0.149)	Data  0.003 ( 0.004)	Loss 8.9693e-01 (9.4026e-01)	Acc@1  72.66 ( 73.93)	Acc@5  95.31 ( 94.11)
03-Mar-22 09:12:57 - Epoch: [3][ 60/352]	Time  0.144 ( 0.155)	Data  0.002 ( 0.006)	Loss 8.1843e-01 (9.6425e-01)	Acc@1  78.12 ( 73.74)	Acc@5  97.66 ( 93.71)
03-Mar-22 09:12:58 - Epoch: [4][130/352]	Time  0.149 ( 0.149)	Data  0.003 ( 0.004)	Loss 8.0814e-01 (9.3640e-01)	Acc@1  75.78 ( 74.06)	Acc@5  95.31 ( 94.16)
03-Mar-22 09:12:58 - Epoch: [3][ 70/352]	Time  0.162 ( 0.154)	Data  0.002 ( 0.006)	Loss 9.0184e-01 (9.6869e-01)	Acc@1  81.25 ( 73.71)	Acc@5  92.19 ( 93.72)
03-Mar-22 09:12:59 - Epoch: [4][140/352]	Time  0.136 ( 0.149)	Data  0.003 ( 0.004)	Loss 1.0134e+00 (9.4075e-01)	Acc@1  71.09 ( 73.88)	Acc@5  94.53 ( 94.15)
03-Mar-22 09:13:00 - Epoch: [3][ 80/352]	Time  0.130 ( 0.154)	Data  0.002 ( 0.005)	Loss 1.1108e+00 (9.7007e-01)	Acc@1  69.53 ( 73.74)	Acc@5  90.62 ( 93.70)
03-Mar-22 09:13:01 - Epoch: [4][150/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.9636e-01 (9.4262e-01)	Acc@1  71.09 ( 73.85)	Acc@5  92.97 ( 94.10)
03-Mar-22 09:13:01 - Epoch: [3][ 90/352]	Time  0.160 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.7781e-01 (9.7182e-01)	Acc@1  79.69 ( 73.71)	Acc@5  94.53 ( 93.64)
03-Mar-22 09:13:02 - Epoch: [4][160/352]	Time  0.150 ( 0.149)	Data  0.002 ( 0.004)	Loss 1.0534e+00 (9.4130e-01)	Acc@1  68.75 ( 73.87)	Acc@5  95.31 ( 94.11)
03-Mar-22 09:13:03 - Epoch: [3][100/352]	Time  0.163 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.2609e-01 (9.7283e-01)	Acc@1  80.47 ( 73.56)	Acc@5  96.09 ( 93.62)
03-Mar-22 09:13:04 - Epoch: [4][170/352]	Time  0.155 ( 0.149)	Data  0.002 ( 0.004)	Loss 7.9606e-01 (9.3750e-01)	Acc@1  79.69 ( 73.94)	Acc@5  96.09 ( 94.16)
03-Mar-22 09:13:04 - Epoch: [3][110/352]	Time  0.144 ( 0.153)	Data  0.004 ( 0.004)	Loss 9.1392e-01 (9.6656e-01)	Acc@1  75.00 ( 73.80)	Acc@5  95.31 ( 93.71)
03-Mar-22 09:13:05 - Epoch: [4][180/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.004)	Loss 1.0266e+00 (9.3902e-01)	Acc@1  71.88 ( 73.90)	Acc@5  89.84 ( 94.13)
03-Mar-22 09:13:06 - Epoch: [3][120/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.7681e-01 (9.6914e-01)	Acc@1  71.88 ( 73.66)	Acc@5  95.31 ( 93.67)
03-Mar-22 09:13:07 - Epoch: [4][190/352]	Time  0.169 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.1897e-01 (9.4001e-01)	Acc@1  81.25 ( 73.86)	Acc@5  96.09 ( 94.09)
03-Mar-22 09:13:07 - Epoch: [3][130/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.5037e-01 (9.7063e-01)	Acc@1  71.09 ( 73.52)	Acc@5  93.75 ( 93.67)
03-Mar-22 09:13:08 - Epoch: [4][200/352]	Time  0.138 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.9632e-01 (9.4310e-01)	Acc@1  75.00 ( 73.78)	Acc@5  91.41 ( 94.03)
03-Mar-22 09:13:09 - Epoch: [3][140/352]	Time  0.142 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.1502e-01 (9.6659e-01)	Acc@1  77.34 ( 73.59)	Acc@5  92.97 ( 93.71)
03-Mar-22 09:13:10 - Epoch: [4][210/352]	Time  0.186 ( 0.149)	Data  0.003 ( 0.004)	Loss 1.0144e+00 (9.4351e-01)	Acc@1  71.88 ( 73.79)	Acc@5  93.75 ( 94.05)
03-Mar-22 09:13:10 - Epoch: [3][150/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.0140e-01 (9.6566e-01)	Acc@1  78.91 ( 73.56)	Acc@5  93.75 ( 93.74)
03-Mar-22 09:13:11 - Epoch: [4][220/352]	Time  0.160 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0686e+00 (9.4365e-01)	Acc@1  71.09 ( 73.82)	Acc@5  91.41 ( 94.03)
03-Mar-22 09:13:12 - Epoch: [3][160/352]	Time  0.152 ( 0.153)	Data  0.003 ( 0.004)	Loss 1.0174e+00 (9.6169e-01)	Acc@1  71.88 ( 73.70)	Acc@5  90.62 ( 93.79)
03-Mar-22 09:13:13 - Epoch: [4][230/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0720e+00 (9.4445e-01)	Acc@1  72.66 ( 73.82)	Acc@5  93.75 ( 94.03)
03-Mar-22 09:13:13 - Epoch: [3][170/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.3211e-01 (9.6370e-01)	Acc@1  78.91 ( 73.66)	Acc@5  94.53 ( 93.76)
03-Mar-22 09:13:14 - Epoch: [4][240/352]	Time  0.149 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.1776e+00 (9.4686e-01)	Acc@1  69.53 ( 73.76)	Acc@5  89.06 ( 93.98)
03-Mar-22 09:13:15 - Epoch: [3][180/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.6844e-01 (9.6605e-01)	Acc@1  74.22 ( 73.68)	Acc@5  92.97 ( 93.75)
03-Mar-22 09:13:16 - Epoch: [4][250/352]	Time  0.155 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.0163e-01 (9.4555e-01)	Acc@1  75.78 ( 73.80)	Acc@5  94.53 ( 94.01)
03-Mar-22 09:13:16 - Epoch: [3][190/352]	Time  0.163 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.1825e-01 (9.6519e-01)	Acc@1  81.25 ( 73.69)	Acc@5  96.09 ( 93.78)
03-Mar-22 09:13:17 - Epoch: [4][260/352]	Time  0.150 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.9228e-01 (9.4845e-01)	Acc@1  75.00 ( 73.70)	Acc@5  92.19 ( 93.97)
03-Mar-22 09:13:18 - Epoch: [3][200/352]	Time  0.152 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.9695e-01 (9.6714e-01)	Acc@1  75.00 ( 73.59)	Acc@5  92.19 ( 93.74)
03-Mar-22 09:13:19 - Epoch: [4][270/352]	Time  0.181 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.9150e-01 (9.4882e-01)	Acc@1  74.22 ( 73.67)	Acc@5  96.88 ( 93.99)
03-Mar-22 09:13:19 - Epoch: [3][210/352]	Time  0.135 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1206e+00 (9.6812e-01)	Acc@1  68.75 ( 73.54)	Acc@5  89.84 ( 93.72)
03-Mar-22 09:13:21 - Epoch: [4][280/352]	Time  0.156 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0049e+00 (9.4666e-01)	Acc@1  68.75 ( 73.73)	Acc@5  93.75 ( 94.06)
03-Mar-22 09:13:21 - Epoch: [3][220/352]	Time  0.131 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.6242e-01 (9.6657e-01)	Acc@1  73.44 ( 73.56)	Acc@5  96.88 ( 93.71)
03-Mar-22 09:13:22 - Epoch: [4][290/352]	Time  0.168 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0052e+00 (9.4778e-01)	Acc@1  73.44 ( 73.69)	Acc@5  92.97 ( 94.05)
03-Mar-22 09:13:23 - Epoch: [3][230/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0571e+00 (9.6727e-01)	Acc@1  68.75 ( 73.55)	Acc@5  92.97 ( 93.70)
03-Mar-22 09:13:24 - Epoch: [4][300/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1002e-01 (9.4717e-01)	Acc@1  78.12 ( 73.70)	Acc@5  95.31 ( 94.07)
03-Mar-22 09:13:24 - Epoch: [3][240/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0679e+00 (9.6632e-01)	Acc@1  71.09 ( 73.54)	Acc@5  91.41 ( 93.73)
03-Mar-22 09:13:25 - Epoch: [4][310/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.8484e-01 (9.4558e-01)	Acc@1  73.44 ( 73.74)	Acc@5  91.41 ( 94.06)
03-Mar-22 09:13:26 - Epoch: [3][250/352]	Time  0.160 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.9316e-01 (9.6559e-01)	Acc@1  82.03 ( 73.55)	Acc@5  94.53 ( 93.77)
03-Mar-22 09:13:27 - Epoch: [4][320/352]	Time  0.156 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.7276e-01 (9.4740e-01)	Acc@1  71.88 ( 73.68)	Acc@5  92.97 ( 94.03)
03-Mar-22 09:13:27 - Epoch: [3][260/352]	Time  0.135 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0236e+00 (9.6495e-01)	Acc@1  73.44 ( 73.55)	Acc@5  94.53 ( 93.82)
03-Mar-22 09:13:28 - Epoch: [4][330/352]	Time  0.140 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0123e+00 (9.4708e-01)	Acc@1  71.88 ( 73.68)	Acc@5  96.09 ( 94.05)
03-Mar-22 09:13:29 - Epoch: [3][270/352]	Time  0.155 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.9737e-01 (9.6500e-01)	Acc@1  71.88 ( 73.52)	Acc@5  92.97 ( 93.81)
03-Mar-22 09:13:30 - Epoch: [4][340/352]	Time  0.151 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.4033e-01 (9.4678e-01)	Acc@1  69.53 ( 73.69)	Acc@5  95.31 ( 94.05)
03-Mar-22 09:13:30 - Epoch: [3][280/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.9813e-01 (9.6622e-01)	Acc@1  66.41 ( 73.42)	Acc@5  93.75 ( 93.79)
03-Mar-22 09:13:31 - Epoch: [4][350/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0806e+00 (9.4796e-01)	Acc@1  71.88 ( 73.67)	Acc@5  92.97 ( 94.03)
03-Mar-22 09:13:32 - Epoch: [3][290/352]	Time  0.141 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.8646e-01 (9.6365e-01)	Acc@1  77.34 ( 73.48)	Acc@5  99.22 ( 93.84)
03-Mar-22 09:13:32 - Test: [ 0/20]	Time  0.357 ( 0.357)	Loss 9.5988e-01 (9.5988e-01)	Acc@1  74.61 ( 74.61)	Acc@5  95.70 ( 95.70)
03-Mar-22 09:13:33 - Test: [10/20]	Time  0.105 ( 0.126)	Loss 8.2218e-01 (9.3248e-01)	Acc@1  75.39 ( 74.61)	Acc@5  95.31 ( 94.25)
03-Mar-22 09:13:33 - Epoch: [3][300/352]	Time  0.172 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.6466e-01 (9.6284e-01)	Acc@1  69.53 ( 73.48)	Acc@5  92.19 ( 93.86)
03-Mar-22 09:13:34 -  * Acc@1 73.980 Acc@5 93.840
03-Mar-22 09:13:34 - Best acc at epoch 4: 73.97999572753906
03-Mar-22 09:13:34 - Epoch: [5][  0/352]	Time  0.365 ( 0.365)	Data  0.227 ( 0.227)	Loss 8.6514e-01 (8.6514e-01)	Acc@1  77.34 ( 77.34)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:13:34 - Epoch: [3][310/352]	Time  0.155 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.2032e+00 (9.6483e-01)	Acc@1  62.50 ( 73.39)	Acc@5  95.31 ( 93.84)
03-Mar-22 09:13:36 - Epoch: [5][ 10/352]	Time  0.136 ( 0.158)	Data  0.002 ( 0.023)	Loss 9.4775e-01 (9.1139e-01)	Acc@1  75.00 ( 75.78)	Acc@5  94.53 ( 95.10)
03-Mar-22 09:13:36 - Epoch: [3][320/352]	Time  0.155 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.1116e-01 (9.6510e-01)	Acc@1  72.66 ( 73.39)	Acc@5  95.31 ( 93.83)
03-Mar-22 09:13:37 - Epoch: [5][ 20/352]	Time  0.163 ( 0.152)	Data  0.002 ( 0.013)	Loss 8.8826e-01 (9.2683e-01)	Acc@1  70.31 ( 74.96)	Acc@5  96.88 ( 94.49)
03-Mar-22 09:13:37 - Epoch: [3][330/352]	Time  0.173 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.8909e-01 (9.6516e-01)	Acc@1  71.88 ( 73.39)	Acc@5  92.97 ( 93.83)
03-Mar-22 09:13:39 - Epoch: [5][ 30/352]	Time  0.154 ( 0.154)	Data  0.002 ( 0.010)	Loss 1.0104e+00 (9.1550e-01)	Acc@1  72.66 ( 75.10)	Acc@5  93.75 ( 94.71)
03-Mar-22 09:13:39 - Epoch: [3][340/352]	Time  0.162 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.7129e-01 (9.6605e-01)	Acc@1  77.34 ( 73.36)	Acc@5  93.75 ( 93.81)
03-Mar-22 09:13:40 - Epoch: [3][350/352]	Time  0.144 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.4757e-01 (9.6488e-01)	Acc@1  78.12 ( 73.39)	Acc@5  95.31 ( 93.83)
03-Mar-22 09:13:40 - Epoch: [5][ 40/352]	Time  0.162 ( 0.157)	Data  0.003 ( 0.008)	Loss 8.0733e-01 (9.2383e-01)	Acc@1  79.69 ( 74.73)	Acc@5  94.53 ( 94.61)
03-Mar-22 09:13:41 - Test: [ 0/20]	Time  0.340 ( 0.340)	Loss 1.0497e+00 (1.0497e+00)	Acc@1  69.14 ( 69.14)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:13:42 - Epoch: [5][ 50/352]	Time  0.179 ( 0.155)	Data  0.002 ( 0.007)	Loss 1.0667e+00 (9.4955e-01)	Acc@1  67.97 ( 74.10)	Acc@5  92.97 ( 94.16)
03-Mar-22 09:13:42 - Test: [10/20]	Time  0.108 ( 0.125)	Loss 8.7967e-01 (9.9434e-01)	Acc@1  75.78 ( 72.94)	Acc@5  96.48 ( 93.47)
03-Mar-22 09:13:43 -  * Acc@1 72.360 Acc@5 93.420
03-Mar-22 09:13:43 - Best acc at epoch 3: 73.54000091552734
03-Mar-22 09:13:43 - Epoch: [5][ 60/352]	Time  0.131 ( 0.153)	Data  0.002 ( 0.006)	Loss 9.1982e-01 (9.4187e-01)	Acc@1  74.22 ( 74.23)	Acc@5  92.97 ( 94.16)
03-Mar-22 09:13:43 - Epoch: [4][  0/352]	Time  0.349 ( 0.349)	Data  0.219 ( 0.219)	Loss 9.0541e-01 (9.0541e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.88 ( 96.88)
03-Mar-22 09:13:45 - Epoch: [5][ 70/352]	Time  0.144 ( 0.151)	Data  0.003 ( 0.006)	Loss 8.6178e-01 (9.4450e-01)	Acc@1  75.00 ( 74.04)	Acc@5  95.31 ( 94.17)
03-Mar-22 09:13:45 - Epoch: [4][ 10/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.022)	Loss 9.6264e-01 (9.6963e-01)	Acc@1  71.88 ( 72.66)	Acc@5  96.09 ( 94.53)
03-Mar-22 09:13:46 - Epoch: [5][ 80/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.005)	Loss 9.0904e-01 (9.4892e-01)	Acc@1  73.44 ( 73.75)	Acc@5  92.19 ( 94.11)
03-Mar-22 09:13:46 - Epoch: [4][ 20/352]	Time  0.160 ( 0.165)	Data  0.002 ( 0.012)	Loss 7.8881e-01 (9.2807e-01)	Acc@1  78.12 ( 74.11)	Acc@5  96.09 ( 94.49)
03-Mar-22 09:13:48 - Epoch: [5][ 90/352]	Time  0.137 ( 0.151)	Data  0.002 ( 0.005)	Loss 8.8044e-01 (9.4898e-01)	Acc@1  80.47 ( 73.90)	Acc@5  92.19 ( 94.03)
03-Mar-22 09:13:48 - Epoch: [4][ 30/352]	Time  0.137 ( 0.160)	Data  0.002 ( 0.009)	Loss 9.4488e-01 (9.4250e-01)	Acc@1  75.00 ( 73.89)	Acc@5  95.31 ( 94.25)
03-Mar-22 09:13:49 - Epoch: [5][100/352]	Time  0.145 ( 0.150)	Data  0.002 ( 0.005)	Loss 1.0309e+00 (9.4711e-01)	Acc@1  70.31 ( 73.93)	Acc@5  96.09 ( 94.11)
03-Mar-22 09:13:49 - Epoch: [4][ 40/352]	Time  0.155 ( 0.158)	Data  0.002 ( 0.007)	Loss 1.0671e+00 (9.3598e-01)	Acc@1  70.31 ( 74.28)	Acc@5  93.75 ( 94.26)
03-Mar-22 09:13:51 - Epoch: [5][110/352]	Time  0.166 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.4978e-01 (9.4198e-01)	Acc@1  78.12 ( 74.08)	Acc@5  95.31 ( 94.17)
03-Mar-22 09:13:51 - Epoch: [4][ 50/352]	Time  0.140 ( 0.157)	Data  0.002 ( 0.007)	Loss 9.3809e-01 (9.2070e-01)	Acc@1  73.44 ( 74.59)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:13:52 - Epoch: [5][120/352]	Time  0.135 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.8204e-01 (9.4221e-01)	Acc@1  72.66 ( 74.03)	Acc@5  96.09 ( 94.12)
03-Mar-22 09:13:52 - Epoch: [4][ 60/352]	Time  0.152 ( 0.155)	Data  0.003 ( 0.006)	Loss 9.5727e-01 (9.3075e-01)	Acc@1  75.00 ( 74.46)	Acc@5  92.19 ( 94.34)
03-Mar-22 09:13:54 - Epoch: [5][130/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.3983e-01 (9.4268e-01)	Acc@1  75.00 ( 73.93)	Acc@5  95.31 ( 94.17)
03-Mar-22 09:13:54 - Epoch: [4][ 70/352]	Time  0.168 ( 0.155)	Data  0.002 ( 0.005)	Loss 1.0612e+00 (9.3435e-01)	Acc@1  65.62 ( 74.35)	Acc@5  90.62 ( 94.19)
03-Mar-22 09:13:55 - Epoch: [5][140/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0160e+00 (9.4118e-01)	Acc@1  71.88 ( 73.96)	Acc@5  92.19 ( 94.19)
03-Mar-22 09:13:55 - Epoch: [4][ 80/352]	Time  0.146 ( 0.155)	Data  0.002 ( 0.005)	Loss 8.8740e-01 (9.3728e-01)	Acc@1  71.09 ( 74.20)	Acc@5  94.53 ( 94.03)
03-Mar-22 09:13:57 - Epoch: [5][150/352]	Time  0.155 ( 0.150)	Data  0.003 ( 0.004)	Loss 1.0998e+00 (9.3812e-01)	Acc@1  67.97 ( 74.03)	Acc@5  91.41 ( 94.21)
03-Mar-22 09:13:57 - Epoch: [4][ 90/352]	Time  0.139 ( 0.155)	Data  0.003 ( 0.005)	Loss 1.1746e+00 (9.4376e-01)	Acc@1  67.97 ( 74.07)	Acc@5  89.84 ( 93.92)
03-Mar-22 09:13:58 - Epoch: [5][160/352]	Time  0.128 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.1129e+00 (9.3933e-01)	Acc@1  66.41 ( 73.97)	Acc@5  91.41 ( 94.16)
03-Mar-22 09:13:59 - Epoch: [4][100/352]	Time  0.144 ( 0.155)	Data  0.002 ( 0.005)	Loss 1.0001e+00 (9.4798e-01)	Acc@1  72.66 ( 73.90)	Acc@5  92.97 ( 93.87)
03-Mar-22 09:13:59 - Epoch: [5][170/352]	Time  0.155 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.1682e-01 (9.4267e-01)	Acc@1  78.12 ( 73.84)	Acc@5  93.75 ( 94.13)
03-Mar-22 09:14:00 - Epoch: [4][110/352]	Time  0.152 ( 0.155)	Data  0.002 ( 0.004)	Loss 1.0108e+00 (9.4840e-01)	Acc@1  68.75 ( 73.79)	Acc@5  96.09 ( 93.95)
03-Mar-22 09:14:01 - Epoch: [5][180/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0119e+00 (9.4383e-01)	Acc@1  71.88 ( 73.77)	Acc@5  92.97 ( 94.15)
03-Mar-22 09:14:02 - Epoch: [4][120/352]	Time  0.157 ( 0.155)	Data  0.002 ( 0.004)	Loss 1.0029e+00 (9.4702e-01)	Acc@1  75.00 ( 73.89)	Acc@5  90.62 ( 93.91)
03-Mar-22 09:14:02 - Epoch: [5][190/352]	Time  0.155 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.5223e-01 (9.4410e-01)	Acc@1  75.78 ( 73.72)	Acc@5  93.75 ( 94.16)
03-Mar-22 09:14:03 - Epoch: [4][130/352]	Time  0.152 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.1182e+00 (9.4770e-01)	Acc@1  72.66 ( 73.84)	Acc@5  91.41 ( 93.97)
03-Mar-22 09:14:04 - Epoch: [5][200/352]	Time  0.149 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.9303e-01 (9.4246e-01)	Acc@1  75.00 ( 73.78)	Acc@5  95.31 ( 94.17)
03-Mar-22 09:14:05 - Epoch: [4][140/352]	Time  0.169 ( 0.155)	Data  0.003 ( 0.004)	Loss 1.0028e+00 (9.4621e-01)	Acc@1  72.66 ( 73.81)	Acc@5  94.53 ( 94.00)
03-Mar-22 09:14:05 - Epoch: [5][210/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.4446e-01 (9.4076e-01)	Acc@1  69.53 ( 73.73)	Acc@5  97.66 ( 94.21)
03-Mar-22 09:14:06 - Epoch: [4][150/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.004)	Loss 8.1710e-01 (9.4621e-01)	Acc@1  79.69 ( 73.79)	Acc@5  96.09 ( 94.00)
03-Mar-22 09:14:07 - Epoch: [5][220/352]	Time  0.122 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.4311e-01 (9.4186e-01)	Acc@1  70.31 ( 73.66)	Acc@5  93.75 ( 94.20)
03-Mar-22 09:14:08 - Epoch: [4][160/352]	Time  0.153 ( 0.155)	Data  0.002 ( 0.004)	Loss 9.1854e-01 (9.4920e-01)	Acc@1  75.78 ( 73.69)	Acc@5  90.62 ( 94.01)
03-Mar-22 09:14:08 - Epoch: [5][230/352]	Time  0.137 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.1702e-01 (9.4146e-01)	Acc@1  71.09 ( 73.68)	Acc@5  93.75 ( 94.23)
03-Mar-22 09:14:10 - Epoch: [4][170/352]	Time  0.157 ( 0.156)	Data  0.003 ( 0.004)	Loss 9.6566e-01 (9.5047e-01)	Acc@1  70.31 ( 73.64)	Acc@5  92.97 ( 93.98)
03-Mar-22 09:14:10 - Epoch: [5][240/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.0620e-01 (9.4116e-01)	Acc@1  71.09 ( 73.70)	Acc@5  96.88 ( 94.25)
03-Mar-22 09:14:11 - Epoch: [4][180/352]	Time  0.169 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.1141e+00 (9.5144e-01)	Acc@1  72.66 ( 73.61)	Acc@5  93.75 ( 93.99)
03-Mar-22 09:14:11 - Epoch: [5][250/352]	Time  0.131 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.3310e-01 (9.4059e-01)	Acc@1  75.00 ( 73.74)	Acc@5  96.09 ( 94.26)
03-Mar-22 09:14:13 - Epoch: [5][260/352]	Time  0.145 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0963e+00 (9.4236e-01)	Acc@1  67.97 ( 73.73)	Acc@5  93.75 ( 94.25)
03-Mar-22 09:14:13 - Epoch: [4][190/352]	Time  0.180 ( 0.156)	Data  0.003 ( 0.004)	Loss 8.5839e-01 (9.5031e-01)	Acc@1  71.88 ( 73.67)	Acc@5  96.88 ( 93.99)
03-Mar-22 09:14:14 - Epoch: [5][270/352]	Time  0.156 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0549e+00 (9.4374e-01)	Acc@1  69.53 ( 73.65)	Acc@5  95.31 ( 94.24)
03-Mar-22 09:14:14 - Epoch: [4][200/352]	Time  0.154 ( 0.156)	Data  0.002 ( 0.004)	Loss 8.5366e-01 (9.5181e-01)	Acc@1  78.91 ( 73.62)	Acc@5  95.31 ( 94.00)
03-Mar-22 09:14:16 - Epoch: [5][280/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1367e+00 (9.4394e-01)	Acc@1  68.75 ( 73.64)	Acc@5  89.06 ( 94.21)
03-Mar-22 09:14:16 - Epoch: [4][210/352]	Time  0.153 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.0263e+00 (9.5479e-01)	Acc@1  72.66 ( 73.51)	Acc@5  92.19 ( 93.95)
03-Mar-22 09:14:17 - Epoch: [5][290/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.8536e-01 (9.4299e-01)	Acc@1  68.75 ( 73.66)	Acc@5  94.53 ( 94.20)
03-Mar-22 09:14:17 - Epoch: [4][220/352]	Time  0.157 ( 0.156)	Data  0.003 ( 0.003)	Loss 1.0355e+00 (9.5817e-01)	Acc@1  72.66 ( 73.39)	Acc@5  90.62 ( 93.89)
03-Mar-22 09:14:19 - Epoch: [5][300/352]	Time  0.129 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.8354e-01 (9.4401e-01)	Acc@1  71.88 ( 73.66)	Acc@5  94.53 ( 94.18)
03-Mar-22 09:14:19 - Epoch: [4][230/352]	Time  0.165 ( 0.156)	Data  0.003 ( 0.003)	Loss 9.8744e-01 (9.5958e-01)	Acc@1  67.19 ( 73.35)	Acc@5  96.09 ( 93.89)
03-Mar-22 09:14:20 - Epoch: [5][310/352]	Time  0.153 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.9412e-01 (9.4222e-01)	Acc@1  71.88 ( 73.67)	Acc@5  93.75 ( 94.19)
03-Mar-22 09:14:20 - Epoch: [4][240/352]	Time  0.144 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.0091e+00 (9.5811e-01)	Acc@1  69.53 ( 73.36)	Acc@5  95.31 ( 93.90)
03-Mar-22 09:14:22 - Epoch: [5][320/352]	Time  0.154 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.4952e-01 (9.4171e-01)	Acc@1  72.66 ( 73.69)	Acc@5  93.75 ( 94.19)
03-Mar-22 09:14:22 - Epoch: [4][250/352]	Time  0.175 ( 0.156)	Data  0.003 ( 0.003)	Loss 1.0214e+00 (9.5555e-01)	Acc@1  72.66 ( 73.45)	Acc@5  92.19 ( 93.90)
03-Mar-22 09:14:23 - Epoch: [5][330/352]	Time  0.144 ( 0.149)	Data  0.003 ( 0.003)	Loss 9.6615e-01 (9.4111e-01)	Acc@1  75.00 ( 73.70)	Acc@5  91.41 ( 94.21)
03-Mar-22 09:14:24 - Epoch: [4][260/352]	Time  0.147 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.1533e+00 (9.5520e-01)	Acc@1  69.53 ( 73.48)	Acc@5  90.62 ( 93.90)
03-Mar-22 09:14:25 - Epoch: [5][340/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.7124e-01 (9.4065e-01)	Acc@1  70.31 ( 73.70)	Acc@5  92.97 ( 94.22)
03-Mar-22 09:14:25 - Epoch: [4][270/352]	Time  0.156 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.6715e-01 (9.5342e-01)	Acc@1  78.91 ( 73.53)	Acc@5  95.31 ( 93.93)
03-Mar-22 09:14:26 - Epoch: [5][350/352]	Time  0.129 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1022e+00 (9.3889e-01)	Acc@1  72.66 ( 73.77)	Acc@5  89.06 ( 94.23)
03-Mar-22 09:14:27 - Epoch: [4][280/352]	Time  0.105 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.7212e-01 (9.5519e-01)	Acc@1  70.31 ( 73.50)	Acc@5  94.53 ( 93.91)
03-Mar-22 09:14:27 - Test: [ 0/20]	Time  0.367 ( 0.367)	Loss 9.6177e-01 (9.6177e-01)	Acc@1  73.44 ( 73.44)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:14:28 - Test: [10/20]	Time  0.116 ( 0.128)	Loss 7.9424e-01 (9.4898e-01)	Acc@1  76.17 ( 73.44)	Acc@5  95.70 ( 93.79)
03-Mar-22 09:14:28 - Epoch: [4][290/352]	Time  0.186 ( 0.156)	Data  0.003 ( 0.003)	Loss 1.1813e+00 (9.5670e-01)	Acc@1  67.97 ( 73.41)	Acc@5  90.62 ( 93.88)
03-Mar-22 09:14:29 -  * Acc@1 73.720 Acc@5 93.580
03-Mar-22 09:14:29 - Best acc at epoch 5: 73.97999572753906
03-Mar-22 09:14:29 - Epoch: [6][  0/352]	Time  0.361 ( 0.361)	Data  0.222 ( 0.222)	Loss 1.0536e+00 (1.0536e+00)	Acc@1  74.22 ( 74.22)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:14:30 - Epoch: [4][300/352]	Time  0.153 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.0857e+00 (9.5734e-01)	Acc@1  69.53 ( 73.41)	Acc@5  92.97 ( 93.86)
03-Mar-22 09:14:31 - Epoch: [6][ 10/352]	Time  0.160 ( 0.168)	Data  0.002 ( 0.022)	Loss 1.0978e+00 (9.5124e-01)	Acc@1  70.31 ( 73.08)	Acc@5  89.84 ( 94.74)
03-Mar-22 09:14:31 - Epoch: [4][310/352]	Time  0.144 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.0356e+00 (9.5593e-01)	Acc@1  71.09 ( 73.47)	Acc@5  89.84 ( 93.87)
03-Mar-22 09:14:32 - Epoch: [6][ 20/352]	Time  0.137 ( 0.158)	Data  0.002 ( 0.013)	Loss 1.0306e+00 (9.3624e-01)	Acc@1  72.66 ( 73.59)	Acc@5  92.97 ( 94.61)
03-Mar-22 09:14:33 - Epoch: [4][320/352]	Time  0.136 ( 0.155)	Data  0.003 ( 0.003)	Loss 9.0386e-01 (9.5536e-01)	Acc@1  72.66 ( 73.43)	Acc@5  95.31 ( 93.89)
03-Mar-22 09:14:34 - Epoch: [6][ 30/352]	Time  0.142 ( 0.156)	Data  0.002 ( 0.009)	Loss 8.6593e-01 (9.2335e-01)	Acc@1  77.34 ( 74.09)	Acc@5  95.31 ( 94.68)
03-Mar-22 09:14:34 - Epoch: [4][330/352]	Time  0.156 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.9541e-01 (9.5391e-01)	Acc@1  78.12 ( 73.47)	Acc@5  93.75 ( 93.92)
03-Mar-22 09:14:35 - Epoch: [6][ 40/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.008)	Loss 1.0515e+00 (9.2208e-01)	Acc@1  70.31 ( 74.28)	Acc@5  92.97 ( 94.59)
03-Mar-22 09:14:36 - Epoch: [4][340/352]	Time  0.131 ( 0.155)	Data  0.002 ( 0.003)	Loss 9.9845e-01 (9.5435e-01)	Acc@1  67.19 ( 73.44)	Acc@5  92.97 ( 93.91)
03-Mar-22 09:14:37 - Epoch: [6][ 50/352]	Time  0.156 ( 0.155)	Data  0.002 ( 0.007)	Loss 9.2302e-01 (9.1710e-01)	Acc@1  74.22 ( 74.36)	Acc@5  95.31 ( 94.59)
03-Mar-22 09:14:37 - Epoch: [4][350/352]	Time  0.155 ( 0.155)	Data  0.002 ( 0.003)	Loss 9.0359e-01 (9.5405e-01)	Acc@1  71.09 ( 73.45)	Acc@5  95.31 ( 93.90)
03-Mar-22 09:14:38 - Test: [ 0/20]	Time  0.359 ( 0.359)	Loss 9.7461e-01 (9.7461e-01)	Acc@1  71.48 ( 71.48)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:14:38 - Epoch: [6][ 60/352]	Time  0.142 ( 0.152)	Data  0.002 ( 0.006)	Loss 9.1412e-01 (9.2380e-01)	Acc@1  75.00 ( 74.27)	Acc@5  92.97 ( 94.54)
03-Mar-22 09:14:39 - Test: [10/20]	Time  0.091 ( 0.123)	Loss 8.3714e-01 (9.4499e-01)	Acc@1  74.22 ( 73.05)	Acc@5  95.31 ( 93.96)
03-Mar-22 09:14:40 -  * Acc@1 73.040 Acc@5 93.760
03-Mar-22 09:14:40 - Best acc at epoch 4: 73.54000091552734
03-Mar-22 09:14:40 - Epoch: [6][ 70/352]	Time  0.125 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.6373e-01 (9.2076e-01)	Acc@1  70.31 ( 74.30)	Acc@5  95.31 ( 94.58)
03-Mar-22 09:14:40 - Epoch: [5][  0/352]	Time  0.390 ( 0.390)	Data  0.230 ( 0.230)	Loss 1.0075e+00 (1.0075e+00)	Acc@1  72.66 ( 72.66)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:14:41 - Epoch: [6][ 80/352]	Time  0.128 ( 0.149)	Data  0.001 ( 0.005)	Loss 9.4328e-01 (9.1520e-01)	Acc@1  71.09 ( 74.35)	Acc@5  93.75 ( 94.65)
03-Mar-22 09:14:42 - Epoch: [5][ 10/352]	Time  0.153 ( 0.175)	Data  0.002 ( 0.023)	Loss 9.1507e-01 (9.3679e-01)	Acc@1  75.00 ( 73.08)	Acc@5  92.97 ( 94.89)
03-Mar-22 09:14:42 - Epoch: [6][ 90/352]	Time  0.151 ( 0.147)	Data  0.002 ( 0.005)	Loss 6.9844e-01 (9.1968e-01)	Acc@1  83.59 ( 74.22)	Acc@5  95.31 ( 94.60)
03-Mar-22 09:14:43 - Epoch: [5][ 20/352]	Time  0.157 ( 0.161)	Data  0.002 ( 0.013)	Loss 8.6007e-01 (9.4246e-01)	Acc@1  77.34 ( 73.51)	Acc@5  96.88 ( 94.72)
03-Mar-22 09:14:44 - Epoch: [6][100/352]	Time  0.135 ( 0.147)	Data  0.002 ( 0.004)	Loss 9.1066e-01 (9.2093e-01)	Acc@1  71.88 ( 74.20)	Acc@5  96.09 ( 94.57)
03-Mar-22 09:14:45 - Epoch: [5][ 30/352]	Time  0.145 ( 0.159)	Data  0.002 ( 0.010)	Loss 9.5417e-01 (9.3272e-01)	Acc@1  70.31 ( 73.54)	Acc@5  92.19 ( 94.53)
03-Mar-22 09:14:45 - Epoch: [6][110/352]	Time  0.151 ( 0.146)	Data  0.002 ( 0.004)	Loss 9.1088e-01 (9.2096e-01)	Acc@1  75.00 ( 74.15)	Acc@5  94.53 ( 94.63)
03-Mar-22 09:14:46 - Epoch: [5][ 40/352]	Time  0.151 ( 0.155)	Data  0.002 ( 0.008)	Loss 9.6572e-01 (9.3412e-01)	Acc@1  74.22 ( 73.30)	Acc@5  93.75 ( 94.42)
03-Mar-22 09:14:47 - Epoch: [6][120/352]	Time  0.149 ( 0.146)	Data  0.002 ( 0.004)	Loss 8.5427e-01 (9.2457e-01)	Acc@1  77.34 ( 74.08)	Acc@5  96.88 ( 94.56)
03-Mar-22 09:14:48 - Epoch: [5][ 50/352]	Time  0.152 ( 0.154)	Data  0.002 ( 0.007)	Loss 9.0912e-01 (9.2535e-01)	Acc@1  75.78 ( 73.65)	Acc@5  92.97 ( 94.53)
03-Mar-22 09:14:48 - Epoch: [6][130/352]	Time  0.147 ( 0.146)	Data  0.002 ( 0.004)	Loss 7.6709e-01 (9.2580e-01)	Acc@1  82.03 ( 74.11)	Acc@5  96.09 ( 94.53)
03-Mar-22 09:14:49 - Epoch: [5][ 60/352]	Time  0.155 ( 0.153)	Data  0.002 ( 0.006)	Loss 9.5367e-01 (9.3113e-01)	Acc@1  72.66 ( 73.40)	Acc@5  92.97 ( 94.44)
03-Mar-22 09:14:50 - Epoch: [6][140/352]	Time  0.145 ( 0.147)	Data  0.002 ( 0.004)	Loss 8.2970e-01 (9.2389e-01)	Acc@1  78.91 ( 74.16)	Acc@5  94.53 ( 94.51)
03-Mar-22 09:14:51 - Epoch: [5][ 70/352]	Time  0.157 ( 0.153)	Data  0.002 ( 0.006)	Loss 8.6418e-01 (9.2522e-01)	Acc@1  74.22 ( 73.61)	Acc@5  96.88 ( 94.56)
03-Mar-22 09:14:51 - Epoch: [6][150/352]	Time  0.145 ( 0.147)	Data  0.002 ( 0.004)	Loss 8.3639e-01 (9.2181e-01)	Acc@1  76.56 ( 74.16)	Acc@5  96.88 ( 94.53)
03-Mar-22 09:14:52 - Epoch: [5][ 80/352]	Time  0.146 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.8612e-01 (9.2562e-01)	Acc@1  76.56 ( 73.72)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:14:52 - Epoch: [6][160/352]	Time  0.133 ( 0.147)	Data  0.002 ( 0.004)	Loss 9.5735e-01 (9.2168e-01)	Acc@1  70.31 ( 74.17)	Acc@5  95.31 ( 94.54)
03-Mar-22 09:14:54 - Epoch: [5][ 90/352]	Time  0.132 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.5665e-01 (9.2951e-01)	Acc@1  76.56 ( 73.74)	Acc@5  93.75 ( 94.37)
03-Mar-22 09:14:54 - Epoch: [6][170/352]	Time  0.146 ( 0.147)	Data  0.002 ( 0.004)	Loss 1.0792e+00 (9.2318e-01)	Acc@1  69.53 ( 74.17)	Acc@5  91.41 ( 94.45)
03-Mar-22 09:14:55 - Epoch: [5][100/352]	Time  0.178 ( 0.153)	Data  0.002 ( 0.005)	Loss 1.1271e+00 (9.3063e-01)	Acc@1  68.75 ( 73.68)	Acc@5  90.62 ( 94.37)
03-Mar-22 09:14:55 - Epoch: [6][180/352]	Time  0.153 ( 0.147)	Data  0.002 ( 0.003)	Loss 8.3549e-01 (9.1968e-01)	Acc@1  75.78 ( 74.22)	Acc@5  98.44 ( 94.49)
03-Mar-22 09:14:57 - Epoch: [5][110/352]	Time  0.154 ( 0.153)	Data  0.003 ( 0.004)	Loss 6.5815e-01 (9.3637e-01)	Acc@1  82.03 ( 73.65)	Acc@5  96.88 ( 94.31)
03-Mar-22 09:14:57 - Epoch: [6][190/352]	Time  0.148 ( 0.147)	Data  0.002 ( 0.003)	Loss 8.4863e-01 (9.1990e-01)	Acc@1  75.78 ( 74.17)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:14:58 - Epoch: [5][120/352]	Time  0.159 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.7769e-01 (9.3438e-01)	Acc@1  79.69 ( 73.67)	Acc@5  94.53 ( 94.36)
03-Mar-22 09:14:59 - Epoch: [6][200/352]	Time  0.171 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.1546e+00 (9.2083e-01)	Acc@1  71.88 ( 74.14)	Acc@5  90.62 ( 94.40)
03-Mar-22 09:15:00 - Epoch: [5][130/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.4061e-01 (9.3255e-01)	Acc@1  73.44 ( 73.69)	Acc@5  93.75 ( 94.35)
03-Mar-22 09:15:00 - Epoch: [6][210/352]	Time  0.152 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.4792e-01 (9.2327e-01)	Acc@1  75.00 ( 74.04)	Acc@5  93.75 ( 94.40)
03-Mar-22 09:15:02 - Epoch: [6][220/352]	Time  0.142 ( 0.148)	Data  0.002 ( 0.003)	Loss 7.3405e-01 (9.2229e-01)	Acc@1  78.12 ( 74.10)	Acc@5  97.66 ( 94.43)
03-Mar-22 09:15:02 - Epoch: [5][140/352]	Time  0.177 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0517e+00 (9.3350e-01)	Acc@1  72.66 ( 73.74)	Acc@5  92.97 ( 94.32)
03-Mar-22 09:15:03 - Epoch: [6][230/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.6663e-01 (9.2454e-01)	Acc@1  71.09 ( 74.00)	Acc@5  89.84 ( 94.34)
03-Mar-22 09:15:03 - Epoch: [5][150/352]	Time  0.149 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.4920e-01 (9.3328e-01)	Acc@1  71.09 ( 73.72)	Acc@5  91.41 ( 94.27)
03-Mar-22 09:15:04 - Epoch: [6][240/352]	Time  0.178 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.2040e+00 (9.2791e-01)	Acc@1  65.62 ( 73.90)	Acc@5  89.84 ( 94.28)
03-Mar-22 09:15:05 - Epoch: [5][160/352]	Time  0.166 ( 0.154)	Data  0.003 ( 0.004)	Loss 9.4094e-01 (9.3829e-01)	Acc@1  75.00 ( 73.58)	Acc@5  92.97 ( 94.22)
03-Mar-22 09:15:06 - Epoch: [6][250/352]	Time  0.147 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.4142e-01 (9.2645e-01)	Acc@1  77.34 ( 73.96)	Acc@5  94.53 ( 94.27)
03-Mar-22 09:15:06 - Epoch: [5][170/352]	Time  0.137 ( 0.154)	Data  0.003 ( 0.004)	Loss 8.9468e-01 (9.3725e-01)	Acc@1  72.66 ( 73.66)	Acc@5  94.53 ( 94.23)
03-Mar-22 09:15:07 - Epoch: [6][260/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.7259e-01 (9.2671e-01)	Acc@1  71.88 ( 73.96)	Acc@5  94.53 ( 94.26)
03-Mar-22 09:15:08 - Epoch: [5][180/352]	Time  0.176 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.7539e-01 (9.3589e-01)	Acc@1  74.22 ( 73.78)	Acc@5  96.09 ( 94.25)
03-Mar-22 09:15:09 - Epoch: [6][270/352]	Time  0.138 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.1035e-01 (9.2848e-01)	Acc@1  77.34 ( 73.88)	Acc@5  95.31 ( 94.27)
03-Mar-22 09:15:09 - Epoch: [5][190/352]	Time  0.152 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.7465e-01 (9.3571e-01)	Acc@1  78.12 ( 73.82)	Acc@5  95.31 ( 94.26)
03-Mar-22 09:15:10 - Epoch: [6][280/352]	Time  0.151 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.3580e-01 (9.2805e-01)	Acc@1  75.00 ( 73.88)	Acc@5  92.97 ( 94.27)
03-Mar-22 09:15:11 - Epoch: [5][200/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.7852e-01 (9.3511e-01)	Acc@1  76.56 ( 73.88)	Acc@5  92.19 ( 94.20)
03-Mar-22 09:15:12 - Epoch: [6][290/352]	Time  0.147 ( 0.148)	Data  0.003 ( 0.003)	Loss 8.6346e-01 (9.2807e-01)	Acc@1  72.66 ( 73.87)	Acc@5  96.88 ( 94.28)
03-Mar-22 09:15:12 - Epoch: [5][210/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.6652e-01 (9.3399e-01)	Acc@1  77.34 ( 73.92)	Acc@5  94.53 ( 94.22)
03-Mar-22 09:15:13 - Epoch: [6][300/352]	Time  0.144 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.4541e-01 (9.2757e-01)	Acc@1  74.22 ( 73.91)	Acc@5  95.31 ( 94.27)
03-Mar-22 09:15:14 - Epoch: [5][220/352]	Time  0.177 ( 0.155)	Data  0.002 ( 0.003)	Loss 9.3780e-01 (9.3472e-01)	Acc@1  74.22 ( 73.94)	Acc@5  92.97 ( 94.20)
03-Mar-22 09:15:15 - Epoch: [6][310/352]	Time  0.150 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.5698e-01 (9.2717e-01)	Acc@1  72.66 ( 73.91)	Acc@5  93.75 ( 94.29)
03-Mar-22 09:15:15 - Epoch: [5][230/352]	Time  0.141 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0108e+00 (9.3554e-01)	Acc@1  72.66 ( 73.95)	Acc@5  91.41 ( 94.16)
03-Mar-22 09:15:16 - Epoch: [6][320/352]	Time  0.145 ( 0.147)	Data  0.003 ( 0.003)	Loss 9.2870e-01 (9.2711e-01)	Acc@1  75.00 ( 73.91)	Acc@5  93.75 ( 94.28)
03-Mar-22 09:15:17 - Epoch: [5][240/352]	Time  0.133 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.4844e-01 (9.3420e-01)	Acc@1  78.12 ( 74.04)	Acc@5  94.53 ( 94.14)
03-Mar-22 09:15:18 - Epoch: [6][330/352]	Time  0.148 ( 0.147)	Data  0.002 ( 0.003)	Loss 9.2471e-01 (9.2806e-01)	Acc@1  73.44 ( 73.89)	Acc@5  93.75 ( 94.27)
03-Mar-22 09:15:19 - Epoch: [5][250/352]	Time  0.169 ( 0.154)	Data  0.003 ( 0.003)	Loss 9.0862e-01 (9.3442e-01)	Acc@1  76.56 ( 74.04)	Acc@5  96.09 ( 94.14)
03-Mar-22 09:15:19 - Epoch: [6][340/352]	Time  0.158 ( 0.147)	Data  0.002 ( 0.003)	Loss 9.1154e-01 (9.3080e-01)	Acc@1  75.78 ( 73.79)	Acc@5  93.75 ( 94.23)
03-Mar-22 09:15:20 - Epoch: [5][260/352]	Time  0.173 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.4226e-01 (9.3360e-01)	Acc@1  78.12 ( 74.02)	Acc@5  95.31 ( 94.16)
03-Mar-22 09:15:21 - Epoch: [6][350/352]	Time  0.125 ( 0.147)	Data  0.001 ( 0.003)	Loss 1.0316e+00 (9.3124e-01)	Acc@1  75.78 ( 73.77)	Acc@5  92.97 ( 94.22)
03-Mar-22 09:15:21 - Test: [ 0/20]	Time  0.354 ( 0.354)	Loss 1.0155e+00 (1.0155e+00)	Acc@1  69.53 ( 69.53)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:15:21 - Epoch: [5][270/352]	Time  0.149 ( 0.154)	Data  0.001 ( 0.003)	Loss 1.0541e+00 (9.3311e-01)	Acc@1  67.19 ( 74.01)	Acc@5  91.41 ( 94.16)
03-Mar-22 09:15:22 - Test: [10/20]	Time  0.121 ( 0.130)	Loss 8.3065e-01 (9.5389e-01)	Acc@1  76.95 ( 72.59)	Acc@5  94.92 ( 94.03)
03-Mar-22 09:15:23 - Epoch: [5][280/352]	Time  0.157 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.7186e-01 (9.3002e-01)	Acc@1  75.00 ( 74.08)	Acc@5  94.53 ( 94.22)
03-Mar-22 09:15:23 -  * Acc@1 73.080 Acc@5 94.040
03-Mar-22 09:15:23 - Best acc at epoch 6: 73.97999572753906
03-Mar-22 09:15:24 - Epoch: [7][  0/352]	Time  0.362 ( 0.362)	Data  0.228 ( 0.228)	Loss 9.9459e-01 (9.9459e-01)	Acc@1  75.78 ( 75.78)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:15:24 - Epoch: [5][290/352]	Time  0.161 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0572e+00 (9.3237e-01)	Acc@1  71.09 ( 74.02)	Acc@5  92.19 ( 94.21)
03-Mar-22 09:15:25 - Epoch: [7][ 10/352]	Time  0.151 ( 0.173)	Data  0.003 ( 0.023)	Loss 9.3858e-01 (9.0491e-01)	Acc@1  74.22 ( 74.64)	Acc@5  96.09 ( 94.25)
03-Mar-22 09:15:26 - Epoch: [5][300/352]	Time  0.137 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.1205e-01 (9.3213e-01)	Acc@1  75.78 ( 73.97)	Acc@5  96.88 ( 94.21)
03-Mar-22 09:15:27 - Epoch: [7][ 20/352]	Time  0.147 ( 0.163)	Data  0.002 ( 0.013)	Loss 8.7821e-01 (9.2335e-01)	Acc@1  74.22 ( 73.88)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:15:27 - Epoch: [5][310/352]	Time  0.134 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.4686e-01 (9.3266e-01)	Acc@1  79.69 ( 73.96)	Acc@5  96.88 ( 94.19)
03-Mar-22 09:15:28 - Epoch: [7][ 30/352]	Time  0.146 ( 0.158)	Data  0.003 ( 0.010)	Loss 8.6116e-01 (9.2649e-01)	Acc@1  75.00 ( 74.12)	Acc@5  97.66 ( 94.61)
03-Mar-22 09:15:29 - Epoch: [5][320/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.6290e-01 (9.3462e-01)	Acc@1  75.00 ( 73.91)	Acc@5  92.97 ( 94.19)
03-Mar-22 09:15:30 - Epoch: [7][ 40/352]	Time  0.145 ( 0.156)	Data  0.002 ( 0.008)	Loss 1.1467e+00 (9.3250e-01)	Acc@1  67.19 ( 73.69)	Acc@5  89.06 ( 94.25)
03-Mar-22 09:15:30 - Epoch: [5][330/352]	Time  0.149 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.0937e-01 (9.3391e-01)	Acc@1  75.78 ( 73.92)	Acc@5  96.09 ( 94.20)
03-Mar-22 09:15:31 - Epoch: [7][ 50/352]	Time  0.150 ( 0.155)	Data  0.002 ( 0.007)	Loss 9.8636e-01 (9.3921e-01)	Acc@1  70.31 ( 73.45)	Acc@5  93.75 ( 94.15)
03-Mar-22 09:15:32 - Epoch: [5][340/352]	Time  0.150 ( 0.153)	Data  0.003 ( 0.003)	Loss 8.7669e-01 (9.3390e-01)	Acc@1  72.66 ( 73.89)	Acc@5  95.31 ( 94.20)
03-Mar-22 09:15:33 - Epoch: [7][ 60/352]	Time  0.144 ( 0.154)	Data  0.002 ( 0.006)	Loss 8.7777e-01 (9.3384e-01)	Acc@1  78.91 ( 73.60)	Acc@5  95.31 ( 94.25)
03-Mar-22 09:15:33 - Epoch: [5][350/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.1318e+00 (9.3456e-01)	Acc@1  68.75 ( 73.87)	Acc@5  92.97 ( 94.18)
03-Mar-22 09:15:34 - Epoch: [7][ 70/352]	Time  0.121 ( 0.151)	Data  0.002 ( 0.006)	Loss 1.0011e+00 (9.3742e-01)	Acc@1  72.66 ( 73.40)	Acc@5  89.84 ( 94.18)
03-Mar-22 09:15:34 - Test: [ 0/20]	Time  0.358 ( 0.358)	Loss 9.6162e-01 (9.6162e-01)	Acc@1  71.88 ( 71.88)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:15:35 - Test: [10/20]	Time  0.102 ( 0.128)	Loss 8.3993e-01 (9.3975e-01)	Acc@1  77.73 ( 73.72)	Acc@5  93.75 ( 93.57)
03-Mar-22 09:15:36 - Epoch: [7][ 80/352]	Time  0.160 ( 0.152)	Data  0.002 ( 0.005)	Loss 8.6425e-01 (9.3529e-01)	Acc@1  75.00 ( 73.52)	Acc@5  95.31 ( 94.21)
03-Mar-22 09:15:36 -  * Acc@1 73.360 Acc@5 93.640
03-Mar-22 09:15:36 - Best acc at epoch 5: 73.54000091552734
03-Mar-22 09:15:37 - Epoch: [6][  0/352]	Time  0.364 ( 0.364)	Data  0.227 ( 0.227)	Loss 9.0162e-01 (9.0162e-01)	Acc@1  71.88 ( 71.88)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:15:37 - Epoch: [7][ 90/352]	Time  0.140 ( 0.150)	Data  0.002 ( 0.005)	Loss 8.4213e-01 (9.3278e-01)	Acc@1  75.00 ( 73.76)	Acc@5  95.31 ( 94.19)
03-Mar-22 09:15:38 - Epoch: [6][ 10/352]	Time  0.187 ( 0.183)	Data  0.003 ( 0.023)	Loss 8.9382e-01 (9.0851e-01)	Acc@1  73.44 ( 73.86)	Acc@5  96.88 ( 94.89)
03-Mar-22 09:15:38 - Epoch: [7][100/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.005)	Loss 8.3958e-01 (9.3175e-01)	Acc@1  76.56 ( 73.89)	Acc@5  93.75 ( 94.18)
03-Mar-22 09:15:40 - Epoch: [6][ 20/352]	Time  0.152 ( 0.169)	Data  0.002 ( 0.013)	Loss 8.2687e-01 (8.9104e-01)	Acc@1  77.34 ( 74.14)	Acc@5  95.31 ( 95.13)
03-Mar-22 09:15:40 - Epoch: [7][110/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.9959e-01 (9.2832e-01)	Acc@1  71.09 ( 73.91)	Acc@5  96.09 ( 94.26)
03-Mar-22 09:15:41 - Epoch: [6][ 30/352]	Time  0.153 ( 0.164)	Data  0.002 ( 0.010)	Loss 7.8558e-01 (8.8274e-01)	Acc@1  80.47 ( 74.75)	Acc@5  95.31 ( 94.88)
03-Mar-22 09:15:42 - Epoch: [7][120/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.4573e-01 (9.2552e-01)	Acc@1  74.22 ( 74.04)	Acc@5  94.53 ( 94.29)
03-Mar-22 09:15:43 - Epoch: [6][ 40/352]	Time  0.154 ( 0.161)	Data  0.002 ( 0.008)	Loss 9.3694e-01 (8.8875e-01)	Acc@1  78.12 ( 74.66)	Acc@5  92.97 ( 94.82)
03-Mar-22 09:15:43 - Epoch: [7][130/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.1002e+00 (9.2343e-01)	Acc@1  67.97 ( 74.06)	Acc@5  92.19 ( 94.33)
03-Mar-22 09:15:44 - Epoch: [6][ 50/352]	Time  0.148 ( 0.158)	Data  0.002 ( 0.007)	Loss 1.0896e+00 (8.9226e-01)	Acc@1  75.00 ( 74.91)	Acc@5  87.50 ( 94.59)
03-Mar-22 09:15:45 - Epoch: [7][140/352]	Time  0.130 ( 0.151)	Data  0.003 ( 0.004)	Loss 7.7682e-01 (9.2433e-01)	Acc@1  76.56 ( 73.97)	Acc@5  96.09 ( 94.30)
03-Mar-22 09:15:46 - Epoch: [6][ 60/352]	Time  0.153 ( 0.157)	Data  0.002 ( 0.006)	Loss 8.4252e-01 (8.9981e-01)	Acc@1  78.12 ( 74.62)	Acc@5  95.31 ( 94.38)
03-Mar-22 09:15:46 - Epoch: [7][150/352]	Time  0.149 ( 0.151)	Data  0.003 ( 0.004)	Loss 1.0716e+00 (9.2342e-01)	Acc@1  68.75 ( 74.04)	Acc@5  90.62 ( 94.26)
03-Mar-22 09:15:47 - Epoch: [6][ 70/352]	Time  0.148 ( 0.156)	Data  0.002 ( 0.006)	Loss 9.5684e-01 (9.0186e-01)	Acc@1  71.09 ( 74.60)	Acc@5  92.19 ( 94.33)
03-Mar-22 09:15:48 - Epoch: [7][160/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.004)	Loss 7.8906e-01 (9.2227e-01)	Acc@1  78.12 ( 74.04)	Acc@5  95.31 ( 94.22)
03-Mar-22 09:15:49 - Epoch: [6][ 80/352]	Time  0.159 ( 0.155)	Data  0.002 ( 0.005)	Loss 9.8933e-01 (9.1001e-01)	Acc@1  71.09 ( 74.43)	Acc@5  92.97 ( 94.20)
03-Mar-22 09:15:49 - Epoch: [7][170/352]	Time  0.161 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.6516e-01 (9.2296e-01)	Acc@1  74.22 ( 74.04)	Acc@5  96.09 ( 94.24)
03-Mar-22 09:15:50 - Epoch: [6][ 90/352]	Time  0.150 ( 0.155)	Data  0.002 ( 0.005)	Loss 1.1895e+00 (9.1770e-01)	Acc@1  70.31 ( 74.15)	Acc@5  88.28 ( 94.12)
03-Mar-22 09:15:51 - Epoch: [7][180/352]	Time  0.139 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.1979e-01 (9.2289e-01)	Acc@1  75.78 ( 74.05)	Acc@5  97.66 ( 94.28)
03-Mar-22 09:15:52 - Epoch: [6][100/352]	Time  0.147 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.4669e-01 (9.1381e-01)	Acc@1  68.75 ( 74.27)	Acc@5  93.75 ( 94.15)
03-Mar-22 09:15:52 - Epoch: [7][190/352]	Time  0.150 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.1999e-01 (9.2262e-01)	Acc@1  73.44 ( 74.09)	Acc@5  92.97 ( 94.26)
03-Mar-22 09:15:53 - Epoch: [6][110/352]	Time  0.130 ( 0.154)	Data  0.001 ( 0.004)	Loss 1.1052e+00 (9.1639e-01)	Acc@1  68.75 ( 74.22)	Acc@5  88.28 ( 94.02)
03-Mar-22 09:15:54 - Epoch: [7][200/352]	Time  0.148 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.6218e-01 (9.2279e-01)	Acc@1  74.22 ( 74.09)	Acc@5  94.53 ( 94.27)
03-Mar-22 09:15:55 - Epoch: [6][120/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.9340e-01 (9.1666e-01)	Acc@1  68.75 ( 74.05)	Acc@5  92.97 ( 94.12)
03-Mar-22 09:15:55 - Epoch: [7][210/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.9742e-01 (9.2340e-01)	Acc@1  71.88 ( 74.00)	Acc@5  93.75 ( 94.29)
03-Mar-22 09:15:56 - Epoch: [6][130/352]	Time  0.161 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.3631e-01 (9.1303e-01)	Acc@1  75.00 ( 74.14)	Acc@5  95.31 ( 94.20)
03-Mar-22 09:15:57 - Epoch: [7][220/352]	Time  0.147 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.5913e-01 (9.2179e-01)	Acc@1  76.56 ( 74.08)	Acc@5  94.53 ( 94.33)
03-Mar-22 09:15:58 - Epoch: [6][140/352]	Time  0.160 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0117e+00 (9.1467e-01)	Acc@1  70.31 ( 74.11)	Acc@5  92.19 ( 94.13)
03-Mar-22 09:15:58 - Epoch: [7][230/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1800e-01 (9.2012e-01)	Acc@1  72.66 ( 74.10)	Acc@5  95.31 ( 94.32)
03-Mar-22 09:15:59 - Epoch: [6][150/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.5738e-01 (9.1770e-01)	Acc@1  77.34 ( 74.04)	Acc@5  96.09 ( 94.07)
03-Mar-22 09:16:00 - Epoch: [7][240/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.4896e-01 (9.2105e-01)	Acc@1  71.88 ( 74.04)	Acc@5  93.75 ( 94.30)
03-Mar-22 09:16:01 - Epoch: [6][160/352]	Time  0.157 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.2454e-01 (9.1719e-01)	Acc@1  73.44 ( 74.07)	Acc@5  96.09 ( 94.07)
03-Mar-22 09:16:01 - Epoch: [7][250/352]	Time  0.140 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0039e+00 (9.2367e-01)	Acc@1  71.09 ( 74.00)	Acc@5  95.31 ( 94.27)
03-Mar-22 09:16:02 - Epoch: [6][170/352]	Time  0.149 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.5687e-01 (9.1662e-01)	Acc@1  76.56 ( 74.05)	Acc@5  96.09 ( 94.09)
03-Mar-22 09:16:03 - Epoch: [7][260/352]	Time  0.145 ( 0.151)	Data  0.003 ( 0.003)	Loss 7.9790e-01 (9.2226e-01)	Acc@1  78.12 ( 74.02)	Acc@5  96.88 ( 94.32)
03-Mar-22 09:16:04 - Epoch: [6][180/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0851e+00 (9.1773e-01)	Acc@1  71.88 ( 74.05)	Acc@5  91.41 ( 94.08)
03-Mar-22 09:16:04 - Epoch: [7][270/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0948e+00 (9.2255e-01)	Acc@1  67.97 ( 73.99)	Acc@5  93.75 ( 94.32)
03-Mar-22 09:16:05 - Epoch: [6][190/352]	Time  0.177 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0325e+00 (9.1820e-01)	Acc@1  71.09 ( 74.08)	Acc@5  96.09 ( 94.09)
03-Mar-22 09:16:06 - Epoch: [7][280/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0680e+00 (9.2285e-01)	Acc@1  68.75 ( 73.98)	Acc@5  93.75 ( 94.31)
03-Mar-22 09:16:07 - Epoch: [6][200/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0774e+00 (9.2078e-01)	Acc@1  70.31 ( 74.03)	Acc@5  95.31 ( 94.06)
03-Mar-22 09:16:07 - Epoch: [7][290/352]	Time  0.154 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.9740e-01 (9.2493e-01)	Acc@1  68.75 ( 73.93)	Acc@5  95.31 ( 94.30)
03-Mar-22 09:16:08 - Epoch: [6][210/352]	Time  0.151 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.0767e-01 (9.2106e-01)	Acc@1  75.78 ( 73.99)	Acc@5  93.75 ( 94.08)
03-Mar-22 09:16:09 - Epoch: [7][300/352]	Time  0.145 ( 0.151)	Data  0.003 ( 0.003)	Loss 8.4308e-01 (9.2367e-01)	Acc@1  74.22 ( 73.92)	Acc@5  93.75 ( 94.33)
03-Mar-22 09:16:10 - Epoch: [6][220/352]	Time  0.152 ( 0.153)	Data  0.003 ( 0.004)	Loss 7.1022e-01 (9.2150e-01)	Acc@1  80.47 ( 73.96)	Acc@5  98.44 ( 94.12)
03-Mar-22 09:16:10 - Epoch: [7][310/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.4588e-01 (9.2235e-01)	Acc@1  74.22 ( 74.01)	Acc@5  96.88 ( 94.34)
03-Mar-22 09:16:11 - Epoch: [6][230/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.5846e-01 (9.2116e-01)	Acc@1  79.69 ( 73.97)	Acc@5  94.53 ( 94.13)
03-Mar-22 09:16:12 - Epoch: [7][320/352]	Time  0.151 ( 0.150)	Data  0.003 ( 0.003)	Loss 9.6994e-01 (9.2272e-01)	Acc@1  72.66 ( 73.99)	Acc@5  94.53 ( 94.32)
03-Mar-22 09:16:13 - Epoch: [6][240/352]	Time  0.159 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0304e+00 (9.2285e-01)	Acc@1  68.75 ( 73.95)	Acc@5  93.75 ( 94.14)
03-Mar-22 09:16:13 - Epoch: [7][330/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.8418e-01 (9.2292e-01)	Acc@1  72.66 ( 73.98)	Acc@5  94.53 ( 94.32)
03-Mar-22 09:16:15 - Epoch: [7][340/352]	Time  0.154 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.8314e-01 (9.2289e-01)	Acc@1  71.88 ( 73.99)	Acc@5  93.75 ( 94.30)
03-Mar-22 09:16:15 - Epoch: [6][250/352]	Time  0.154 ( 0.153)	Data  0.003 ( 0.003)	Loss 1.0723e+00 (9.2347e-01)	Acc@1  72.66 ( 73.97)	Acc@5  93.75 ( 94.15)
03-Mar-22 09:16:16 - Epoch: [7][350/352]	Time  0.128 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.0098e-01 (9.2132e-01)	Acc@1  73.44 ( 74.05)	Acc@5  93.75 ( 94.31)
03-Mar-22 09:16:16 - Epoch: [6][260/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.9729e-01 (9.2516e-01)	Acc@1  77.34 ( 73.98)	Acc@5  93.75 ( 94.16)
03-Mar-22 09:16:17 - Test: [ 0/20]	Time  0.359 ( 0.359)	Loss 1.0057e+00 (1.0057e+00)	Acc@1  69.53 ( 69.53)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:16:18 - Epoch: [6][270/352]	Time  0.159 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.4080e-01 (9.2427e-01)	Acc@1  79.69 ( 74.00)	Acc@5  96.88 ( 94.22)
03-Mar-22 09:16:18 - Test: [10/20]	Time  0.118 ( 0.131)	Loss 8.0035e-01 (9.3659e-01)	Acc@1  80.08 ( 74.64)	Acc@5  95.70 ( 93.93)
03-Mar-22 09:16:19 -  * Acc@1 73.840 Acc@5 93.660
03-Mar-22 09:16:19 - Best acc at epoch 7: 73.97999572753906
03-Mar-22 09:16:19 - Epoch: [6][280/352]	Time  0.133 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.7216e-01 (9.2560e-01)	Acc@1  73.44 ( 73.91)	Acc@5  91.41 ( 94.23)
03-Mar-22 09:16:19 - Epoch: [8][  0/352]	Time  0.363 ( 0.363)	Data  0.232 ( 0.232)	Loss 9.2094e-01 (9.2094e-01)	Acc@1  69.53 ( 69.53)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:16:21 - Epoch: [8][ 10/352]	Time  0.149 ( 0.166)	Data  0.002 ( 0.024)	Loss 8.2953e-01 (8.6726e-01)	Acc@1  76.56 ( 75.21)	Acc@5  95.31 ( 95.45)
03-Mar-22 09:16:21 - Epoch: [6][290/352]	Time  0.124 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.3022e-01 (9.2572e-01)	Acc@1  73.44 ( 73.94)	Acc@5  92.19 ( 94.22)
03-Mar-22 09:16:22 - Epoch: [8][ 20/352]	Time  0.136 ( 0.153)	Data  0.001 ( 0.013)	Loss 9.3763e-01 (8.9830e-01)	Acc@1  74.22 ( 75.15)	Acc@5  92.97 ( 94.42)
03-Mar-22 09:16:22 - Epoch: [6][300/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.3697e-01 (9.2618e-01)	Acc@1  67.19 ( 73.88)	Acc@5  98.44 ( 94.24)
03-Mar-22 09:16:23 - Epoch: [8][ 30/352]	Time  0.147 ( 0.151)	Data  0.002 ( 0.010)	Loss 8.2094e-01 (8.8954e-01)	Acc@1  77.34 ( 75.23)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:16:23 - Epoch: [6][310/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.3657e-01 (9.2612e-01)	Acc@1  73.44 ( 73.89)	Acc@5  94.53 ( 94.23)
03-Mar-22 09:16:25 - Epoch: [8][ 40/352]	Time  0.138 ( 0.151)	Data  0.002 ( 0.008)	Loss 9.6064e-01 (9.0191e-01)	Acc@1  75.00 ( 74.83)	Acc@5  92.97 ( 94.40)
03-Mar-22 09:16:25 - Epoch: [6][320/352]	Time  0.173 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.7516e-01 (9.2618e-01)	Acc@1  74.22 ( 73.90)	Acc@5  94.53 ( 94.22)
03-Mar-22 09:16:26 - Epoch: [8][ 50/352]	Time  0.174 ( 0.151)	Data  0.002 ( 0.007)	Loss 7.6632e-01 (9.0229e-01)	Acc@1  79.69 ( 74.57)	Acc@5  96.09 ( 94.47)
03-Mar-22 09:16:27 - Epoch: [6][330/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.1668e-01 (9.2729e-01)	Acc@1  71.09 ( 73.86)	Acc@5  93.75 ( 94.19)
03-Mar-22 09:16:28 - Epoch: [8][ 60/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.006)	Loss 8.5654e-01 (9.1458e-01)	Acc@1  76.56 ( 74.03)	Acc@5  95.31 ( 94.39)
03-Mar-22 09:16:28 - Epoch: [6][340/352]	Time  0.135 ( 0.152)	Data  0.001 ( 0.003)	Loss 1.1296e+00 (9.2607e-01)	Acc@1  69.53 ( 73.89)	Acc@5  92.97 ( 94.21)
03-Mar-22 09:16:29 - Epoch: [8][ 70/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.006)	Loss 9.7137e-01 (9.1212e-01)	Acc@1  69.53 ( 73.89)	Acc@5  93.75 ( 94.42)
03-Mar-22 09:16:30 - Epoch: [6][350/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.8696e-01 (9.2581e-01)	Acc@1  72.66 ( 73.87)	Acc@5  95.31 ( 94.24)
03-Mar-22 09:16:30 - Test: [ 0/20]	Time  0.361 ( 0.361)	Loss 9.9207e-01 (9.9207e-01)	Acc@1  71.09 ( 71.09)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:16:31 - Epoch: [8][ 80/352]	Time  0.160 ( 0.152)	Data  0.002 ( 0.005)	Loss 8.6914e-01 (9.0511e-01)	Acc@1  71.88 ( 74.20)	Acc@5  98.44 ( 94.50)
03-Mar-22 09:16:31 - Test: [10/20]	Time  0.104 ( 0.127)	Loss 8.3633e-01 (9.4249e-01)	Acc@1  76.17 ( 73.97)	Acc@5  95.70 ( 93.47)
03-Mar-22 09:16:32 -  * Acc@1 74.040 Acc@5 93.840
03-Mar-22 09:16:32 - Best acc at epoch 6: 74.04000091552734
03-Mar-22 09:16:32 - Epoch: [8][ 90/352]	Time  0.117 ( 0.152)	Data  0.002 ( 0.005)	Loss 9.1522e-01 (9.0105e-01)	Acc@1  74.22 ( 74.35)	Acc@5  94.53 ( 94.55)
03-Mar-22 09:16:33 - Epoch: [7][  0/352]	Time  0.357 ( 0.357)	Data  0.216 ( 0.216)	Loss 9.0551e-01 (9.0551e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.88 ( 96.88)
03-Mar-22 09:16:34 - Epoch: [8][100/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.005)	Loss 1.0787e+00 (9.0722e-01)	Acc@1  68.75 ( 74.26)	Acc@5  95.31 ( 94.51)
03-Mar-22 09:16:34 - Epoch: [7][ 10/352]	Time  0.131 ( 0.166)	Data  0.002 ( 0.022)	Loss 8.3136e-01 (9.3745e-01)	Acc@1  76.56 ( 73.30)	Acc@5  94.53 ( 94.60)
03-Mar-22 09:16:35 - Epoch: [8][110/352]	Time  0.154 ( 0.150)	Data  0.002 ( 0.005)	Loss 9.8195e-01 (9.0976e-01)	Acc@1  71.88 ( 74.14)	Acc@5  93.75 ( 94.43)
03-Mar-22 09:16:35 - Epoch: [7][ 20/352]	Time  0.150 ( 0.157)	Data  0.002 ( 0.012)	Loss 9.3964e-01 (9.2215e-01)	Acc@1  75.78 ( 74.22)	Acc@5  90.62 ( 94.46)
03-Mar-22 09:16:37 - Epoch: [8][120/352]	Time  0.157 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.9901e-01 (9.0998e-01)	Acc@1  67.19 ( 74.25)	Acc@5  92.19 ( 94.36)
03-Mar-22 09:16:37 - Epoch: [7][ 30/352]	Time  0.154 ( 0.154)	Data  0.002 ( 0.009)	Loss 1.1229e+00 (9.2360e-01)	Acc@1  69.53 ( 74.09)	Acc@5  89.84 ( 94.35)
03-Mar-22 09:16:38 - Epoch: [8][130/352]	Time  0.150 ( 0.150)	Data  0.003 ( 0.004)	Loss 9.0641e-01 (9.1491e-01)	Acc@1  73.44 ( 74.04)	Acc@5  93.75 ( 94.30)
03-Mar-22 09:16:39 - Epoch: [7][ 40/352]	Time  0.149 ( 0.154)	Data  0.003 ( 0.008)	Loss 9.1774e-01 (9.0718e-01)	Acc@1  75.00 ( 74.60)	Acc@5  96.09 ( 94.51)
03-Mar-22 09:16:40 - Epoch: [8][140/352]	Time  0.170 ( 0.150)	Data  0.003 ( 0.004)	Loss 7.7502e-01 (9.1218e-01)	Acc@1  79.69 ( 74.12)	Acc@5  97.66 ( 94.34)
03-Mar-22 09:16:40 - Epoch: [7][ 50/352]	Time  0.155 ( 0.154)	Data  0.003 ( 0.007)	Loss 9.2839e-01 (9.0472e-01)	Acc@1  75.00 ( 74.92)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:16:41 - Epoch: [8][150/352]	Time  0.147 ( 0.150)	Data  0.003 ( 0.004)	Loss 9.5294e-01 (9.1255e-01)	Acc@1  73.44 ( 74.12)	Acc@5  92.19 ( 94.29)
03-Mar-22 09:16:42 - Epoch: [7][ 60/352]	Time  0.151 ( 0.153)	Data  0.002 ( 0.006)	Loss 1.1887e+00 (9.0839e-01)	Acc@1  64.84 ( 74.67)	Acc@5  91.41 ( 94.49)
03-Mar-22 09:16:43 - Epoch: [8][160/352]	Time  0.148 ( 0.150)	Data  0.003 ( 0.004)	Loss 1.1133e+00 (9.1479e-01)	Acc@1  65.62 ( 74.04)	Acc@5  90.62 ( 94.28)
03-Mar-22 09:16:43 - Epoch: [7][ 70/352]	Time  0.151 ( 0.153)	Data  0.003 ( 0.006)	Loss 8.5015e-01 (9.0896e-01)	Acc@1  75.00 ( 74.55)	Acc@5  95.31 ( 94.55)
03-Mar-22 09:16:44 - Epoch: [8][170/352]	Time  0.145 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.4237e-01 (9.1499e-01)	Acc@1  74.22 ( 73.91)	Acc@5  94.53 ( 94.27)
03-Mar-22 09:16:45 - Epoch: [7][ 80/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.7374e-01 (9.0980e-01)	Acc@1  70.31 ( 74.44)	Acc@5  92.19 ( 94.56)
03-Mar-22 09:16:46 - Epoch: [8][180/352]	Time  0.151 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.7021e-01 (9.1583e-01)	Acc@1  75.00 ( 73.86)	Acc@5  97.66 ( 94.33)
03-Mar-22 09:16:46 - Epoch: [7][ 90/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.005)	Loss 8.0499e-01 (9.0738e-01)	Acc@1  78.91 ( 74.43)	Acc@5  96.88 ( 94.59)
03-Mar-22 09:16:48 - Epoch: [7][100/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.005)	Loss 9.5827e-01 (9.0556e-01)	Acc@1  69.53 ( 74.44)	Acc@5  93.75 ( 94.60)
03-Mar-22 09:16:48 - Epoch: [8][190/352]	Time  0.175 ( 0.152)	Data  0.003 ( 0.004)	Loss 9.7321e-01 (9.1490e-01)	Acc@1  75.00 ( 73.94)	Acc@5  92.19 ( 94.31)
03-Mar-22 09:16:49 - Epoch: [7][110/352]	Time  0.145 ( 0.152)	Data  0.003 ( 0.004)	Loss 9.7243e-01 (9.1040e-01)	Acc@1  67.19 ( 74.23)	Acc@5  93.75 ( 94.47)
03-Mar-22 09:16:49 - Epoch: [8][200/352]	Time  0.183 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.5068e-01 (9.1619e-01)	Acc@1  75.00 ( 73.95)	Acc@5  94.53 ( 94.33)
03-Mar-22 09:16:51 - Epoch: [7][120/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.1163e-01 (9.0878e-01)	Acc@1  75.78 ( 74.23)	Acc@5  96.09 ( 94.51)
03-Mar-22 09:16:51 - Epoch: [8][210/352]	Time  0.147 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.8656e-01 (9.1465e-01)	Acc@1  73.44 ( 74.03)	Acc@5  95.31 ( 94.36)
03-Mar-22 09:16:52 - Epoch: [7][130/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.8388e-01 (9.0841e-01)	Acc@1  78.91 ( 74.36)	Acc@5  96.09 ( 94.61)
03-Mar-22 09:16:53 - Epoch: [8][220/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0278e+00 (9.1599e-01)	Acc@1  71.88 ( 73.96)	Acc@5  92.97 ( 94.31)
03-Mar-22 09:16:54 - Epoch: [7][140/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.4362e-01 (9.0896e-01)	Acc@1  74.22 ( 74.32)	Acc@5  97.66 ( 94.63)
03-Mar-22 09:16:54 - Epoch: [8][230/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0876e+00 (9.1604e-01)	Acc@1  68.75 ( 73.96)	Acc@5  91.41 ( 94.32)
03-Mar-22 09:16:55 - Epoch: [7][150/352]	Time  0.152 ( 0.151)	Data  0.003 ( 0.004)	Loss 9.8753e-01 (9.1003e-01)	Acc@1  71.88 ( 74.27)	Acc@5  92.97 ( 94.63)
03-Mar-22 09:16:56 - Epoch: [8][240/352]	Time  0.176 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.7689e-01 (9.1558e-01)	Acc@1  77.34 ( 73.98)	Acc@5  96.09 ( 94.34)
03-Mar-22 09:16:57 - Epoch: [7][160/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.7035e-01 (9.0911e-01)	Acc@1  75.78 ( 74.27)	Acc@5  94.53 ( 94.65)
03-Mar-22 09:16:58 - Epoch: [8][250/352]	Time  0.168 ( 0.155)	Data  0.002 ( 0.003)	Loss 9.6440e-01 (9.1735e-01)	Acc@1  74.22 ( 73.90)	Acc@5  94.53 ( 94.32)
03-Mar-22 09:16:58 - Epoch: [7][170/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.1185e-01 (9.1398e-01)	Acc@1  71.09 ( 74.12)	Acc@5  96.88 ( 94.60)
03-Mar-22 09:16:59 - Epoch: [8][260/352]	Time  0.146 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.8074e-01 (9.1765e-01)	Acc@1  74.22 ( 73.87)	Acc@5  96.09 ( 94.31)
03-Mar-22 09:16:59 - Epoch: [7][180/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.0177e-01 (9.1016e-01)	Acc@1  72.66 ( 74.28)	Acc@5  95.31 ( 94.62)
03-Mar-22 09:17:01 - Epoch: [8][270/352]	Time  0.172 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.4573e-01 (9.1649e-01)	Acc@1  78.91 ( 73.88)	Acc@5  96.88 ( 94.35)
03-Mar-22 09:17:01 - Epoch: [7][190/352]	Time  0.136 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.0124e+00 (9.1031e-01)	Acc@1  73.44 ( 74.26)	Acc@5  94.53 ( 94.62)
03-Mar-22 09:17:02 - Epoch: [8][280/352]	Time  0.170 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.0292e+00 (9.1779e-01)	Acc@1  69.53 ( 73.88)	Acc@5  94.53 ( 94.34)
03-Mar-22 09:17:02 - Epoch: [7][200/352]	Time  0.148 ( 0.151)	Data  0.003 ( 0.003)	Loss 7.7231e-01 (9.0974e-01)	Acc@1  74.22 ( 74.26)	Acc@5  96.88 ( 94.61)
03-Mar-22 09:17:04 - Epoch: [8][290/352]	Time  0.157 ( 0.156)	Data  0.003 ( 0.003)	Loss 8.3222e-01 (9.1736e-01)	Acc@1  75.00 ( 73.83)	Acc@5  96.88 ( 94.36)
03-Mar-22 09:17:04 - Epoch: [7][210/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0043e+00 (9.1336e-01)	Acc@1  71.88 ( 74.15)	Acc@5  92.97 ( 94.54)
03-Mar-22 09:17:05 - Epoch: [7][220/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0290e+00 (9.1365e-01)	Acc@1  73.44 ( 74.12)	Acc@5  91.41 ( 94.52)
03-Mar-22 09:17:06 - Epoch: [8][300/352]	Time  0.178 ( 0.156)	Data  0.003 ( 0.003)	Loss 8.4967e-01 (9.1680e-01)	Acc@1  77.34 ( 73.88)	Acc@5  95.31 ( 94.34)
03-Mar-22 09:17:07 - Epoch: [7][230/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0901e+00 (9.1432e-01)	Acc@1  71.09 ( 74.06)	Acc@5  90.62 ( 94.53)
03-Mar-22 09:17:07 - Epoch: [8][310/352]	Time  0.176 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.0823e-01 (9.1618e-01)	Acc@1  67.19 ( 73.87)	Acc@5  97.66 ( 94.37)
03-Mar-22 09:17:08 - Epoch: [7][240/352]	Time  0.149 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.7801e-01 (9.1322e-01)	Acc@1  77.34 ( 74.15)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:17:09 - Epoch: [8][320/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.5109e-01 (9.1584e-01)	Acc@1  76.56 ( 73.92)	Acc@5  95.31 ( 94.35)
03-Mar-22 09:17:10 - Epoch: [7][250/352]	Time  0.135 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0543e+00 (9.1435e-01)	Acc@1  67.97 ( 74.08)	Acc@5  93.75 ( 94.51)
03-Mar-22 09:17:11 - Epoch: [8][330/352]	Time  0.152 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.0552e+00 (9.1675e-01)	Acc@1  71.88 ( 73.86)	Acc@5  90.62 ( 94.35)
03-Mar-22 09:17:11 - Epoch: [7][260/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.1392e-01 (9.1581e-01)	Acc@1  77.34 ( 74.05)	Acc@5  99.22 ( 94.50)
03-Mar-22 09:17:12 - Epoch: [8][340/352]	Time  0.154 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.6867e-01 (9.1764e-01)	Acc@1  74.22 ( 73.86)	Acc@5  96.09 ( 94.33)
03-Mar-22 09:17:13 - Epoch: [7][270/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.003)	Loss 6.7272e-01 (9.1578e-01)	Acc@1  83.59 ( 74.07)	Acc@5  96.09 ( 94.49)
03-Mar-22 09:17:14 - Epoch: [8][350/352]	Time  0.136 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.0010e+00 (9.1877e-01)	Acc@1  69.53 ( 73.82)	Acc@5  95.31 ( 94.29)
03-Mar-22 09:17:14 - Epoch: [7][280/352]	Time  0.096 ( 0.149)	Data  0.001 ( 0.003)	Loss 7.4493e-01 (9.1695e-01)	Acc@1  76.56 ( 74.04)	Acc@5  97.66 ( 94.49)
03-Mar-22 09:17:14 - Test: [ 0/20]	Time  0.368 ( 0.368)	Loss 1.0369e+00 (1.0369e+00)	Acc@1  70.31 ( 70.31)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:17:15 - Test: [10/20]	Time  0.106 ( 0.135)	Loss 8.6322e-01 (9.5752e-01)	Acc@1  75.00 ( 72.34)	Acc@5  94.53 ( 93.61)
03-Mar-22 09:17:16 - Epoch: [7][290/352]	Time  0.160 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.5598e-01 (9.1806e-01)	Acc@1  78.12 ( 74.05)	Acc@5  94.53 ( 94.45)
03-Mar-22 09:17:16 -  * Acc@1 72.680 Acc@5 93.580
03-Mar-22 09:17:17 - Best acc at epoch 8: 73.97999572753906
03-Mar-22 09:17:17 - Epoch: [9][  0/352]	Time  0.368 ( 0.368)	Data  0.218 ( 0.218)	Loss 1.0948e+00 (1.0948e+00)	Acc@1  67.97 ( 67.97)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:17:17 - Epoch: [7][300/352]	Time  0.126 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.8600e-01 (9.1970e-01)	Acc@1  78.91 ( 74.02)	Acc@5  92.19 ( 94.41)
03-Mar-22 09:17:18 - Epoch: [9][ 10/352]	Time  0.138 ( 0.156)	Data  0.002 ( 0.022)	Loss 7.9139e-01 (9.1585e-01)	Acc@1  78.91 ( 73.58)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:17:18 - Epoch: [7][310/352]	Time  0.129 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.1772e-01 (9.1896e-01)	Acc@1  74.22 ( 74.05)	Acc@5  92.97 ( 94.39)
03-Mar-22 09:17:20 - Epoch: [7][320/352]	Time  0.118 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.7745e-01 (9.1992e-01)	Acc@1  74.22 ( 73.99)	Acc@5  94.53 ( 94.40)
03-Mar-22 09:17:20 - Epoch: [9][ 20/352]	Time  0.134 ( 0.152)	Data  0.002 ( 0.012)	Loss 1.0410e+00 (9.2203e-01)	Acc@1  70.31 ( 74.00)	Acc@5  92.19 ( 94.27)
03-Mar-22 09:17:21 - Epoch: [7][330/352]	Time  0.133 ( 0.147)	Data  0.002 ( 0.003)	Loss 7.9056e-01 (9.1891e-01)	Acc@1  79.69 ( 74.02)	Acc@5  95.31 ( 94.39)
03-Mar-22 09:17:21 - Epoch: [9][ 30/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.009)	Loss 8.4749e-01 (9.3150e-01)	Acc@1  77.34 ( 73.92)	Acc@5  94.53 ( 94.13)
03-Mar-22 09:17:22 - Epoch: [7][340/352]	Time  0.141 ( 0.147)	Data  0.002 ( 0.003)	Loss 1.0592e+00 (9.2079e-01)	Acc@1  68.75 ( 73.94)	Acc@5  92.19 ( 94.37)
03-Mar-22 09:17:23 - Epoch: [9][ 40/352]	Time  0.149 ( 0.150)	Data  0.002 ( 0.008)	Loss 9.6588e-01 (9.2938e-01)	Acc@1  74.22 ( 74.09)	Acc@5  93.75 ( 93.88)
03-Mar-22 09:17:24 - Epoch: [7][350/352]	Time  0.127 ( 0.147)	Data  0.002 ( 0.003)	Loss 1.0301e+00 (9.2075e-01)	Acc@1  72.66 ( 73.98)	Acc@5  92.97 ( 94.36)
03-Mar-22 09:17:24 - Epoch: [9][ 50/352]	Time  0.126 ( 0.148)	Data  0.002 ( 0.006)	Loss 7.9641e-01 (9.2102e-01)	Acc@1  78.91 ( 74.42)	Acc@5  96.09 ( 94.03)
03-Mar-22 09:17:24 - Test: [ 0/20]	Time  0.380 ( 0.380)	Loss 9.9213e-01 (9.9213e-01)	Acc@1  69.92 ( 69.92)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:17:25 - Test: [10/20]	Time  0.107 ( 0.127)	Loss 8.4714e-01 (9.5769e-01)	Acc@1  77.73 ( 73.47)	Acc@5  92.58 ( 93.82)
03-Mar-22 09:17:26 - Epoch: [9][ 60/352]	Time  0.148 ( 0.147)	Data  0.002 ( 0.006)	Loss 8.6641e-01 (9.2358e-01)	Acc@1  76.56 ( 73.95)	Acc@5  96.09 ( 94.15)
03-Mar-22 09:17:26 -  * Acc@1 73.380 Acc@5 93.920
03-Mar-22 09:17:26 - Best acc at epoch 7: 74.04000091552734
03-Mar-22 09:17:27 - Epoch: [8][  0/352]	Time  0.385 ( 0.385)	Data  0.218 ( 0.218)	Loss 7.9514e-01 (7.9514e-01)	Acc@1  75.78 ( 75.78)	Acc@5  96.88 ( 96.88)
03-Mar-22 09:17:27 - Epoch: [9][ 70/352]	Time  0.132 ( 0.145)	Data  0.002 ( 0.005)	Loss 8.0040e-01 (9.1199e-01)	Acc@1  75.78 ( 74.24)	Acc@5  96.09 ( 94.29)
03-Mar-22 09:17:28 - Epoch: [8][ 10/352]	Time  0.155 ( 0.172)	Data  0.002 ( 0.022)	Loss 8.5178e-01 (8.4981e-01)	Acc@1  77.34 ( 77.70)	Acc@5  93.75 ( 95.24)
03-Mar-22 09:17:28 - Epoch: [9][ 80/352]	Time  0.162 ( 0.147)	Data  0.002 ( 0.005)	Loss 1.0043e+00 (9.0797e-01)	Acc@1  71.09 ( 74.26)	Acc@5  92.97 ( 94.40)
03-Mar-22 09:17:30 - Epoch: [8][ 20/352]	Time  0.150 ( 0.159)	Data  0.002 ( 0.012)	Loss 9.3686e-01 (8.8721e-01)	Acc@1  69.53 ( 75.07)	Acc@5  96.09 ( 94.87)
03-Mar-22 09:17:30 - Epoch: [9][ 90/352]	Time  0.152 ( 0.148)	Data  0.002 ( 0.005)	Loss 9.2124e-01 (9.0665e-01)	Acc@1  74.22 ( 74.48)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:17:31 - Epoch: [8][ 30/352]	Time  0.151 ( 0.157)	Data  0.003 ( 0.009)	Loss 9.2735e-01 (9.0020e-01)	Acc@1  75.00 ( 74.77)	Acc@5  92.97 ( 94.43)
03-Mar-22 09:17:32 - Epoch: [9][100/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.004)	Loss 7.9841e-01 (9.0235e-01)	Acc@1  78.91 ( 74.54)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:17:33 - Epoch: [8][ 40/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.008)	Loss 1.0580e+00 (8.9946e-01)	Acc@1  68.75 ( 74.85)	Acc@5  92.97 ( 94.26)
03-Mar-22 09:17:33 - Epoch: [9][110/352]	Time  0.154 ( 0.149)	Data  0.002 ( 0.004)	Loss 1.0197e+00 (9.0590e-01)	Acc@1  71.88 ( 74.51)	Acc@5  94.53 ( 94.47)
03-Mar-22 09:17:34 - Epoch: [8][ 50/352]	Time  0.155 ( 0.156)	Data  0.002 ( 0.007)	Loss 9.8837e-01 (9.0391e-01)	Acc@1  71.88 ( 74.66)	Acc@5  95.31 ( 94.39)
03-Mar-22 09:17:35 - Epoch: [9][120/352]	Time  0.134 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.5753e-01 (9.0600e-01)	Acc@1  71.88 ( 74.42)	Acc@5  97.66 ( 94.51)
03-Mar-22 09:17:36 - Epoch: [8][ 60/352]	Time  0.149 ( 0.155)	Data  0.002 ( 0.006)	Loss 7.3123e-01 (9.0202e-01)	Acc@1  78.91 ( 74.77)	Acc@5  97.66 ( 94.43)
03-Mar-22 09:17:36 - Epoch: [9][130/352]	Time  0.161 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0837e+00 (9.0350e-01)	Acc@1  67.97 ( 74.49)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:17:37 - Epoch: [8][ 70/352]	Time  0.149 ( 0.155)	Data  0.002 ( 0.005)	Loss 8.0217e-01 (9.0024e-01)	Acc@1  80.47 ( 74.71)	Acc@5  93.75 ( 94.43)
03-Mar-22 09:17:38 - Epoch: [9][140/352]	Time  0.156 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.7801e-01 (9.0297e-01)	Acc@1  75.00 ( 74.50)	Acc@5  94.53 ( 94.59)
03-Mar-22 09:17:39 - Epoch: [8][ 80/352]	Time  0.148 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.5595e-01 (8.9897e-01)	Acc@1  71.09 ( 74.67)	Acc@5  92.97 ( 94.41)
03-Mar-22 09:17:39 - Epoch: [9][150/352]	Time  0.165 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0125e+00 (9.0383e-01)	Acc@1  66.41 ( 74.42)	Acc@5  91.41 ( 94.59)
03-Mar-22 09:17:40 - Epoch: [8][ 90/352]	Time  0.147 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.1464e-01 (9.0242e-01)	Acc@1  77.34 ( 74.48)	Acc@5  95.31 ( 94.35)
03-Mar-22 09:17:41 - Epoch: [9][160/352]	Time  0.164 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.0876e-01 (9.0257e-01)	Acc@1  79.69 ( 74.45)	Acc@5  96.09 ( 94.58)
03-Mar-22 09:17:42 - Epoch: [8][100/352]	Time  0.150 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.1472e-01 (9.0308e-01)	Acc@1  74.22 ( 74.40)	Acc@5  95.31 ( 94.38)
03-Mar-22 09:17:43 - Epoch: [9][170/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1155e+00 (9.0616e-01)	Acc@1  65.62 ( 74.35)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:17:44 - Epoch: [8][110/352]	Time  0.158 ( 0.153)	Data  0.003 ( 0.004)	Loss 1.0887e+00 (9.0337e-01)	Acc@1  71.09 ( 74.44)	Acc@5  90.62 ( 94.40)
03-Mar-22 09:17:44 - Epoch: [9][180/352]	Time  0.134 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.6375e-01 (9.1156e-01)	Acc@1  73.44 ( 74.08)	Acc@5  95.31 ( 94.50)
03-Mar-22 09:17:45 - Epoch: [8][120/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.9916e-01 (9.0058e-01)	Acc@1  77.34 ( 74.57)	Acc@5  94.53 ( 94.43)
03-Mar-22 09:17:46 - Epoch: [9][190/352]	Time  0.149 ( 0.152)	Data  0.003 ( 0.003)	Loss 8.8607e-01 (9.1132e-01)	Acc@1  74.22 ( 74.15)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:17:46 - Epoch: [8][130/352]	Time  0.154 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.3322e-01 (9.0240e-01)	Acc@1  77.34 ( 74.47)	Acc@5  95.31 ( 94.41)
03-Mar-22 09:17:47 - Epoch: [9][200/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.5439e-01 (9.1351e-01)	Acc@1  78.12 ( 74.15)	Acc@5  95.31 ( 94.39)
03-Mar-22 09:17:48 - Epoch: [8][140/352]	Time  0.143 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.8821e-01 (9.0265e-01)	Acc@1  76.56 ( 74.49)	Acc@5  97.66 ( 94.50)
03-Mar-22 09:17:49 - Epoch: [9][210/352]	Time  0.146 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0060e+00 (9.1516e-01)	Acc@1  74.22 ( 74.08)	Acc@5  92.97 ( 94.38)
03-Mar-22 09:17:49 - Epoch: [8][150/352]	Time  0.137 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.3144e-01 (9.0064e-01)	Acc@1  74.22 ( 74.47)	Acc@5  94.53 ( 94.60)
03-Mar-22 09:17:50 - Epoch: [9][220/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.7958e-01 (9.1460e-01)	Acc@1  77.34 ( 74.08)	Acc@5  93.75 ( 94.37)
03-Mar-22 09:17:51 - Epoch: [8][160/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.2688e-01 (9.0164e-01)	Acc@1  73.44 ( 74.45)	Acc@5  96.09 ( 94.62)
03-Mar-22 09:17:51 - Epoch: [9][230/352]	Time  0.141 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.5385e-01 (9.1615e-01)	Acc@1  71.88 ( 74.03)	Acc@5  92.97 ( 94.35)
03-Mar-22 09:17:52 - Epoch: [8][170/352]	Time  0.152 ( 0.151)	Data  0.003 ( 0.004)	Loss 1.0540e+00 (9.0215e-01)	Acc@1  71.09 ( 74.47)	Acc@5  92.19 ( 94.56)
03-Mar-22 09:17:53 - Epoch: [9][240/352]	Time  0.131 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3520e-01 (9.1625e-01)	Acc@1  75.00 ( 74.02)	Acc@5  94.53 ( 94.33)
03-Mar-22 09:17:54 - Epoch: [8][180/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.0239e+00 (9.0077e-01)	Acc@1  67.19 ( 74.47)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:17:54 - Epoch: [9][250/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.8426e-01 (9.1568e-01)	Acc@1  78.12 ( 74.05)	Acc@5  95.31 ( 94.32)
03-Mar-22 09:17:55 - Epoch: [8][190/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.4311e-01 (9.0290e-01)	Acc@1  71.88 ( 74.43)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:17:56 - Epoch: [9][260/352]	Time  0.144 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.6517e-01 (9.1521e-01)	Acc@1  77.34 ( 74.09)	Acc@5  93.75 ( 94.33)
03-Mar-22 09:17:57 - Epoch: [8][200/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.9863e-01 (9.0286e-01)	Acc@1  81.25 ( 74.47)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:17:57 - Epoch: [9][270/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.8675e-01 (9.1577e-01)	Acc@1  75.00 ( 74.07)	Acc@5  92.97 ( 94.31)
03-Mar-22 09:17:58 - Epoch: [8][210/352]	Time  0.154 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.9855e-01 (9.0303e-01)	Acc@1  71.88 ( 74.49)	Acc@5  92.97 ( 94.52)
03-Mar-22 09:17:59 - Epoch: [9][280/352]	Time  0.152 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.9383e-01 (9.1576e-01)	Acc@1  78.12 ( 74.11)	Acc@5  95.31 ( 94.31)
03-Mar-22 09:18:00 - Epoch: [8][220/352]	Time  0.156 ( 0.151)	Data  0.003 ( 0.003)	Loss 8.8316e-01 (9.0118e-01)	Acc@1  76.56 ( 74.55)	Acc@5  94.53 ( 94.57)
03-Mar-22 09:18:00 - Epoch: [9][290/352]	Time  0.142 ( 0.150)	Data  0.002 ( 0.003)	Loss 6.9669e-01 (9.1264e-01)	Acc@1  80.47 ( 74.19)	Acc@5  94.53 ( 94.34)
03-Mar-22 09:18:01 - Epoch: [8][230/352]	Time  0.146 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.0842e-01 (9.0333e-01)	Acc@1  73.44 ( 74.45)	Acc@5  93.75 ( 94.52)
03-Mar-22 09:18:02 - Epoch: [9][300/352]	Time  0.150 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.4125e-01 (9.1203e-01)	Acc@1  82.03 ( 74.23)	Acc@5  94.53 ( 94.32)
03-Mar-22 09:18:03 - Epoch: [8][240/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.3760e-01 (9.0688e-01)	Acc@1  74.22 ( 74.38)	Acc@5  91.41 ( 94.45)
03-Mar-22 09:18:03 - Epoch: [9][310/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.6293e-01 (9.1097e-01)	Acc@1  77.34 ( 74.22)	Acc@5  92.97 ( 94.32)
03-Mar-22 09:18:04 - Epoch: [8][250/352]	Time  0.147 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.4947e-01 (9.0649e-01)	Acc@1  71.88 ( 74.43)	Acc@5  94.53 ( 94.45)
03-Mar-22 09:18:05 - Epoch: [9][320/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.9308e-01 (9.1158e-01)	Acc@1  72.66 ( 74.20)	Acc@5  92.97 ( 94.30)
03-Mar-22 09:18:06 - Epoch: [8][260/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.3599e-01 (9.0543e-01)	Acc@1  71.88 ( 74.45)	Acc@5  96.09 ( 94.47)
03-Mar-22 09:18:06 - Epoch: [9][330/352]	Time  0.148 ( 0.150)	Data  0.003 ( 0.003)	Loss 1.0837e+00 (9.1160e-01)	Acc@1  66.41 ( 74.20)	Acc@5  91.41 ( 94.30)
03-Mar-22 09:18:07 - Epoch: [8][270/352]	Time  0.150 ( 0.151)	Data  0.003 ( 0.003)	Loss 8.7250e-01 (9.0682e-01)	Acc@1  78.12 ( 74.40)	Acc@5  95.31 ( 94.47)
03-Mar-22 09:18:08 - Epoch: [9][340/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.7953e-01 (9.1017e-01)	Acc@1  71.88 ( 74.28)	Acc@5  92.97 ( 94.33)
03-Mar-22 09:18:09 - Epoch: [8][280/352]	Time  0.142 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.4824e-01 (9.0740e-01)	Acc@1  73.44 ( 74.36)	Acc@5  92.97 ( 94.46)
03-Mar-22 09:18:09 - Epoch: [9][350/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.7550e-01 (9.1051e-01)	Acc@1  77.34 ( 74.29)	Acc@5  95.31 ( 94.33)
03-Mar-22 09:18:10 - Test: [ 0/20]	Time  0.361 ( 0.361)	Loss 1.0141e+00 (1.0141e+00)	Acc@1  70.70 ( 70.70)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:18:10 - Epoch: [8][290/352]	Time  0.160 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.6805e-01 (9.0790e-01)	Acc@1  78.12 ( 74.38)	Acc@5  94.53 ( 94.43)
03-Mar-22 09:18:11 - Test: [10/20]	Time  0.094 ( 0.122)	Loss 8.0585e-01 (9.2974e-01)	Acc@1  77.34 ( 73.93)	Acc@5  97.27 ( 93.79)
03-Mar-22 09:18:12 -  * Acc@1 73.440 Acc@5 93.840
03-Mar-22 09:18:12 - Best acc at epoch 9: 73.97999572753906
03-Mar-22 09:18:12 - Epoch: [8][300/352]	Time  0.112 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.4593e-01 (9.0921e-01)	Acc@1  75.78 ( 74.33)	Acc@5  96.88 ( 94.43)
03-Mar-22 09:18:12 - Epoch: [10][  0/352]	Time  0.375 ( 0.375)	Data  0.228 ( 0.228)	Loss 1.0807e+00 (1.0807e+00)	Acc@1  67.97 ( 67.97)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:18:13 - Epoch: [8][310/352]	Time  0.120 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0098e+00 (9.0995e-01)	Acc@1  75.78 ( 74.33)	Acc@5  93.75 ( 94.43)
03-Mar-22 09:18:14 - Epoch: [10][ 10/352]	Time  0.155 ( 0.171)	Data  0.002 ( 0.023)	Loss 8.4546e-01 (8.9209e-01)	Acc@1  76.56 ( 74.15)	Acc@5  96.09 ( 95.03)
03-Mar-22 09:18:14 - Epoch: [8][320/352]	Time  0.128 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0568e+00 (9.1082e-01)	Acc@1  69.53 ( 74.30)	Acc@5  95.31 ( 94.41)
03-Mar-22 09:18:15 - Epoch: [10][ 20/352]	Time  0.155 ( 0.162)	Data  0.002 ( 0.013)	Loss 9.2348e-01 (8.6505e-01)	Acc@1  72.66 ( 74.93)	Acc@5  94.53 ( 95.24)
03-Mar-22 09:18:16 - Epoch: [8][330/352]	Time  0.150 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.3329e-01 (9.1109e-01)	Acc@1  73.44 ( 74.29)	Acc@5  91.41 ( 94.38)
03-Mar-22 09:18:17 - Epoch: [10][ 30/352]	Time  0.133 ( 0.157)	Data  0.002 ( 0.010)	Loss 1.1159e+00 (8.9407e-01)	Acc@1  68.75 ( 74.12)	Acc@5  91.41 ( 94.61)
03-Mar-22 09:18:17 - Epoch: [8][340/352]	Time  0.145 ( 0.148)	Data  0.002 ( 0.003)	Loss 7.9963e-01 (9.0962e-01)	Acc@1  79.69 ( 74.35)	Acc@5  94.53 ( 94.37)
03-Mar-22 09:18:18 - Epoch: [10][ 40/352]	Time  0.155 ( 0.155)	Data  0.002 ( 0.008)	Loss 9.8772e-01 (9.0057e-01)	Acc@1  74.22 ( 73.80)	Acc@5  92.97 ( 94.40)
03-Mar-22 09:18:19 - Epoch: [8][350/352]	Time  0.155 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.0788e+00 (9.0878e-01)	Acc@1  74.22 ( 74.40)	Acc@5  89.06 ( 94.38)
03-Mar-22 09:18:19 - Test: [ 0/20]	Time  0.356 ( 0.356)	Loss 9.6605e-01 (9.6605e-01)	Acc@1  72.27 ( 72.27)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:18:20 - Epoch: [10][ 50/352]	Time  0.186 ( 0.154)	Data  0.003 ( 0.007)	Loss 8.4382e-01 (8.9662e-01)	Acc@1  76.56 ( 74.17)	Acc@5  93.75 ( 94.41)
03-Mar-22 09:18:20 - Test: [10/20]	Time  0.089 ( 0.120)	Loss 8.2554e-01 (9.3767e-01)	Acc@1  76.17 ( 73.40)	Acc@5  96.09 ( 93.79)
03-Mar-22 09:18:21 -  * Acc@1 73.420 Acc@5 93.940
03-Mar-22 09:18:21 - Best acc at epoch 8: 74.04000091552734
03-Mar-22 09:18:21 - Epoch: [10][ 60/352]	Time  0.131 ( 0.156)	Data  0.002 ( 0.006)	Loss 1.0083e+00 (8.9396e-01)	Acc@1  70.31 ( 74.17)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:18:21 - Epoch: [9][  0/352]	Time  0.389 ( 0.389)	Data  0.221 ( 0.221)	Loss 9.3042e-01 (9.3042e-01)	Acc@1  71.09 ( 71.09)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:18:22 - Epoch: [10][ 70/352]	Time  0.125 ( 0.151)	Data  0.002 ( 0.005)	Loss 8.1142e-01 (8.9591e-01)	Acc@1  75.00 ( 74.22)	Acc@5  95.31 ( 94.54)
03-Mar-22 09:18:23 - Epoch: [9][ 10/352]	Time  0.156 ( 0.175)	Data  0.002 ( 0.023)	Loss 9.9385e-01 (9.3616e-01)	Acc@1  73.44 ( 72.59)	Acc@5  95.31 ( 93.96)
03-Mar-22 09:18:24 - Epoch: [10][ 80/352]	Time  0.129 ( 0.148)	Data  0.002 ( 0.005)	Loss 9.3920e-01 (8.9477e-01)	Acc@1  79.69 ( 74.44)	Acc@5  92.19 ( 94.48)
03-Mar-22 09:18:25 - Epoch: [9][ 20/352]	Time  0.155 ( 0.164)	Data  0.002 ( 0.013)	Loss 8.3270e-01 (9.2507e-01)	Acc@1  75.78 ( 72.99)	Acc@5  96.09 ( 94.12)
03-Mar-22 09:18:25 - Epoch: [10][ 90/352]	Time  0.124 ( 0.145)	Data  0.002 ( 0.005)	Loss 8.5309e-01 (8.9737e-01)	Acc@1  76.56 ( 74.30)	Acc@5  96.09 ( 94.45)
03-Mar-22 09:18:26 - Epoch: [9][ 30/352]	Time  0.156 ( 0.157)	Data  0.002 ( 0.009)	Loss 9.7847e-01 (9.2300e-01)	Acc@1  72.66 ( 73.26)	Acc@5  93.75 ( 94.10)
03-Mar-22 09:18:26 - Epoch: [10][100/352]	Time  0.128 ( 0.144)	Data  0.002 ( 0.004)	Loss 7.9782e-01 (8.9762e-01)	Acc@1  75.00 ( 74.29)	Acc@5  96.09 ( 94.41)
03-Mar-22 09:18:27 - Epoch: [10][110/352]	Time  0.122 ( 0.142)	Data  0.002 ( 0.004)	Loss 6.3581e-01 (8.8966e-01)	Acc@1  84.38 ( 74.64)	Acc@5  99.22 ( 94.52)
03-Mar-22 09:18:27 - Epoch: [9][ 40/352]	Time  0.159 ( 0.156)	Data  0.002 ( 0.008)	Loss 9.6613e-01 (9.1557e-01)	Acc@1  73.44 ( 73.55)	Acc@5  93.75 ( 94.26)
03-Mar-22 09:18:29 - Epoch: [10][120/352]	Time  0.126 ( 0.140)	Data  0.002 ( 0.004)	Loss 1.1580e+00 (8.9342e-01)	Acc@1  65.62 ( 74.54)	Acc@5  90.62 ( 94.47)
03-Mar-22 09:18:29 - Epoch: [9][ 50/352]	Time  0.156 ( 0.156)	Data  0.003 ( 0.007)	Loss 1.0235e+00 (9.0660e-01)	Acc@1  71.88 ( 74.07)	Acc@5  96.09 ( 94.50)
03-Mar-22 09:18:30 - Epoch: [10][130/352]	Time  0.126 ( 0.139)	Data  0.002 ( 0.004)	Loss 9.1386e-01 (8.9974e-01)	Acc@1  72.66 ( 74.41)	Acc@5  94.53 ( 94.44)
03-Mar-22 09:18:31 - Epoch: [9][ 60/352]	Time  0.158 ( 0.156)	Data  0.002 ( 0.006)	Loss 8.8992e-01 (9.1346e-01)	Acc@1  78.91 ( 74.10)	Acc@5  93.75 ( 94.42)
03-Mar-22 09:18:31 - Epoch: [10][140/352]	Time  0.128 ( 0.138)	Data  0.002 ( 0.004)	Loss 1.1512e+00 (9.0055e-01)	Acc@1  67.19 ( 74.40)	Acc@5  89.84 ( 94.44)
03-Mar-22 09:18:32 - Epoch: [9][ 70/352]	Time  0.157 ( 0.155)	Data  0.002 ( 0.005)	Loss 1.1725e+00 (9.2333e-01)	Acc@1  69.53 ( 74.05)	Acc@5  94.53 ( 94.29)
03-Mar-22 09:18:32 - Epoch: [10][150/352]	Time  0.118 ( 0.137)	Data  0.002 ( 0.003)	Loss 9.0738e-01 (8.9618e-01)	Acc@1  75.78 ( 74.58)	Acc@5  92.19 ( 94.46)
03-Mar-22 09:18:34 - Epoch: [10][160/352]	Time  0.118 ( 0.136)	Data  0.002 ( 0.003)	Loss 8.8973e-01 (8.9718e-01)	Acc@1  71.88 ( 74.54)	Acc@5  96.88 ( 94.42)
03-Mar-22 09:18:34 - Epoch: [9][ 80/352]	Time  0.151 ( 0.155)	Data  0.002 ( 0.005)	Loss 8.5306e-01 (9.2456e-01)	Acc@1  77.34 ( 74.07)	Acc@5  94.53 ( 94.24)
03-Mar-22 09:18:35 - Epoch: [10][170/352]	Time  0.121 ( 0.135)	Data  0.002 ( 0.003)	Loss 8.6629e-01 (8.9955e-01)	Acc@1  73.44 ( 74.42)	Acc@5  92.97 ( 94.38)
03-Mar-22 09:18:35 - Epoch: [9][ 90/352]	Time  0.155 ( 0.155)	Data  0.002 ( 0.005)	Loss 8.9941e-01 (9.2443e-01)	Acc@1  76.56 ( 74.06)	Acc@5  96.09 ( 94.31)
03-Mar-22 09:18:36 - Epoch: [10][180/352]	Time  0.121 ( 0.135)	Data  0.002 ( 0.003)	Loss 1.0031e+00 (8.9708e-01)	Acc@1  75.78 ( 74.46)	Acc@5  95.31 ( 94.41)
03-Mar-22 09:18:37 - Epoch: [9][100/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.6930e-01 (9.2458e-01)	Acc@1  67.97 ( 74.02)	Acc@5  92.19 ( 94.31)
03-Mar-22 09:18:38 - Epoch: [10][190/352]	Time  0.150 ( 0.135)	Data  0.002 ( 0.003)	Loss 7.9938e-01 (8.9747e-01)	Acc@1  77.34 ( 74.46)	Acc@5  94.53 ( 94.41)
03-Mar-22 09:18:38 - Epoch: [9][110/352]	Time  0.155 ( 0.154)	Data  0.003 ( 0.004)	Loss 9.5380e-01 (9.2554e-01)	Acc@1  67.97 ( 73.99)	Acc@5  95.31 ( 94.26)
03-Mar-22 09:18:39 - Epoch: [10][200/352]	Time  0.153 ( 0.136)	Data  0.002 ( 0.003)	Loss 8.7034e-01 (8.9595e-01)	Acc@1  73.44 ( 74.53)	Acc@5  96.88 ( 94.47)
03-Mar-22 09:18:40 - Epoch: [9][120/352]	Time  0.147 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.1575e-01 (9.2816e-01)	Acc@1  76.56 ( 73.81)	Acc@5  95.31 ( 94.25)
03-Mar-22 09:18:41 - Epoch: [10][210/352]	Time  0.136 ( 0.136)	Data  0.002 ( 0.003)	Loss 8.0351e-01 (8.9685e-01)	Acc@1  72.66 ( 74.43)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:18:41 - Epoch: [9][130/352]	Time  0.132 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0128e+00 (9.2653e-01)	Acc@1  71.88 ( 73.84)	Acc@5  94.53 ( 94.22)
03-Mar-22 09:18:42 - Epoch: [10][220/352]	Time  0.150 ( 0.137)	Data  0.002 ( 0.003)	Loss 1.0823e+00 (8.9877e-01)	Acc@1  73.44 ( 74.43)	Acc@5  90.62 ( 94.40)
03-Mar-22 09:18:43 - Epoch: [9][140/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.2894e-01 (9.2530e-01)	Acc@1  78.12 ( 73.91)	Acc@5  96.09 ( 94.28)
03-Mar-22 09:18:43 - Epoch: [10][230/352]	Time  0.143 ( 0.137)	Data  0.002 ( 0.003)	Loss 7.2496e-01 (8.9765e-01)	Acc@1  80.47 ( 74.53)	Acc@5  96.09 ( 94.40)
03-Mar-22 09:18:44 - Epoch: [9][150/352]	Time  0.150 ( 0.152)	Data  0.003 ( 0.004)	Loss 1.0084e+00 (9.2466e-01)	Acc@1  76.56 ( 73.93)	Acc@5  91.41 ( 94.28)
03-Mar-22 09:18:45 - Epoch: [10][240/352]	Time  0.154 ( 0.138)	Data  0.002 ( 0.003)	Loss 8.5213e-01 (9.0209e-01)	Acc@1  75.78 ( 74.39)	Acc@5  96.09 ( 94.39)
03-Mar-22 09:18:46 - Epoch: [9][160/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.6604e-01 (9.2292e-01)	Acc@1  78.12 ( 74.06)	Acc@5  99.22 ( 94.28)
03-Mar-22 09:18:46 - Epoch: [10][250/352]	Time  0.152 ( 0.138)	Data  0.002 ( 0.003)	Loss 9.7267e-01 (9.0159e-01)	Acc@1  72.66 ( 74.39)	Acc@5  93.75 ( 94.42)
03-Mar-22 09:18:47 - Epoch: [9][170/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.8526e-01 (9.2664e-01)	Acc@1  75.00 ( 73.94)	Acc@5  97.66 ( 94.25)
03-Mar-22 09:18:48 - Epoch: [10][260/352]	Time  0.154 ( 0.139)	Data  0.002 ( 0.003)	Loss 9.1750e-01 (9.0188e-01)	Acc@1  77.34 ( 74.40)	Acc@5  91.41 ( 94.40)
03-Mar-22 09:18:49 - Epoch: [9][180/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0404e+00 (9.2619e-01)	Acc@1  66.41 ( 73.88)	Acc@5  93.75 ( 94.25)
03-Mar-22 09:18:49 - Epoch: [10][270/352]	Time  0.155 ( 0.139)	Data  0.002 ( 0.003)	Loss 9.4760e-01 (9.0143e-01)	Acc@1  71.09 ( 74.37)	Acc@5  92.97 ( 94.42)
03-Mar-22 09:18:50 - Epoch: [9][190/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0018e+00 (9.2738e-01)	Acc@1  68.75 ( 73.83)	Acc@5  90.62 ( 94.20)
03-Mar-22 09:18:51 - Epoch: [10][280/352]	Time  0.147 ( 0.140)	Data  0.002 ( 0.003)	Loss 9.6830e-01 (9.0302e-01)	Acc@1  70.31 ( 74.31)	Acc@5  95.31 ( 94.42)
03-Mar-22 09:18:52 - Epoch: [9][200/352]	Time  0.136 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0382e+00 (9.2804e-01)	Acc@1  73.44 ( 73.79)	Acc@5  92.19 ( 94.17)
03-Mar-22 09:18:52 - Epoch: [10][290/352]	Time  0.152 ( 0.140)	Data  0.002 ( 0.003)	Loss 7.1983e-01 (9.0188e-01)	Acc@1  78.12 ( 74.33)	Acc@5  98.44 ( 94.45)
03-Mar-22 09:18:53 - Epoch: [9][210/352]	Time  0.130 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.1704e+00 (9.2906e-01)	Acc@1  67.19 ( 73.78)	Acc@5  89.84 ( 94.11)
03-Mar-22 09:18:54 - Epoch: [10][300/352]	Time  0.134 ( 0.140)	Data  0.002 ( 0.003)	Loss 9.8303e-01 (9.0354e-01)	Acc@1  68.75 ( 74.29)	Acc@5  93.75 ( 94.43)
03-Mar-22 09:18:54 - Epoch: [9][220/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.5936e-01 (9.2544e-01)	Acc@1  71.09 ( 73.87)	Acc@5  96.09 ( 94.15)
03-Mar-22 09:18:55 - Epoch: [10][310/352]	Time  0.155 ( 0.140)	Data  0.002 ( 0.003)	Loss 8.5672e-01 (9.0219e-01)	Acc@1  75.00 ( 74.31)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:18:56 - Epoch: [9][230/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.0795e-01 (9.2435e-01)	Acc@1  78.12 ( 73.94)	Acc@5  96.09 ( 94.17)
03-Mar-22 09:18:57 - Epoch: [10][320/352]	Time  0.145 ( 0.140)	Data  0.002 ( 0.003)	Loss 7.2375e-01 (9.0251e-01)	Acc@1  77.34 ( 74.33)	Acc@5  96.88 ( 94.45)
03-Mar-22 09:18:57 - Epoch: [9][240/352]	Time  0.152 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.4505e-01 (9.2263e-01)	Acc@1  71.88 ( 73.98)	Acc@5  93.75 ( 94.20)
03-Mar-22 09:18:58 - Epoch: [10][330/352]	Time  0.149 ( 0.140)	Data  0.002 ( 0.003)	Loss 8.5258e-01 (9.0176e-01)	Acc@1  77.34 ( 74.35)	Acc@5  92.97 ( 94.43)
03-Mar-22 09:18:59 - Epoch: [9][250/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.0855e-01 (9.1972e-01)	Acc@1  81.25 ( 74.07)	Acc@5  93.75 ( 94.20)
03-Mar-22 09:19:00 - Epoch: [10][340/352]	Time  0.152 ( 0.141)	Data  0.002 ( 0.003)	Loss 9.7033e-01 (9.0192e-01)	Acc@1  72.66 ( 74.36)	Acc@5  94.53 ( 94.44)
03-Mar-22 09:19:00 - Epoch: [9][260/352]	Time  0.156 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.7751e-01 (9.1814e-01)	Acc@1  74.22 ( 74.09)	Acc@5  93.75 ( 94.23)
03-Mar-22 09:19:01 - Epoch: [10][350/352]	Time  0.153 ( 0.141)	Data  0.002 ( 0.003)	Loss 9.7728e-01 (9.0137e-01)	Acc@1  72.66 ( 74.39)	Acc@5  96.88 ( 94.48)
03-Mar-22 09:19:02 - Epoch: [9][270/352]	Time  0.108 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.7549e-01 (9.1684e-01)	Acc@1  75.78 ( 74.12)	Acc@5  97.66 ( 94.26)
03-Mar-22 09:19:02 - Test: [ 0/20]	Time  0.365 ( 0.365)	Loss 1.0782e+00 (1.0782e+00)	Acc@1  67.58 ( 67.58)	Acc@5  91.41 ( 91.41)
03-Mar-22 09:19:03 - Test: [10/20]	Time  0.107 ( 0.123)	Loss 8.4485e-01 (9.3410e-01)	Acc@1  73.44 ( 73.05)	Acc@5  94.53 ( 94.03)
03-Mar-22 09:19:03 - Epoch: [9][280/352]	Time  0.155 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.6464e-01 (9.1647e-01)	Acc@1  78.12 ( 74.15)	Acc@5  96.88 ( 94.26)
03-Mar-22 09:19:04 -  * Acc@1 73.640 Acc@5 94.220
03-Mar-22 09:19:04 - Best acc at epoch 10: 73.97999572753906
03-Mar-22 09:19:04 - Epoch: [11][  0/352]	Time  0.389 ( 0.389)	Data  0.257 ( 0.257)	Loss 9.9409e-01 (9.9409e-01)	Acc@1  70.31 ( 70.31)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:19:05 - Epoch: [9][290/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.0416e-01 (9.1429e-01)	Acc@1  82.03 ( 74.21)	Acc@5  96.88 ( 94.27)
03-Mar-22 09:19:06 - Epoch: [11][ 10/352]	Time  0.127 ( 0.164)	Data  0.002 ( 0.025)	Loss 8.7583e-01 (9.0003e-01)	Acc@1  78.12 ( 74.43)	Acc@5  93.75 ( 93.68)
03-Mar-22 09:19:06 - Epoch: [9][300/352]	Time  0.134 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1357e+00 (9.1402e-01)	Acc@1  66.41 ( 74.23)	Acc@5  92.97 ( 94.25)
03-Mar-22 09:19:07 - Epoch: [11][ 20/352]	Time  0.175 ( 0.168)	Data  0.002 ( 0.014)	Loss 8.4828e-01 (8.8765e-01)	Acc@1  78.12 ( 74.96)	Acc@5  96.09 ( 94.08)
03-Mar-22 09:19:07 - Epoch: [9][310/352]	Time  0.156 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0603e+00 (9.1439e-01)	Acc@1  71.88 ( 74.21)	Acc@5  92.97 ( 94.24)
03-Mar-22 09:19:09 - Epoch: [9][320/352]	Time  0.137 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.6211e-01 (9.1171e-01)	Acc@1  79.69 ( 74.25)	Acc@5  96.09 ( 94.28)
03-Mar-22 09:19:09 - Epoch: [11][ 30/352]	Time  0.174 ( 0.169)	Data  0.002 ( 0.010)	Loss 7.5157e-01 (8.9484e-01)	Acc@1  76.56 ( 74.55)	Acc@5  95.31 ( 94.13)
03-Mar-22 09:19:10 - Epoch: [9][330/352]	Time  0.150 ( 0.149)	Data  0.003 ( 0.003)	Loss 9.0582e-01 (9.1005e-01)	Acc@1  73.44 ( 74.27)	Acc@5  94.53 ( 94.32)
03-Mar-22 09:19:11 - Epoch: [11][ 40/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.008)	Loss 8.1962e-01 (8.9967e-01)	Acc@1  75.78 ( 74.24)	Acc@5  95.31 ( 94.32)
03-Mar-22 09:19:12 - Epoch: [9][340/352]	Time  0.156 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0312e+00 (9.0969e-01)	Acc@1  71.88 ( 74.24)	Acc@5  91.41 ( 94.33)
03-Mar-22 09:19:12 - Epoch: [11][ 50/352]	Time  0.175 ( 0.169)	Data  0.002 ( 0.007)	Loss 8.9016e-01 (8.9423e-01)	Acc@1  77.34 ( 74.46)	Acc@5  92.97 ( 94.27)
03-Mar-22 09:19:13 - Epoch: [9][350/352]	Time  0.145 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.8558e-01 (9.0965e-01)	Acc@1  71.88 ( 74.26)	Acc@5  93.75 ( 94.34)
03-Mar-22 09:19:14 - Epoch: [11][ 60/352]	Time  0.101 ( 0.167)	Data  0.002 ( 0.006)	Loss 8.8277e-01 (8.8753e-01)	Acc@1  71.88 ( 74.54)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:19:14 - Test: [ 0/20]	Time  0.366 ( 0.366)	Loss 1.0390e+00 (1.0390e+00)	Acc@1  68.75 ( 68.75)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:19:15 - Test: [10/20]	Time  0.094 ( 0.124)	Loss 8.2269e-01 (9.6485e-01)	Acc@1  75.78 ( 73.01)	Acc@5  94.14 ( 93.39)
03-Mar-22 09:19:16 - Epoch: [11][ 70/352]	Time  0.142 ( 0.165)	Data  0.002 ( 0.006)	Loss 1.0492e+00 (8.9041e-01)	Acc@1  73.44 ( 74.33)	Acc@5  92.19 ( 94.54)
03-Mar-22 09:19:16 -  * Acc@1 72.460 Acc@5 93.700
03-Mar-22 09:19:16 - Best acc at epoch 9: 74.04000091552734
03-Mar-22 09:19:17 - Epoch: [10][  0/352]	Time  0.347 ( 0.347)	Data  0.218 ( 0.218)	Loss 7.5992e-01 (7.5992e-01)	Acc@1  78.91 ( 78.91)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:19:17 - Epoch: [11][ 80/352]	Time  0.141 ( 0.161)	Data  0.002 ( 0.005)	Loss 9.2002e-01 (8.8842e-01)	Acc@1  73.44 ( 74.36)	Acc@5  93.75 ( 94.51)
03-Mar-22 09:19:18 - Epoch: [10][ 10/352]	Time  0.151 ( 0.163)	Data  0.002 ( 0.023)	Loss 8.9890e-01 (8.4083e-01)	Acc@1  69.53 ( 75.99)	Acc@5  94.53 ( 94.60)
03-Mar-22 09:19:18 - Epoch: [11][ 90/352]	Time  0.154 ( 0.159)	Data  0.002 ( 0.005)	Loss 9.1407e-01 (8.8657e-01)	Acc@1  78.91 ( 74.54)	Acc@5  90.62 ( 94.46)
03-Mar-22 09:19:20 - Epoch: [10][ 20/352]	Time  0.146 ( 0.159)	Data  0.002 ( 0.013)	Loss 9.0409e-01 (8.6172e-01)	Acc@1  75.00 ( 75.74)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:19:20 - Epoch: [11][100/352]	Time  0.148 ( 0.158)	Data  0.002 ( 0.005)	Loss 8.3074e-01 (8.8453e-01)	Acc@1  76.56 ( 74.63)	Acc@5  92.97 ( 94.52)
03-Mar-22 09:19:21 - Epoch: [10][ 30/352]	Time  0.137 ( 0.156)	Data  0.001 ( 0.009)	Loss 1.0043e+00 (8.8398e-01)	Acc@1  67.97 ( 75.13)	Acc@5  96.88 ( 94.81)
03-Mar-22 09:19:21 - Epoch: [11][110/352]	Time  0.145 ( 0.157)	Data  0.002 ( 0.004)	Loss 7.6199e-01 (8.8523e-01)	Acc@1  78.91 ( 74.66)	Acc@5  96.88 ( 94.51)
03-Mar-22 09:19:23 - Epoch: [10][ 40/352]	Time  0.153 ( 0.156)	Data  0.003 ( 0.008)	Loss 7.4039e-01 (8.9510e-01)	Acc@1  78.12 ( 74.89)	Acc@5  96.09 ( 94.59)
03-Mar-22 09:19:23 - Epoch: [11][120/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.0166e+00 (8.8751e-01)	Acc@1  68.75 ( 74.61)	Acc@5  92.97 ( 94.49)
03-Mar-22 09:19:24 - Epoch: [10][ 50/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.007)	Loss 8.9051e-01 (8.9237e-01)	Acc@1  75.00 ( 74.91)	Acc@5  95.31 ( 94.61)
03-Mar-22 09:19:24 - Epoch: [11][130/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.004)	Loss 9.0574e-01 (8.8791e-01)	Acc@1  75.00 ( 74.62)	Acc@5  92.97 ( 94.50)
03-Mar-22 09:19:26 - Epoch: [10][ 60/352]	Time  0.155 ( 0.154)	Data  0.003 ( 0.006)	Loss 7.6705e-01 (8.8333e-01)	Acc@1  81.25 ( 75.12)	Acc@5  95.31 ( 94.80)
03-Mar-22 09:19:26 - Epoch: [11][140/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.004)	Loss 8.4786e-01 (8.8621e-01)	Acc@1  76.56 ( 74.75)	Acc@5  94.53 ( 94.50)
03-Mar-22 09:19:27 - Epoch: [10][ 70/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.006)	Loss 7.7875e-01 (8.8564e-01)	Acc@1  76.56 ( 74.91)	Acc@5  98.44 ( 94.78)
03-Mar-22 09:19:27 - Epoch: [11][150/352]	Time  0.137 ( 0.155)	Data  0.002 ( 0.004)	Loss 8.4088e-01 (8.8317e-01)	Acc@1  75.78 ( 74.78)	Acc@5  95.31 ( 94.54)
03-Mar-22 09:19:29 - Epoch: [10][ 80/352]	Time  0.139 ( 0.153)	Data  0.002 ( 0.005)	Loss 7.7385e-01 (8.8671e-01)	Acc@1  82.03 ( 74.99)	Acc@5  94.53 ( 94.79)
03-Mar-22 09:19:29 - Epoch: [11][160/352]	Time  0.150 ( 0.155)	Data  0.002 ( 0.004)	Loss 8.7183e-01 (8.8560e-01)	Acc@1  71.88 ( 74.72)	Acc@5  97.66 ( 94.54)
03-Mar-22 09:19:30 - Epoch: [11][170/352]	Time  0.152 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.2016e-01 (8.8641e-01)	Acc@1  77.34 ( 74.73)	Acc@5  96.88 ( 94.56)
03-Mar-22 09:19:30 - Epoch: [10][ 90/352]	Time  0.136 ( 0.153)	Data  0.002 ( 0.005)	Loss 1.1963e+00 (8.8848e-01)	Acc@1  68.75 ( 74.98)	Acc@5  89.84 ( 94.76)
03-Mar-22 09:19:32 - Epoch: [11][180/352]	Time  0.151 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.9187e-01 (8.8718e-01)	Acc@1  72.66 ( 74.74)	Acc@5  96.09 ( 94.60)
03-Mar-22 09:19:32 - Epoch: [10][100/352]	Time  0.151 ( 0.153)	Data  0.003 ( 0.005)	Loss 9.5711e-01 (8.9028e-01)	Acc@1  72.66 ( 74.92)	Acc@5  94.53 ( 94.72)
03-Mar-22 09:19:33 - Epoch: [11][190/352]	Time  0.141 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0813e+00 (8.8821e-01)	Acc@1  68.75 ( 74.70)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:19:33 - Epoch: [10][110/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.1434e-01 (8.8726e-01)	Acc@1  82.03 ( 74.99)	Acc@5  95.31 ( 94.77)
03-Mar-22 09:19:35 - Epoch: [11][200/352]	Time  0.149 ( 0.153)	Data  0.003 ( 0.003)	Loss 1.2133e+00 (8.9154e-01)	Acc@1  65.62 ( 74.61)	Acc@5  92.97 ( 94.52)
03-Mar-22 09:19:35 - Epoch: [10][120/352]	Time  0.158 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.4208e-01 (8.8976e-01)	Acc@1  72.66 ( 74.93)	Acc@5  93.75 ( 94.75)
03-Mar-22 09:19:36 - Epoch: [11][210/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0244e+00 (8.9199e-01)	Acc@1  71.88 ( 74.60)	Acc@5  93.75 ( 94.53)
03-Mar-22 09:19:36 - Epoch: [10][130/352]	Time  0.147 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.4258e-01 (8.8946e-01)	Acc@1  70.31 ( 74.97)	Acc@5  94.53 ( 94.76)
03-Mar-22 09:19:38 - Epoch: [11][220/352]	Time  0.149 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.5576e-01 (8.9366e-01)	Acc@1  71.09 ( 74.52)	Acc@5  94.53 ( 94.49)
03-Mar-22 09:19:38 - Epoch: [10][140/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.5425e-01 (8.9404e-01)	Acc@1  68.75 ( 74.82)	Acc@5  95.31 ( 94.67)
03-Mar-22 09:19:39 - Epoch: [11][230/352]	Time  0.155 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.7986e-01 (8.9382e-01)	Acc@1  73.44 ( 74.47)	Acc@5  92.97 ( 94.46)
03-Mar-22 09:19:39 - Epoch: [10][150/352]	Time  0.156 ( 0.153)	Data  0.003 ( 0.004)	Loss 1.0385e+00 (8.9802e-01)	Acc@1  70.31 ( 74.74)	Acc@5  95.31 ( 94.66)
03-Mar-22 09:19:41 - Epoch: [11][240/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.6135e-01 (8.9506e-01)	Acc@1  75.00 ( 74.42)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:19:41 - Epoch: [10][160/352]	Time  0.153 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.9758e-01 (9.0253e-01)	Acc@1  70.31 ( 74.55)	Acc@5  91.41 ( 94.58)
03-Mar-22 09:19:42 - Epoch: [11][250/352]	Time  0.154 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0527e+00 (8.9796e-01)	Acc@1  69.53 ( 74.27)	Acc@5  93.75 ( 94.44)
03-Mar-22 09:19:42 - Epoch: [10][170/352]	Time  0.151 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0308e+00 (9.0424e-01)	Acc@1  68.75 ( 74.52)	Acc@5  92.97 ( 94.58)
03-Mar-22 09:19:44 - Epoch: [11][260/352]	Time  0.152 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.2103e-01 (8.9915e-01)	Acc@1  76.56 ( 74.26)	Acc@5  92.19 ( 94.44)
03-Mar-22 09:19:44 - Epoch: [10][180/352]	Time  0.146 ( 0.153)	Data  0.003 ( 0.004)	Loss 1.0714e+00 (9.0609e-01)	Acc@1  71.09 ( 74.34)	Acc@5  92.19 ( 94.56)
03-Mar-22 09:19:45 - Epoch: [11][270/352]	Time  0.135 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.8694e-01 (8.9996e-01)	Acc@1  75.00 ( 74.26)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:19:45 - Epoch: [10][190/352]	Time  0.155 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.7345e-01 (9.0335e-01)	Acc@1  67.97 ( 74.37)	Acc@5  94.53 ( 94.61)
03-Mar-22 09:19:47 - Epoch: [11][280/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.4668e-01 (9.0020e-01)	Acc@1  71.09 ( 74.21)	Acc@5  89.06 ( 94.44)
03-Mar-22 09:19:47 - Epoch: [10][200/352]	Time  0.132 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0464e+00 (9.0532e-01)	Acc@1  67.19 ( 74.30)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:19:48 - Epoch: [11][290/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.6674e-01 (8.9944e-01)	Acc@1  75.00 ( 74.23)	Acc@5  93.75 ( 94.46)
03-Mar-22 09:19:48 - Epoch: [10][210/352]	Time  0.156 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.1762e-01 (9.0552e-01)	Acc@1  78.91 ( 74.34)	Acc@5  96.88 ( 94.55)
03-Mar-22 09:19:50 - Epoch: [11][300/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.003)	Loss 1.0135e+00 (8.9895e-01)	Acc@1  76.56 ( 74.31)	Acc@5  89.06 ( 94.46)
03-Mar-22 09:19:50 - Epoch: [10][220/352]	Time  0.157 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.7270e-01 (9.0515e-01)	Acc@1  68.75 ( 74.34)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:19:51 - Epoch: [11][310/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.4600e-01 (8.9841e-01)	Acc@1  79.69 ( 74.35)	Acc@5  95.31 ( 94.47)
03-Mar-22 09:19:51 - Epoch: [10][230/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.3381e-01 (9.0151e-01)	Acc@1  75.78 ( 74.43)	Acc@5  96.88 ( 94.62)
03-Mar-22 09:19:53 - Epoch: [11][320/352]	Time  0.169 ( 0.153)	Data  0.003 ( 0.003)	Loss 1.0894e+00 (9.0015e-01)	Acc@1  68.75 ( 74.29)	Acc@5  91.41 ( 94.44)
03-Mar-22 09:19:53 - Epoch: [10][240/352]	Time  0.151 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.6282e-01 (9.0309e-01)	Acc@1  71.88 ( 74.39)	Acc@5  92.19 ( 94.56)
03-Mar-22 09:19:54 - Epoch: [10][250/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.1786e-01 (9.0512e-01)	Acc@1  74.22 ( 74.27)	Acc@5  96.88 ( 94.57)
03-Mar-22 09:19:55 - Epoch: [11][330/352]	Time  0.175 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.2412e-01 (9.0099e-01)	Acc@1  74.22 ( 74.29)	Acc@5  94.53 ( 94.39)
03-Mar-22 09:19:56 - Epoch: [10][260/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.9492e-01 (9.0423e-01)	Acc@1  73.44 ( 74.30)	Acc@5  96.09 ( 94.57)
03-Mar-22 09:19:56 - Epoch: [11][340/352]	Time  0.169 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.6397e-01 (9.0087e-01)	Acc@1  73.44 ( 74.33)	Acc@5  97.66 ( 94.38)
03-Mar-22 09:19:57 - Epoch: [10][270/352]	Time  0.131 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.8377e-01 (9.0382e-01)	Acc@1  76.56 ( 74.32)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:19:58 - Epoch: [11][350/352]	Time  0.174 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0265e+00 (9.0292e-01)	Acc@1  72.66 ( 74.27)	Acc@5  93.75 ( 94.35)
03-Mar-22 09:19:59 - Test: [ 0/20]	Time  0.354 ( 0.354)	Loss 9.7291e-01 (9.7291e-01)	Acc@1  71.48 ( 71.48)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:19:59 - Epoch: [10][280/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.9288e-01 (9.0481e-01)	Acc@1  64.84 ( 74.25)	Acc@5  96.09 ( 94.56)
03-Mar-22 09:20:00 - Test: [10/20]	Time  0.097 ( 0.121)	Loss 8.5112e-01 (9.3193e-01)	Acc@1  76.17 ( 73.08)	Acc@5  95.31 ( 94.11)
03-Mar-22 09:20:00 - Epoch: [10][290/352]	Time  0.145 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0570e+00 (9.0596e-01)	Acc@1  68.75 ( 74.25)	Acc@5  92.19 ( 94.53)
03-Mar-22 09:20:00 -  * Acc@1 73.040 Acc@5 94.020
03-Mar-22 09:20:01 - Best acc at epoch 11: 73.97999572753906
03-Mar-22 09:20:01 - Epoch: [12][  0/352]	Time  0.385 ( 0.385)	Data  0.226 ( 0.226)	Loss 8.1578e-01 (8.1578e-01)	Acc@1  79.69 ( 79.69)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:20:02 - Epoch: [10][300/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1482e-01 (9.0721e-01)	Acc@1  81.25 ( 74.22)	Acc@5  93.75 ( 94.51)
03-Mar-22 09:20:02 - Epoch: [12][ 10/352]	Time  0.147 ( 0.165)	Data  0.002 ( 0.022)	Loss 1.0606e+00 (8.8892e-01)	Acc@1  74.22 ( 74.72)	Acc@5  89.84 ( 94.39)
03-Mar-22 09:20:03 - Epoch: [10][310/352]	Time  0.132 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0958e+00 (9.0592e-01)	Acc@1  64.84 ( 74.26)	Acc@5  92.97 ( 94.52)
03-Mar-22 09:20:04 - Epoch: [12][ 20/352]	Time  0.174 ( 0.160)	Data  0.003 ( 0.013)	Loss 8.8961e-01 (8.8113e-01)	Acc@1  78.12 ( 74.93)	Acc@5  92.97 ( 94.72)
03-Mar-22 09:20:05 - Epoch: [10][320/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.5756e-01 (9.0528e-01)	Acc@1  78.91 ( 74.29)	Acc@5  96.88 ( 94.52)
03-Mar-22 09:20:05 - Epoch: [12][ 30/352]	Time  0.169 ( 0.159)	Data  0.002 ( 0.010)	Loss 8.3255e-01 (8.5911e-01)	Acc@1  73.44 ( 75.53)	Acc@5  95.31 ( 94.86)
03-Mar-22 09:20:06 - Epoch: [10][330/352]	Time  0.150 ( 0.151)	Data  0.003 ( 0.003)	Loss 8.0659e-01 (9.0281e-01)	Acc@1  76.56 ( 74.36)	Acc@5  97.66 ( 94.56)
03-Mar-22 09:20:07 - Epoch: [12][ 40/352]	Time  0.149 ( 0.159)	Data  0.002 ( 0.008)	Loss 8.5419e-01 (8.6951e-01)	Acc@1  71.88 ( 75.32)	Acc@5  96.09 ( 94.72)
03-Mar-22 09:20:08 - Epoch: [10][340/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.3682e-01 (9.0384e-01)	Acc@1  73.44 ( 74.31)	Acc@5  93.75 ( 94.54)
03-Mar-22 09:20:09 - Epoch: [12][ 50/352]	Time  0.150 ( 0.157)	Data  0.003 ( 0.007)	Loss 9.0951e-01 (8.7385e-01)	Acc@1  71.88 ( 75.20)	Acc@5  92.97 ( 94.81)
03-Mar-22 09:20:09 - Epoch: [10][350/352]	Time  0.147 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.2127e-01 (9.0429e-01)	Acc@1  71.09 ( 74.28)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:20:10 - Test: [ 0/20]	Time  0.357 ( 0.357)	Loss 1.0434e+00 (1.0434e+00)	Acc@1  67.97 ( 67.97)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:20:10 - Epoch: [12][ 60/352]	Time  0.160 ( 0.154)	Data  0.002 ( 0.006)	Loss 9.4926e-01 (8.7749e-01)	Acc@1  72.66 ( 74.90)	Acc@5  93.75 ( 94.83)
03-Mar-22 09:20:11 - Test: [10/20]	Time  0.095 ( 0.127)	Loss 7.9182e-01 (9.4513e-01)	Acc@1  79.30 ( 72.66)	Acc@5  96.09 ( 94.07)
03-Mar-22 09:20:12 - Epoch: [12][ 70/352]	Time  0.182 ( 0.155)	Data  0.003 ( 0.006)	Loss 9.0789e-01 (8.7982e-01)	Acc@1  72.66 ( 74.91)	Acc@5  94.53 ( 94.75)
03-Mar-22 09:20:12 -  * Acc@1 72.940 Acc@5 94.040
03-Mar-22 09:20:12 - Best acc at epoch 10: 74.04000091552734
03-Mar-22 09:20:12 - Epoch: [11][  0/352]	Time  0.359 ( 0.359)	Data  0.223 ( 0.223)	Loss 7.4421e-01 (7.4421e-01)	Acc@1  78.91 ( 78.91)	Acc@5  98.44 ( 98.44)
03-Mar-22 09:20:13 - Epoch: [12][ 80/352]	Time  0.151 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.8759e-01 (8.7875e-01)	Acc@1  78.12 ( 75.00)	Acc@5  92.19 ( 94.61)
03-Mar-22 09:20:14 - Epoch: [11][ 10/352]	Time  0.158 ( 0.172)	Data  0.002 ( 0.022)	Loss 7.3580e-01 (8.1209e-01)	Acc@1  82.03 ( 77.06)	Acc@5  98.44 ( 95.53)
03-Mar-22 09:20:15 - Epoch: [12][ 90/352]	Time  0.149 ( 0.155)	Data  0.003 ( 0.005)	Loss 6.8409e-01 (8.7861e-01)	Acc@1  82.81 ( 75.00)	Acc@5  96.88 ( 94.53)
03-Mar-22 09:20:15 - Epoch: [11][ 20/352]	Time  0.145 ( 0.163)	Data  0.002 ( 0.013)	Loss 8.6233e-01 (8.4378e-01)	Acc@1  76.56 ( 75.86)	Acc@5  96.09 ( 95.46)
03-Mar-22 09:20:16 - Epoch: [12][100/352]	Time  0.139 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.9133e-01 (8.7896e-01)	Acc@1  70.31 ( 74.99)	Acc@5  90.62 ( 94.53)
03-Mar-22 09:20:17 - Epoch: [11][ 30/352]	Time  0.148 ( 0.159)	Data  0.002 ( 0.010)	Loss 8.9358e-01 (8.6235e-01)	Acc@1  74.22 ( 75.18)	Acc@5  94.53 ( 94.88)
03-Mar-22 09:20:18 - Epoch: [12][110/352]	Time  0.147 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.1705e-01 (8.7454e-01)	Acc@1  72.66 ( 75.18)	Acc@5  95.31 ( 94.64)
03-Mar-22 09:20:18 - Epoch: [11][ 40/352]	Time  0.154 ( 0.157)	Data  0.002 ( 0.008)	Loss 7.9466e-01 (8.7089e-01)	Acc@1  73.44 ( 74.92)	Acc@5  97.66 ( 94.87)
03-Mar-22 09:20:19 - Epoch: [12][120/352]	Time  0.140 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0586e+00 (8.7604e-01)	Acc@1  69.53 ( 75.09)	Acc@5  92.97 ( 94.65)
03-Mar-22 09:20:20 - Epoch: [11][ 50/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.007)	Loss 8.7640e-01 (8.7269e-01)	Acc@1  77.34 ( 75.15)	Acc@5  93.75 ( 94.75)
03-Mar-22 09:20:21 - Epoch: [12][130/352]	Time  0.138 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.6749e-01 (8.7634e-01)	Acc@1  75.00 ( 75.02)	Acc@5  90.62 ( 94.64)
03-Mar-22 09:20:21 - Epoch: [11][ 60/352]	Time  0.143 ( 0.156)	Data  0.003 ( 0.006)	Loss 8.3885e-01 (8.8177e-01)	Acc@1  76.56 ( 74.86)	Acc@5  97.66 ( 94.57)
03-Mar-22 09:20:22 - Epoch: [12][140/352]	Time  0.147 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.0260e-01 (8.7975e-01)	Acc@1  72.66 ( 74.90)	Acc@5  92.97 ( 94.56)
03-Mar-22 09:20:23 - Epoch: [11][ 70/352]	Time  0.147 ( 0.155)	Data  0.002 ( 0.006)	Loss 8.8405e-01 (8.7608e-01)	Acc@1  75.00 ( 74.99)	Acc@5  95.31 ( 94.71)
03-Mar-22 09:20:24 - Epoch: [12][150/352]	Time  0.156 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.7687e-01 (8.8157e-01)	Acc@1  74.22 ( 74.81)	Acc@5  92.97 ( 94.56)
03-Mar-22 09:20:24 - Epoch: [11][ 80/352]	Time  0.152 ( 0.154)	Data  0.003 ( 0.005)	Loss 9.4191e-01 (8.7583e-01)	Acc@1  71.88 ( 74.98)	Acc@5  95.31 ( 94.65)
03-Mar-22 09:20:25 - Epoch: [12][160/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.9136e-01 (8.7996e-01)	Acc@1  74.22 ( 74.87)	Acc@5  92.97 ( 94.58)
03-Mar-22 09:20:26 - Epoch: [11][ 90/352]	Time  0.151 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.9062e-01 (8.7999e-01)	Acc@1  75.78 ( 75.08)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:20:27 - Epoch: [12][170/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.004)	Loss 6.6925e-01 (8.7946e-01)	Acc@1  83.59 ( 74.92)	Acc@5  95.31 ( 94.60)
03-Mar-22 09:20:27 - Epoch: [11][100/352]	Time  0.154 ( 0.154)	Data  0.003 ( 0.005)	Loss 1.0354e+00 (8.8376e-01)	Acc@1  71.09 ( 74.95)	Acc@5  94.53 ( 94.54)
03-Mar-22 09:20:28 - Epoch: [12][180/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0420e+00 (8.8127e-01)	Acc@1  71.09 ( 74.87)	Acc@5  92.97 ( 94.58)
03-Mar-22 09:20:29 - Epoch: [11][110/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.8836e-01 (8.8336e-01)	Acc@1  64.84 ( 74.82)	Acc@5  94.53 ( 94.59)
03-Mar-22 09:20:30 - Epoch: [12][190/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.9896e-01 (8.8468e-01)	Acc@1  76.56 ( 74.81)	Acc@5  93.75 ( 94.54)
03-Mar-22 09:20:30 - Epoch: [11][120/352]	Time  0.155 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.2520e-01 (8.8579e-01)	Acc@1  73.44 ( 74.74)	Acc@5  93.75 ( 94.50)
03-Mar-22 09:20:31 - Epoch: [12][200/352]	Time  0.174 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0137e+00 (8.8672e-01)	Acc@1  75.78 ( 74.72)	Acc@5  92.19 ( 94.53)
03-Mar-22 09:20:32 - Epoch: [11][130/352]	Time  0.131 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.3626e-01 (8.8397e-01)	Acc@1  75.00 ( 74.81)	Acc@5  92.19 ( 94.49)
03-Mar-22 09:20:33 - Epoch: [12][210/352]	Time  0.172 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.2998e-01 (8.8527e-01)	Acc@1  71.88 ( 74.76)	Acc@5  93.75 ( 94.53)
03-Mar-22 09:20:33 - Epoch: [11][140/352]	Time  0.158 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0376e+00 (8.8640e-01)	Acc@1  71.09 ( 74.75)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:20:35 - Epoch: [12][220/352]	Time  0.172 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.1023e+00 (8.8746e-01)	Acc@1  66.41 ( 74.68)	Acc@5  93.75 ( 94.50)
03-Mar-22 09:20:35 - Epoch: [11][150/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.9717e-01 (8.8946e-01)	Acc@1  78.12 ( 74.74)	Acc@5  94.53 ( 94.40)
03-Mar-22 09:20:36 - Epoch: [12][230/352]	Time  0.146 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0641e+00 (8.8784e-01)	Acc@1  71.09 ( 74.73)	Acc@5  92.97 ( 94.49)
03-Mar-22 09:20:36 - Epoch: [11][160/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0187e+00 (8.9440e-01)	Acc@1  70.31 ( 74.57)	Acc@5  92.97 ( 94.42)
03-Mar-22 09:20:38 - Epoch: [12][240/352]	Time  0.138 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.0707e-01 (8.8734e-01)	Acc@1  75.00 ( 74.74)	Acc@5  96.88 ( 94.52)
03-Mar-22 09:20:38 - Epoch: [11][170/352]	Time  0.146 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.5085e-01 (8.9447e-01)	Acc@1  73.44 ( 74.59)	Acc@5  96.88 ( 94.47)
03-Mar-22 09:20:39 - Epoch: [12][250/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.003)	Loss 7.9902e-01 (8.8755e-01)	Acc@1  78.91 ( 74.78)	Acc@5  95.31 ( 94.51)
03-Mar-22 09:20:39 - Epoch: [11][180/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.3420e-01 (8.9557e-01)	Acc@1  75.78 ( 74.53)	Acc@5  95.31 ( 94.49)
03-Mar-22 09:20:41 - Epoch: [12][260/352]	Time  0.150 ( 0.154)	Data  0.002 ( 0.003)	Loss 7.2785e-01 (8.8853e-01)	Acc@1  80.47 ( 74.78)	Acc@5  97.66 ( 94.52)
03-Mar-22 09:20:41 - Epoch: [11][190/352]	Time  0.144 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.0796e-01 (8.9654e-01)	Acc@1  73.44 ( 74.43)	Acc@5  94.53 ( 94.48)
03-Mar-22 09:20:42 - Epoch: [11][200/352]	Time  0.129 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.4355e-01 (8.9828e-01)	Acc@1  78.12 ( 74.36)	Acc@5  96.88 ( 94.48)
03-Mar-22 09:20:42 - Epoch: [12][270/352]	Time  0.175 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.8216e-01 (8.8734e-01)	Acc@1  75.00 ( 74.84)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:20:44 - Epoch: [11][210/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.2861e-01 (8.9895e-01)	Acc@1  73.44 ( 74.37)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:20:44 - Epoch: [12][280/352]	Time  0.170 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.2181e-01 (8.8838e-01)	Acc@1  73.44 ( 74.78)	Acc@5  94.53 ( 94.52)
03-Mar-22 09:20:45 - Epoch: [11][220/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0770e+00 (9.0018e-01)	Acc@1  61.72 ( 74.34)	Acc@5  96.09 ( 94.45)
03-Mar-22 09:20:45 - Epoch: [12][290/352]	Time  0.146 ( 0.154)	Data  0.002 ( 0.003)	Loss 7.9403e-01 (8.8892e-01)	Acc@1  75.00 ( 74.75)	Acc@5  96.88 ( 94.52)
03-Mar-22 09:20:47 - Epoch: [11][230/352]	Time  0.156 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.3386e-01 (8.9949e-01)	Acc@1  70.31 ( 74.41)	Acc@5  97.66 ( 94.45)
03-Mar-22 09:20:47 - Epoch: [12][300/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.9606e-01 (8.9089e-01)	Acc@1  76.56 ( 74.68)	Acc@5  95.31 ( 94.49)
03-Mar-22 09:20:48 - Epoch: [11][240/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.7283e-01 (8.9873e-01)	Acc@1  78.12 ( 74.42)	Acc@5  93.75 ( 94.43)
03-Mar-22 09:20:48 - Epoch: [12][310/352]	Time  0.148 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.6453e-01 (8.9196e-01)	Acc@1  73.44 ( 74.65)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:20:50 - Epoch: [11][250/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.0054e-01 (8.9761e-01)	Acc@1  71.09 ( 74.34)	Acc@5  96.88 ( 94.47)
03-Mar-22 09:20:50 - Epoch: [12][320/352]	Time  0.145 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.7409e-01 (8.9095e-01)	Acc@1  74.22 ( 74.65)	Acc@5  95.31 ( 94.48)
03-Mar-22 09:20:51 - Epoch: [11][260/352]	Time  0.146 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.9588e-01 (8.9672e-01)	Acc@1  71.09 ( 74.36)	Acc@5  92.19 ( 94.51)
03-Mar-22 09:20:51 - Epoch: [12][330/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.7950e-01 (8.9171e-01)	Acc@1  75.78 ( 74.61)	Acc@5  95.31 ( 94.46)
03-Mar-22 09:20:53 - Epoch: [11][270/352]	Time  0.146 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.7364e-01 (8.9764e-01)	Acc@1  75.78 ( 74.31)	Acc@5  92.19 ( 94.49)
03-Mar-22 09:20:53 - Epoch: [12][340/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.2834e-01 (8.9120e-01)	Acc@1  78.91 ( 74.62)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:20:54 - Epoch: [11][280/352]	Time  0.130 ( 0.150)	Data  0.001 ( 0.003)	Loss 8.2206e-01 (8.9971e-01)	Acc@1  77.34 ( 74.30)	Acc@5  94.53 ( 94.45)
03-Mar-22 09:20:54 - Epoch: [12][350/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.2911e-01 (8.9014e-01)	Acc@1  74.22 ( 74.65)	Acc@5  96.09 ( 94.47)
03-Mar-22 09:20:55 - Test: [ 0/20]	Time  0.393 ( 0.393)	Loss 1.0150e+00 (1.0150e+00)	Acc@1  71.48 ( 71.48)	Acc@5  91.41 ( 91.41)
03-Mar-22 09:20:55 - Epoch: [11][290/352]	Time  0.154 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.5191e-01 (8.9926e-01)	Acc@1  75.00 ( 74.33)	Acc@5  94.53 ( 94.45)
03-Mar-22 09:20:56 - Test: [10/20]	Time  0.116 ( 0.135)	Loss 8.3822e-01 (9.5305e-01)	Acc@1  75.39 ( 73.40)	Acc@5  94.92 ( 93.61)
03-Mar-22 09:20:57 - Epoch: [11][300/352]	Time  0.143 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.9455e-01 (9.0079e-01)	Acc@1  78.91 ( 74.28)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:20:57 -  * Acc@1 72.900 Acc@5 93.640
03-Mar-22 09:20:57 - Best acc at epoch 12: 73.97999572753906
03-Mar-22 09:20:57 - Epoch: [13][  0/352]	Time  0.405 ( 0.405)	Data  0.255 ( 0.255)	Loss 9.4827e-01 (9.4827e-01)	Acc@1  73.44 ( 73.44)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:20:58 - Epoch: [11][310/352]	Time  0.167 ( 0.150)	Data  0.003 ( 0.003)	Loss 9.3349e-01 (9.0148e-01)	Acc@1  74.22 ( 74.28)	Acc@5  91.41 ( 94.42)
03-Mar-22 09:20:59 - Epoch: [13][ 10/352]	Time  0.171 ( 0.180)	Data  0.002 ( 0.025)	Loss 9.2685e-01 (8.7962e-01)	Acc@1  74.22 ( 74.22)	Acc@5  92.19 ( 94.53)
03-Mar-22 09:21:00 - Epoch: [11][320/352]	Time  0.153 ( 0.149)	Data  0.003 ( 0.003)	Loss 8.2313e-01 (9.0135e-01)	Acc@1  77.34 ( 74.24)	Acc@5  95.31 ( 94.43)
03-Mar-22 09:21:01 - Epoch: [13][ 20/352]	Time  0.140 ( 0.171)	Data  0.003 ( 0.014)	Loss 8.0421e-01 (8.8453e-01)	Acc@1  80.47 ( 74.26)	Acc@5  95.31 ( 94.42)
03-Mar-22 09:21:01 - Epoch: [11][330/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0179e+00 (9.0211e-01)	Acc@1  69.53 ( 74.21)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:21:02 - Epoch: [13][ 30/352]	Time  0.150 ( 0.163)	Data  0.002 ( 0.010)	Loss 1.1159e+00 (8.9358e-01)	Acc@1  66.41 ( 74.19)	Acc@5  90.62 ( 94.33)
03-Mar-22 09:21:03 - Epoch: [11][340/352]	Time  0.135 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.1092e-01 (9.0288e-01)	Acc@1  72.66 ( 74.15)	Acc@5  93.75 ( 94.45)
03-Mar-22 09:21:04 - Epoch: [13][ 40/352]	Time  0.172 ( 0.166)	Data  0.003 ( 0.009)	Loss 8.7053e-01 (9.0358e-01)	Acc@1  74.22 ( 73.91)	Acc@5  94.53 ( 94.30)
03-Mar-22 09:21:04 - Epoch: [11][350/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.0808e-01 (9.0325e-01)	Acc@1  75.78 ( 74.15)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:21:05 - Test: [ 0/20]	Time  0.356 ( 0.356)	Loss 9.8638e-01 (9.8638e-01)	Acc@1  73.83 ( 73.83)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:21:05 - Epoch: [13][ 50/352]	Time  0.146 ( 0.163)	Data  0.002 ( 0.007)	Loss 9.0374e-01 (9.1103e-01)	Acc@1  71.88 ( 73.58)	Acc@5  96.09 ( 94.41)
03-Mar-22 09:21:06 - Test: [10/20]	Time  0.096 ( 0.122)	Loss 8.3975e-01 (9.5003e-01)	Acc@1  74.61 ( 73.54)	Acc@5  94.92 ( 93.68)
03-Mar-22 09:21:07 -  * Acc@1 73.500 Acc@5 93.760
03-Mar-22 09:21:07 - Best acc at epoch 11: 74.04000091552734
03-Mar-22 09:21:07 - Epoch: [13][ 60/352]	Time  0.133 ( 0.163)	Data  0.002 ( 0.007)	Loss 1.0531e+00 (9.0727e-01)	Acc@1  72.66 ( 73.78)	Acc@5  89.84 ( 94.42)
03-Mar-22 09:21:07 - Epoch: [12][  0/352]	Time  0.372 ( 0.372)	Data  0.242 ( 0.242)	Loss 9.9779e-01 (9.9779e-01)	Acc@1  68.75 ( 68.75)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:21:09 - Epoch: [13][ 70/352]	Time  0.146 ( 0.162)	Data  0.002 ( 0.006)	Loss 8.3850e-01 (9.0107e-01)	Acc@1  77.34 ( 74.03)	Acc@5  94.53 ( 94.49)
03-Mar-22 09:21:09 - Epoch: [12][ 10/352]	Time  0.148 ( 0.168)	Data  0.002 ( 0.024)	Loss 1.0389e+00 (9.1082e-01)	Acc@1  72.66 ( 73.58)	Acc@5  93.75 ( 94.60)
03-Mar-22 09:21:10 - Epoch: [13][ 80/352]	Time  0.153 ( 0.161)	Data  0.002 ( 0.006)	Loss 9.4656e-01 (8.9669e-01)	Acc@1  73.44 ( 74.27)	Acc@5  92.19 ( 94.49)
03-Mar-22 09:21:10 - Epoch: [12][ 20/352]	Time  0.140 ( 0.159)	Data  0.002 ( 0.014)	Loss 9.8916e-01 (8.9378e-01)	Acc@1  69.53 ( 74.07)	Acc@5  92.97 ( 94.87)
03-Mar-22 09:21:12 - Epoch: [13][ 90/352]	Time  0.137 ( 0.159)	Data  0.002 ( 0.005)	Loss 9.9198e-01 (8.9665e-01)	Acc@1  71.88 ( 74.42)	Acc@5  94.53 ( 94.51)
03-Mar-22 09:21:12 - Epoch: [12][ 30/352]	Time  0.135 ( 0.154)	Data  0.002 ( 0.010)	Loss 1.0084e+00 (8.8308e-01)	Acc@1  70.31 ( 74.34)	Acc@5  93.75 ( 95.06)
03-Mar-22 09:21:13 - Epoch: [13][100/352]	Time  0.135 ( 0.158)	Data  0.002 ( 0.005)	Loss 9.0208e-01 (8.9281e-01)	Acc@1  75.00 ( 74.57)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:21:13 - Epoch: [12][ 40/352]	Time  0.152 ( 0.153)	Data  0.003 ( 0.008)	Loss 8.4344e-01 (8.8916e-01)	Acc@1  75.00 ( 74.45)	Acc@5  94.53 ( 94.87)
03-Mar-22 09:21:15 - Epoch: [13][110/352]	Time  0.152 ( 0.157)	Data  0.002 ( 0.005)	Loss 1.1064e+00 (8.9323e-01)	Acc@1  67.19 ( 74.58)	Acc@5  92.97 ( 94.46)
03-Mar-22 09:21:15 - Epoch: [12][ 50/352]	Time  0.134 ( 0.153)	Data  0.002 ( 0.007)	Loss 7.9779e-01 (8.8657e-01)	Acc@1  78.12 ( 74.71)	Acc@5  94.53 ( 94.87)
03-Mar-22 09:21:16 - Epoch: [13][120/352]	Time  0.152 ( 0.157)	Data  0.002 ( 0.005)	Loss 7.1679e-01 (8.9427e-01)	Acc@1  77.34 ( 74.43)	Acc@5  96.09 ( 94.45)
03-Mar-22 09:21:16 - Epoch: [12][ 60/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.006)	Loss 8.2193e-01 (8.8296e-01)	Acc@1  73.44 ( 74.85)	Acc@5  96.88 ( 94.94)
03-Mar-22 09:21:17 - Epoch: [13][130/352]	Time  0.141 ( 0.156)	Data  0.002 ( 0.004)	Loss 7.7095e-01 (8.9137e-01)	Acc@1  78.12 ( 74.53)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:21:18 - Epoch: [12][ 70/352]	Time  0.131 ( 0.153)	Data  0.002 ( 0.006)	Loss 8.7095e-01 (8.8954e-01)	Acc@1  73.44 ( 74.48)	Acc@5  94.53 ( 94.94)
03-Mar-22 09:21:19 - Epoch: [13][140/352]	Time  0.168 ( 0.156)	Data  0.002 ( 0.004)	Loss 9.2561e-01 (8.8722e-01)	Acc@1  74.22 ( 74.70)	Acc@5  92.97 ( 94.49)
03-Mar-22 09:21:19 - Epoch: [12][ 80/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.3899e-01 (8.8818e-01)	Acc@1  76.56 ( 74.54)	Acc@5  92.97 ( 94.88)
03-Mar-22 09:21:21 - Epoch: [12][ 90/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.6437e-01 (8.8750e-01)	Acc@1  76.56 ( 74.67)	Acc@5  92.97 ( 94.82)
03-Mar-22 09:21:21 - Epoch: [13][150/352]	Time  0.176 ( 0.157)	Data  0.002 ( 0.004)	Loss 7.5933e-01 (8.8582e-01)	Acc@1  74.22 ( 74.71)	Acc@5  94.53 ( 94.52)
03-Mar-22 09:21:22 - Epoch: [12][100/352]	Time  0.159 ( 0.153)	Data  0.002 ( 0.005)	Loss 7.6420e-01 (8.8619e-01)	Acc@1  77.34 ( 74.63)	Acc@5  94.53 ( 94.80)
03-Mar-22 09:21:22 - Epoch: [13][160/352]	Time  0.169 ( 0.158)	Data  0.002 ( 0.004)	Loss 1.0106e+00 (8.8588e-01)	Acc@1  71.88 ( 74.71)	Acc@5  94.53 ( 94.54)
03-Mar-22 09:21:24 - Epoch: [12][110/352]	Time  0.149 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.1194e-01 (8.8790e-01)	Acc@1  73.44 ( 74.47)	Acc@5  96.88 ( 94.76)
03-Mar-22 09:21:24 - Epoch: [13][170/352]	Time  0.159 ( 0.158)	Data  0.002 ( 0.004)	Loss 9.1726e-01 (8.8582e-01)	Acc@1  76.56 ( 74.78)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:21:25 - Epoch: [12][120/352]	Time  0.155 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0846e+00 (8.9274e-01)	Acc@1  67.19 ( 74.30)	Acc@5  91.41 ( 94.67)
03-Mar-22 09:21:26 - Epoch: [13][180/352]	Time  0.153 ( 0.158)	Data  0.002 ( 0.004)	Loss 8.4127e-01 (8.8466e-01)	Acc@1  77.34 ( 74.72)	Acc@5  92.97 ( 94.55)
03-Mar-22 09:21:27 - Epoch: [12][130/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.3903e-01 (8.9465e-01)	Acc@1  75.00 ( 74.21)	Acc@5  96.88 ( 94.64)
03-Mar-22 09:21:27 - Epoch: [13][190/352]	Time  0.153 ( 0.158)	Data  0.002 ( 0.004)	Loss 9.8384e-01 (8.8640e-01)	Acc@1  71.09 ( 74.69)	Acc@5  94.53 ( 94.52)
03-Mar-22 09:21:28 - Epoch: [12][140/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.2674e-01 (8.9933e-01)	Acc@1  71.88 ( 74.07)	Acc@5  94.53 ( 94.57)
03-Mar-22 09:21:29 - Epoch: [13][200/352]	Time  0.136 ( 0.158)	Data  0.002 ( 0.004)	Loss 8.9220e-01 (8.8636e-01)	Acc@1  75.00 ( 74.68)	Acc@5  92.97 ( 94.49)
03-Mar-22 09:21:30 - Epoch: [12][150/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.5312e-01 (8.9911e-01)	Acc@1  75.00 ( 74.12)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:21:30 - Epoch: [13][210/352]	Time  0.133 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.2740e-01 (8.8820e-01)	Acc@1  75.00 ( 74.57)	Acc@5  95.31 ( 94.46)
03-Mar-22 09:21:31 - Epoch: [12][160/352]	Time  0.152 ( 0.152)	Data  0.003 ( 0.004)	Loss 9.2173e-01 (8.9634e-01)	Acc@1  75.00 ( 74.23)	Acc@5  90.62 ( 94.57)
03-Mar-22 09:21:32 - Epoch: [13][220/352]	Time  0.150 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.3227e-01 (8.8487e-01)	Acc@1  75.00 ( 74.71)	Acc@5  94.53 ( 94.48)
03-Mar-22 09:21:33 - Epoch: [12][170/352]	Time  0.128 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.5239e-01 (8.9490e-01)	Acc@1  77.34 ( 74.21)	Acc@5  99.22 ( 94.60)
03-Mar-22 09:21:33 - Epoch: [13][230/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.1387e+00 (8.8475e-01)	Acc@1  69.53 ( 74.75)	Acc@5  88.28 ( 94.49)
03-Mar-22 09:21:34 - Epoch: [12][180/352]	Time  0.140 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.9324e-01 (8.9517e-01)	Acc@1  71.09 ( 74.18)	Acc@5  92.97 ( 94.57)
03-Mar-22 09:21:35 - Epoch: [13][240/352]	Time  0.150 ( 0.156)	Data  0.002 ( 0.003)	Loss 7.4896e-01 (8.8175e-01)	Acc@1  79.69 ( 74.86)	Acc@5  96.09 ( 94.55)
03-Mar-22 09:21:36 - Epoch: [12][190/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.004)	Loss 9.1656e-01 (8.9451e-01)	Acc@1  75.00 ( 74.26)	Acc@5  96.09 ( 94.55)
03-Mar-22 09:21:36 - Epoch: [13][250/352]	Time  0.127 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.6723e-01 (8.8167e-01)	Acc@1  70.31 ( 74.83)	Acc@5  92.97 ( 94.57)
03-Mar-22 09:21:37 - Epoch: [12][200/352]	Time  0.155 ( 0.152)	Data  0.003 ( 0.004)	Loss 7.7349e-01 (8.9473e-01)	Acc@1  79.69 ( 74.23)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:21:38 - Epoch: [13][260/352]	Time  0.149 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.4602e-01 (8.8219e-01)	Acc@1  75.00 ( 74.81)	Acc@5  95.31 ( 94.55)
03-Mar-22 09:21:39 - Epoch: [12][210/352]	Time  0.145 ( 0.152)	Data  0.001 ( 0.003)	Loss 9.4956e-01 (8.9443e-01)	Acc@1  71.88 ( 74.22)	Acc@5  94.53 ( 94.54)
03-Mar-22 09:21:39 - Epoch: [13][270/352]	Time  0.132 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.9561e-01 (8.8507e-01)	Acc@1  75.78 ( 74.78)	Acc@5  92.19 ( 94.52)
03-Mar-22 09:21:40 - Epoch: [12][220/352]	Time  0.157 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.2961e-01 (8.9725e-01)	Acc@1  74.22 ( 74.07)	Acc@5  95.31 ( 94.55)
03-Mar-22 09:21:41 - Epoch: [13][280/352]	Time  0.150 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.2585e-01 (8.8431e-01)	Acc@1  77.34 ( 74.81)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:21:42 - Epoch: [12][230/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.0800e-01 (9.0080e-01)	Acc@1  75.78 ( 74.01)	Acc@5  94.53 ( 94.51)
03-Mar-22 09:21:42 - Epoch: [13][290/352]	Time  0.170 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.5371e-01 (8.8529e-01)	Acc@1  73.44 ( 74.83)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:21:43 - Epoch: [12][240/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0577e+00 (9.0112e-01)	Acc@1  69.53 ( 73.98)	Acc@5  93.75 ( 94.51)
03-Mar-22 09:21:44 - Epoch: [13][300/352]	Time  0.154 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.1774e-01 (8.8629e-01)	Acc@1  75.00 ( 74.83)	Acc@5  93.75 ( 94.49)
03-Mar-22 09:21:45 - Epoch: [12][250/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.0672e-01 (9.0205e-01)	Acc@1  71.88 ( 73.96)	Acc@5  92.97 ( 94.49)
03-Mar-22 09:21:46 - Epoch: [13][310/352]	Time  0.176 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.5817e-01 (8.8582e-01)	Acc@1  72.66 ( 74.82)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:21:46 - Epoch: [12][260/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.5658e-01 (9.0225e-01)	Acc@1  72.66 ( 73.97)	Acc@5  92.97 ( 94.50)
03-Mar-22 09:21:47 - Epoch: [13][320/352]	Time  0.177 ( 0.157)	Data  0.002 ( 0.003)	Loss 7.5285e-01 (8.8571e-01)	Acc@1  82.03 ( 74.82)	Acc@5  98.44 ( 94.52)
03-Mar-22 09:21:48 - Epoch: [12][270/352]	Time  0.138 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.6555e-01 (9.0140e-01)	Acc@1  74.22 ( 74.00)	Acc@5  96.88 ( 94.52)
03-Mar-22 09:21:49 - Epoch: [13][330/352]	Time  0.156 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.8413e-01 (8.8529e-01)	Acc@1  75.00 ( 74.84)	Acc@5  93.75 ( 94.52)
03-Mar-22 09:21:49 - Epoch: [12][280/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.7026e-01 (9.0148e-01)	Acc@1  74.22 ( 74.03)	Acc@5  95.31 ( 94.51)
03-Mar-22 09:21:51 - Epoch: [12][290/352]	Time  0.146 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.8568e-01 (8.9952e-01)	Acc@1  77.34 ( 74.09)	Acc@5  98.44 ( 94.54)
03-Mar-22 09:21:51 - Epoch: [13][340/352]	Time  0.177 ( 0.158)	Data  0.002 ( 0.003)	Loss 7.9734e-01 (8.8479e-01)	Acc@1  78.12 ( 74.88)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:21:52 - Epoch: [12][300/352]	Time  0.157 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.4345e-01 (8.9902e-01)	Acc@1  78.12 ( 74.17)	Acc@5  93.75 ( 94.52)
03-Mar-22 09:21:53 - Epoch: [13][350/352]	Time  0.172 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.0684e-01 (8.8507e-01)	Acc@1  71.09 ( 74.86)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:21:53 - Test: [ 0/20]	Time  0.397 ( 0.397)	Loss 1.0813e+00 (1.0813e+00)	Acc@1  69.14 ( 69.14)	Acc@5  91.02 ( 91.02)
03-Mar-22 09:21:54 - Epoch: [12][310/352]	Time  0.143 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3415e-01 (8.9956e-01)	Acc@1  77.34 ( 74.17)	Acc@5  96.88 ( 94.52)
03-Mar-22 09:21:54 - Test: [10/20]	Time  0.106 ( 0.141)	Loss 7.5184e-01 (9.4661e-01)	Acc@1  78.52 ( 73.19)	Acc@5  95.31 ( 93.57)
03-Mar-22 09:21:55 - Epoch: [12][320/352]	Time  0.140 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.8453e-01 (8.9993e-01)	Acc@1  75.00 ( 74.18)	Acc@5  92.19 ( 94.52)
03-Mar-22 09:21:55 -  * Acc@1 72.980 Acc@5 93.420
03-Mar-22 09:21:55 - Best acc at epoch 13: 73.97999572753906
03-Mar-22 09:21:56 - Epoch: [14][  0/352]	Time  0.376 ( 0.376)	Data  0.220 ( 0.220)	Loss 6.9882e-01 (6.9882e-01)	Acc@1  78.91 ( 78.91)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:21:57 - Epoch: [12][330/352]	Time  0.130 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.5758e-01 (8.9975e-01)	Acc@1  75.78 ( 74.23)	Acc@5  94.53 ( 94.49)
03-Mar-22 09:21:58 - Epoch: [14][ 10/352]	Time  0.174 ( 0.190)	Data  0.003 ( 0.023)	Loss 7.8175e-01 (8.2127e-01)	Acc@1  76.56 ( 75.50)	Acc@5  96.88 ( 95.38)
03-Mar-22 09:21:58 - Epoch: [12][340/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.3639e-01 (9.0024e-01)	Acc@1  79.69 ( 74.21)	Acc@5  95.31 ( 94.48)
03-Mar-22 09:21:59 - Epoch: [14][ 20/352]	Time  0.158 ( 0.177)	Data  0.002 ( 0.013)	Loss 9.8266e-01 (8.8328e-01)	Acc@1  72.66 ( 74.63)	Acc@5  92.97 ( 94.27)
03-Mar-22 09:21:59 - Epoch: [12][350/352]	Time  0.145 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.8969e-01 (8.9945e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.88 ( 94.49)
03-Mar-22 09:22:00 - Test: [ 0/20]	Time  0.352 ( 0.352)	Loss 9.9855e-01 (9.9855e-01)	Acc@1  68.36 ( 68.36)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:22:01 - Epoch: [14][ 30/352]	Time  0.159 ( 0.164)	Data  0.002 ( 0.009)	Loss 9.2460e-01 (8.8363e-01)	Acc@1  71.88 ( 75.13)	Acc@5  96.88 ( 94.46)
03-Mar-22 09:22:01 - Test: [10/20]	Time  0.096 ( 0.124)	Loss 8.7156e-01 (9.3880e-01)	Acc@1  73.05 ( 73.08)	Acc@5  95.31 ( 93.82)
03-Mar-22 09:22:02 -  * Acc@1 73.340 Acc@5 93.620
03-Mar-22 09:22:02 - Best acc at epoch 12: 74.04000091552734
03-Mar-22 09:22:02 - Epoch: [14][ 40/352]	Time  0.114 ( 0.165)	Data  0.002 ( 0.008)	Loss 8.2627e-01 (8.8409e-01)	Acc@1  77.34 ( 75.25)	Acc@5  95.31 ( 94.42)
03-Mar-22 09:22:02 - Epoch: [13][  0/352]	Time  0.359 ( 0.359)	Data  0.218 ( 0.218)	Loss 7.9315e-01 (7.9315e-01)	Acc@1  75.78 ( 75.78)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:22:04 - Epoch: [14][ 50/352]	Time  0.136 ( 0.161)	Data  0.002 ( 0.007)	Loss 8.5970e-01 (8.8600e-01)	Acc@1  76.56 ( 75.26)	Acc@5  96.09 ( 94.44)
03-Mar-22 09:22:04 - Epoch: [13][ 10/352]	Time  0.145 ( 0.168)	Data  0.003 ( 0.022)	Loss 9.8455e-01 (9.3915e-01)	Acc@1  70.31 ( 73.30)	Acc@5  96.88 ( 94.18)
03-Mar-22 09:22:05 - Epoch: [14][ 60/352]	Time  0.149 ( 0.160)	Data  0.003 ( 0.006)	Loss 7.7619e-01 (8.7897e-01)	Acc@1  77.34 ( 75.46)	Acc@5  96.09 ( 94.48)
03-Mar-22 09:22:05 - Epoch: [13][ 20/352]	Time  0.171 ( 0.160)	Data  0.003 ( 0.013)	Loss 1.0422e+00 (9.2541e-01)	Acc@1  71.09 ( 73.77)	Acc@5  92.97 ( 94.53)
03-Mar-22 09:22:07 - Epoch: [14][ 70/352]	Time  0.148 ( 0.159)	Data  0.002 ( 0.005)	Loss 1.0384e+00 (8.7122e-01)	Acc@1  73.44 ( 75.66)	Acc@5  91.41 ( 94.48)
03-Mar-22 09:22:07 - Epoch: [13][ 30/352]	Time  0.170 ( 0.159)	Data  0.003 ( 0.009)	Loss 1.0283e+00 (9.2117e-01)	Acc@1  75.78 ( 74.04)	Acc@5  89.84 ( 94.51)
03-Mar-22 09:22:08 - Epoch: [14][ 80/352]	Time  0.149 ( 0.157)	Data  0.002 ( 0.005)	Loss 8.0824e-01 (8.6874e-01)	Acc@1  75.00 ( 75.66)	Acc@5  99.22 ( 94.52)
03-Mar-22 09:22:09 - Epoch: [13][ 40/352]	Time  0.159 ( 0.158)	Data  0.003 ( 0.008)	Loss 8.5819e-01 (9.0464e-01)	Acc@1  76.56 ( 74.75)	Acc@5  94.53 ( 94.57)
03-Mar-22 09:22:10 - Epoch: [14][ 90/352]	Time  0.172 ( 0.157)	Data  0.002 ( 0.005)	Loss 8.6144e-01 (8.6876e-01)	Acc@1  76.56 ( 75.59)	Acc@5  92.19 ( 94.60)
03-Mar-22 09:22:10 - Epoch: [13][ 50/352]	Time  0.149 ( 0.157)	Data  0.002 ( 0.007)	Loss 7.6894e-01 (9.0090e-01)	Acc@1  78.91 ( 74.77)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:22:11 - Epoch: [14][100/352]	Time  0.177 ( 0.158)	Data  0.003 ( 0.004)	Loss 9.6100e-01 (8.6581e-01)	Acc@1  73.44 ( 75.63)	Acc@5  96.09 ( 94.66)
03-Mar-22 09:22:12 - Epoch: [13][ 60/352]	Time  0.151 ( 0.158)	Data  0.003 ( 0.006)	Loss 7.4166e-01 (8.9090e-01)	Acc@1  76.56 ( 74.85)	Acc@5  96.09 ( 94.67)
03-Mar-22 09:22:13 - Epoch: [14][110/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.004)	Loss 8.6580e-01 (8.6828e-01)	Acc@1  74.22 ( 75.57)	Acc@5  92.19 ( 94.67)
03-Mar-22 09:22:13 - Epoch: [13][ 70/352]	Time  0.136 ( 0.158)	Data  0.002 ( 0.005)	Loss 8.8211e-01 (8.9389e-01)	Acc@1  74.22 ( 74.68)	Acc@5  94.53 ( 94.72)
03-Mar-22 09:22:15 - Epoch: [14][120/352]	Time  0.151 ( 0.158)	Data  0.002 ( 0.004)	Loss 9.8918e-01 (8.7188e-01)	Acc@1  71.88 ( 75.38)	Acc@5  91.41 ( 94.64)
03-Mar-22 09:22:15 - Epoch: [13][ 80/352]	Time  0.151 ( 0.158)	Data  0.002 ( 0.005)	Loss 7.3672e-01 (8.8752e-01)	Acc@1  78.12 ( 74.74)	Acc@5  97.66 ( 94.75)
03-Mar-22 09:22:16 - Epoch: [14][130/352]	Time  0.151 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.2115e-01 (8.6659e-01)	Acc@1  75.78 ( 75.51)	Acc@5  92.97 ( 94.71)
03-Mar-22 09:22:16 - Epoch: [13][ 90/352]	Time  0.156 ( 0.158)	Data  0.002 ( 0.005)	Loss 9.0052e-01 (8.8382e-01)	Acc@1  71.88 ( 74.88)	Acc@5  93.75 ( 94.75)
03-Mar-22 09:22:17 - Epoch: [14][140/352]	Time  0.130 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.0340e+00 (8.7116e-01)	Acc@1  70.31 ( 75.39)	Acc@5  88.28 ( 94.61)
03-Mar-22 09:22:18 - Epoch: [13][100/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.005)	Loss 9.8472e-01 (8.8400e-01)	Acc@1  76.56 ( 74.80)	Acc@5  92.19 ( 94.76)
03-Mar-22 09:22:19 - Epoch: [14][150/352]	Time  0.149 ( 0.156)	Data  0.002 ( 0.004)	Loss 8.8211e-01 (8.7132e-01)	Acc@1  74.22 ( 75.35)	Acc@5  92.97 ( 94.61)
03-Mar-22 09:22:20 - Epoch: [13][110/352]	Time  0.144 ( 0.158)	Data  0.002 ( 0.004)	Loss 6.5002e-01 (8.8041e-01)	Acc@1  80.47 ( 74.78)	Acc@5  98.44 ( 94.86)
03-Mar-22 09:22:21 - Epoch: [14][160/352]	Time  0.156 ( 0.156)	Data  0.002 ( 0.004)	Loss 8.3958e-01 (8.6806e-01)	Acc@1  79.69 ( 75.42)	Acc@5  96.88 ( 94.68)
03-Mar-22 09:22:21 - Epoch: [13][120/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.004)	Loss 9.3972e-01 (8.8561e-01)	Acc@1  71.09 ( 74.65)	Acc@5  96.88 ( 94.79)
03-Mar-22 09:22:22 - Epoch: [14][170/352]	Time  0.176 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.6633e-01 (8.7206e-01)	Acc@1  77.34 ( 75.34)	Acc@5  94.53 ( 94.60)
03-Mar-22 09:22:23 - Epoch: [13][130/352]	Time  0.131 ( 0.157)	Data  0.002 ( 0.004)	Loss 9.5521e-01 (8.8322e-01)	Acc@1  75.00 ( 74.73)	Acc@5  96.09 ( 94.79)
03-Mar-22 09:22:24 - Epoch: [14][180/352]	Time  0.144 ( 0.157)	Data  0.002 ( 0.004)	Loss 1.0784e+00 (8.7403e-01)	Acc@1  66.41 ( 75.11)	Acc@5  94.53 ( 94.64)
03-Mar-22 09:22:24 - Epoch: [13][140/352]	Time  0.158 ( 0.156)	Data  0.003 ( 0.004)	Loss 8.9359e-01 (8.8476e-01)	Acc@1  77.34 ( 74.66)	Acc@5  92.97 ( 94.71)
03-Mar-22 09:22:26 - Epoch: [14][190/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.0500e+00 (8.7511e-01)	Acc@1  70.31 ( 75.09)	Acc@5  93.75 ( 94.63)
03-Mar-22 09:22:26 - Epoch: [13][150/352]	Time  0.147 ( 0.156)	Data  0.003 ( 0.004)	Loss 8.7307e-01 (8.8728e-01)	Acc@1  74.22 ( 74.63)	Acc@5  92.19 ( 94.69)
03-Mar-22 09:22:27 - Epoch: [13][160/352]	Time  0.147 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.0498e+00 (8.8705e-01)	Acc@1  68.75 ( 74.71)	Acc@5  92.19 ( 94.69)
03-Mar-22 09:22:27 - Epoch: [14][200/352]	Time  0.171 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.9339e-01 (8.7825e-01)	Acc@1  72.66 ( 75.04)	Acc@5  90.62 ( 94.54)
03-Mar-22 09:22:29 - Epoch: [13][170/352]	Time  0.153 ( 0.156)	Data  0.003 ( 0.004)	Loss 8.4923e-01 (8.8807e-01)	Acc@1  74.22 ( 74.67)	Acc@5  97.66 ( 94.68)
03-Mar-22 09:22:29 - Epoch: [14][210/352]	Time  0.156 ( 0.158)	Data  0.003 ( 0.003)	Loss 9.9491e-01 (8.7881e-01)	Acc@1  69.53 ( 74.97)	Acc@5  92.97 ( 94.57)
03-Mar-22 09:22:30 - Epoch: [13][180/352]	Time  0.171 ( 0.156)	Data  0.003 ( 0.004)	Loss 9.7907e-01 (8.9264e-01)	Acc@1  70.31 ( 74.56)	Acc@5  96.09 ( 94.64)
03-Mar-22 09:22:30 - Epoch: [14][220/352]	Time  0.136 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.3014e-01 (8.7970e-01)	Acc@1  76.56 ( 74.93)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:22:32 - Epoch: [13][190/352]	Time  0.152 ( 0.156)	Data  0.002 ( 0.004)	Loss 7.0321e-01 (8.9167e-01)	Acc@1  80.47 ( 74.54)	Acc@5  97.66 ( 94.68)
03-Mar-22 09:22:32 - Epoch: [14][230/352]	Time  0.181 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.7938e-01 (8.7999e-01)	Acc@1  75.78 ( 74.87)	Acc@5  90.62 ( 94.55)
03-Mar-22 09:22:33 - Epoch: [13][200/352]	Time  0.158 ( 0.155)	Data  0.002 ( 0.003)	Loss 9.0730e-01 (8.9288e-01)	Acc@1  72.66 ( 74.49)	Acc@5  92.19 ( 94.63)
03-Mar-22 09:22:34 - Epoch: [14][240/352]	Time  0.152 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.7041e-01 (8.7928e-01)	Acc@1  76.56 ( 74.88)	Acc@5  96.09 ( 94.55)
03-Mar-22 09:22:35 - Epoch: [13][210/352]	Time  0.154 ( 0.156)	Data  0.003 ( 0.003)	Loss 9.2973e-01 (8.9407e-01)	Acc@1  71.88 ( 74.50)	Acc@5  92.97 ( 94.58)
03-Mar-22 09:22:35 - Epoch: [14][250/352]	Time  0.154 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.6627e-01 (8.7789e-01)	Acc@1  75.78 ( 74.94)	Acc@5  94.53 ( 94.57)
03-Mar-22 09:22:36 - Epoch: [13][220/352]	Time  0.170 ( 0.155)	Data  0.003 ( 0.003)	Loss 9.7946e-01 (8.9483e-01)	Acc@1  71.09 ( 74.47)	Acc@5  92.97 ( 94.55)
03-Mar-22 09:22:37 - Epoch: [14][260/352]	Time  0.174 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.8867e-01 (8.7782e-01)	Acc@1  75.78 ( 74.89)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:22:38 - Epoch: [13][230/352]	Time  0.152 ( 0.155)	Data  0.003 ( 0.003)	Loss 9.2287e-01 (8.9529e-01)	Acc@1  71.09 ( 74.37)	Acc@5  93.75 ( 94.51)
03-Mar-22 09:22:38 - Epoch: [14][270/352]	Time  0.156 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.0270e+00 (8.8202e-01)	Acc@1  71.09 ( 74.75)	Acc@5  94.53 ( 94.50)
03-Mar-22 09:22:39 - Epoch: [13][240/352]	Time  0.153 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.9204e-01 (8.9385e-01)	Acc@1  73.44 ( 74.38)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:22:40 - Epoch: [14][280/352]	Time  0.153 ( 0.159)	Data  0.002 ( 0.003)	Loss 7.9068e-01 (8.8201e-01)	Acc@1  80.47 ( 74.74)	Acc@5  95.31 ( 94.50)
03-Mar-22 09:22:41 - Epoch: [13][250/352]	Time  0.158 ( 0.155)	Data  0.002 ( 0.003)	Loss 9.8394e-01 (8.9138e-01)	Acc@1  71.09 ( 74.46)	Acc@5  90.62 ( 94.60)
03-Mar-22 09:22:42 - Epoch: [14][290/352]	Time  0.167 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.0352e+00 (8.8158e-01)	Acc@1  68.75 ( 74.74)	Acc@5  92.97 ( 94.48)
03-Mar-22 09:22:42 - Epoch: [13][260/352]	Time  0.152 ( 0.155)	Data  0.003 ( 0.003)	Loss 9.5089e-01 (8.9086e-01)	Acc@1  67.19 ( 74.47)	Acc@5  93.75 ( 94.58)
03-Mar-22 09:22:43 - Epoch: [14][300/352]	Time  0.151 ( 0.159)	Data  0.002 ( 0.003)	Loss 7.5163e-01 (8.8198e-01)	Acc@1  82.03 ( 74.74)	Acc@5  97.66 ( 94.50)
03-Mar-22 09:22:44 - Epoch: [13][270/352]	Time  0.150 ( 0.154)	Data  0.002 ( 0.003)	Loss 7.8684e-01 (8.9122e-01)	Acc@1  82.03 ( 74.46)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:22:45 - Epoch: [14][310/352]	Time  0.147 ( 0.159)	Data  0.002 ( 0.003)	Loss 9.6638e-01 (8.8343e-01)	Acc@1  71.88 ( 74.69)	Acc@5  94.53 ( 94.49)
03-Mar-22 09:22:45 - Epoch: [13][280/352]	Time  0.149 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.5371e-01 (8.9119e-01)	Acc@1  75.78 ( 74.52)	Acc@5  89.84 ( 94.54)
03-Mar-22 09:22:46 - Epoch: [14][320/352]	Time  0.148 ( 0.158)	Data  0.002 ( 0.003)	Loss 7.6975e-01 (8.8262e-01)	Acc@1  79.69 ( 74.72)	Acc@5  96.09 ( 94.52)
03-Mar-22 09:22:47 - Epoch: [13][290/352]	Time  0.151 ( 0.154)	Data  0.003 ( 0.003)	Loss 8.7343e-01 (8.9014e-01)	Acc@1  74.22 ( 74.52)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:22:48 - Epoch: [14][330/352]	Time  0.147 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.3084e-01 (8.8368e-01)	Acc@1  75.00 ( 74.67)	Acc@5  92.19 ( 94.51)
03-Mar-22 09:22:48 - Epoch: [13][300/352]	Time  0.156 ( 0.154)	Data  0.003 ( 0.003)	Loss 8.7021e-01 (8.9073e-01)	Acc@1  73.44 ( 74.46)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:22:49 - Epoch: [14][340/352]	Time  0.154 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.0820e-01 (8.8326e-01)	Acc@1  76.56 ( 74.70)	Acc@5  96.09 ( 94.51)
03-Mar-22 09:22:50 - Epoch: [13][310/352]	Time  0.138 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.5174e-01 (8.9080e-01)	Acc@1  74.22 ( 74.49)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:22:51 - Epoch: [14][350/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.3704e-01 (8.8342e-01)	Acc@1  71.09 ( 74.68)	Acc@5  91.41 ( 94.50)
03-Mar-22 09:22:51 - Epoch: [13][320/352]	Time  0.101 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.3711e-01 (8.9282e-01)	Acc@1  74.22 ( 74.48)	Acc@5  96.09 ( 94.53)
03-Mar-22 09:22:51 - Test: [ 0/20]	Time  0.391 ( 0.391)	Loss 1.0349e+00 (1.0349e+00)	Acc@1  70.31 ( 70.31)	Acc@5  91.80 ( 91.80)
03-Mar-22 09:22:52 - Test: [10/20]	Time  0.098 ( 0.129)	Loss 7.8009e-01 (9.3339e-01)	Acc@1  76.95 ( 73.47)	Acc@5  96.48 ( 93.61)
03-Mar-22 09:22:53 - Epoch: [13][330/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.7227e-01 (8.9202e-01)	Acc@1  73.44 ( 74.47)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:22:53 -  * Acc@1 73.500 Acc@5 93.660
03-Mar-22 09:22:53 - Best acc at epoch 14: 73.97999572753906
03-Mar-22 09:22:54 - Epoch: [15][  0/352]	Time  0.370 ( 0.370)	Data  0.248 ( 0.248)	Loss 9.5236e-01 (9.5236e-01)	Acc@1  69.53 ( 69.53)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:22:54 - Epoch: [13][340/352]	Time  0.146 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.5438e-01 (8.9275e-01)	Acc@1  76.56 ( 74.47)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:22:55 - Epoch: [15][ 10/352]	Time  0.179 ( 0.177)	Data  0.002 ( 0.025)	Loss 8.9228e-01 (8.6453e-01)	Acc@1  71.88 ( 74.01)	Acc@5  90.62 ( 94.60)
03-Mar-22 09:22:56 - Epoch: [13][350/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.1444e+00 (8.9393e-01)	Acc@1  64.06 ( 74.42)	Acc@5  92.19 ( 94.50)
03-Mar-22 09:22:56 - Test: [ 0/20]	Time  0.342 ( 0.342)	Loss 1.0447e+00 (1.0447e+00)	Acc@1  71.09 ( 71.09)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:22:57 - Epoch: [15][ 20/352]	Time  0.156 ( 0.164)	Data  0.002 ( 0.014)	Loss 9.2573e-01 (8.7684e-01)	Acc@1  71.88 ( 74.48)	Acc@5  92.97 ( 94.75)
03-Mar-22 09:22:57 - Test: [10/20]	Time  0.091 ( 0.122)	Loss 8.2450e-01 (9.5757e-01)	Acc@1  76.17 ( 72.69)	Acc@5  95.31 ( 93.79)
03-Mar-22 09:22:58 -  * Acc@1 72.320 Acc@5 93.600
03-Mar-22 09:22:58 - Best acc at epoch 13: 74.04000091552734
03-Mar-22 09:22:58 - Epoch: [15][ 30/352]	Time  0.104 ( 0.158)	Data  0.002 ( 0.010)	Loss 8.7249e-01 (8.6605e-01)	Acc@1  72.66 ( 75.05)	Acc@5  94.53 ( 94.73)
03-Mar-22 09:22:58 - Epoch: [14][  0/352]	Time  0.362 ( 0.362)	Data  0.217 ( 0.217)	Loss 9.7796e-01 (9.7796e-01)	Acc@1  71.09 ( 71.09)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:23:00 - Epoch: [15][ 40/352]	Time  0.147 ( 0.154)	Data  0.003 ( 0.008)	Loss 8.4790e-01 (8.6487e-01)	Acc@1  71.88 ( 74.68)	Acc@5  93.75 ( 94.93)
03-Mar-22 09:23:00 - Epoch: [14][ 10/352]	Time  0.147 ( 0.168)	Data  0.002 ( 0.022)	Loss 9.0449e-01 (8.4306e-01)	Acc@1  74.22 ( 76.28)	Acc@5  93.75 ( 94.89)
03-Mar-22 09:23:01 - Epoch: [15][ 50/352]	Time  0.154 ( 0.154)	Data  0.003 ( 0.007)	Loss 9.1831e-01 (8.6682e-01)	Acc@1  76.56 ( 74.97)	Acc@5  94.53 ( 94.85)
03-Mar-22 09:23:01 - Epoch: [14][ 20/352]	Time  0.151 ( 0.158)	Data  0.002 ( 0.012)	Loss 9.7377e-01 (8.6214e-01)	Acc@1  65.62 ( 75.00)	Acc@5  95.31 ( 94.64)
03-Mar-22 09:23:03 - Epoch: [14][ 30/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.009)	Loss 9.7395e-01 (8.6759e-01)	Acc@1  72.66 ( 74.77)	Acc@5  92.97 ( 94.76)
03-Mar-22 09:23:03 - Epoch: [15][ 60/352]	Time  0.174 ( 0.155)	Data  0.002 ( 0.006)	Loss 9.7077e-01 (8.7830e-01)	Acc@1  71.09 ( 74.68)	Acc@5  94.53 ( 94.71)
03-Mar-22 09:23:04 - Epoch: [14][ 40/352]	Time  0.152 ( 0.154)	Data  0.002 ( 0.007)	Loss 9.7424e-01 (8.7886e-01)	Acc@1  71.09 ( 74.62)	Acc@5  95.31 ( 94.80)
03-Mar-22 09:23:05 - Epoch: [15][ 70/352]	Time  0.176 ( 0.157)	Data  0.003 ( 0.006)	Loss 8.6683e-01 (8.7890e-01)	Acc@1  75.00 ( 74.65)	Acc@5  94.53 ( 94.70)
03-Mar-22 09:23:06 - Epoch: [14][ 50/352]	Time  0.155 ( 0.153)	Data  0.002 ( 0.006)	Loss 8.9575e-01 (8.7611e-01)	Acc@1  71.88 ( 74.77)	Acc@5  95.31 ( 94.78)
03-Mar-22 09:23:06 - Epoch: [15][ 80/352]	Time  0.158 ( 0.158)	Data  0.002 ( 0.005)	Loss 8.0868e-01 (8.7170e-01)	Acc@1  80.47 ( 74.88)	Acc@5  92.97 ( 94.78)
03-Mar-22 09:23:07 - Epoch: [14][ 60/352]	Time  0.152 ( 0.153)	Data  0.002 ( 0.006)	Loss 1.0939e+00 (8.8148e-01)	Acc@1  68.75 ( 74.77)	Acc@5  91.41 ( 94.67)
03-Mar-22 09:23:08 - Epoch: [15][ 90/352]	Time  0.184 ( 0.158)	Data  0.003 ( 0.005)	Loss 9.7391e-01 (8.7496e-01)	Acc@1  73.44 ( 74.79)	Acc@5  92.19 ( 94.71)
03-Mar-22 09:23:09 - Epoch: [14][ 70/352]	Time  0.147 ( 0.152)	Data  0.002 ( 0.005)	Loss 8.6365e-01 (8.8539e-01)	Acc@1  73.44 ( 74.68)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:23:10 - Epoch: [15][100/352]	Time  0.171 ( 0.160)	Data  0.002 ( 0.005)	Loss 7.0274e-01 (8.7191e-01)	Acc@1  80.47 ( 74.91)	Acc@5  95.31 ( 94.72)
03-Mar-22 09:23:10 - Epoch: [14][ 80/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.005)	Loss 8.1793e-01 (8.8442e-01)	Acc@1  74.22 ( 74.68)	Acc@5  93.75 ( 94.58)
03-Mar-22 09:23:11 - Epoch: [15][110/352]	Time  0.174 ( 0.160)	Data  0.002 ( 0.005)	Loss 8.5418e-01 (8.7887e-01)	Acc@1  71.88 ( 74.72)	Acc@5  94.53 ( 94.71)
03-Mar-22 09:23:12 - Epoch: [14][ 90/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.005)	Loss 8.1648e-01 (8.8125e-01)	Acc@1  75.78 ( 74.78)	Acc@5  93.75 ( 94.63)
03-Mar-22 09:23:13 - Epoch: [15][120/352]	Time  0.174 ( 0.161)	Data  0.003 ( 0.004)	Loss 9.0751e-01 (8.8329e-01)	Acc@1  73.44 ( 74.49)	Acc@5  94.53 ( 94.67)
03-Mar-22 09:23:13 - Epoch: [14][100/352]	Time  0.152 ( 0.150)	Data  0.002 ( 0.004)	Loss 7.7359e-01 (8.7997e-01)	Acc@1  76.56 ( 74.74)	Acc@5  96.88 ( 94.66)
03-Mar-22 09:23:15 - Epoch: [15][130/352]	Time  0.151 ( 0.161)	Data  0.002 ( 0.004)	Loss 9.3772e-01 (8.8076e-01)	Acc@1  71.88 ( 74.57)	Acc@5  92.97 ( 94.67)
03-Mar-22 09:23:15 - Epoch: [14][110/352]	Time  0.129 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.6822e-01 (8.8038e-01)	Acc@1  75.00 ( 74.73)	Acc@5  94.53 ( 94.66)
03-Mar-22 09:23:16 - Epoch: [14][120/352]	Time  0.152 ( 0.150)	Data  0.002 ( 0.004)	Loss 7.7510e-01 (8.8107e-01)	Acc@1  75.78 ( 74.61)	Acc@5  96.88 ( 94.71)
03-Mar-22 09:23:16 - Epoch: [15][140/352]	Time  0.172 ( 0.162)	Data  0.002 ( 0.004)	Loss 8.8046e-01 (8.7980e-01)	Acc@1  78.91 ( 74.61)	Acc@5  92.19 ( 94.68)
03-Mar-22 09:23:18 - Epoch: [14][130/352]	Time  0.146 ( 0.150)	Data  0.002 ( 0.004)	Loss 6.9607e-01 (8.8431e-01)	Acc@1  79.69 ( 74.51)	Acc@5  96.88 ( 94.66)
03-Mar-22 09:23:18 - Epoch: [15][150/352]	Time  0.175 ( 0.163)	Data  0.002 ( 0.004)	Loss 9.6942e-01 (8.7977e-01)	Acc@1  73.44 ( 74.67)	Acc@5  96.09 ( 94.64)
03-Mar-22 09:23:19 - Epoch: [14][140/352]	Time  0.158 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.5772e-01 (8.8811e-01)	Acc@1  70.31 ( 74.42)	Acc@5  94.53 ( 94.65)
03-Mar-22 09:23:20 - Epoch: [15][160/352]	Time  0.177 ( 0.163)	Data  0.002 ( 0.004)	Loss 1.0684e+00 (8.8044e-01)	Acc@1  74.22 ( 74.73)	Acc@5  92.19 ( 94.59)
03-Mar-22 09:23:21 - Epoch: [14][150/352]	Time  0.154 ( 0.150)	Data  0.002 ( 0.004)	Loss 7.8958e-01 (8.8797e-01)	Acc@1  80.47 ( 74.41)	Acc@5  95.31 ( 94.65)
03-Mar-22 09:23:21 - Epoch: [15][170/352]	Time  0.152 ( 0.163)	Data  0.003 ( 0.004)	Loss 8.6777e-01 (8.7945e-01)	Acc@1  75.00 ( 74.78)	Acc@5  93.75 ( 94.60)
03-Mar-22 09:23:22 - Epoch: [14][160/352]	Time  0.149 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.2450e-01 (8.8834e-01)	Acc@1  77.34 ( 74.45)	Acc@5  93.75 ( 94.63)
03-Mar-22 09:23:23 - Epoch: [15][180/352]	Time  0.178 ( 0.163)	Data  0.003 ( 0.004)	Loss 8.6530e-01 (8.7735e-01)	Acc@1  73.44 ( 74.88)	Acc@5  96.09 ( 94.61)
03-Mar-22 09:23:24 - Epoch: [14][170/352]	Time  0.136 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.9865e-01 (8.8693e-01)	Acc@1  72.66 ( 74.43)	Acc@5  94.53 ( 94.64)
03-Mar-22 09:23:25 - Epoch: [15][190/352]	Time  0.171 ( 0.163)	Data  0.002 ( 0.004)	Loss 8.7461e-01 (8.7844e-01)	Acc@1  79.69 ( 74.83)	Acc@5  94.53 ( 94.63)
03-Mar-22 09:23:25 - Epoch: [14][180/352]	Time  0.152 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0972e+00 (8.8598e-01)	Acc@1  73.44 ( 74.55)	Acc@5  92.19 ( 94.58)
03-Mar-22 09:23:26 - Epoch: [15][200/352]	Time  0.150 ( 0.163)	Data  0.002 ( 0.004)	Loss 8.1016e-01 (8.7546e-01)	Acc@1  78.91 ( 74.95)	Acc@5  93.75 ( 94.62)
03-Mar-22 09:23:27 - Epoch: [14][190/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.2212e-01 (8.8475e-01)	Acc@1  79.69 ( 74.66)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:23:28 - Epoch: [15][210/352]	Time  0.180 ( 0.163)	Data  0.003 ( 0.004)	Loss 9.1318e-01 (8.7666e-01)	Acc@1  71.09 ( 74.89)	Acc@5  95.31 ( 94.63)
03-Mar-22 09:23:28 - Epoch: [14][200/352]	Time  0.131 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.6906e-01 (8.8428e-01)	Acc@1  78.91 ( 74.70)	Acc@5  94.53 ( 94.55)
03-Mar-22 09:23:29 - Epoch: [14][210/352]	Time  0.125 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.8048e-01 (8.8514e-01)	Acc@1  76.56 ( 74.67)	Acc@5  96.09 ( 94.54)
03-Mar-22 09:23:30 - Epoch: [15][220/352]	Time  0.169 ( 0.164)	Data  0.002 ( 0.004)	Loss 8.7034e-01 (8.7569e-01)	Acc@1  75.00 ( 74.95)	Acc@5  96.88 ( 94.64)
03-Mar-22 09:23:31 - Epoch: [14][220/352]	Time  0.153 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.8355e-01 (8.8799e-01)	Acc@1  75.78 ( 74.59)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:23:31 - Epoch: [15][230/352]	Time  0.134 ( 0.164)	Data  0.002 ( 0.003)	Loss 9.2487e-01 (8.7630e-01)	Acc@1  71.88 ( 74.88)	Acc@5  93.75 ( 94.61)
03-Mar-22 09:23:32 - Epoch: [14][230/352]	Time  0.150 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.5373e-01 (8.8824e-01)	Acc@1  82.03 ( 74.57)	Acc@5  96.09 ( 94.51)
03-Mar-22 09:23:33 - Epoch: [15][240/352]	Time  0.140 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.8404e-01 (8.7769e-01)	Acc@1  75.00 ( 74.86)	Acc@5  96.09 ( 94.56)
03-Mar-22 09:23:34 - Epoch: [14][240/352]	Time  0.133 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.3128e-01 (8.9027e-01)	Acc@1  75.78 ( 74.53)	Acc@5  93.75 ( 94.43)
03-Mar-22 09:23:34 - Epoch: [15][250/352]	Time  0.160 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.6352e-01 (8.7805e-01)	Acc@1  78.12 ( 74.88)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:23:35 - Epoch: [14][250/352]	Time  0.133 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.6110e-01 (8.9180e-01)	Acc@1  66.41 ( 74.45)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:23:36 - Epoch: [15][260/352]	Time  0.146 ( 0.162)	Data  0.003 ( 0.003)	Loss 7.3959e-01 (8.7808e-01)	Acc@1  77.34 ( 74.84)	Acc@5  96.09 ( 94.55)
03-Mar-22 09:23:37 - Epoch: [14][260/352]	Time  0.144 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.1939e-01 (8.8999e-01)	Acc@1  78.91 ( 74.50)	Acc@5  93.75 ( 94.45)
03-Mar-22 09:23:37 - Epoch: [15][270/352]	Time  0.146 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.0294e-01 (8.7567e-01)	Acc@1  79.69 ( 74.87)	Acc@5  97.66 ( 94.57)
03-Mar-22 09:23:38 - Epoch: [14][270/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.0448e+00 (8.9143e-01)	Acc@1  72.66 ( 74.46)	Acc@5  94.53 ( 94.48)
03-Mar-22 09:23:39 - Epoch: [15][280/352]	Time  0.147 ( 0.161)	Data  0.003 ( 0.003)	Loss 8.3702e-01 (8.7459e-01)	Acc@1  73.44 ( 74.86)	Acc@5  96.88 ( 94.58)
03-Mar-22 09:23:40 - Epoch: [14][280/352]	Time  0.154 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.1956e-01 (8.8866e-01)	Acc@1  74.22 ( 74.57)	Acc@5  93.75 ( 94.51)
03-Mar-22 09:23:40 - Epoch: [15][290/352]	Time  0.155 ( 0.161)	Data  0.002 ( 0.003)	Loss 8.2256e-01 (8.7593e-01)	Acc@1  77.34 ( 74.81)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:23:41 - Epoch: [14][290/352]	Time  0.136 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.7162e-01 (8.8903e-01)	Acc@1  74.22 ( 74.59)	Acc@5  94.53 ( 94.48)
03-Mar-22 09:23:42 - Epoch: [15][300/352]	Time  0.151 ( 0.160)	Data  0.002 ( 0.003)	Loss 7.0062e-01 (8.7620e-01)	Acc@1  80.47 ( 74.84)	Acc@5  98.44 ( 94.54)
03-Mar-22 09:23:43 - Epoch: [14][300/352]	Time  0.148 ( 0.148)	Data  0.002 ( 0.003)	Loss 7.6372e-01 (8.8791e-01)	Acc@1  81.25 ( 74.63)	Acc@5  96.88 ( 94.51)
03-Mar-22 09:23:43 - Epoch: [15][310/352]	Time  0.145 ( 0.160)	Data  0.002 ( 0.003)	Loss 9.1995e-01 (8.7876e-01)	Acc@1  71.88 ( 74.76)	Acc@5  92.97 ( 94.53)
03-Mar-22 09:23:44 - Epoch: [14][310/352]	Time  0.155 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.3380e-01 (8.8747e-01)	Acc@1  75.78 ( 74.61)	Acc@5  96.09 ( 94.54)
03-Mar-22 09:23:45 - Epoch: [15][320/352]	Time  0.146 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.0295e+00 (8.7952e-01)	Acc@1  72.66 ( 74.73)	Acc@5  92.97 ( 94.53)
03-Mar-22 09:23:46 - Epoch: [14][320/352]	Time  0.147 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.9947e-01 (8.8816e-01)	Acc@1  71.09 ( 74.57)	Acc@5  93.75 ( 94.52)
03-Mar-22 09:23:46 - Epoch: [15][330/352]	Time  0.137 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1663e+00 (8.7869e-01)	Acc@1  63.28 ( 74.76)	Acc@5  93.75 ( 94.56)
03-Mar-22 09:23:47 - Epoch: [14][330/352]	Time  0.148 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.3340e-01 (8.8910e-01)	Acc@1  71.88 ( 74.53)	Acc@5  93.75 ( 94.50)
03-Mar-22 09:23:48 - Epoch: [15][340/352]	Time  0.150 ( 0.159)	Data  0.002 ( 0.003)	Loss 8.3361e-01 (8.8147e-01)	Acc@1  75.00 ( 74.70)	Acc@5  94.53 ( 94.51)
03-Mar-22 09:23:49 - Epoch: [14][340/352]	Time  0.148 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.2984e-01 (8.9034e-01)	Acc@1  75.78 ( 74.48)	Acc@5  92.97 ( 94.50)
03-Mar-22 09:23:49 - Epoch: [15][350/352]	Time  0.146 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.5758e-01 (8.8250e-01)	Acc@1  75.00 ( 74.68)	Acc@5  93.75 ( 94.49)
03-Mar-22 09:23:50 - Test: [ 0/20]	Time  0.350 ( 0.350)	Loss 1.0294e+00 (1.0294e+00)	Acc@1  71.88 ( 71.88)	Acc@5  91.41 ( 91.41)
03-Mar-22 09:23:50 - Epoch: [14][350/352]	Time  0.152 ( 0.148)	Data  0.001 ( 0.003)	Loss 8.5507e-01 (8.9087e-01)	Acc@1  75.78 ( 74.47)	Acc@5  94.53 ( 94.50)
03-Mar-22 09:23:51 - Test: [10/20]	Time  0.092 ( 0.110)	Loss 8.1087e-01 (9.3881e-01)	Acc@1  78.12 ( 73.51)	Acc@5  94.53 ( 93.75)
03-Mar-22 09:23:51 - Test: [ 0/20]	Time  0.342 ( 0.342)	Loss 1.0098e+00 (1.0098e+00)	Acc@1  71.09 ( 71.09)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:23:52 -  * Acc@1 73.160 Acc@5 93.860
03-Mar-22 09:23:52 - Test: [10/20]	Time  0.070 ( 0.124)	Loss 8.2507e-01 (9.6389e-01)	Acc@1  75.78 ( 72.37)	Acc@5  96.09 ( 93.08)
03-Mar-22 09:23:52 - Best acc at epoch 15: 73.97999572753906
03-Mar-22 09:23:52 - Epoch: [16][  0/352]	Time  0.410 ( 0.410)	Data  0.253 ( 0.253)	Loss 8.8196e-01 (8.8196e-01)	Acc@1  73.44 ( 73.44)	Acc@5  97.66 ( 97.66)
03-Mar-22 09:23:52 -  * Acc@1 72.440 Acc@5 93.500
03-Mar-22 09:23:52 - Best acc at epoch 14: 74.04000091552734
03-Mar-22 09:23:53 - Epoch: [15][  0/352]	Time  0.359 ( 0.359)	Data  0.233 ( 0.233)	Loss 9.7796e-01 (9.7796e-01)	Acc@1  74.22 ( 74.22)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:23:53 - Epoch: [16][ 10/352]	Time  0.125 ( 0.151)	Data  0.002 ( 0.025)	Loss 1.0805e+00 (8.6919e-01)	Acc@1  70.31 ( 73.93)	Acc@5  92.19 ( 95.95)
03-Mar-22 09:23:54 - Epoch: [15][ 10/352]	Time  0.152 ( 0.173)	Data  0.002 ( 0.024)	Loss 8.1198e-01 (9.4921e-01)	Acc@1  73.44 ( 73.44)	Acc@5  96.09 ( 94.74)
03-Mar-22 09:23:55 - Epoch: [16][ 20/352]	Time  0.151 ( 0.149)	Data  0.002 ( 0.014)	Loss 7.8318e-01 (8.7292e-01)	Acc@1  79.69 ( 74.48)	Acc@5  95.31 ( 95.28)
03-Mar-22 09:23:56 - Epoch: [15][ 20/352]	Time  0.134 ( 0.163)	Data  0.002 ( 0.014)	Loss 9.1968e-01 (9.0608e-01)	Acc@1  75.00 ( 74.59)	Acc@5  94.53 ( 94.68)
03-Mar-22 09:23:57 - Epoch: [16][ 30/352]	Time  0.178 ( 0.157)	Data  0.002 ( 0.010)	Loss 9.4570e-01 (8.5686e-01)	Acc@1  71.88 ( 74.97)	Acc@5  92.19 ( 95.26)
03-Mar-22 09:23:57 - Epoch: [15][ 30/352]	Time  0.121 ( 0.158)	Data  0.002 ( 0.010)	Loss 9.6843e-01 (9.1072e-01)	Acc@1  74.22 ( 74.29)	Acc@5  92.19 ( 94.41)
03-Mar-22 09:23:58 - Epoch: [16][ 40/352]	Time  0.173 ( 0.160)	Data  0.002 ( 0.008)	Loss 8.9035e-01 (8.6147e-01)	Acc@1  76.56 ( 75.08)	Acc@5  92.97 ( 95.16)
03-Mar-22 09:23:59 - Epoch: [15][ 40/352]	Time  0.146 ( 0.156)	Data  0.002 ( 0.008)	Loss 7.6510e-01 (8.8979e-01)	Acc@1  74.22 ( 74.87)	Acc@5  97.66 ( 94.47)
03-Mar-22 09:24:00 - Epoch: [16][ 50/352]	Time  0.172 ( 0.162)	Data  0.002 ( 0.007)	Loss 9.6459e-01 (8.6729e-01)	Acc@1  71.09 ( 74.69)	Acc@5  91.41 ( 95.02)
03-Mar-22 09:24:00 - Epoch: [15][ 50/352]	Time  0.166 ( 0.155)	Data  0.002 ( 0.007)	Loss 7.3345e-01 (8.8841e-01)	Acc@1  78.12 ( 74.75)	Acc@5  96.09 ( 94.42)
03-Mar-22 09:24:02 - Epoch: [16][ 60/352]	Time  0.173 ( 0.162)	Data  0.002 ( 0.006)	Loss 8.5571e-01 (8.6712e-01)	Acc@1  74.22 ( 74.77)	Acc@5  96.88 ( 95.12)
03-Mar-22 09:24:02 - Epoch: [15][ 60/352]	Time  0.163 ( 0.155)	Data  0.002 ( 0.006)	Loss 8.7388e-01 (8.9518e-01)	Acc@1  75.00 ( 74.37)	Acc@5  93.75 ( 94.22)
03-Mar-22 09:24:03 - Epoch: [16][ 70/352]	Time  0.151 ( 0.160)	Data  0.002 ( 0.006)	Loss 9.4395e-01 (8.6707e-01)	Acc@1  77.34 ( 74.90)	Acc@5  92.97 ( 94.99)
03-Mar-22 09:24:03 - Epoch: [15][ 70/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.006)	Loss 9.9887e-01 (9.0227e-01)	Acc@1  71.09 ( 74.17)	Acc@5  93.75 ( 94.19)
03-Mar-22 09:24:04 - Epoch: [16][ 80/352]	Time  0.137 ( 0.158)	Data  0.002 ( 0.005)	Loss 8.5255e-01 (8.6222e-01)	Acc@1  74.22 ( 75.09)	Acc@5  94.53 ( 94.99)
03-Mar-22 09:24:05 - Epoch: [15][ 80/352]	Time  0.152 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.0767e-01 (8.9951e-01)	Acc@1  75.00 ( 74.32)	Acc@5  94.53 ( 94.12)
03-Mar-22 09:24:06 - Epoch: [16][ 90/352]	Time  0.145 ( 0.157)	Data  0.002 ( 0.005)	Loss 8.2007e-01 (8.6203e-01)	Acc@1  75.00 ( 74.97)	Acc@5  96.09 ( 95.05)
03-Mar-22 09:24:06 - Epoch: [15][ 90/352]	Time  0.134 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.2555e-01 (9.0134e-01)	Acc@1  76.56 ( 74.30)	Acc@5  94.53 ( 94.14)
03-Mar-22 09:24:07 - Epoch: [16][100/352]	Time  0.145 ( 0.156)	Data  0.002 ( 0.005)	Loss 7.6419e-01 (8.6245e-01)	Acc@1  80.47 ( 74.94)	Acc@5  95.31 ( 95.06)
03-Mar-22 09:24:08 - Epoch: [15][100/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.005)	Loss 7.0346e-01 (8.9195e-01)	Acc@1  83.59 ( 74.59)	Acc@5  96.88 ( 94.31)
03-Mar-22 09:24:09 - Epoch: [16][110/352]	Time  0.140 ( 0.155)	Data  0.002 ( 0.005)	Loss 8.2444e-01 (8.6079e-01)	Acc@1  80.47 ( 75.03)	Acc@5  92.97 ( 95.07)
03-Mar-22 09:24:09 - Epoch: [15][110/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.005)	Loss 9.6062e-01 (8.9350e-01)	Acc@1  73.44 ( 74.54)	Acc@5  94.53 ( 94.31)
03-Mar-22 09:24:10 - Epoch: [16][120/352]	Time  0.133 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.4324e-01 (8.6633e-01)	Acc@1  78.12 ( 74.95)	Acc@5  95.31 ( 94.96)
03-Mar-22 09:24:11 - Epoch: [15][120/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.2835e-01 (8.9150e-01)	Acc@1  79.69 ( 74.59)	Acc@5  95.31 ( 94.32)
03-Mar-22 09:24:12 - Epoch: [16][130/352]	Time  0.148 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.9281e-01 (8.6551e-01)	Acc@1  74.22 ( 75.02)	Acc@5  92.97 ( 94.91)
03-Mar-22 09:24:12 - Epoch: [15][130/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.004)	Loss 6.2772e-01 (8.9087e-01)	Acc@1  82.81 ( 74.62)	Acc@5  96.09 ( 94.32)
03-Mar-22 09:24:13 - Epoch: [16][140/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.7306e-01 (8.6763e-01)	Acc@1  67.97 ( 74.88)	Acc@5  97.66 ( 94.89)
03-Mar-22 09:24:14 - Epoch: [15][140/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0233e+00 (8.8924e-01)	Acc@1  70.31 ( 74.69)	Acc@5  92.97 ( 94.43)
03-Mar-22 09:24:15 - Epoch: [16][150/352]	Time  0.152 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.7068e-01 (8.7138e-01)	Acc@1  75.00 ( 74.74)	Acc@5  95.31 ( 94.85)
03-Mar-22 09:24:15 - Epoch: [15][150/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.5411e-01 (8.9394e-01)	Acc@1  77.34 ( 74.59)	Acc@5  92.97 ( 94.30)
03-Mar-22 09:24:16 - Epoch: [16][160/352]	Time  0.156 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.1162e-01 (8.7012e-01)	Acc@1  71.09 ( 74.67)	Acc@5  96.88 ( 94.91)
03-Mar-22 09:24:17 - Epoch: [15][160/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.004)	Loss 7.9736e-01 (8.9266e-01)	Acc@1  78.12 ( 74.59)	Acc@5  96.88 ( 94.32)
03-Mar-22 09:24:18 - Epoch: [16][170/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.5619e-01 (8.7097e-01)	Acc@1  74.22 ( 74.72)	Acc@5  93.75 ( 94.86)
03-Mar-22 09:24:18 - Epoch: [15][170/352]	Time  0.155 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.2092e-01 (8.9193e-01)	Acc@1  78.12 ( 74.63)	Acc@5  92.97 ( 94.36)
03-Mar-22 09:24:19 - Epoch: [16][180/352]	Time  0.163 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.4703e-01 (8.7254e-01)	Acc@1  77.34 ( 74.74)	Acc@5  95.31 ( 94.85)
03-Mar-22 09:24:20 - Epoch: [15][180/352]	Time  0.143 ( 0.151)	Data  0.002 ( 0.004)	Loss 7.7122e-01 (8.9260e-01)	Acc@1  75.00 ( 74.60)	Acc@5  96.09 ( 94.33)
03-Mar-22 09:24:21 - Epoch: [16][190/352]	Time  0.155 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.0010e-01 (8.7524e-01)	Acc@1  78.91 ( 74.71)	Acc@5  93.75 ( 94.82)
03-Mar-22 09:24:21 - Epoch: [15][190/352]	Time  0.154 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.2043e-01 (8.9544e-01)	Acc@1  75.78 ( 74.52)	Acc@5  95.31 ( 94.32)
03-Mar-22 09:24:23 - Epoch: [16][200/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.2576e-01 (8.7424e-01)	Acc@1  79.69 ( 74.74)	Acc@5  97.66 ( 94.84)
03-Mar-22 09:24:23 - Epoch: [15][200/352]	Time  0.128 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.4368e-01 (8.9356e-01)	Acc@1  71.09 ( 74.52)	Acc@5  94.53 ( 94.38)
03-Mar-22 09:24:24 - Epoch: [16][210/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.3161e-01 (8.7483e-01)	Acc@1  79.69 ( 74.69)	Acc@5  93.75 ( 94.85)
03-Mar-22 09:24:24 - Epoch: [15][210/352]	Time  0.146 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.7270e-01 (8.9263e-01)	Acc@1  71.09 ( 74.49)	Acc@5  96.88 ( 94.40)
03-Mar-22 09:24:26 - Epoch: [16][220/352]	Time  0.162 ( 0.154)	Data  0.003 ( 0.003)	Loss 8.6713e-01 (8.7662e-01)	Acc@1  80.47 ( 74.68)	Acc@5  95.31 ( 94.80)
03-Mar-22 09:24:26 - Epoch: [15][220/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.1268e-01 (8.9338e-01)	Acc@1  75.00 ( 74.49)	Acc@5  91.41 ( 94.39)
03-Mar-22 09:24:27 - Epoch: [15][230/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.1817e-01 (8.9239e-01)	Acc@1  71.88 ( 74.51)	Acc@5  92.97 ( 94.42)
03-Mar-22 09:24:27 - Epoch: [16][230/352]	Time  0.159 ( 0.154)	Data  0.002 ( 0.003)	Loss 7.6810e-01 (8.7963e-01)	Acc@1  76.56 ( 74.58)	Acc@5  97.66 ( 94.75)
03-Mar-22 09:24:29 - Epoch: [15][240/352]	Time  0.150 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.6746e-01 (8.9206e-01)	Acc@1  76.56 ( 74.50)	Acc@5  95.31 ( 94.43)
03-Mar-22 09:24:29 - Epoch: [16][240/352]	Time  0.156 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.5027e-01 (8.8119e-01)	Acc@1  78.91 ( 74.49)	Acc@5  97.66 ( 94.73)
03-Mar-22 09:24:30 - Epoch: [15][250/352]	Time  0.150 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.7966e-01 (8.9308e-01)	Acc@1  70.31 ( 74.44)	Acc@5  92.19 ( 94.44)
03-Mar-22 09:24:30 - Epoch: [16][250/352]	Time  0.151 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.3286e-01 (8.8006e-01)	Acc@1  75.78 ( 74.51)	Acc@5  93.75 ( 94.70)
03-Mar-22 09:24:31 - Epoch: [15][260/352]	Time  0.136 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1308e+00 (8.9480e-01)	Acc@1  71.09 ( 74.41)	Acc@5  89.06 ( 94.42)
03-Mar-22 09:24:32 - Epoch: [16][260/352]	Time  0.158 ( 0.154)	Data  0.002 ( 0.003)	Loss 6.7397e-01 (8.7933e-01)	Acc@1  78.91 ( 74.52)	Acc@5  97.66 ( 94.73)
03-Mar-22 09:24:33 - Epoch: [15][270/352]	Time  0.136 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.7933e-01 (8.9367e-01)	Acc@1  78.12 ( 74.42)	Acc@5  93.75 ( 94.42)
03-Mar-22 09:24:33 - Epoch: [16][270/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.6610e-01 (8.7986e-01)	Acc@1  69.53 ( 74.48)	Acc@5  94.53 ( 94.74)
03-Mar-22 09:24:34 - Epoch: [15][280/352]	Time  0.138 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.6203e-01 (8.9359e-01)	Acc@1  73.44 ( 74.45)	Acc@5  94.53 ( 94.41)
03-Mar-22 09:24:35 - Epoch: [16][280/352]	Time  0.160 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0686e+00 (8.8257e-01)	Acc@1  73.44 ( 74.42)	Acc@5  89.84 ( 94.67)
03-Mar-22 09:24:36 - Epoch: [15][290/352]	Time  0.166 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.2319e-01 (8.9165e-01)	Acc@1  78.91 ( 74.53)	Acc@5  94.53 ( 94.42)
03-Mar-22 09:24:36 - Epoch: [16][290/352]	Time  0.161 ( 0.154)	Data  0.003 ( 0.003)	Loss 9.7762e-01 (8.8415e-01)	Acc@1  67.19 ( 74.36)	Acc@5  92.97 ( 94.65)
03-Mar-22 09:24:37 - Epoch: [15][300/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.3494e-01 (8.8995e-01)	Acc@1  80.47 ( 74.57)	Acc@5  96.09 ( 94.44)
03-Mar-22 09:24:38 - Epoch: [16][300/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.2638e-01 (8.8477e-01)	Acc@1  71.88 ( 74.34)	Acc@5  95.31 ( 94.65)
03-Mar-22 09:24:39 - Epoch: [15][310/352]	Time  0.120 ( 0.149)	Data  0.001 ( 0.003)	Loss 8.6068e-01 (8.8908e-01)	Acc@1  70.31 ( 74.58)	Acc@5  95.31 ( 94.43)
03-Mar-22 09:24:40 - Epoch: [16][310/352]	Time  0.156 ( 0.154)	Data  0.003 ( 0.003)	Loss 1.0012e+00 (8.8306e-01)	Acc@1  72.66 ( 74.39)	Acc@5  92.19 ( 94.63)
03-Mar-22 09:24:40 - Epoch: [15][320/352]	Time  0.138 ( 0.149)	Data  0.001 ( 0.003)	Loss 8.1665e-01 (8.8725e-01)	Acc@1  76.56 ( 74.63)	Acc@5  94.53 ( 94.45)
03-Mar-22 09:24:41 - Epoch: [16][320/352]	Time  0.159 ( 0.154)	Data  0.002 ( 0.003)	Loss 7.4952e-01 (8.8240e-01)	Acc@1  75.00 ( 74.39)	Acc@5  96.09 ( 94.64)
03-Mar-22 09:24:42 - Epoch: [15][330/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0520e+00 (8.8760e-01)	Acc@1  71.88 ( 74.61)	Acc@5  92.19 ( 94.44)
03-Mar-22 09:24:43 - Epoch: [16][330/352]	Time  0.158 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0425e+00 (8.8440e-01)	Acc@1  70.31 ( 74.34)	Acc@5  92.19 ( 94.61)
03-Mar-22 09:24:43 - Epoch: [15][340/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.5903e-01 (8.8598e-01)	Acc@1  78.12 ( 74.66)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:24:44 - Epoch: [16][340/352]	Time  0.150 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.2861e-01 (8.8525e-01)	Acc@1  75.78 ( 74.34)	Acc@5  92.97 ( 94.60)
03-Mar-22 09:24:45 - Epoch: [15][350/352]	Time  0.137 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0202e+00 (8.8741e-01)	Acc@1  68.75 ( 74.62)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:24:45 - Test: [ 0/20]	Time  0.348 ( 0.348)	Loss 1.0633e+00 (1.0633e+00)	Acc@1  71.09 ( 71.09)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:24:46 - Epoch: [16][350/352]	Time  0.159 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0468e+00 (8.8434e-01)	Acc@1  67.19 ( 74.37)	Acc@5  90.62 ( 94.60)
03-Mar-22 09:24:46 - Test: [ 0/20]	Time  0.351 ( 0.351)	Loss 1.0259e+00 (1.0259e+00)	Acc@1  69.53 ( 69.53)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:24:46 - Test: [10/20]	Time  0.111 ( 0.119)	Loss 8.5694e-01 (9.8052e-01)	Acc@1  75.78 ( 72.90)	Acc@5  95.31 ( 93.15)
03-Mar-22 09:24:47 - Test: [10/20]	Time  0.114 ( 0.130)	Loss 8.1481e-01 (9.5344e-01)	Acc@1  76.56 ( 72.62)	Acc@5  94.92 ( 93.57)
03-Mar-22 09:24:47 -  * Acc@1 73.000 Acc@5 93.440
03-Mar-22 09:24:47 - Best acc at epoch 15: 74.04000091552734
03-Mar-22 09:24:48 - Epoch: [16][  0/352]	Time  0.372 ( 0.372)	Data  0.226 ( 0.226)	Loss 7.3826e-01 (7.3826e-01)	Acc@1  81.25 ( 81.25)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:24:48 -  * Acc@1 72.360 Acc@5 93.700
03-Mar-22 09:24:48 - Best acc at epoch 16: 73.97999572753906
03-Mar-22 09:24:49 - Epoch: [17][  0/352]	Time  0.483 ( 0.483)	Data  0.324 ( 0.324)	Loss 8.3945e-01 (8.3945e-01)	Acc@1  75.78 ( 75.78)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:24:49 - Epoch: [16][ 10/352]	Time  0.142 ( 0.160)	Data  0.002 ( 0.023)	Loss 9.1263e-01 (9.2469e-01)	Acc@1  70.31 ( 73.15)	Acc@5  95.31 ( 93.89)
03-Mar-22 09:24:50 - Epoch: [17][ 10/352]	Time  0.150 ( 0.188)	Data  0.002 ( 0.031)	Loss 9.9491e-01 (8.9155e-01)	Acc@1  70.31 ( 72.87)	Acc@5  90.62 ( 94.11)
03-Mar-22 09:24:51 - Epoch: [16][ 20/352]	Time  0.164 ( 0.158)	Data  0.002 ( 0.013)	Loss 7.5621e-01 (8.4795e-01)	Acc@1  73.44 ( 75.15)	Acc@5  98.44 ( 95.20)
03-Mar-22 09:24:52 - Epoch: [17][ 20/352]	Time  0.138 ( 0.169)	Data  0.002 ( 0.018)	Loss 9.8581e-01 (8.7967e-01)	Acc@1  71.88 ( 74.14)	Acc@5  92.19 ( 94.23)
03-Mar-22 09:24:52 - Epoch: [16][ 30/352]	Time  0.176 ( 0.155)	Data  0.002 ( 0.009)	Loss 7.6091e-01 (8.3765e-01)	Acc@1  78.12 ( 75.81)	Acc@5  96.09 ( 95.19)
03-Mar-22 09:24:53 - Epoch: [17][ 30/352]	Time  0.151 ( 0.162)	Data  0.002 ( 0.013)	Loss 1.0799e+00 (8.7114e-01)	Acc@1  69.53 ( 74.57)	Acc@5  89.84 ( 94.30)
03-Mar-22 09:24:54 - Epoch: [16][ 40/352]	Time  0.137 ( 0.153)	Data  0.002 ( 0.008)	Loss 8.8388e-01 (8.4505e-01)	Acc@1  75.00 ( 75.50)	Acc@5  96.09 ( 95.12)
03-Mar-22 09:24:55 - Epoch: [17][ 40/352]	Time  0.133 ( 0.158)	Data  0.002 ( 0.010)	Loss 8.4523e-01 (8.6467e-01)	Acc@1  78.91 ( 74.92)	Acc@5  93.75 ( 94.46)
03-Mar-22 09:24:55 - Epoch: [16][ 50/352]	Time  0.166 ( 0.152)	Data  0.002 ( 0.007)	Loss 8.4595e-01 (8.6170e-01)	Acc@1  75.00 ( 74.92)	Acc@5  92.97 ( 94.87)
03-Mar-22 09:24:56 - Epoch: [17][ 50/352]	Time  0.148 ( 0.155)	Data  0.002 ( 0.008)	Loss 1.0092e+00 (8.6489e-01)	Acc@1  69.53 ( 75.26)	Acc@5  95.31 ( 94.47)
03-Mar-22 09:24:57 - Epoch: [16][ 60/352]	Time  0.147 ( 0.152)	Data  0.002 ( 0.006)	Loss 7.7932e-01 (8.6148e-01)	Acc@1  75.00 ( 74.81)	Acc@5  98.44 ( 94.97)
03-Mar-22 09:24:58 - Epoch: [17][ 60/352]	Time  0.140 ( 0.154)	Data  0.002 ( 0.007)	Loss 9.5979e-01 (8.7185e-01)	Acc@1  70.31 ( 74.78)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:24:58 - Epoch: [16][ 70/352]	Time  0.183 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.1790e-01 (8.7106e-01)	Acc@1  75.00 ( 74.44)	Acc@5  96.88 ( 94.87)
03-Mar-22 09:24:59 - Epoch: [17][ 70/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.007)	Loss 8.9597e-01 (8.7139e-01)	Acc@1  71.09 ( 74.68)	Acc@5  94.53 ( 94.55)
03-Mar-22 09:25:00 - Epoch: [16][ 80/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.5261e-01 (8.7311e-01)	Acc@1  73.44 ( 74.34)	Acc@5  92.97 ( 94.82)
03-Mar-22 09:25:00 - Epoch: [17][ 80/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.006)	Loss 8.9616e-01 (8.6708e-01)	Acc@1  71.88 ( 74.78)	Acc@5  91.41 ( 94.57)
03-Mar-22 09:25:02 - Epoch: [16][ 90/352]	Time  0.178 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.2434e-01 (8.7033e-01)	Acc@1  80.47 ( 74.59)	Acc@5  92.97 ( 94.84)
03-Mar-22 09:25:02 - Epoch: [17][ 90/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.006)	Loss 9.6531e-01 (8.6944e-01)	Acc@1  74.22 ( 74.69)	Acc@5  91.41 ( 94.56)
03-Mar-22 09:25:03 - Epoch: [16][100/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.9269e-01 (8.7173e-01)	Acc@1  74.22 ( 74.64)	Acc@5  94.53 ( 94.83)
03-Mar-22 09:25:04 - Epoch: [17][100/352]	Time  0.145 ( 0.154)	Data  0.002 ( 0.005)	Loss 7.7750e-01 (8.7109e-01)	Acc@1  78.12 ( 74.74)	Acc@5  97.66 ( 94.55)
03-Mar-22 09:25:05 - Epoch: [16][110/352]	Time  0.166 ( 0.154)	Data  0.003 ( 0.004)	Loss 9.3058e-01 (8.7702e-01)	Acc@1  72.66 ( 74.58)	Acc@5  92.19 ( 94.74)
03-Mar-22 09:25:05 - Epoch: [17][110/352]	Time  0.152 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.5007e-01 (8.7506e-01)	Acc@1  76.56 ( 74.61)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:25:06 - Epoch: [16][120/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.3186e-01 (8.7549e-01)	Acc@1  78.12 ( 74.66)	Acc@5  95.31 ( 94.75)
03-Mar-22 09:25:07 - Epoch: [17][120/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.3138e-01 (8.7883e-01)	Acc@1  74.22 ( 74.55)	Acc@5  94.53 ( 94.52)
03-Mar-22 09:25:08 - Epoch: [16][130/352]	Time  0.154 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.8037e-01 (8.7311e-01)	Acc@1  74.22 ( 74.74)	Acc@5  95.31 ( 94.76)
03-Mar-22 09:25:08 - Epoch: [17][130/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.9410e-01 (8.8117e-01)	Acc@1  71.88 ( 74.48)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:25:09 - Epoch: [16][140/352]	Time  0.157 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.2446e-01 (8.7346e-01)	Acc@1  77.34 ( 74.68)	Acc@5  98.44 ( 94.71)
03-Mar-22 09:25:10 - Epoch: [17][140/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.1086e-01 (8.8328e-01)	Acc@1  78.91 ( 74.46)	Acc@5  94.53 ( 94.40)
03-Mar-22 09:25:11 - Epoch: [16][150/352]	Time  0.183 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0648e+00 (8.7768e-01)	Acc@1  66.41 ( 74.58)	Acc@5  91.41 ( 94.63)
03-Mar-22 09:25:11 - Epoch: [17][150/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.5483e-01 (8.8199e-01)	Acc@1  81.25 ( 74.54)	Acc@5  95.31 ( 94.39)
03-Mar-22 09:25:12 - Epoch: [16][160/352]	Time  0.173 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.0675e-01 (8.8239e-01)	Acc@1  76.56 ( 74.42)	Acc@5  89.84 ( 94.58)
03-Mar-22 09:25:13 - Epoch: [17][160/352]	Time  0.155 ( 0.152)	Data  0.003 ( 0.004)	Loss 9.3909e-01 (8.7797e-01)	Acc@1  76.56 ( 74.72)	Acc@5  93.75 ( 94.45)
03-Mar-22 09:25:14 - Epoch: [16][170/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.6475e-01 (8.8509e-01)	Acc@1  78.91 ( 74.30)	Acc@5  93.75 ( 94.59)
03-Mar-22 09:25:14 - Epoch: [17][170/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.2335e-01 (8.7605e-01)	Acc@1  75.00 ( 74.87)	Acc@5  95.31 ( 94.48)
03-Mar-22 09:25:15 - Epoch: [16][180/352]	Time  0.168 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.4123e-01 (8.8731e-01)	Acc@1  78.91 ( 74.32)	Acc@5  92.97 ( 94.54)
03-Mar-22 09:25:15 - Epoch: [17][180/352]	Time  0.138 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.6502e-01 (8.7756e-01)	Acc@1  70.31 ( 74.75)	Acc@5  94.53 ( 94.50)
03-Mar-22 09:25:17 - Epoch: [16][190/352]	Time  0.135 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.0419e-01 (8.8651e-01)	Acc@1  78.91 ( 74.40)	Acc@5  96.88 ( 94.58)
03-Mar-22 09:25:17 - Epoch: [17][190/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.1085e-01 (8.7856e-01)	Acc@1  75.00 ( 74.68)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:25:18 - Epoch: [16][200/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.9609e-01 (8.8547e-01)	Acc@1  82.03 ( 74.43)	Acc@5  95.31 ( 94.58)
03-Mar-22 09:25:18 - Epoch: [17][200/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.004)	Loss 7.3284e-01 (8.7876e-01)	Acc@1  82.03 ( 74.67)	Acc@5  96.09 ( 94.51)
03-Mar-22 09:25:20 - Epoch: [16][210/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.9425e-01 (8.8531e-01)	Acc@1  77.34 ( 74.44)	Acc@5  93.75 ( 94.61)
03-Mar-22 09:25:20 - Epoch: [17][210/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.9682e-01 (8.7954e-01)	Acc@1  73.44 ( 74.68)	Acc@5  95.31 ( 94.51)
03-Mar-22 09:25:21 - Epoch: [16][220/352]	Time  0.164 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.0394e-01 (8.8520e-01)	Acc@1  74.22 ( 74.44)	Acc@5  94.53 ( 94.61)
03-Mar-22 09:25:22 - Epoch: [17][220/352]	Time  0.156 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.8114e-01 (8.7762e-01)	Acc@1  78.12 ( 74.75)	Acc@5  92.97 ( 94.55)
03-Mar-22 09:25:23 - Epoch: [16][230/352]	Time  0.144 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0487e+00 (8.8536e-01)	Acc@1  67.97 ( 74.44)	Acc@5  95.31 ( 94.62)
03-Mar-22 09:25:23 - Epoch: [17][230/352]	Time  0.159 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0648e+00 (8.7978e-01)	Acc@1  70.31 ( 74.72)	Acc@5  92.97 ( 94.55)
03-Mar-22 09:25:24 - Epoch: [16][240/352]	Time  0.147 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.9914e-01 (8.8726e-01)	Acc@1  68.75 ( 74.40)	Acc@5  93.75 ( 94.59)
03-Mar-22 09:25:25 - Epoch: [17][240/352]	Time  0.144 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.2093e-01 (8.7944e-01)	Acc@1  79.69 ( 74.66)	Acc@5  94.53 ( 94.54)
03-Mar-22 09:25:26 - Epoch: [16][250/352]	Time  0.136 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.0688e-01 (8.8706e-01)	Acc@1  72.66 ( 74.43)	Acc@5  96.88 ( 94.61)
03-Mar-22 09:25:27 - Epoch: [17][250/352]	Time  0.140 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.2882e-01 (8.7946e-01)	Acc@1  75.78 ( 74.63)	Acc@5  97.66 ( 94.57)
03-Mar-22 09:25:27 - Epoch: [16][260/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.1439e-01 (8.8588e-01)	Acc@1  76.56 ( 74.48)	Acc@5  96.88 ( 94.61)
03-Mar-22 09:25:28 - Epoch: [17][260/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.7236e-01 (8.7857e-01)	Acc@1  76.56 ( 74.68)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:25:29 - Epoch: [16][270/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.003)	Loss 6.8510e-01 (8.8575e-01)	Acc@1  81.25 ( 74.51)	Acc@5  95.31 ( 94.63)
03-Mar-22 09:25:29 - Epoch: [17][270/352]	Time  0.147 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.1957e-01 (8.7841e-01)	Acc@1  75.78 ( 74.69)	Acc@5  93.75 ( 94.56)
03-Mar-22 09:25:30 - Epoch: [16][280/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.6808e-01 (8.8618e-01)	Acc@1  71.88 ( 74.49)	Acc@5  94.53 ( 94.62)
03-Mar-22 09:25:31 - Epoch: [17][280/352]	Time  0.161 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0206e+00 (8.7757e-01)	Acc@1  65.62 ( 74.70)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:25:32 - Epoch: [16][290/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.5867e-01 (8.8635e-01)	Acc@1  71.88 ( 74.46)	Acc@5  96.09 ( 94.64)
03-Mar-22 09:25:32 - Epoch: [17][290/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.0787e-01 (8.7790e-01)	Acc@1  75.78 ( 74.67)	Acc@5  96.09 ( 94.52)
03-Mar-22 09:25:33 - Epoch: [16][300/352]	Time  0.147 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0205e+00 (8.8662e-01)	Acc@1  64.06 ( 74.44)	Acc@5  94.53 ( 94.64)
03-Mar-22 09:25:34 - Epoch: [17][300/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.9916e-01 (8.7848e-01)	Acc@1  77.34 ( 74.66)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:25:35 - Epoch: [16][310/352]	Time  0.136 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0078e+00 (8.8756e-01)	Acc@1  71.88 ( 74.43)	Acc@5  92.97 ( 94.64)
03-Mar-22 09:25:35 - Epoch: [17][310/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.8128e-01 (8.7830e-01)	Acc@1  72.66 ( 74.65)	Acc@5  96.09 ( 94.53)
03-Mar-22 09:25:36 - Epoch: [16][320/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.8482e-01 (8.8798e-01)	Acc@1  70.31 ( 74.39)	Acc@5  95.31 ( 94.62)
03-Mar-22 09:25:37 - Epoch: [17][320/352]	Time  0.147 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.2900e-01 (8.7638e-01)	Acc@1  75.78 ( 74.69)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:25:38 - Epoch: [16][330/352]	Time  0.133 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.2942e-01 (8.8734e-01)	Acc@1  76.56 ( 74.41)	Acc@5  95.31 ( 94.65)
03-Mar-22 09:25:38 - Epoch: [17][330/352]	Time  0.158 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.0293e-01 (8.7620e-01)	Acc@1  78.12 ( 74.66)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:25:39 - Epoch: [16][340/352]	Time  0.154 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.8980e-01 (8.8571e-01)	Acc@1  80.47 ( 74.47)	Acc@5  93.75 ( 94.67)
03-Mar-22 09:25:40 - Epoch: [17][340/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0015e+00 (8.7759e-01)	Acc@1  68.75 ( 74.66)	Acc@5  92.97 ( 94.54)
03-Mar-22 09:25:40 - Epoch: [16][350/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3428e-01 (8.8414e-01)	Acc@1  71.88 ( 74.46)	Acc@5  95.31 ( 94.72)
03-Mar-22 09:25:41 - Test: [ 0/20]	Time  0.344 ( 0.344)	Loss 9.9083e-01 (9.9083e-01)	Acc@1  70.31 ( 70.31)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:25:41 - Epoch: [17][350/352]	Time  0.144 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3540e-01 (8.7667e-01)	Acc@1  77.34 ( 74.68)	Acc@5  93.75 ( 94.54)
03-Mar-22 09:25:42 - Test: [ 0/20]	Time  0.415 ( 0.415)	Loss 1.0904e+00 (1.0904e+00)	Acc@1  68.36 ( 68.36)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:25:42 - Test: [10/20]	Time  0.081 ( 0.109)	Loss 8.1155e-01 (9.3823e-01)	Acc@1  76.17 ( 73.15)	Acc@5  94.92 ( 93.39)
03-Mar-22 09:25:43 - Test: [10/20]	Time  0.089 ( 0.136)	Loss 8.1682e-01 (9.4460e-01)	Acc@1  77.73 ( 73.33)	Acc@5  96.09 ( 93.75)
03-Mar-22 09:25:43 -  * Acc@1 73.180 Acc@5 93.540
03-Mar-22 09:25:43 - Best acc at epoch 16: 74.04000091552734
03-Mar-22 09:25:44 - Epoch: [17][  0/352]	Time  0.491 ( 0.491)	Data  0.343 ( 0.343)	Loss 6.9856e-01 (6.9856e-01)	Acc@1  79.69 ( 79.69)	Acc@5  97.66 ( 97.66)
03-Mar-22 09:25:44 -  * Acc@1 73.340 Acc@5 93.920
03-Mar-22 09:25:44 - Best acc at epoch 17: 73.97999572753906
03-Mar-22 09:25:44 - Epoch: [18][  0/352]	Time  0.383 ( 0.383)	Data  0.236 ( 0.236)	Loss 8.7629e-01 (8.7629e-01)	Acc@1  75.00 ( 75.00)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:25:45 - Epoch: [17][ 10/352]	Time  0.158 ( 0.170)	Data  0.002 ( 0.033)	Loss 8.3906e-01 (8.6730e-01)	Acc@1  71.09 ( 74.50)	Acc@5  96.09 ( 94.53)
03-Mar-22 09:25:46 - Epoch: [18][ 10/352]	Time  0.201 ( 0.207)	Data  0.003 ( 0.024)	Loss 8.9050e-01 (8.8507e-01)	Acc@1  69.53 ( 74.36)	Acc@5  97.66 ( 94.25)
03-Mar-22 09:25:46 - Epoch: [17][ 20/352]	Time  0.149 ( 0.160)	Data  0.002 ( 0.018)	Loss 7.7275e-01 (8.8637e-01)	Acc@1  75.00 ( 74.03)	Acc@5  99.22 ( 94.53)
03-Mar-22 09:25:48 - Epoch: [18][ 20/352]	Time  0.169 ( 0.190)	Data  0.002 ( 0.013)	Loss 9.8464e-01 (8.8275e-01)	Acc@1  68.75 ( 74.22)	Acc@5  93.75 ( 94.53)
03-Mar-22 09:25:48 - Epoch: [17][ 30/352]	Time  0.151 ( 0.158)	Data  0.002 ( 0.013)	Loss 7.0990e-01 (8.8466e-01)	Acc@1  77.34 ( 74.07)	Acc@5  96.09 ( 94.28)
03-Mar-22 09:25:49 - Epoch: [17][ 40/352]	Time  0.133 ( 0.156)	Data  0.002 ( 0.010)	Loss 7.7173e-01 (8.7606e-01)	Acc@1  79.69 ( 74.49)	Acc@5  96.88 ( 94.53)
03-Mar-22 09:25:50 - Epoch: [18][ 30/352]	Time  0.188 ( 0.184)	Data  0.002 ( 0.010)	Loss 7.5895e-01 (8.7238e-01)	Acc@1  78.12 ( 74.57)	Acc@5  96.88 ( 94.93)
03-Mar-22 09:25:51 - Epoch: [17][ 50/352]	Time  0.157 ( 0.155)	Data  0.002 ( 0.009)	Loss 8.7668e-01 (8.7469e-01)	Acc@1  80.47 ( 74.53)	Acc@5  92.19 ( 94.70)
03-Mar-22 09:25:51 - Epoch: [18][ 40/352]	Time  0.145 ( 0.179)	Data  0.002 ( 0.008)	Loss 9.0035e-01 (8.7557e-01)	Acc@1  69.53 ( 74.52)	Acc@5  94.53 ( 94.95)
03-Mar-22 09:25:53 - Epoch: [17][ 60/352]	Time  0.155 ( 0.155)	Data  0.003 ( 0.008)	Loss 8.2796e-01 (8.7937e-01)	Acc@1  74.22 ( 74.41)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:25:53 - Epoch: [18][ 50/352]	Time  0.159 ( 0.173)	Data  0.002 ( 0.007)	Loss 7.9641e-01 (8.7293e-01)	Acc@1  77.34 ( 74.92)	Acc@5  96.88 ( 94.96)
03-Mar-22 09:25:54 - Epoch: [17][ 70/352]	Time  0.168 ( 0.156)	Data  0.002 ( 0.007)	Loss 9.7952e-01 (8.8011e-01)	Acc@1  72.66 ( 74.44)	Acc@5  92.97 ( 94.54)
03-Mar-22 09:25:54 - Epoch: [18][ 60/352]	Time  0.152 ( 0.170)	Data  0.002 ( 0.006)	Loss 6.8256e-01 (8.7232e-01)	Acc@1  79.69 ( 74.99)	Acc@5  97.66 ( 94.94)
03-Mar-22 09:25:56 - Epoch: [17][ 80/352]	Time  0.155 ( 0.155)	Data  0.002 ( 0.006)	Loss 7.0781e-01 (8.8146e-01)	Acc@1  78.91 ( 74.59)	Acc@5  96.88 ( 94.48)
03-Mar-22 09:25:56 - Epoch: [18][ 70/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.006)	Loss 1.0875e+00 (8.6853e-01)	Acc@1  66.41 ( 75.10)	Acc@5  93.75 ( 95.05)
03-Mar-22 09:25:57 - Epoch: [18][ 80/352]	Time  0.146 ( 0.165)	Data  0.002 ( 0.005)	Loss 7.9726e-01 (8.6814e-01)	Acc@1  80.47 ( 75.14)	Acc@5  95.31 ( 95.01)
03-Mar-22 09:25:57 - Epoch: [17][ 90/352]	Time  0.182 ( 0.156)	Data  0.002 ( 0.006)	Loss 7.7810e-01 (8.8046e-01)	Acc@1  78.91 ( 74.56)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:25:59 - Epoch: [18][ 90/352]	Time  0.152 ( 0.163)	Data  0.002 ( 0.005)	Loss 9.2226e-01 (8.6216e-01)	Acc@1  70.31 ( 75.31)	Acc@5  98.44 ( 95.12)
03-Mar-22 09:25:59 - Epoch: [17][100/352]	Time  0.153 ( 0.156)	Data  0.002 ( 0.006)	Loss 8.4537e-01 (8.7810e-01)	Acc@1  75.78 ( 74.58)	Acc@5  96.09 ( 94.52)
03-Mar-22 09:26:00 - Epoch: [18][100/352]	Time  0.152 ( 0.162)	Data  0.002 ( 0.005)	Loss 9.9383e-01 (8.6241e-01)	Acc@1  70.31 ( 75.29)	Acc@5  93.75 ( 95.12)
03-Mar-22 09:26:00 - Epoch: [17][110/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.005)	Loss 7.8183e-01 (8.7686e-01)	Acc@1  71.09 ( 74.51)	Acc@5  97.66 ( 94.57)
03-Mar-22 09:26:02 - Epoch: [18][110/352]	Time  0.143 ( 0.161)	Data  0.003 ( 0.004)	Loss 9.7943e-01 (8.6612e-01)	Acc@1  68.75 ( 75.06)	Acc@5  93.75 ( 95.06)
03-Mar-22 09:26:02 - Epoch: [17][120/352]	Time  0.154 ( 0.156)	Data  0.002 ( 0.005)	Loss 8.2652e-01 (8.7613e-01)	Acc@1  73.44 ( 74.44)	Acc@5  94.53 ( 94.63)
03-Mar-22 09:26:03 - Epoch: [18][120/352]	Time  0.136 ( 0.158)	Data  0.002 ( 0.004)	Loss 9.0624e-01 (8.6560e-01)	Acc@1  73.44 ( 75.01)	Acc@5  96.09 ( 95.06)
03-Mar-22 09:26:03 - Epoch: [17][130/352]	Time  0.162 ( 0.156)	Data  0.003 ( 0.005)	Loss 7.9939e-01 (8.8037e-01)	Acc@1  77.34 ( 74.31)	Acc@5  95.31 ( 94.60)
03-Mar-22 09:26:04 - Epoch: [18][130/352]	Time  0.150 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.2790e-01 (8.6673e-01)	Acc@1  70.31 ( 74.77)	Acc@5  96.88 ( 95.06)
03-Mar-22 09:26:05 - Epoch: [17][140/352]	Time  0.146 ( 0.156)	Data  0.002 ( 0.005)	Loss 9.0565e-01 (8.7984e-01)	Acc@1  75.78 ( 74.39)	Acc@5  95.31 ( 94.60)
03-Mar-22 09:26:06 - Epoch: [18][140/352]	Time  0.133 ( 0.157)	Data  0.002 ( 0.004)	Loss 7.6569e-01 (8.6764e-01)	Acc@1  75.78 ( 74.76)	Acc@5  95.31 ( 95.02)
03-Mar-22 09:26:07 - Epoch: [17][150/352]	Time  0.178 ( 0.157)	Data  0.002 ( 0.005)	Loss 9.5004e-01 (8.8071e-01)	Acc@1  75.78 ( 74.41)	Acc@5  93.75 ( 94.60)
03-Mar-22 09:26:07 - Epoch: [18][150/352]	Time  0.146 ( 0.156)	Data  0.002 ( 0.004)	Loss 8.5882e-01 (8.6634e-01)	Acc@1  76.56 ( 74.82)	Acc@5  95.31 ( 94.99)
03-Mar-22 09:26:08 - Epoch: [17][160/352]	Time  0.157 ( 0.157)	Data  0.002 ( 0.004)	Loss 1.0042e+00 (8.8507e-01)	Acc@1  67.97 ( 74.33)	Acc@5  96.09 ( 94.57)
03-Mar-22 09:26:09 - Epoch: [18][160/352]	Time  0.129 ( 0.155)	Data  0.002 ( 0.004)	Loss 8.7354e-01 (8.6712e-01)	Acc@1  77.34 ( 74.87)	Acc@5  93.75 ( 94.98)
03-Mar-22 09:26:10 - Epoch: [17][170/352]	Time  0.153 ( 0.157)	Data  0.003 ( 0.004)	Loss 9.5267e-01 (8.8697e-01)	Acc@1  75.00 ( 74.26)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:26:10 - Epoch: [18][170/352]	Time  0.136 ( 0.154)	Data  0.002 ( 0.004)	Loss 7.3205e-01 (8.6541e-01)	Acc@1  83.59 ( 75.00)	Acc@5  95.31 ( 94.96)
03-Mar-22 09:26:11 - Epoch: [17][180/352]	Time  0.159 ( 0.157)	Data  0.002 ( 0.004)	Loss 7.7695e-01 (8.8597e-01)	Acc@1  78.91 ( 74.27)	Acc@5  96.88 ( 94.60)
03-Mar-22 09:26:12 - Epoch: [18][180/352]	Time  0.146 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0330e+00 (8.6744e-01)	Acc@1  71.88 ( 74.92)	Acc@5  92.19 ( 94.93)
03-Mar-22 09:26:13 - Epoch: [17][190/352]	Time  0.177 ( 0.157)	Data  0.002 ( 0.004)	Loss 6.6256e-01 (8.8468e-01)	Acc@1  82.81 ( 74.35)	Acc@5  97.66 ( 94.62)
03-Mar-22 09:26:13 - Epoch: [18][190/352]	Time  0.143 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.4720e-01 (8.7036e-01)	Acc@1  78.12 ( 74.87)	Acc@5  93.75 ( 94.87)
03-Mar-22 09:26:15 - Epoch: [17][200/352]	Time  0.154 ( 0.157)	Data  0.002 ( 0.004)	Loss 1.0367e+00 (8.8307e-01)	Acc@1  68.75 ( 74.41)	Acc@5  92.19 ( 94.65)
03-Mar-22 09:26:15 - Epoch: [18][200/352]	Time  0.151 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.5403e-01 (8.6897e-01)	Acc@1  73.44 ( 74.93)	Acc@5  95.31 ( 94.87)
03-Mar-22 09:26:16 - Epoch: [18][210/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.1070e-01 (8.6859e-01)	Acc@1  70.31 ( 74.91)	Acc@5  93.75 ( 94.86)
03-Mar-22 09:26:16 - Epoch: [17][210/352]	Time  0.180 ( 0.157)	Data  0.003 ( 0.004)	Loss 7.7017e-01 (8.8262e-01)	Acc@1  77.34 ( 74.37)	Acc@5  95.31 ( 94.66)
03-Mar-22 09:26:18 - Epoch: [18][220/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.9487e-01 (8.6925e-01)	Acc@1  73.44 ( 74.88)	Acc@5  92.19 ( 94.84)
03-Mar-22 09:26:18 - Epoch: [17][220/352]	Time  0.146 ( 0.157)	Data  0.002 ( 0.004)	Loss 9.7436e-01 (8.8061e-01)	Acc@1  71.88 ( 74.47)	Acc@5  92.97 ( 94.69)
03-Mar-22 09:26:19 - Epoch: [18][230/352]	Time  0.146 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.4937e-01 (8.6898e-01)	Acc@1  70.31 ( 74.89)	Acc@5  92.19 ( 94.82)
03-Mar-22 09:26:19 - Epoch: [17][230/352]	Time  0.185 ( 0.157)	Data  0.009 ( 0.004)	Loss 1.0141e+00 (8.8261e-01)	Acc@1  67.97 ( 74.39)	Acc@5  90.62 ( 94.66)
03-Mar-22 09:26:20 - Epoch: [18][240/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.3720e-01 (8.6659e-01)	Acc@1  73.44 ( 74.98)	Acc@5  93.75 ( 94.86)
03-Mar-22 09:26:21 - Epoch: [17][240/352]	Time  0.151 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.6990e-01 (8.8016e-01)	Acc@1  78.91 ( 74.48)	Acc@5  92.97 ( 94.69)
03-Mar-22 09:26:22 - Epoch: [18][250/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.9906e-01 (8.6845e-01)	Acc@1  73.44 ( 74.94)	Acc@5  94.53 ( 94.82)
03-Mar-22 09:26:22 - Epoch: [17][250/352]	Time  0.165 ( 0.157)	Data  0.002 ( 0.004)	Loss 7.6272e-01 (8.7893e-01)	Acc@1  77.34 ( 74.54)	Acc@5  96.88 ( 94.71)
03-Mar-22 09:26:23 - Epoch: [18][260/352]	Time  0.146 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.3754e-01 (8.7106e-01)	Acc@1  71.88 ( 74.87)	Acc@5  94.53 ( 94.79)
03-Mar-22 09:26:24 - Epoch: [17][260/352]	Time  0.151 ( 0.157)	Data  0.002 ( 0.004)	Loss 7.5044e-01 (8.8034e-01)	Acc@1  80.47 ( 74.52)	Acc@5  95.31 ( 94.67)
03-Mar-22 09:26:25 - Epoch: [18][270/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0648e+00 (8.7024e-01)	Acc@1  69.53 ( 74.88)	Acc@5  91.41 ( 94.77)
03-Mar-22 09:26:26 - Epoch: [17][270/352]	Time  0.181 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.5717e-01 (8.8085e-01)	Acc@1  74.22 ( 74.47)	Acc@5  95.31 ( 94.65)
03-Mar-22 09:26:26 - Epoch: [18][280/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.9108e-01 (8.7150e-01)	Acc@1  73.44 ( 74.83)	Acc@5  94.53 ( 94.77)
03-Mar-22 09:26:27 - Epoch: [17][280/352]	Time  0.156 ( 0.157)	Data  0.002 ( 0.004)	Loss 9.4374e-01 (8.8119e-01)	Acc@1  74.22 ( 74.44)	Acc@5  95.31 ( 94.64)
03-Mar-22 09:26:28 - Epoch: [18][290/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.0361e-01 (8.7196e-01)	Acc@1  72.66 ( 74.81)	Acc@5  92.97 ( 94.74)
03-Mar-22 09:26:29 - Epoch: [17][290/352]	Time  0.182 ( 0.157)	Data  0.003 ( 0.004)	Loss 9.9500e-01 (8.8205e-01)	Acc@1  70.31 ( 74.43)	Acc@5  92.19 ( 94.59)
03-Mar-22 09:26:29 - Epoch: [18][300/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.3520e-01 (8.7243e-01)	Acc@1  71.09 ( 74.78)	Acc@5  97.66 ( 94.76)
03-Mar-22 09:26:30 - Epoch: [17][300/352]	Time  0.151 ( 0.157)	Data  0.002 ( 0.004)	Loss 9.0406e-01 (8.8270e-01)	Acc@1  73.44 ( 74.42)	Acc@5  90.62 ( 94.57)
03-Mar-22 09:26:31 - Epoch: [18][310/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 6.9840e-01 (8.7240e-01)	Acc@1  78.12 ( 74.75)	Acc@5  98.44 ( 94.76)
03-Mar-22 09:26:32 - Epoch: [17][310/352]	Time  0.178 ( 0.157)	Data  0.003 ( 0.004)	Loss 7.8743e-01 (8.8143e-01)	Acc@1  78.12 ( 74.46)	Acc@5  96.09 ( 94.58)
03-Mar-22 09:26:32 - Epoch: [18][320/352]	Time  0.135 ( 0.151)	Data  0.001 ( 0.003)	Loss 7.5952e-01 (8.7354e-01)	Acc@1  75.78 ( 74.72)	Acc@5  96.09 ( 94.74)
03-Mar-22 09:26:33 - Epoch: [17][320/352]	Time  0.153 ( 0.157)	Data  0.003 ( 0.004)	Loss 8.3305e-01 (8.8139e-01)	Acc@1  75.78 ( 74.48)	Acc@5  96.09 ( 94.59)
03-Mar-22 09:26:34 - Epoch: [18][330/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.5351e-01 (8.7348e-01)	Acc@1  79.69 ( 74.73)	Acc@5  98.44 ( 94.73)
03-Mar-22 09:26:35 - Epoch: [17][330/352]	Time  0.181 ( 0.156)	Data  0.003 ( 0.003)	Loss 9.2880e-01 (8.8038e-01)	Acc@1  78.12 ( 74.52)	Acc@5  90.62 ( 94.59)
03-Mar-22 09:26:35 - Epoch: [18][340/352]	Time  0.144 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.3345e-01 (8.7221e-01)	Acc@1  75.78 ( 74.73)	Acc@5  92.97 ( 94.75)
03-Mar-22 09:26:36 - Epoch: [17][340/352]	Time  0.151 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.2951e-01 (8.8090e-01)	Acc@1  75.00 ( 74.52)	Acc@5  96.09 ( 94.59)
03-Mar-22 09:26:37 - Epoch: [18][350/352]	Time  0.147 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0200e+00 (8.7307e-01)	Acc@1  72.66 ( 74.70)	Acc@5  92.19 ( 94.75)
03-Mar-22 09:26:37 - Test: [ 0/20]	Time  0.371 ( 0.371)	Loss 9.8515e-01 (9.8515e-01)	Acc@1  73.05 ( 73.05)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:26:38 - Epoch: [17][350/352]	Time  0.173 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.5634e-01 (8.8031e-01)	Acc@1  75.00 ( 74.57)	Acc@5  93.75 ( 94.58)
03-Mar-22 09:26:38 - Test: [10/20]	Time  0.070 ( 0.111)	Loss 8.2861e-01 (9.3327e-01)	Acc@1  74.22 ( 72.90)	Acc@5  96.09 ( 93.61)
03-Mar-22 09:26:38 - Test: [ 0/20]	Time  0.429 ( 0.429)	Loss 1.0368e+00 (1.0368e+00)	Acc@1  70.31 ( 70.31)	Acc@5  91.80 ( 91.80)
03-Mar-22 09:26:39 -  * Acc@1 73.180 Acc@5 93.880
03-Mar-22 09:26:39 - Best acc at epoch 18: 73.97999572753906
03-Mar-22 09:26:39 - Test: [10/20]	Time  0.070 ( 0.128)	Loss 8.5495e-01 (9.4888e-01)	Acc@1  74.61 ( 72.59)	Acc@5  94.14 ( 93.15)
03-Mar-22 09:26:40 - Epoch: [19][  0/352]	Time  0.398 ( 0.398)	Data  0.234 ( 0.234)	Loss 6.8738e-01 (6.8738e-01)	Acc@1  82.03 ( 82.03)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:26:40 -  * Acc@1 72.720 Acc@5 93.320
03-Mar-22 09:26:40 - Best acc at epoch 17: 74.04000091552734
03-Mar-22 09:26:41 - Epoch: [18][  0/352]	Time  0.359 ( 0.359)	Data  0.215 ( 0.215)	Loss 8.7634e-01 (8.7634e-01)	Acc@1  74.22 ( 74.22)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:26:41 - Epoch: [19][ 10/352]	Time  0.150 ( 0.169)	Data  0.002 ( 0.024)	Loss 8.8526e-01 (9.2155e-01)	Acc@1  71.09 ( 73.30)	Acc@5  94.53 ( 93.39)
03-Mar-22 09:26:42 - Epoch: [18][ 10/352]	Time  0.151 ( 0.172)	Data  0.002 ( 0.022)	Loss 8.4519e-01 (8.6045e-01)	Acc@1  74.22 ( 74.86)	Acc@5  96.88 ( 95.10)
03-Mar-22 09:26:43 - Epoch: [19][ 20/352]	Time  0.156 ( 0.161)	Data  0.003 ( 0.014)	Loss 9.5139e-01 (9.2211e-01)	Acc@1  74.22 ( 73.85)	Acc@5  91.41 ( 93.23)
03-Mar-22 09:26:44 - Epoch: [18][ 20/352]	Time  0.149 ( 0.162)	Data  0.002 ( 0.013)	Loss 8.7481e-01 (8.8030e-01)	Acc@1  72.66 ( 75.07)	Acc@5  92.97 ( 94.20)
03-Mar-22 09:26:44 - Epoch: [19][ 30/352]	Time  0.173 ( 0.161)	Data  0.003 ( 0.010)	Loss 7.5414e-01 (9.0301e-01)	Acc@1  79.69 ( 74.50)	Acc@5  96.09 ( 93.65)
03-Mar-22 09:26:45 - Epoch: [18][ 30/352]	Time  0.149 ( 0.159)	Data  0.003 ( 0.009)	Loss 7.9891e-01 (8.5920e-01)	Acc@1  79.69 ( 75.45)	Acc@5  94.53 ( 94.61)
03-Mar-22 09:26:46 - Epoch: [19][ 40/352]	Time  0.173 ( 0.161)	Data  0.003 ( 0.008)	Loss 8.1138e-01 (8.8163e-01)	Acc@1  76.56 ( 75.25)	Acc@5  96.09 ( 93.90)
03-Mar-22 09:26:47 - Epoch: [18][ 40/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.008)	Loss 9.0730e-01 (8.6122e-01)	Acc@1  77.34 ( 75.40)	Acc@5  94.53 ( 94.57)
03-Mar-22 09:26:48 - Epoch: [19][ 50/352]	Time  0.173 ( 0.162)	Data  0.002 ( 0.007)	Loss 7.1900e-01 (8.8482e-01)	Acc@1  75.78 ( 75.11)	Acc@5  97.66 ( 94.01)
03-Mar-22 09:26:48 - Epoch: [18][ 50/352]	Time  0.143 ( 0.155)	Data  0.002 ( 0.007)	Loss 1.0014e+00 (8.7428e-01)	Acc@1  70.31 ( 74.77)	Acc@5  92.19 ( 94.62)
03-Mar-22 09:26:49 - Epoch: [19][ 60/352]	Time  0.131 ( 0.162)	Data  0.002 ( 0.006)	Loss 9.1656e-01 (8.8778e-01)	Acc@1  70.31 ( 74.94)	Acc@5  96.09 ( 94.19)
03-Mar-22 09:26:50 - Epoch: [18][ 60/352]	Time  0.153 ( 0.155)	Data  0.003 ( 0.006)	Loss 8.5310e-01 (8.7065e-01)	Acc@1  76.56 ( 75.04)	Acc@5  96.88 ( 94.63)
03-Mar-22 09:26:51 - Epoch: [19][ 70/352]	Time  0.155 ( 0.161)	Data  0.002 ( 0.006)	Loss 9.7482e-01 (8.8574e-01)	Acc@1  73.44 ( 74.92)	Acc@5  92.19 ( 94.34)
03-Mar-22 09:26:51 - Epoch: [18][ 70/352]	Time  0.147 ( 0.155)	Data  0.002 ( 0.005)	Loss 1.0285e+00 (8.7478e-01)	Acc@1  70.31 ( 74.97)	Acc@5  93.75 ( 94.61)
03-Mar-22 09:26:52 - Epoch: [19][ 80/352]	Time  0.141 ( 0.161)	Data  0.003 ( 0.005)	Loss 9.8899e-01 (8.8323e-01)	Acc@1  71.88 ( 74.96)	Acc@5  92.19 ( 94.34)
03-Mar-22 09:26:53 - Epoch: [18][ 80/352]	Time  0.149 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.1022e-01 (8.7860e-01)	Acc@1  75.00 ( 74.89)	Acc@5  92.97 ( 94.49)
03-Mar-22 09:26:54 - Epoch: [19][ 90/352]	Time  0.147 ( 0.160)	Data  0.002 ( 0.005)	Loss 8.3497e-01 (8.8414e-01)	Acc@1  81.25 ( 74.84)	Acc@5  95.31 ( 94.35)
03-Mar-22 09:26:54 - Epoch: [18][ 90/352]	Time  0.152 ( 0.154)	Data  0.003 ( 0.005)	Loss 7.7329e-01 (8.7421e-01)	Acc@1  78.12 ( 75.03)	Acc@5  96.88 ( 94.55)
03-Mar-22 09:26:56 - Epoch: [19][100/352]	Time  0.143 ( 0.159)	Data  0.002 ( 0.005)	Loss 1.0156e+00 (8.7564e-01)	Acc@1  66.41 ( 74.99)	Acc@5  92.97 ( 94.48)
03-Mar-22 09:26:56 - Epoch: [18][100/352]	Time  0.154 ( 0.154)	Data  0.002 ( 0.005)	Loss 7.9675e-01 (8.7406e-01)	Acc@1  81.25 ( 75.05)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:26:57 - Epoch: [19][110/352]	Time  0.171 ( 0.160)	Data  0.003 ( 0.005)	Loss 8.0971e-01 (8.7890e-01)	Acc@1  76.56 ( 74.75)	Acc@5  96.88 ( 94.45)
03-Mar-22 09:26:58 - Epoch: [18][110/352]	Time  0.155 ( 0.154)	Data  0.003 ( 0.004)	Loss 9.1250e-01 (8.7585e-01)	Acc@1  75.00 ( 74.89)	Acc@5  92.97 ( 94.55)
03-Mar-22 09:26:59 - Epoch: [19][120/352]	Time  0.174 ( 0.161)	Data  0.003 ( 0.004)	Loss 8.8056e-01 (8.8003e-01)	Acc@1  73.44 ( 74.68)	Acc@5  96.09 ( 94.45)
03-Mar-22 09:26:59 - Epoch: [18][120/352]	Time  0.151 ( 0.154)	Data  0.003 ( 0.004)	Loss 1.0010e+00 (8.8164e-01)	Acc@1  73.44 ( 74.72)	Acc@5  92.19 ( 94.52)
03-Mar-22 09:27:00 - Epoch: [19][130/352]	Time  0.158 ( 0.160)	Data  0.002 ( 0.004)	Loss 8.8862e-01 (8.7756e-01)	Acc@1  76.56 ( 74.79)	Acc@5  95.31 ( 94.51)
03-Mar-22 09:27:01 - Epoch: [18][130/352]	Time  0.140 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.9376e-01 (8.8287e-01)	Acc@1  71.88 ( 74.75)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:27:02 - Epoch: [18][140/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.0486e-01 (8.8114e-01)	Acc@1  75.00 ( 74.88)	Acc@5  93.75 ( 94.58)
03-Mar-22 09:27:02 - Epoch: [19][140/352]	Time  0.175 ( 0.162)	Data  0.003 ( 0.004)	Loss 6.7712e-01 (8.7673e-01)	Acc@1  80.47 ( 74.72)	Acc@5  97.66 ( 94.56)
03-Mar-22 09:27:03 - Epoch: [18][150/352]	Time  0.147 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.5827e-01 (8.8111e-01)	Acc@1  69.53 ( 74.87)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:27:04 - Epoch: [19][150/352]	Time  0.171 ( 0.161)	Data  0.002 ( 0.004)	Loss 8.2417e-01 (8.7829e-01)	Acc@1  74.22 ( 74.68)	Acc@5  95.31 ( 94.54)
03-Mar-22 09:27:05 - Epoch: [18][160/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.004)	Loss 7.9389e-01 (8.8097e-01)	Acc@1  75.78 ( 74.75)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:27:06 - Epoch: [19][160/352]	Time  0.176 ( 0.162)	Data  0.003 ( 0.004)	Loss 7.0273e-01 (8.7691e-01)	Acc@1  76.56 ( 74.70)	Acc@5  96.88 ( 94.55)
03-Mar-22 09:27:06 - Epoch: [18][170/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.0626e-01 (8.8375e-01)	Acc@1  75.00 ( 74.68)	Acc@5  95.31 ( 94.54)
03-Mar-22 09:27:07 - Epoch: [19][170/352]	Time  0.149 ( 0.161)	Data  0.002 ( 0.004)	Loss 9.0478e-01 (8.7448e-01)	Acc@1  73.44 ( 74.78)	Acc@5  92.97 ( 94.58)
03-Mar-22 09:27:08 - Epoch: [18][180/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.9724e-01 (8.8290e-01)	Acc@1  75.00 ( 74.76)	Acc@5  93.75 ( 94.54)
03-Mar-22 09:27:09 - Epoch: [19][180/352]	Time  0.181 ( 0.161)	Data  0.003 ( 0.004)	Loss 7.8044e-01 (8.7318e-01)	Acc@1  78.91 ( 74.89)	Acc@5  95.31 ( 94.55)
03-Mar-22 09:27:09 - Epoch: [18][190/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.1106e-01 (8.8528e-01)	Acc@1  75.00 ( 74.67)	Acc@5  92.97 ( 94.51)
03-Mar-22 09:27:10 - Epoch: [19][190/352]	Time  0.171 ( 0.161)	Data  0.002 ( 0.004)	Loss 9.0965e-01 (8.7371e-01)	Acc@1  75.00 ( 74.90)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:27:11 - Epoch: [18][200/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.5389e-01 (8.8417e-01)	Acc@1  75.78 ( 74.69)	Acc@5  94.53 ( 94.52)
03-Mar-22 09:27:12 - Epoch: [19][200/352]	Time  0.172 ( 0.161)	Data  0.002 ( 0.004)	Loss 8.5674e-01 (8.7295e-01)	Acc@1  75.00 ( 74.93)	Acc@5  96.09 ( 94.56)
03-Mar-22 09:27:12 - Epoch: [18][210/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.9420e-01 (8.8291e-01)	Acc@1  73.44 ( 74.73)	Acc@5  92.97 ( 94.52)
03-Mar-22 09:27:14 - Epoch: [19][210/352]	Time  0.147 ( 0.161)	Data  0.002 ( 0.004)	Loss 8.6102e-01 (8.7314e-01)	Acc@1  75.00 ( 74.89)	Acc@5  94.53 ( 94.59)
03-Mar-22 09:27:14 - Epoch: [18][220/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.9145e-01 (8.8484e-01)	Acc@1  75.78 ( 74.65)	Acc@5  97.66 ( 94.52)
03-Mar-22 09:27:15 - Epoch: [19][220/352]	Time  0.170 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.8889e-01 (8.7313e-01)	Acc@1  71.09 ( 74.88)	Acc@5  94.53 ( 94.60)
03-Mar-22 09:27:15 - Epoch: [18][230/352]	Time  0.140 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.6880e-01 (8.8675e-01)	Acc@1  70.31 ( 74.67)	Acc@5  92.97 ( 94.45)
03-Mar-22 09:27:17 - Epoch: [19][230/352]	Time  0.130 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.1298e-01 (8.7313e-01)	Acc@1  78.12 ( 74.86)	Acc@5  95.31 ( 94.60)
03-Mar-22 09:27:17 - Epoch: [18][240/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.8725e-01 (8.8672e-01)	Acc@1  77.34 ( 74.62)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:27:18 - Epoch: [19][240/352]	Time  0.149 ( 0.161)	Data  0.002 ( 0.003)	Loss 8.2392e-01 (8.7323e-01)	Acc@1  78.91 ( 74.89)	Acc@5  92.19 ( 94.57)
03-Mar-22 09:27:18 - Epoch: [18][250/352]	Time  0.154 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.0308e-01 (8.8672e-01)	Acc@1  73.44 ( 74.66)	Acc@5  92.97 ( 94.45)
03-Mar-22 09:27:20 - Epoch: [19][250/352]	Time  0.150 ( 0.160)	Data  0.002 ( 0.003)	Loss 7.7821e-01 (8.7407e-01)	Acc@1  82.81 ( 74.86)	Acc@5  92.97 ( 94.57)
03-Mar-22 09:27:20 - Epoch: [18][260/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.8467e-01 (8.8713e-01)	Acc@1  71.09 ( 74.65)	Acc@5  96.09 ( 94.43)
03-Mar-22 09:27:21 - Epoch: [19][260/352]	Time  0.135 ( 0.160)	Data  0.002 ( 0.003)	Loss 8.4807e-01 (8.7372e-01)	Acc@1  73.44 ( 74.82)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:27:21 - Epoch: [18][270/352]	Time  0.161 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1170e-01 (8.8661e-01)	Acc@1  78.12 ( 74.63)	Acc@5  92.19 ( 94.46)
03-Mar-22 09:27:23 - Epoch: [19][270/352]	Time  0.151 ( 0.160)	Data  0.002 ( 0.003)	Loss 9.6729e-01 (8.7304e-01)	Acc@1  71.09 ( 74.89)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:27:23 - Epoch: [18][280/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.003)	Loss 7.3332e-01 (8.8571e-01)	Acc@1  78.12 ( 74.67)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:27:24 - Epoch: [19][280/352]	Time  0.146 ( 0.159)	Data  0.002 ( 0.003)	Loss 8.1393e-01 (8.7468e-01)	Acc@1  76.56 ( 74.84)	Acc@5  96.09 ( 94.56)
03-Mar-22 09:27:25 - Epoch: [18][290/352]	Time  0.147 ( 0.152)	Data  0.003 ( 0.003)	Loss 1.0119e+00 (8.8312e-01)	Acc@1  69.53 ( 74.75)	Acc@5  90.62 ( 94.46)
03-Mar-22 09:27:26 - Epoch: [19][290/352]	Time  0.156 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.0326e+00 (8.7563e-01)	Acc@1  68.75 ( 74.79)	Acc@5  92.19 ( 94.57)
03-Mar-22 09:27:26 - Epoch: [18][300/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.6978e-01 (8.8357e-01)	Acc@1  73.44 ( 74.74)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:27:27 - Epoch: [19][300/352]	Time  0.151 ( 0.159)	Data  0.002 ( 0.003)	Loss 8.6360e-01 (8.7641e-01)	Acc@1  75.78 ( 74.75)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:27:28 - Epoch: [18][310/352]	Time  0.158 ( 0.152)	Data  0.003 ( 0.003)	Loss 8.7010e-01 (8.8446e-01)	Acc@1  76.56 ( 74.70)	Acc@5  94.53 ( 94.45)
03-Mar-22 09:27:29 - Epoch: [19][310/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.6906e-01 (8.7711e-01)	Acc@1  71.09 ( 74.74)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:27:29 - Epoch: [18][320/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.1324e-01 (8.8218e-01)	Acc@1  71.09 ( 74.74)	Acc@5  95.31 ( 94.48)
03-Mar-22 09:27:30 - Epoch: [19][320/352]	Time  0.149 ( 0.158)	Data  0.002 ( 0.003)	Loss 6.3576e-01 (8.7732e-01)	Acc@1  82.03 ( 74.70)	Acc@5  96.88 ( 94.57)
03-Mar-22 09:27:31 - Epoch: [18][330/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.9764e-01 (8.8278e-01)	Acc@1  77.34 ( 74.71)	Acc@5  92.97 ( 94.47)
03-Mar-22 09:27:32 - Epoch: [19][330/352]	Time  0.152 ( 0.157)	Data  0.002 ( 0.003)	Loss 7.2942e-01 (8.7825e-01)	Acc@1  81.25 ( 74.72)	Acc@5  96.88 ( 94.57)
03-Mar-22 09:27:32 - Epoch: [18][340/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.8389e-01 (8.8213e-01)	Acc@1  81.25 ( 74.73)	Acc@5  96.09 ( 94.49)
03-Mar-22 09:27:33 - Epoch: [19][340/352]	Time  0.153 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.9418e-01 (8.7790e-01)	Acc@1  67.97 ( 74.69)	Acc@5  94.53 ( 94.60)
03-Mar-22 09:27:34 - Epoch: [18][350/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.8595e-01 (8.8161e-01)	Acc@1  80.47 ( 74.75)	Acc@5  95.31 ( 94.49)
03-Mar-22 09:27:34 - Test: [ 0/20]	Time  0.345 ( 0.345)	Loss 1.0131e+00 (1.0131e+00)	Acc@1  68.36 ( 68.36)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:27:34 - Epoch: [19][350/352]	Time  0.143 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.1545e+00 (8.7822e-01)	Acc@1  65.62 ( 74.71)	Acc@5  89.06 ( 94.57)
03-Mar-22 09:27:35 - Test: [ 0/20]	Time  0.365 ( 0.365)	Loss 9.9966e-01 (9.9966e-01)	Acc@1  68.75 ( 68.75)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:27:35 - Test: [10/20]	Time  0.117 ( 0.117)	Loss 8.0115e-01 (9.4575e-01)	Acc@1  77.34 ( 72.48)	Acc@5  95.31 ( 94.00)
03-Mar-22 09:27:36 -  * Acc@1 71.940 Acc@5 93.580
03-Mar-22 09:27:36 - Best acc at epoch 18: 74.04000091552734
03-Mar-22 09:27:36 - Test: [10/20]	Time  0.086 ( 0.139)	Loss 8.3104e-01 (9.5184e-01)	Acc@1  75.00 ( 72.48)	Acc@5  94.53 ( 93.47)
03-Mar-22 09:27:37 - Epoch: [19][  0/352]	Time  0.386 ( 0.386)	Data  0.229 ( 0.229)	Loss 7.3511e-01 (7.3511e-01)	Acc@1  75.78 ( 75.78)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:27:37 -  * Acc@1 72.480 Acc@5 93.640
03-Mar-22 09:27:37 - Best acc at epoch 19: 73.97999572753906
03-Mar-22 09:27:38 - Epoch: [20][  0/352]	Time  0.401 ( 0.401)	Data  0.230 ( 0.230)	Loss 7.9055e-01 (7.9055e-01)	Acc@1  78.91 ( 78.91)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:27:38 - Epoch: [19][ 10/352]	Time  0.121 ( 0.155)	Data  0.002 ( 0.023)	Loss 8.7749e-01 (9.1504e-01)	Acc@1  74.22 ( 73.79)	Acc@5  95.31 ( 94.25)
03-Mar-22 09:27:39 - Epoch: [19][ 20/352]	Time  0.117 ( 0.145)	Data  0.001 ( 0.013)	Loss 9.1569e-01 (8.8140e-01)	Acc@1  74.22 ( 74.37)	Acc@5  91.41 ( 94.46)
03-Mar-22 09:27:39 - Epoch: [20][ 10/352]	Time  0.179 ( 0.193)	Data  0.003 ( 0.023)	Loss 9.4961e-01 (8.8409e-01)	Acc@1  73.44 ( 74.29)	Acc@5  96.09 ( 94.39)
03-Mar-22 09:27:41 - Epoch: [19][ 30/352]	Time  0.149 ( 0.142)	Data  0.002 ( 0.009)	Loss 9.3937e-01 (8.8331e-01)	Acc@1  72.66 ( 74.62)	Acc@5  92.19 ( 94.15)
03-Mar-22 09:27:41 - Epoch: [20][ 20/352]	Time  0.177 ( 0.182)	Data  0.002 ( 0.013)	Loss 9.5205e-01 (8.5495e-01)	Acc@1  73.44 ( 75.11)	Acc@5  91.41 ( 94.42)
03-Mar-22 09:27:42 - Epoch: [19][ 40/352]	Time  0.127 ( 0.142)	Data  0.002 ( 0.008)	Loss 8.4310e-01 (8.8047e-01)	Acc@1  77.34 ( 74.60)	Acc@5  94.53 ( 94.26)
03-Mar-22 09:27:43 - Epoch: [20][ 30/352]	Time  0.176 ( 0.179)	Data  0.003 ( 0.010)	Loss 7.8170e-01 (8.6164e-01)	Acc@1  79.69 ( 75.28)	Acc@5  96.88 ( 94.56)
03-Mar-22 09:27:43 - Epoch: [19][ 50/352]	Time  0.130 ( 0.139)	Data  0.002 ( 0.007)	Loss 8.4634e-01 (8.7811e-01)	Acc@1  71.09 ( 74.54)	Acc@5  96.88 ( 94.35)
03-Mar-22 09:27:45 - Epoch: [20][ 40/352]	Time  0.175 ( 0.178)	Data  0.002 ( 0.008)	Loss 9.0920e-01 (8.5624e-01)	Acc@1  75.00 ( 75.34)	Acc@5  96.09 ( 94.70)
03-Mar-22 09:27:45 - Epoch: [19][ 60/352]	Time  0.149 ( 0.140)	Data  0.002 ( 0.006)	Loss 8.5649e-01 (8.7677e-01)	Acc@1  75.00 ( 74.62)	Acc@5  96.09 ( 94.40)
03-Mar-22 09:27:46 - Epoch: [19][ 70/352]	Time  0.139 ( 0.140)	Data  0.002 ( 0.005)	Loss 9.1017e-01 (8.8395e-01)	Acc@1  73.44 ( 74.21)	Acc@5  92.97 ( 94.37)
03-Mar-22 09:27:46 - Epoch: [20][ 50/352]	Time  0.173 ( 0.176)	Data  0.002 ( 0.007)	Loss 9.1989e-01 (8.6139e-01)	Acc@1  74.22 ( 75.09)	Acc@5  92.19 ( 94.64)
03-Mar-22 09:27:48 - Epoch: [19][ 80/352]	Time  0.122 ( 0.140)	Data  0.002 ( 0.005)	Loss 7.9452e-01 (8.8574e-01)	Acc@1  75.00 ( 74.24)	Acc@5  93.75 ( 94.21)
03-Mar-22 09:27:48 - Epoch: [20][ 60/352]	Time  0.182 ( 0.176)	Data  0.005 ( 0.006)	Loss 9.0048e-01 (8.6513e-01)	Acc@1  78.12 ( 74.94)	Acc@5  96.09 ( 94.60)
03-Mar-22 09:27:49 - Epoch: [19][ 90/352]	Time  0.146 ( 0.140)	Data  0.002 ( 0.005)	Loss 1.0163e+00 (8.8365e-01)	Acc@1  68.75 ( 74.29)	Acc@5  92.19 ( 94.23)
03-Mar-22 09:27:50 - Epoch: [20][ 70/352]	Time  0.150 ( 0.175)	Data  0.002 ( 0.006)	Loss 1.0464e+00 (8.6785e-01)	Acc@1  69.53 ( 74.70)	Acc@5  92.97 ( 94.66)
03-Mar-22 09:27:50 - Epoch: [19][100/352]	Time  0.157 ( 0.141)	Data  0.002 ( 0.004)	Loss 7.9288e-01 (8.8241e-01)	Acc@1  78.91 ( 74.37)	Acc@5  94.53 ( 94.28)
03-Mar-22 09:27:51 - Epoch: [20][ 80/352]	Time  0.158 ( 0.174)	Data  0.002 ( 0.005)	Loss 9.0589e-01 (8.6574e-01)	Acc@1  74.22 ( 74.89)	Acc@5  93.75 ( 94.76)
03-Mar-22 09:27:52 - Epoch: [19][110/352]	Time  0.156 ( 0.142)	Data  0.003 ( 0.004)	Loss 8.9540e-01 (8.7270e-01)	Acc@1  75.00 ( 74.85)	Acc@5  92.19 ( 94.38)
03-Mar-22 09:27:53 - Epoch: [20][ 90/352]	Time  0.154 ( 0.173)	Data  0.002 ( 0.005)	Loss 9.4807e-01 (8.6261e-01)	Acc@1  75.78 ( 75.12)	Acc@5  92.19 ( 94.72)
03-Mar-22 09:27:53 - Epoch: [19][120/352]	Time  0.119 ( 0.142)	Data  0.002 ( 0.004)	Loss 7.5094e-01 (8.6668e-01)	Acc@1  75.78 ( 75.01)	Acc@5  96.88 ( 94.47)
03-Mar-22 09:27:55 - Epoch: [20][100/352]	Time  0.157 ( 0.172)	Data  0.002 ( 0.005)	Loss 7.8209e-01 (8.6405e-01)	Acc@1  78.91 ( 75.02)	Acc@5  96.09 ( 94.63)
03-Mar-22 09:27:55 - Epoch: [19][130/352]	Time  0.128 ( 0.142)	Data  0.002 ( 0.004)	Loss 8.4854e-01 (8.6965e-01)	Acc@1  74.22 ( 74.84)	Acc@5  95.31 ( 94.50)
03-Mar-22 09:27:56 - Epoch: [19][140/352]	Time  0.118 ( 0.142)	Data  0.002 ( 0.004)	Loss 9.5081e-01 (8.6784e-01)	Acc@1  72.66 ( 74.86)	Acc@5  96.09 ( 94.54)
03-Mar-22 09:27:56 - Epoch: [20][110/352]	Time  0.171 ( 0.172)	Data  0.002 ( 0.004)	Loss 7.1794e-01 (8.6228e-01)	Acc@1  75.00 ( 75.12)	Acc@5  96.88 ( 94.61)
03-Mar-22 09:27:58 - Epoch: [19][150/352]	Time  0.143 ( 0.143)	Data  0.003 ( 0.004)	Loss 7.6023e-01 (8.6771e-01)	Acc@1  75.78 ( 74.74)	Acc@5  97.66 ( 94.62)
03-Mar-22 09:27:58 - Epoch: [20][120/352]	Time  0.142 ( 0.170)	Data  0.002 ( 0.004)	Loss 9.0537e-01 (8.6723e-01)	Acc@1  73.44 ( 74.94)	Acc@5  93.75 ( 94.56)
03-Mar-22 09:27:59 - Epoch: [19][160/352]	Time  0.130 ( 0.143)	Data  0.002 ( 0.004)	Loss 8.7294e-01 (8.7225e-01)	Acc@1  73.44 ( 74.57)	Acc@5  96.09 ( 94.58)
03-Mar-22 09:27:59 - Epoch: [20][130/352]	Time  0.148 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.9790e-01 (8.6962e-01)	Acc@1  69.53 ( 74.90)	Acc@5  92.19 ( 94.57)
03-Mar-22 09:28:01 - Epoch: [19][170/352]	Time  0.163 ( 0.143)	Data  0.002 ( 0.003)	Loss 7.1083e-01 (8.6942e-01)	Acc@1  78.12 ( 74.64)	Acc@5  96.88 ( 94.66)
03-Mar-22 09:28:01 - Epoch: [20][140/352]	Time  0.151 ( 0.167)	Data  0.003 ( 0.004)	Loss 9.7328e-01 (8.7075e-01)	Acc@1  76.56 ( 74.90)	Acc@5  92.19 ( 94.54)
03-Mar-22 09:28:02 - Epoch: [19][180/352]	Time  0.154 ( 0.143)	Data  0.002 ( 0.003)	Loss 8.3233e-01 (8.6683e-01)	Acc@1  78.12 ( 74.74)	Acc@5  96.09 ( 94.71)
03-Mar-22 09:28:02 - Epoch: [20][150/352]	Time  0.149 ( 0.165)	Data  0.002 ( 0.004)	Loss 7.5290e-01 (8.7190e-01)	Acc@1  78.91 ( 74.93)	Acc@5  96.88 ( 94.54)
03-Mar-22 09:28:04 - Epoch: [19][190/352]	Time  0.132 ( 0.144)	Data  0.002 ( 0.003)	Loss 7.4950e-01 (8.6297e-01)	Acc@1  79.69 ( 74.87)	Acc@5  96.09 ( 94.75)
03-Mar-22 09:28:04 - Epoch: [20][160/352]	Time  0.152 ( 0.164)	Data  0.002 ( 0.004)	Loss 9.4302e-01 (8.7840e-01)	Acc@1  75.00 ( 74.77)	Acc@5  93.75 ( 94.45)
03-Mar-22 09:28:05 - Epoch: [20][170/352]	Time  0.141 ( 0.163)	Data  0.002 ( 0.004)	Loss 6.5822e-01 (8.7751e-01)	Acc@1  80.47 ( 74.80)	Acc@5  97.66 ( 94.45)
03-Mar-22 09:28:05 - Epoch: [19][200/352]	Time  0.149 ( 0.144)	Data  0.002 ( 0.003)	Loss 8.6774e-01 (8.6172e-01)	Acc@1  78.91 ( 74.95)	Acc@5  93.75 ( 94.75)
03-Mar-22 09:28:07 - Epoch: [20][180/352]	Time  0.150 ( 0.162)	Data  0.002 ( 0.004)	Loss 8.2314e-01 (8.7454e-01)	Acc@1  78.12 ( 74.87)	Acc@5  96.88 ( 94.52)
03-Mar-22 09:28:07 - Epoch: [19][210/352]	Time  0.152 ( 0.145)	Data  0.002 ( 0.003)	Loss 8.4376e-01 (8.6276e-01)	Acc@1  74.22 ( 74.94)	Acc@5  93.75 ( 94.70)
03-Mar-22 09:28:08 - Epoch: [20][190/352]	Time  0.149 ( 0.161)	Data  0.002 ( 0.004)	Loss 9.1503e-01 (8.7370e-01)	Acc@1  72.66 ( 74.92)	Acc@5  95.31 ( 94.55)
03-Mar-22 09:28:08 - Epoch: [19][220/352]	Time  0.161 ( 0.145)	Data  0.002 ( 0.003)	Loss 8.6333e-01 (8.6519e-01)	Acc@1  74.22 ( 74.87)	Acc@5  94.53 ( 94.66)
03-Mar-22 09:28:10 - Epoch: [20][200/352]	Time  0.151 ( 0.160)	Data  0.002 ( 0.003)	Loss 9.0071e-01 (8.7363e-01)	Acc@1  74.22 ( 74.90)	Acc@5  96.88 ( 94.62)
03-Mar-22 09:28:10 - Epoch: [19][230/352]	Time  0.151 ( 0.146)	Data  0.003 ( 0.003)	Loss 8.0481e-01 (8.6487e-01)	Acc@1  76.56 ( 74.89)	Acc@5  96.09 ( 94.66)
03-Mar-22 09:28:11 - Epoch: [20][210/352]	Time  0.173 ( 0.160)	Data  0.002 ( 0.003)	Loss 9.0595e-01 (8.7364e-01)	Acc@1  75.78 ( 74.89)	Acc@5  95.31 ( 94.63)
03-Mar-22 09:28:11 - Epoch: [19][240/352]	Time  0.155 ( 0.146)	Data  0.002 ( 0.003)	Loss 1.0458e+00 (8.6814e-01)	Acc@1  67.97 ( 74.77)	Acc@5  91.41 ( 94.58)
03-Mar-22 09:28:13 - Epoch: [20][220/352]	Time  0.154 ( 0.161)	Data  0.002 ( 0.003)	Loss 8.2996e-01 (8.7275e-01)	Acc@1  77.34 ( 74.95)	Acc@5  96.09 ( 94.67)
03-Mar-22 09:28:13 - Epoch: [19][250/352]	Time  0.151 ( 0.146)	Data  0.002 ( 0.003)	Loss 7.6135e-01 (8.7019e-01)	Acc@1  78.91 ( 74.71)	Acc@5  98.44 ( 94.61)
03-Mar-22 09:28:14 - Epoch: [19][260/352]	Time  0.152 ( 0.146)	Data  0.002 ( 0.003)	Loss 9.6814e-01 (8.7168e-01)	Acc@1  73.44 ( 74.63)	Acc@5  92.97 ( 94.58)
03-Mar-22 09:28:15 - Epoch: [20][230/352]	Time  0.198 ( 0.162)	Data  0.003 ( 0.003)	Loss 6.0829e-01 (8.6995e-01)	Acc@1  82.03 ( 75.08)	Acc@5  97.66 ( 94.66)
03-Mar-22 09:28:16 - Epoch: [19][270/352]	Time  0.154 ( 0.146)	Data  0.003 ( 0.003)	Loss 7.1556e-01 (8.7186e-01)	Acc@1  79.69 ( 74.60)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:28:16 - Epoch: [20][240/352]	Time  0.161 ( 0.162)	Data  0.002 ( 0.003)	Loss 9.9713e-01 (8.7155e-01)	Acc@1  69.53 ( 75.02)	Acc@5  91.41 ( 94.64)
03-Mar-22 09:28:17 - Epoch: [19][280/352]	Time  0.152 ( 0.146)	Data  0.002 ( 0.003)	Loss 9.1619e-01 (8.7306e-01)	Acc@1  75.00 ( 74.59)	Acc@5  92.97 ( 94.51)
03-Mar-22 09:28:18 - Epoch: [20][250/352]	Time  0.173 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.9987e-01 (8.7051e-01)	Acc@1  75.00 ( 74.98)	Acc@5  97.66 ( 94.68)
03-Mar-22 09:28:19 - Epoch: [19][290/352]	Time  0.146 ( 0.146)	Data  0.002 ( 0.003)	Loss 8.1976e-01 (8.7224e-01)	Acc@1  79.69 ( 74.65)	Acc@5  95.31 ( 94.55)
03-Mar-22 09:28:20 - Epoch: [20][260/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.4413e-01 (8.6963e-01)	Acc@1  78.91 ( 75.04)	Acc@5  96.88 ( 94.67)
03-Mar-22 09:28:20 - Epoch: [19][300/352]	Time  0.140 ( 0.146)	Data  0.002 ( 0.003)	Loss 1.0189e+00 (8.7308e-01)	Acc@1  72.66 ( 74.65)	Acc@5  93.75 ( 94.54)
03-Mar-22 09:28:21 - Epoch: [20][270/352]	Time  0.172 ( 0.163)	Data  0.003 ( 0.003)	Loss 9.6910e-01 (8.7008e-01)	Acc@1  73.44 ( 75.06)	Acc@5  92.97 ( 94.64)
03-Mar-22 09:28:22 - Epoch: [19][310/352]	Time  0.138 ( 0.146)	Data  0.002 ( 0.003)	Loss 8.2403e-01 (8.7205e-01)	Acc@1  81.25 ( 74.69)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:28:23 - Epoch: [19][320/352]	Time  0.156 ( 0.146)	Data  0.002 ( 0.003)	Loss 1.0456e+00 (8.7228e-01)	Acc@1  69.53 ( 74.69)	Acc@5  89.06 ( 94.56)
03-Mar-22 09:28:23 - Epoch: [20][280/352]	Time  0.173 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.5519e-01 (8.7088e-01)	Acc@1  72.66 ( 74.97)	Acc@5  94.53 ( 94.66)
03-Mar-22 09:28:24 - Epoch: [19][330/352]	Time  0.152 ( 0.146)	Data  0.002 ( 0.003)	Loss 9.9747e-01 (8.7460e-01)	Acc@1  70.31 ( 74.65)	Acc@5  92.97 ( 94.53)
03-Mar-22 09:28:25 - Epoch: [20][290/352]	Time  0.153 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.0820e-01 (8.7092e-01)	Acc@1  74.22 ( 74.95)	Acc@5  97.66 ( 94.66)
03-Mar-22 09:28:26 - Epoch: [19][340/352]	Time  0.144 ( 0.146)	Data  0.002 ( 0.003)	Loss 7.5136e-01 (8.7409e-01)	Acc@1  80.47 ( 74.66)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:28:27 - Epoch: [20][300/352]	Time  0.174 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.9760e-01 (8.7184e-01)	Acc@1  76.56 ( 74.92)	Acc@5  94.53 ( 94.65)
03-Mar-22 09:28:27 - Epoch: [19][350/352]	Time  0.156 ( 0.146)	Data  0.002 ( 0.003)	Loss 9.1398e-01 (8.7512e-01)	Acc@1  72.66 ( 74.69)	Acc@5  92.97 ( 94.56)
03-Mar-22 09:28:28 - Test: [ 0/20]	Time  0.325 ( 0.325)	Loss 1.0313e+00 (1.0313e+00)	Acc@1  73.44 ( 73.44)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:28:28 - Epoch: [20][310/352]	Time  0.160 ( 0.164)	Data  0.002 ( 0.003)	Loss 6.9697e-01 (8.7241e-01)	Acc@1  82.03 ( 74.92)	Acc@5  97.66 ( 94.66)
03-Mar-22 09:28:29 - Test: [10/20]	Time  0.106 ( 0.129)	Loss 8.4908e-01 (9.5587e-01)	Acc@1  73.05 ( 73.58)	Acc@5  94.53 ( 93.22)
03-Mar-22 09:28:30 - Epoch: [20][320/352]	Time  0.142 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.5329e-01 (8.7249e-01)	Acc@1  78.12 ( 74.92)	Acc@5  94.53 ( 94.66)
03-Mar-22 09:28:30 -  * Acc@1 73.260 Acc@5 93.460
03-Mar-22 09:28:30 - Best acc at epoch 19: 74.04000091552734
03-Mar-22 09:28:30 - Epoch: [20][  0/352]	Time  0.365 ( 0.365)	Data  0.228 ( 0.228)	Loss 9.9669e-01 (9.9669e-01)	Acc@1  71.88 ( 71.88)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:28:31 - Epoch: [20][330/352]	Time  0.144 ( 0.162)	Data  0.002 ( 0.003)	Loss 9.7235e-01 (8.7470e-01)	Acc@1  67.97 ( 74.88)	Acc@5  93.75 ( 94.62)
03-Mar-22 09:28:32 - Epoch: [20][ 10/352]	Time  0.132 ( 0.160)	Data  0.002 ( 0.022)	Loss 9.4972e-01 (8.9178e-01)	Acc@1  71.88 ( 74.93)	Acc@5  95.31 ( 94.60)
03-Mar-22 09:28:32 - Epoch: [20][340/352]	Time  0.146 ( 0.161)	Data  0.002 ( 0.003)	Loss 7.9241e-01 (8.7463e-01)	Acc@1  77.34 ( 74.86)	Acc@5  95.31 ( 94.65)
03-Mar-22 09:28:33 - Epoch: [20][ 20/352]	Time  0.137 ( 0.152)	Data  0.002 ( 0.013)	Loss 9.0585e-01 (8.7974e-01)	Acc@1  75.00 ( 74.81)	Acc@5  92.19 ( 94.53)
03-Mar-22 09:28:34 - Epoch: [20][350/352]	Time  0.145 ( 0.161)	Data  0.002 ( 0.003)	Loss 7.5283e-01 (8.7250e-01)	Acc@1  75.78 ( 74.92)	Acc@5  96.09 ( 94.67)
03-Mar-22 09:28:34 - Test: [ 0/20]	Time  0.332 ( 0.332)	Loss 9.5214e-01 (9.5214e-01)	Acc@1  71.48 ( 71.48)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:28:35 - Epoch: [20][ 30/352]	Time  0.179 ( 0.146)	Data  0.002 ( 0.010)	Loss 9.2104e-01 (8.8322e-01)	Acc@1  71.88 ( 74.57)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:28:35 - Test: [10/20]	Time  0.092 ( 0.125)	Loss 9.0232e-01 (9.4781e-01)	Acc@1  71.88 ( 72.34)	Acc@5  94.53 ( 93.75)
03-Mar-22 09:28:36 - Epoch: [20][ 40/352]	Time  0.158 ( 0.150)	Data  0.002 ( 0.008)	Loss 7.9950e-01 (8.6014e-01)	Acc@1  76.56 ( 75.04)	Acc@5  95.31 ( 94.76)
03-Mar-22 09:28:36 -  * Acc@1 72.580 Acc@5 93.680
03-Mar-22 09:28:36 - Best acc at epoch 20: 73.97999572753906
03-Mar-22 09:28:37 - Epoch: [21][  0/352]	Time  0.369 ( 0.369)	Data  0.218 ( 0.218)	Loss 9.0287e-01 (9.0287e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.88 ( 96.88)
03-Mar-22 09:28:38 - Epoch: [20][ 50/352]	Time  0.166 ( 0.148)	Data  0.002 ( 0.007)	Loss 1.2895e+00 (8.6954e-01)	Acc@1  58.59 ( 74.72)	Acc@5  89.06 ( 94.70)
03-Mar-22 09:28:38 - Epoch: [21][ 10/352]	Time  0.148 ( 0.173)	Data  0.003 ( 0.022)	Loss 9.1496e-01 (8.2753e-01)	Acc@1  75.00 ( 76.85)	Acc@5  92.97 ( 94.96)
03-Mar-22 09:28:39 - Epoch: [20][ 60/352]	Time  0.144 ( 0.147)	Data  0.002 ( 0.006)	Loss 7.4490e-01 (8.6816e-01)	Acc@1  80.47 ( 74.83)	Acc@5  97.66 ( 94.75)
03-Mar-22 09:28:40 - Epoch: [21][ 20/352]	Time  0.152 ( 0.161)	Data  0.002 ( 0.013)	Loss 9.7082e-01 (8.3886e-01)	Acc@1  74.22 ( 76.41)	Acc@5  94.53 ( 95.20)
03-Mar-22 09:28:41 - Epoch: [20][ 70/352]	Time  0.150 ( 0.148)	Data  0.002 ( 0.006)	Loss 7.2092e-01 (8.6760e-01)	Acc@1  82.81 ( 74.90)	Acc@5  93.75 ( 94.66)
03-Mar-22 09:28:41 - Epoch: [21][ 30/352]	Time  0.153 ( 0.158)	Data  0.002 ( 0.009)	Loss 9.6612e-01 (8.4839e-01)	Acc@1  71.09 ( 75.66)	Acc@5  93.75 ( 95.09)
03-Mar-22 09:28:42 - Epoch: [20][ 80/352]	Time  0.155 ( 0.149)	Data  0.002 ( 0.005)	Loss 8.0445e-01 (8.5972e-01)	Acc@1  72.66 ( 75.18)	Acc@5  95.31 ( 94.79)
03-Mar-22 09:28:43 - Epoch: [21][ 40/352]	Time  0.153 ( 0.157)	Data  0.002 ( 0.008)	Loss 8.8858e-01 (8.5525e-01)	Acc@1  74.22 ( 75.50)	Acc@5  94.53 ( 95.14)
03-Mar-22 09:28:44 - Epoch: [20][ 90/352]	Time  0.149 ( 0.148)	Data  0.002 ( 0.005)	Loss 8.7348e-01 (8.6227e-01)	Acc@1  75.00 ( 75.20)	Acc@5  93.75 ( 94.74)
03-Mar-22 09:28:44 - Epoch: [21][ 50/352]	Time  0.146 ( 0.155)	Data  0.002 ( 0.007)	Loss 8.8284e-01 (8.5807e-01)	Acc@1  78.12 ( 75.64)	Acc@5  93.75 ( 95.07)
03-Mar-22 09:28:45 - Epoch: [20][100/352]	Time  0.163 ( 0.149)	Data  0.002 ( 0.005)	Loss 8.4243e-01 (8.6021e-01)	Acc@1  75.78 ( 75.28)	Acc@5  95.31 ( 94.80)
03-Mar-22 09:28:46 - Epoch: [21][ 60/352]	Time  0.147 ( 0.154)	Data  0.002 ( 0.006)	Loss 6.0172e-01 (8.5205e-01)	Acc@1  78.12 ( 75.61)	Acc@5  99.22 ( 95.08)
03-Mar-22 09:28:47 - Epoch: [20][110/352]	Time  0.153 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.8279e-01 (8.6608e-01)	Acc@1  72.66 ( 75.10)	Acc@5  95.31 ( 94.73)
03-Mar-22 09:28:47 - Epoch: [21][ 70/352]	Time  0.149 ( 0.153)	Data  0.002 ( 0.005)	Loss 1.0027e+00 (8.5325e-01)	Acc@1  75.78 ( 75.66)	Acc@5  92.19 ( 95.06)
03-Mar-22 09:28:48 - Epoch: [20][120/352]	Time  0.162 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.1110e-01 (8.6473e-01)	Acc@1  71.88 ( 75.15)	Acc@5  93.75 ( 94.75)
03-Mar-22 09:28:49 - Epoch: [21][ 80/352]	Time  0.151 ( 0.153)	Data  0.002 ( 0.005)	Loss 1.0655e+00 (8.6320e-01)	Acc@1  67.97 ( 75.38)	Acc@5  90.62 ( 94.90)
03-Mar-22 09:28:50 - Epoch: [20][130/352]	Time  0.152 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.4161e-01 (8.6547e-01)	Acc@1  75.78 ( 75.10)	Acc@5  95.31 ( 94.71)
03-Mar-22 09:28:50 - Epoch: [21][ 90/352]	Time  0.135 ( 0.152)	Data  0.002 ( 0.005)	Loss 8.3994e-01 (8.5728e-01)	Acc@1  75.00 ( 75.49)	Acc@5  93.75 ( 94.88)
03-Mar-22 09:28:51 - Epoch: [20][140/352]	Time  0.161 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.1269e-01 (8.6504e-01)	Acc@1  76.56 ( 75.07)	Acc@5  92.97 ( 94.66)
03-Mar-22 09:28:52 - Epoch: [21][100/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.6818e-01 (8.5276e-01)	Acc@1  75.00 ( 75.62)	Acc@5  92.97 ( 94.88)
03-Mar-22 09:28:53 - Epoch: [20][150/352]	Time  0.152 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.1505e-01 (8.6629e-01)	Acc@1  70.31 ( 74.96)	Acc@5  97.66 ( 94.67)
03-Mar-22 09:28:53 - Epoch: [21][110/352]	Time  0.140 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.1165e-01 (8.5541e-01)	Acc@1  74.22 ( 75.53)	Acc@5  92.19 ( 94.74)
03-Mar-22 09:28:54 - Epoch: [20][160/352]	Time  0.166 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.1748e-01 (8.6244e-01)	Acc@1  75.00 ( 75.08)	Acc@5  96.09 ( 94.73)
03-Mar-22 09:28:55 - Epoch: [21][120/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0567e+00 (8.5654e-01)	Acc@1  71.88 ( 75.44)	Acc@5  92.97 ( 94.72)
03-Mar-22 09:28:56 - Epoch: [20][170/352]	Time  0.153 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.3078e-01 (8.6438e-01)	Acc@1  74.22 ( 75.01)	Acc@5  96.09 ( 94.72)
03-Mar-22 09:28:56 - Epoch: [21][130/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.1647e-01 (8.5992e-01)	Acc@1  76.56 ( 75.33)	Acc@5  96.88 ( 94.74)
03-Mar-22 09:28:57 - Epoch: [20][180/352]	Time  0.167 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.8979e-01 (8.6403e-01)	Acc@1  76.56 ( 75.00)	Acc@5  92.19 ( 94.72)
03-Mar-22 09:28:58 - Epoch: [21][140/352]	Time  0.147 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.8749e-01 (8.6048e-01)	Acc@1  70.31 ( 75.21)	Acc@5  95.31 ( 94.82)
03-Mar-22 09:28:59 - Epoch: [20][190/352]	Time  0.152 ( 0.149)	Data  0.002 ( 0.004)	Loss 6.5822e-01 (8.6244e-01)	Acc@1  85.94 ( 75.15)	Acc@5  96.09 ( 94.71)
03-Mar-22 09:28:59 - Epoch: [21][150/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.9905e-01 (8.6037e-01)	Acc@1  76.56 ( 75.16)	Acc@5  95.31 ( 94.80)
03-Mar-22 09:29:00 - Epoch: [20][200/352]	Time  0.163 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.2020e-01 (8.6347e-01)	Acc@1  74.22 ( 75.10)	Acc@5  95.31 ( 94.71)
03-Mar-22 09:29:01 - Epoch: [21][160/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.004)	Loss 7.9454e-01 (8.6092e-01)	Acc@1  78.91 ( 75.16)	Acc@5  96.88 ( 94.79)
03-Mar-22 09:29:02 - Epoch: [20][210/352]	Time  0.149 ( 0.150)	Data  0.003 ( 0.003)	Loss 9.1411e-01 (8.6244e-01)	Acc@1  73.44 ( 75.09)	Acc@5  92.97 ( 94.71)
03-Mar-22 09:29:02 - Epoch: [21][170/352]	Time  0.138 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.0588e-01 (8.6360e-01)	Acc@1  79.69 ( 75.06)	Acc@5  96.09 ( 94.74)
03-Mar-22 09:29:03 - Epoch: [20][220/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.6035e-01 (8.6205e-01)	Acc@1  73.44 ( 75.10)	Acc@5  95.31 ( 94.72)
03-Mar-22 09:29:04 - Epoch: [21][180/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.0247e+00 (8.6488e-01)	Acc@1  69.53 ( 75.04)	Acc@5  93.75 ( 94.69)
03-Mar-22 09:29:05 - Epoch: [20][230/352]	Time  0.149 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.2713e-01 (8.6543e-01)	Acc@1  75.00 ( 75.02)	Acc@5  94.53 ( 94.67)
03-Mar-22 09:29:05 - Epoch: [21][190/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.7719e-01 (8.6753e-01)	Acc@1  75.78 ( 75.00)	Acc@5  93.75 ( 94.61)
03-Mar-22 09:29:06 - Epoch: [20][240/352]	Time  0.149 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.7056e-01 (8.6504e-01)	Acc@1  76.56 ( 75.00)	Acc@5  94.53 ( 94.67)
03-Mar-22 09:29:07 - Epoch: [21][200/352]	Time  0.146 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3513e-01 (8.6666e-01)	Acc@1  78.12 ( 75.05)	Acc@5  95.31 ( 94.66)
03-Mar-22 09:29:08 - Epoch: [20][250/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.8946e-01 (8.6560e-01)	Acc@1  72.66 ( 74.98)	Acc@5  92.19 ( 94.65)
03-Mar-22 09:29:08 - Epoch: [21][210/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.6595e-01 (8.6471e-01)	Acc@1  76.56 ( 75.11)	Acc@5  93.75 ( 94.68)
03-Mar-22 09:29:09 - Epoch: [20][260/352]	Time  0.132 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.1790e-01 (8.6874e-01)	Acc@1  78.91 ( 74.91)	Acc@5  96.88 ( 94.63)
03-Mar-22 09:29:10 - Epoch: [21][220/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.9623e-01 (8.6530e-01)	Acc@1  71.09 ( 75.14)	Acc@5  93.75 ( 94.64)
03-Mar-22 09:29:11 - Epoch: [20][270/352]	Time  0.149 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.7222e-01 (8.6987e-01)	Acc@1  74.22 ( 74.85)	Acc@5  96.88 ( 94.61)
03-Mar-22 09:29:11 - Epoch: [21][230/352]	Time  0.145 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.1349e-01 (8.6602e-01)	Acc@1  72.66 ( 75.08)	Acc@5  93.75 ( 94.64)
03-Mar-22 09:29:12 - Epoch: [20][280/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1442e+00 (8.6928e-01)	Acc@1  69.53 ( 74.91)	Acc@5  94.53 ( 94.62)
03-Mar-22 09:29:13 - Epoch: [21][240/352]	Time  0.136 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0100e+00 (8.6465e-01)	Acc@1  73.44 ( 75.08)	Acc@5  92.97 ( 94.68)
03-Mar-22 09:29:13 - Epoch: [20][290/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.6357e-01 (8.7021e-01)	Acc@1  69.53 ( 74.90)	Acc@5  92.19 ( 94.60)
03-Mar-22 09:29:14 - Epoch: [21][250/352]	Time  0.146 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.9165e-01 (8.6302e-01)	Acc@1  76.56 ( 75.12)	Acc@5  95.31 ( 94.68)
03-Mar-22 09:29:15 - Epoch: [20][300/352]	Time  0.134 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.0323e-01 (8.7090e-01)	Acc@1  78.91 ( 74.87)	Acc@5  96.09 ( 94.59)
03-Mar-22 09:29:16 - Epoch: [21][260/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.5439e-01 (8.6247e-01)	Acc@1  77.34 ( 75.16)	Acc@5  93.75 ( 94.68)
03-Mar-22 09:29:16 - Epoch: [20][310/352]	Time  0.149 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.2519e-01 (8.7148e-01)	Acc@1  76.56 ( 74.85)	Acc@5  96.09 ( 94.59)
03-Mar-22 09:29:17 - Epoch: [21][270/352]	Time  0.144 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.2736e-01 (8.6419e-01)	Acc@1  81.25 ( 75.08)	Acc@5  96.09 ( 94.66)
03-Mar-22 09:29:18 - Epoch: [20][320/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.8832e-01 (8.7287e-01)	Acc@1  70.31 ( 74.83)	Acc@5  92.97 ( 94.56)
03-Mar-22 09:29:18 - Epoch: [21][280/352]	Time  0.146 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.5197e-01 (8.6433e-01)	Acc@1  78.12 ( 75.12)	Acc@5  95.31 ( 94.64)
03-Mar-22 09:29:19 - Epoch: [20][330/352]	Time  0.154 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.5149e-01 (8.7295e-01)	Acc@1  77.34 ( 74.81)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:29:20 - Epoch: [21][290/352]	Time  0.151 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.9480e-01 (8.6266e-01)	Acc@1  73.44 ( 75.19)	Acc@5  95.31 ( 94.64)
03-Mar-22 09:29:21 - Epoch: [20][340/352]	Time  0.152 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.5370e-01 (8.7248e-01)	Acc@1  76.56 ( 74.85)	Acc@5  96.09 ( 94.56)
03-Mar-22 09:29:21 - Epoch: [21][300/352]	Time  0.154 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.0867e-01 (8.6410e-01)	Acc@1  75.00 ( 75.13)	Acc@5  95.31 ( 94.61)
03-Mar-22 09:29:22 - Epoch: [20][350/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.1778e-01 (8.7342e-01)	Acc@1  78.91 ( 74.84)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:29:23 - Epoch: [21][310/352]	Time  0.095 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0338e+00 (8.6427e-01)	Acc@1  65.62 ( 75.10)	Acc@5  94.53 ( 94.64)
03-Mar-22 09:29:23 - Test: [ 0/20]	Time  0.375 ( 0.375)	Loss 1.0047e+00 (1.0047e+00)	Acc@1  69.14 ( 69.14)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:29:24 - Test: [10/20]	Time  0.096 ( 0.127)	Loss 7.6748e-01 (9.5346e-01)	Acc@1  79.69 ( 72.02)	Acc@5  96.09 ( 93.57)
03-Mar-22 09:29:24 - Epoch: [21][320/352]	Time  0.142 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.6268e-01 (8.6471e-01)	Acc@1  75.78 ( 75.10)	Acc@5  91.41 ( 94.63)
03-Mar-22 09:29:25 -  * Acc@1 71.900 Acc@5 93.700
03-Mar-22 09:29:25 - Best acc at epoch 20: 74.04000091552734
03-Mar-22 09:29:25 - Epoch: [21][330/352]	Time  0.110 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.9629e-01 (8.6532e-01)	Acc@1  71.88 ( 75.07)	Acc@5  94.53 ( 94.64)
03-Mar-22 09:29:25 - Epoch: [21][  0/352]	Time  0.371 ( 0.371)	Data  0.218 ( 0.218)	Loss 7.7336e-01 (7.7336e-01)	Acc@1  77.34 ( 77.34)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:29:27 - Epoch: [21][340/352]	Time  0.135 ( 0.147)	Data  0.001 ( 0.003)	Loss 7.3039e-01 (8.6453e-01)	Acc@1  78.12 ( 75.11)	Acc@5  94.53 ( 94.64)
03-Mar-22 09:29:27 - Epoch: [21][ 10/352]	Time  0.167 ( 0.178)	Data  0.002 ( 0.022)	Loss 9.2127e-01 (8.5107e-01)	Acc@1  76.56 ( 75.50)	Acc@5  93.75 ( 94.74)
03-Mar-22 09:29:28 - Epoch: [21][350/352]	Time  0.149 ( 0.147)	Data  0.001 ( 0.003)	Loss 8.0513e-01 (8.6476e-01)	Acc@1  78.12 ( 75.10)	Acc@5  96.88 ( 94.64)
03-Mar-22 09:29:29 - Epoch: [21][ 20/352]	Time  0.133 ( 0.166)	Data  0.003 ( 0.013)	Loss 9.6130e-01 (8.6597e-01)	Acc@1  73.44 ( 75.37)	Acc@5  92.19 ( 94.38)
03-Mar-22 09:29:29 - Test: [ 0/20]	Time  0.361 ( 0.361)	Loss 1.0340e+00 (1.0340e+00)	Acc@1  67.97 ( 67.97)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:29:30 - Test: [10/20]	Time  0.111 ( 0.133)	Loss 7.9253e-01 (9.6260e-01)	Acc@1  77.34 ( 72.59)	Acc@5  95.70 ( 93.29)
03-Mar-22 09:29:30 - Epoch: [21][ 30/352]	Time  0.157 ( 0.161)	Data  0.002 ( 0.009)	Loss 9.1562e-01 (8.7108e-01)	Acc@1  77.34 ( 75.00)	Acc@5  91.41 ( 94.15)
03-Mar-22 09:29:31 -  * Acc@1 73.000 Acc@5 93.340
03-Mar-22 09:29:31 - Best acc at epoch 21: 73.97999572753906
03-Mar-22 09:29:31 - Epoch: [22][  0/352]	Time  0.398 ( 0.398)	Data  0.223 ( 0.223)	Loss 8.2066e-01 (8.2066e-01)	Acc@1  76.56 ( 76.56)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:29:31 - Epoch: [21][ 40/352]	Time  0.149 ( 0.156)	Data  0.002 ( 0.007)	Loss 9.0566e-01 (8.7798e-01)	Acc@1  71.09 ( 74.52)	Acc@5  92.19 ( 94.15)
03-Mar-22 09:29:33 - Epoch: [22][ 10/352]	Time  0.144 ( 0.168)	Data  0.002 ( 0.022)	Loss 1.1677e+00 (8.9726e-01)	Acc@1  61.72 ( 73.79)	Acc@5  90.62 ( 94.11)
03-Mar-22 09:29:33 - Epoch: [21][ 50/352]	Time  0.165 ( 0.156)	Data  0.002 ( 0.006)	Loss 8.9880e-01 (8.7021e-01)	Acc@1  76.56 ( 74.85)	Acc@5  92.97 ( 94.21)
03-Mar-22 09:29:34 - Epoch: [22][ 20/352]	Time  0.147 ( 0.156)	Data  0.002 ( 0.013)	Loss 7.0258e-01 (8.8348e-01)	Acc@1  78.12 ( 75.07)	Acc@5  98.44 ( 94.05)
03-Mar-22 09:29:34 - Epoch: [21][ 60/352]	Time  0.154 ( 0.154)	Data  0.002 ( 0.006)	Loss 6.8335e-01 (8.6330e-01)	Acc@1  79.69 ( 75.14)	Acc@5  96.09 ( 94.30)
03-Mar-22 09:29:36 - Epoch: [22][ 30/352]	Time  0.143 ( 0.152)	Data  0.002 ( 0.009)	Loss 9.7983e-01 (8.6549e-01)	Acc@1  71.88 ( 75.30)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:29:36 - Epoch: [21][ 70/352]	Time  0.156 ( 0.154)	Data  0.002 ( 0.005)	Loss 7.1681e-01 (8.6275e-01)	Acc@1  78.91 ( 74.91)	Acc@5  96.88 ( 94.38)
03-Mar-22 09:29:37 - Epoch: [22][ 40/352]	Time  0.133 ( 0.151)	Data  0.002 ( 0.008)	Loss 1.0003e+00 (8.6605e-01)	Acc@1  71.09 ( 75.21)	Acc@5  92.19 ( 94.59)
03-Mar-22 09:29:37 - Epoch: [21][ 80/352]	Time  0.157 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.2182e-01 (8.6477e-01)	Acc@1  76.56 ( 74.81)	Acc@5  97.66 ( 94.51)
03-Mar-22 09:29:39 - Epoch: [22][ 50/352]	Time  0.136 ( 0.150)	Data  0.002 ( 0.007)	Loss 8.3144e-01 (8.6616e-01)	Acc@1  75.00 ( 74.95)	Acc@5  93.75 ( 94.62)
03-Mar-22 09:29:39 - Epoch: [21][ 90/352]	Time  0.140 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.9361e-01 (8.5605e-01)	Acc@1  76.56 ( 75.07)	Acc@5  92.97 ( 94.64)
03-Mar-22 09:29:40 - Epoch: [22][ 60/352]	Time  0.134 ( 0.149)	Data  0.002 ( 0.006)	Loss 8.4439e-01 (8.6200e-01)	Acc@1  78.91 ( 75.12)	Acc@5  95.31 ( 94.67)
03-Mar-22 09:29:41 - Epoch: [21][100/352]	Time  0.162 ( 0.154)	Data  0.002 ( 0.004)	Loss 7.6210e-01 (8.5486e-01)	Acc@1  77.34 ( 75.12)	Acc@5  98.44 ( 94.71)
03-Mar-22 09:29:42 - Epoch: [22][ 70/352]	Time  0.149 ( 0.149)	Data  0.002 ( 0.005)	Loss 7.3792e-01 (8.5771e-01)	Acc@1  79.69 ( 75.30)	Acc@5  96.88 ( 94.84)
03-Mar-22 09:29:42 - Epoch: [21][110/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.004)	Loss 6.3349e-01 (8.5698e-01)	Acc@1  82.81 ( 75.18)	Acc@5  96.88 ( 94.66)
03-Mar-22 09:29:43 - Epoch: [22][ 80/352]	Time  0.152 ( 0.148)	Data  0.002 ( 0.005)	Loss 1.0108e+00 (8.6218e-01)	Acc@1  74.22 ( 75.17)	Acc@5  94.53 ( 94.83)
03-Mar-22 09:29:44 - Epoch: [21][120/352]	Time  0.154 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.8850e-01 (8.5989e-01)	Acc@1  74.22 ( 75.19)	Acc@5  97.66 ( 94.67)
03-Mar-22 09:29:44 - Epoch: [22][ 90/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.005)	Loss 9.0197e-01 (8.5630e-01)	Acc@1  67.19 ( 75.29)	Acc@5  96.09 ( 94.88)
03-Mar-22 09:29:45 - Epoch: [21][130/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.8820e-01 (8.5989e-01)	Acc@1  78.12 ( 75.22)	Acc@5  94.53 ( 94.62)
03-Mar-22 09:29:46 - Epoch: [22][100/352]	Time  0.137 ( 0.148)	Data  0.002 ( 0.004)	Loss 1.0994e+00 (8.5917e-01)	Acc@1  73.44 ( 75.25)	Acc@5  88.28 ( 94.84)
03-Mar-22 09:29:47 - Epoch: [21][140/352]	Time  0.146 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.7430e-01 (8.5997e-01)	Acc@1  67.19 ( 75.21)	Acc@5  90.62 ( 94.61)
03-Mar-22 09:29:47 - Epoch: [22][110/352]	Time  0.154 ( 0.148)	Data  0.002 ( 0.004)	Loss 7.8405e-01 (8.5965e-01)	Acc@1  75.00 ( 75.18)	Acc@5  94.53 ( 94.84)
03-Mar-22 09:29:48 - Epoch: [21][150/352]	Time  0.154 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.3104e-01 (8.5860e-01)	Acc@1  75.00 ( 75.29)	Acc@5  93.75 ( 94.65)
03-Mar-22 09:29:49 - Epoch: [22][120/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.4325e-01 (8.5895e-01)	Acc@1  70.31 ( 75.19)	Acc@5  94.53 ( 94.91)
03-Mar-22 09:29:50 - Epoch: [21][160/352]	Time  0.132 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.5790e-01 (8.6323e-01)	Acc@1  73.44 ( 75.22)	Acc@5  94.53 ( 94.59)
03-Mar-22 09:29:50 - Epoch: [22][130/352]	Time  0.135 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.0847e-01 (8.6160e-01)	Acc@1  73.44 ( 75.15)	Acc@5  91.41 ( 94.87)
03-Mar-22 09:29:51 - Epoch: [21][170/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0737e+00 (8.6408e-01)	Acc@1  70.31 ( 75.16)	Acc@5  93.75 ( 94.61)
03-Mar-22 09:29:52 - Epoch: [22][140/352]	Time  0.157 ( 0.149)	Data  0.002 ( 0.004)	Loss 7.3346e-01 (8.5870e-01)	Acc@1  78.91 ( 75.21)	Acc@5  96.88 ( 94.90)
03-Mar-22 09:29:53 - Epoch: [21][180/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.7563e-01 (8.6448e-01)	Acc@1  75.00 ( 75.09)	Acc@5  95.31 ( 94.62)
03-Mar-22 09:29:53 - Epoch: [22][150/352]	Time  0.151 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.2715e-01 (8.5835e-01)	Acc@1  75.00 ( 75.21)	Acc@5  95.31 ( 94.91)
03-Mar-22 09:29:54 - Epoch: [21][190/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.5317e-01 (8.6360e-01)	Acc@1  76.56 ( 75.11)	Acc@5  96.88 ( 94.63)
03-Mar-22 09:29:55 - Epoch: [22][160/352]	Time  0.151 ( 0.149)	Data  0.003 ( 0.004)	Loss 8.4052e-01 (8.5978e-01)	Acc@1  78.12 ( 75.16)	Acc@5  94.53 ( 94.89)
03-Mar-22 09:29:56 - Epoch: [21][200/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0105e+00 (8.6333e-01)	Acc@1  75.00 ( 75.12)	Acc@5  92.97 ( 94.64)
03-Mar-22 09:29:56 - Epoch: [22][170/352]	Time  0.152 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.1621e-01 (8.5879e-01)	Acc@1  71.88 ( 75.21)	Acc@5  93.75 ( 94.88)
03-Mar-22 09:29:57 - Epoch: [21][210/352]	Time  0.140 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.7936e-01 (8.6542e-01)	Acc@1  67.97 ( 75.03)	Acc@5  95.31 ( 94.61)
03-Mar-22 09:29:58 - Epoch: [22][180/352]	Time  0.135 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.7955e-01 (8.6133e-01)	Acc@1  76.56 ( 75.11)	Acc@5  94.53 ( 94.85)
03-Mar-22 09:29:59 - Epoch: [21][220/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.3005e-01 (8.6757e-01)	Acc@1  75.78 ( 75.05)	Acc@5  96.09 ( 94.59)
03-Mar-22 09:29:59 - Epoch: [22][190/352]	Time  0.155 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.6342e-01 (8.6268e-01)	Acc@1  73.44 ( 75.11)	Acc@5  89.84 ( 94.81)
03-Mar-22 09:30:00 - Epoch: [21][230/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.1070e-01 (8.6814e-01)	Acc@1  75.00 ( 75.02)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:30:01 - Epoch: [22][200/352]	Time  0.148 ( 0.148)	Data  0.002 ( 0.003)	Loss 7.0372e-01 (8.6203e-01)	Acc@1  78.12 ( 75.12)	Acc@5  96.88 ( 94.83)
03-Mar-22 09:30:02 - Epoch: [21][240/352]	Time  0.134 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.8843e-01 (8.6962e-01)	Acc@1  74.22 ( 74.97)	Acc@5  97.66 ( 94.58)
03-Mar-22 09:30:02 - Epoch: [22][210/352]	Time  0.145 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.0016e-01 (8.6203e-01)	Acc@1  73.44 ( 75.07)	Acc@5  93.75 ( 94.85)
03-Mar-22 09:30:03 - Epoch: [21][250/352]	Time  0.130 ( 0.151)	Data  0.002 ( 0.003)	Loss 6.6495e-01 (8.6630e-01)	Acc@1  80.47 ( 75.03)	Acc@5  98.44 ( 94.65)
03-Mar-22 09:30:04 - Epoch: [22][220/352]	Time  0.147 ( 0.148)	Data  0.003 ( 0.003)	Loss 1.0184e+00 (8.6094e-01)	Acc@1  70.31 ( 75.08)	Acc@5  92.97 ( 94.86)
03-Mar-22 09:30:04 - Epoch: [21][260/352]	Time  0.147 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.0874e-01 (8.6439e-01)	Acc@1  74.22 ( 75.10)	Acc@5  93.75 ( 94.66)
03-Mar-22 09:30:05 - Epoch: [22][230/352]	Time  0.148 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.2070e-01 (8.6100e-01)	Acc@1  71.09 ( 75.07)	Acc@5  93.75 ( 94.85)
03-Mar-22 09:30:06 - Epoch: [21][270/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0101e+00 (8.6504e-01)	Acc@1  70.31 ( 75.04)	Acc@5  91.41 ( 94.67)
03-Mar-22 09:30:07 - Epoch: [22][240/352]	Time  0.139 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.7418e-01 (8.6093e-01)	Acc@1  70.31 ( 75.02)	Acc@5  92.19 ( 94.86)
03-Mar-22 09:30:07 - Epoch: [21][280/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.2146e-01 (8.6614e-01)	Acc@1  75.00 ( 74.99)	Acc@5  97.66 ( 94.73)
03-Mar-22 09:30:08 - Epoch: [22][250/352]	Time  0.152 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.4634e-01 (8.6019e-01)	Acc@1  76.56 ( 75.08)	Acc@5  95.31 ( 94.86)
03-Mar-22 09:30:09 - Epoch: [21][290/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1857e-01 (8.6712e-01)	Acc@1  79.69 ( 75.01)	Acc@5  94.53 ( 94.69)
03-Mar-22 09:30:10 - Epoch: [22][260/352]	Time  0.136 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.7150e-01 (8.6281e-01)	Acc@1  75.00 ( 75.04)	Acc@5  91.41 ( 94.83)
03-Mar-22 09:30:10 - Epoch: [21][300/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3713e-01 (8.6664e-01)	Acc@1  76.56 ( 74.96)	Acc@5  94.53 ( 94.74)
03-Mar-22 09:30:11 - Epoch: [22][270/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.0756e-01 (8.6313e-01)	Acc@1  75.78 ( 75.04)	Acc@5  93.75 ( 94.83)
03-Mar-22 09:30:12 - Epoch: [21][310/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0530e+00 (8.6947e-01)	Acc@1  70.31 ( 74.89)	Acc@5  92.19 ( 94.70)
03-Mar-22 09:30:13 - Epoch: [22][280/352]	Time  0.156 ( 0.148)	Data  0.003 ( 0.003)	Loss 8.1424e-01 (8.6175e-01)	Acc@1  79.69 ( 75.09)	Acc@5  93.75 ( 94.84)
03-Mar-22 09:30:13 - Epoch: [21][320/352]	Time  0.128 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.9957e-01 (8.7033e-01)	Acc@1  76.56 ( 74.86)	Acc@5  96.09 ( 94.70)
03-Mar-22 09:30:14 - Epoch: [22][290/352]	Time  0.148 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.3282e-01 (8.6054e-01)	Acc@1  74.22 ( 75.10)	Acc@5  95.31 ( 94.85)
03-Mar-22 09:30:15 - Epoch: [21][330/352]	Time  0.147 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1918e-01 (8.7198e-01)	Acc@1  78.91 ( 74.81)	Acc@5  94.53 ( 94.68)
03-Mar-22 09:30:16 - Epoch: [22][300/352]	Time  0.140 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.1351e-01 (8.6025e-01)	Acc@1  77.34 ( 75.11)	Acc@5  94.53 ( 94.83)
03-Mar-22 09:30:16 - Epoch: [21][340/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.3034e-01 (8.7104e-01)	Acc@1  77.34 ( 74.80)	Acc@5  96.88 ( 94.69)
03-Mar-22 09:30:17 - Epoch: [22][310/352]	Time  0.150 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.8513e-01 (8.6038e-01)	Acc@1  77.34 ( 75.13)	Acc@5  94.53 ( 94.82)
03-Mar-22 09:30:18 - Epoch: [21][350/352]	Time  0.148 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0412e+00 (8.7035e-01)	Acc@1  67.19 ( 74.79)	Acc@5  92.19 ( 94.71)
03-Mar-22 09:30:18 - Epoch: [22][320/352]	Time  0.118 ( 0.148)	Data  0.002 ( 0.003)	Loss 7.9669e-01 (8.6033e-01)	Acc@1  80.47 ( 75.15)	Acc@5  95.31 ( 94.80)
03-Mar-22 09:30:18 - Test: [ 0/20]	Time  0.359 ( 0.359)	Loss 1.0808e+00 (1.0808e+00)	Acc@1  69.14 ( 69.14)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:30:19 - Test: [10/20]	Time  0.094 ( 0.127)	Loss 8.7151e-01 (9.5311e-01)	Acc@1  72.27 ( 72.44)	Acc@5  95.70 ( 93.36)
03-Mar-22 09:30:20 - Epoch: [22][330/352]	Time  0.133 ( 0.147)	Data  0.001 ( 0.003)	Loss 9.2514e-01 (8.5969e-01)	Acc@1  75.00 ( 75.16)	Acc@5  92.97 ( 94.81)
03-Mar-22 09:30:20 -  * Acc@1 72.700 Acc@5 93.400
03-Mar-22 09:30:21 - Best acc at epoch 21: 74.04000091552734
03-Mar-22 09:30:21 - Epoch: [22][340/352]	Time  0.106 ( 0.146)	Data  0.002 ( 0.003)	Loss 1.0083e+00 (8.6141e-01)	Acc@1  73.44 ( 75.14)	Acc@5  91.41 ( 94.76)
03-Mar-22 09:30:21 - Epoch: [22][  0/352]	Time  0.390 ( 0.390)	Data  0.242 ( 0.242)	Loss 9.5159e-01 (9.5159e-01)	Acc@1  76.56 ( 76.56)	Acc@5  90.62 ( 90.62)
03-Mar-22 09:30:22 - Epoch: [22][350/352]	Time  0.136 ( 0.146)	Data  0.001 ( 0.003)	Loss 9.3999e-01 (8.6230e-01)	Acc@1  72.66 ( 75.13)	Acc@5  92.97 ( 94.72)
03-Mar-22 09:30:22 - Epoch: [22][ 10/352]	Time  0.145 ( 0.167)	Data  0.002 ( 0.024)	Loss 8.6034e-01 (8.8857e-01)	Acc@1  72.66 ( 74.36)	Acc@5  96.88 ( 94.18)
03-Mar-22 09:30:23 - Test: [ 0/20]	Time  0.374 ( 0.374)	Loss 9.7895e-01 (9.7895e-01)	Acc@1  71.88 ( 71.88)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:30:24 - Epoch: [22][ 20/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.013)	Loss 7.5429e-01 (8.8471e-01)	Acc@1  79.69 ( 74.63)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:30:24 - Test: [10/20]	Time  0.094 ( 0.124)	Loss 9.0476e-01 (9.4640e-01)	Acc@1  72.66 ( 72.34)	Acc@5  94.92 ( 93.54)
03-Mar-22 09:30:25 -  * Acc@1 72.280 Acc@5 93.620
03-Mar-22 09:30:25 - Best acc at epoch 22: 73.97999572753906
03-Mar-22 09:30:25 - Epoch: [22][ 30/352]	Time  0.118 ( 0.149)	Data  0.002 ( 0.010)	Loss 8.2062e-01 (8.6888e-01)	Acc@1  77.34 ( 74.87)	Acc@5  94.53 ( 94.78)
03-Mar-22 09:30:25 - Epoch: [23][  0/352]	Time  0.374 ( 0.374)	Data  0.228 ( 0.228)	Loss 9.2306e-01 (9.2306e-01)	Acc@1  71.09 ( 71.09)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:30:27 - Epoch: [22][ 40/352]	Time  0.158 ( 0.149)	Data  0.002 ( 0.008)	Loss 8.9937e-01 (8.8859e-01)	Acc@1  72.66 ( 74.60)	Acc@5  96.88 ( 94.47)
03-Mar-22 09:30:27 - Epoch: [23][ 10/352]	Time  0.152 ( 0.170)	Data  0.002 ( 0.023)	Loss 7.8765e-01 (8.8304e-01)	Acc@1  77.34 ( 75.21)	Acc@5  94.53 ( 94.11)
03-Mar-22 09:30:28 - Epoch: [23][ 20/352]	Time  0.141 ( 0.156)	Data  0.002 ( 0.013)	Loss 7.8644e-01 (8.7558e-01)	Acc@1  74.22 ( 74.37)	Acc@5  96.88 ( 94.68)
03-Mar-22 09:30:28 - Epoch: [22][ 50/352]	Time  0.151 ( 0.150)	Data  0.003 ( 0.007)	Loss 7.6245e-01 (8.7392e-01)	Acc@1  75.78 ( 74.97)	Acc@5  97.66 ( 94.81)
03-Mar-22 09:30:30 - Epoch: [23][ 30/352]	Time  0.151 ( 0.155)	Data  0.002 ( 0.009)	Loss 9.9355e-01 (8.6637e-01)	Acc@1  73.44 ( 74.80)	Acc@5  93.75 ( 94.88)
03-Mar-22 09:30:30 - Epoch: [22][ 60/352]	Time  0.150 ( 0.150)	Data  0.003 ( 0.006)	Loss 9.2775e-01 (8.7750e-01)	Acc@1  78.12 ( 75.12)	Acc@5  96.09 ( 94.72)
03-Mar-22 09:30:31 - Epoch: [23][ 40/352]	Time  0.137 ( 0.153)	Data  0.002 ( 0.008)	Loss 9.0498e-01 (8.6798e-01)	Acc@1  68.75 ( 74.89)	Acc@5  92.97 ( 94.72)
03-Mar-22 09:30:31 - Epoch: [22][ 70/352]	Time  0.154 ( 0.150)	Data  0.002 ( 0.006)	Loss 9.4548e-01 (8.7170e-01)	Acc@1  73.44 ( 75.19)	Acc@5  92.97 ( 94.81)
03-Mar-22 09:30:33 - Epoch: [23][ 50/352]	Time  0.134 ( 0.151)	Data  0.002 ( 0.007)	Loss 9.5192e-01 (8.6931e-01)	Acc@1  71.88 ( 74.92)	Acc@5  92.97 ( 94.68)
03-Mar-22 09:30:33 - Epoch: [22][ 80/352]	Time  0.158 ( 0.151)	Data  0.002 ( 0.005)	Loss 8.2285e-01 (8.6755e-01)	Acc@1  74.22 ( 75.23)	Acc@5  95.31 ( 94.81)
03-Mar-22 09:30:34 - Epoch: [23][ 60/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.006)	Loss 8.1515e-01 (8.6199e-01)	Acc@1  80.47 ( 75.08)	Acc@5  93.75 ( 94.81)
03-Mar-22 09:30:34 - Epoch: [22][ 90/352]	Time  0.148 ( 0.150)	Data  0.002 ( 0.005)	Loss 9.4838e-01 (8.6256e-01)	Acc@1  71.88 ( 75.46)	Acc@5  92.97 ( 94.84)
03-Mar-22 09:30:36 - Epoch: [23][ 70/352]	Time  0.134 ( 0.150)	Data  0.002 ( 0.005)	Loss 8.2953e-01 (8.6261e-01)	Acc@1  75.78 ( 74.97)	Acc@5  96.09 ( 94.82)
03-Mar-22 09:30:36 - Epoch: [22][100/352]	Time  0.172 ( 0.150)	Data  0.003 ( 0.005)	Loss 8.8901e-01 (8.6875e-01)	Acc@1  80.47 ( 75.32)	Acc@5  94.53 ( 94.71)
03-Mar-22 09:30:37 - Epoch: [23][ 80/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.005)	Loss 8.7312e-01 (8.6690e-01)	Acc@1  73.44 ( 74.74)	Acc@5  95.31 ( 94.82)
03-Mar-22 09:30:37 - Epoch: [22][110/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.6322e-01 (8.6683e-01)	Acc@1  76.56 ( 75.44)	Acc@5  92.97 ( 94.71)
03-Mar-22 09:30:38 - Epoch: [23][ 90/352]	Time  0.134 ( 0.148)	Data  0.002 ( 0.005)	Loss 1.0618e+00 (8.7639e-01)	Acc@1  67.97 ( 74.49)	Acc@5  90.62 ( 94.69)
03-Mar-22 09:30:39 - Epoch: [22][120/352]	Time  0.156 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.0017e+00 (8.6642e-01)	Acc@1  74.22 ( 75.46)	Acc@5  92.97 ( 94.71)
03-Mar-22 09:30:40 - Epoch: [23][100/352]	Time  0.138 ( 0.148)	Data  0.002 ( 0.004)	Loss 7.4615e-01 (8.7068e-01)	Acc@1  78.91 ( 74.78)	Acc@5  97.66 ( 94.76)
03-Mar-22 09:30:40 - Epoch: [22][130/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.7823e-01 (8.7036e-01)	Acc@1  74.22 ( 75.32)	Acc@5  92.19 ( 94.70)
03-Mar-22 09:30:41 - Epoch: [23][110/352]	Time  0.139 ( 0.148)	Data  0.002 ( 0.004)	Loss 7.0499e-01 (8.6774e-01)	Acc@1  78.91 ( 74.93)	Acc@5  98.44 ( 94.80)
03-Mar-22 09:30:42 - Epoch: [22][140/352]	Time  0.157 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.1720e-01 (8.6744e-01)	Acc@1  75.78 ( 75.37)	Acc@5  93.75 ( 94.74)
03-Mar-22 09:30:43 - Epoch: [23][120/352]	Time  0.152 ( 0.148)	Data  0.002 ( 0.004)	Loss 7.7609e-01 (8.6892e-01)	Acc@1  77.34 ( 74.88)	Acc@5  93.75 ( 94.79)
03-Mar-22 09:30:43 - Epoch: [22][150/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.1073e-01 (8.6659e-01)	Acc@1  70.31 ( 75.32)	Acc@5  94.53 ( 94.75)
03-Mar-22 09:30:44 - Epoch: [23][130/352]	Time  0.144 ( 0.148)	Data  0.003 ( 0.004)	Loss 9.7439e-01 (8.6216e-01)	Acc@1  70.31 ( 75.07)	Acc@5  96.09 ( 94.90)
03-Mar-22 09:30:45 - Epoch: [22][160/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.004)	Loss 7.0769e-01 (8.6004e-01)	Acc@1  78.91 ( 75.54)	Acc@5  96.88 ( 94.83)
03-Mar-22 09:30:46 - Epoch: [23][140/352]	Time  0.144 ( 0.147)	Data  0.002 ( 0.004)	Loss 7.4583e-01 (8.6149e-01)	Acc@1  82.03 ( 75.02)	Acc@5  96.88 ( 94.91)
03-Mar-22 09:30:46 - Epoch: [22][170/352]	Time  0.157 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.0143e+00 (8.6154e-01)	Acc@1  68.75 ( 75.42)	Acc@5  92.97 ( 94.80)
03-Mar-22 09:30:47 - Epoch: [23][150/352]	Time  0.154 ( 0.147)	Data  0.002 ( 0.004)	Loss 6.8811e-01 (8.6368e-01)	Acc@1  78.91 ( 75.03)	Acc@5  96.09 ( 94.83)
03-Mar-22 09:30:48 - Epoch: [22][180/352]	Time  0.165 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.1894e-01 (8.6076e-01)	Acc@1  75.00 ( 75.45)	Acc@5  95.31 ( 94.82)
03-Mar-22 09:30:49 - Epoch: [23][160/352]	Time  0.145 ( 0.147)	Data  0.002 ( 0.004)	Loss 8.6345e-01 (8.6046e-01)	Acc@1  74.22 ( 75.05)	Acc@5  92.19 ( 94.81)
03-Mar-22 09:30:49 - Epoch: [22][190/352]	Time  0.158 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.1887e-01 (8.6028e-01)	Acc@1  75.00 ( 75.46)	Acc@5  95.31 ( 94.79)
03-Mar-22 09:30:50 - Epoch: [23][170/352]	Time  0.127 ( 0.147)	Data  0.002 ( 0.004)	Loss 7.3783e-01 (8.6065e-01)	Acc@1  78.91 ( 75.03)	Acc@5  96.88 ( 94.84)
03-Mar-22 09:30:51 - Epoch: [22][200/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.4414e-01 (8.6232e-01)	Acc@1  72.66 ( 75.38)	Acc@5  94.53 ( 94.75)
03-Mar-22 09:30:52 - Epoch: [23][180/352]	Time  0.145 ( 0.147)	Data  0.002 ( 0.003)	Loss 1.0122e+00 (8.6277e-01)	Acc@1  72.66 ( 74.95)	Acc@5  93.75 ( 94.82)
03-Mar-22 09:30:52 - Epoch: [22][210/352]	Time  0.151 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.3500e-01 (8.6117e-01)	Acc@1  72.66 ( 75.43)	Acc@5  97.66 ( 94.77)
03-Mar-22 09:30:53 - Epoch: [23][190/352]	Time  0.140 ( 0.147)	Data  0.002 ( 0.003)	Loss 9.9764e-01 (8.6487e-01)	Acc@1  70.31 ( 74.89)	Acc@5  91.41 ( 94.81)
03-Mar-22 09:30:54 - Epoch: [22][220/352]	Time  0.162 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3417e-01 (8.6130e-01)	Acc@1  77.34 ( 75.42)	Acc@5  96.88 ( 94.78)
03-Mar-22 09:30:54 - Epoch: [23][200/352]	Time  0.139 ( 0.147)	Data  0.002 ( 0.003)	Loss 7.4597e-01 (8.6666e-01)	Acc@1  78.12 ( 74.89)	Acc@5  97.66 ( 94.75)
03-Mar-22 09:30:55 - Epoch: [22][230/352]	Time  0.131 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1783e-01 (8.6138e-01)	Acc@1  75.00 ( 75.42)	Acc@5  95.31 ( 94.78)
03-Mar-22 09:30:56 - Epoch: [23][210/352]	Time  0.140 ( 0.147)	Data  0.002 ( 0.003)	Loss 7.7749e-01 (8.6500e-01)	Acc@1  78.91 ( 74.97)	Acc@5  94.53 ( 94.72)
03-Mar-22 09:30:57 - Epoch: [22][240/352]	Time  0.155 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0718e+00 (8.6445e-01)	Acc@1  64.84 ( 75.32)	Acc@5  93.75 ( 94.79)
03-Mar-22 09:30:57 - Epoch: [23][220/352]	Time  0.140 ( 0.147)	Data  0.002 ( 0.003)	Loss 8.1269e-01 (8.6449e-01)	Acc@1  75.78 ( 74.96)	Acc@5  95.31 ( 94.72)
03-Mar-22 09:32:40 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:32:40 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:32:40 - match all modules defined in bit_config: False
03-Mar-22 09:32:40 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:32:45 - Epoch: [0][  0/352]	Time  0.407 ( 0.407)	Data  0.230 ( 0.230)	Loss 3.0511e-01 (3.0511e-01)	Acc@1  91.41 ( 91.41)	Acc@5  98.44 ( 98.44)
03-Mar-22 09:32:46 - Epoch: [0][ 10/352]	Time  0.128 ( 0.153)	Data  0.002 ( 0.023)	Loss 2.8913e-01 (3.0537e-01)	Acc@1  92.97 ( 91.62)	Acc@5  99.22 ( 99.36)
03-Mar-22 09:32:47 - Epoch: [0][ 20/352]	Time  0.134 ( 0.141)	Data  0.002 ( 0.013)	Loss 3.8745e-01 (2.9305e-01)	Acc@1  87.50 ( 91.85)	Acc@5  98.44 ( 99.59)
03-Mar-22 09:32:49 - Epoch: [0][ 30/352]	Time  0.139 ( 0.138)	Data  0.002 ( 0.010)	Loss 3.3961e-01 (2.7593e-01)	Acc@1  88.28 ( 92.14)	Acc@5 100.00 ( 99.65)
03-Mar-22 09:32:50 - Epoch: [0][ 40/352]	Time  0.133 ( 0.137)	Data  0.002 ( 0.008)	Loss 2.5080e-01 (2.6919e-01)	Acc@1  92.19 ( 92.07)	Acc@5 100.00 ( 99.73)
03-Mar-22 09:32:51 - Epoch: [0][ 50/352]	Time  0.140 ( 0.136)	Data  0.002 ( 0.007)	Loss 1.6434e-01 (2.6213e-01)	Acc@1  95.31 ( 92.17)	Acc@5 100.00 ( 99.77)
03-Mar-22 09:32:53 - Epoch: [0][ 60/352]	Time  0.126 ( 0.136)	Data  0.003 ( 0.006)	Loss 2.4038e-01 (2.5864e-01)	Acc@1  91.41 ( 92.16)	Acc@5 100.00 ( 99.78)
03-Mar-22 09:32:59 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:32:59 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:32:59 - match all modules defined in bit_config: False
03-Mar-22 09:32:59 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:33:03 - Epoch: [0][  0/352]	Time  0.439 ( 0.439)	Data  0.260 ( 0.260)	Loss 3.2302e-01 (3.2302e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
03-Mar-22 09:33:05 - Epoch: [0][ 10/352]	Time  0.127 ( 0.166)	Data  0.002 ( 0.026)	Loss 2.8748e-01 (2.9797e-01)	Acc@1  90.62 ( 91.34)	Acc@5 100.00 ( 99.72)
03-Mar-22 09:33:06 - Epoch: [0][ 20/352]	Time  0.124 ( 0.153)	Data  0.002 ( 0.014)	Loss 2.4996e-01 (2.8716e-01)	Acc@1  92.19 ( 91.63)	Acc@5 100.00 ( 99.67)
03-Mar-22 09:33:07 - Epoch: [0][ 30/352]	Time  0.142 ( 0.146)	Data  0.002 ( 0.010)	Loss 2.2613e-01 (2.7712e-01)	Acc@1  93.75 ( 91.78)	Acc@5 100.00 ( 99.70)
03-Mar-22 09:33:08 - Epoch: [0][ 40/352]	Time  0.124 ( 0.141)	Data  0.002 ( 0.008)	Loss 2.7349e-01 (2.6950e-01)	Acc@1  89.84 ( 92.05)	Acc@5  99.22 ( 99.70)
03-Mar-22 09:33:10 - Epoch: [0][ 50/352]	Time  0.123 ( 0.139)	Data  0.003 ( 0.007)	Loss 2.0644e-01 (2.6620e-01)	Acc@1  94.53 ( 92.23)	Acc@5 100.00 ( 99.71)
03-Mar-22 09:33:10 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:33:10 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:33:10 - match all modules defined in bit_config: False
03-Mar-22 09:33:10 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:33:11 - Epoch: [0][ 60/352]	Time  0.125 ( 0.138)	Data  0.003 ( 0.006)	Loss 1.9474e-01 (2.5711e-01)	Acc@1  93.75 ( 92.35)	Acc@5 100.00 ( 99.72)
03-Mar-22 09:33:12 - Epoch: [0][ 70/352]	Time  0.128 ( 0.136)	Data  0.002 ( 0.006)	Loss 2.7776e-01 (2.5185e-01)	Acc@1  93.75 ( 92.46)	Acc@5  99.22 ( 99.72)
03-Mar-22 09:33:14 - Epoch: [0][ 80/352]	Time  0.129 ( 0.135)	Data  0.002 ( 0.006)	Loss 1.7303e-01 (2.4546e-01)	Acc@1  96.09 ( 92.70)	Acc@5 100.00 ( 99.72)
03-Mar-22 09:33:15 - Epoch: [0][ 90/352]	Time  0.140 ( 0.134)	Data  0.002 ( 0.005)	Loss 1.4778e-01 (2.4315e-01)	Acc@1  94.53 ( 92.66)	Acc@5 100.00 ( 99.73)
03-Mar-22 09:33:15 - Epoch: [0][  0/352]	Time  0.456 ( 0.456)	Data  0.223 ( 0.223)	Loss 3.4624e-01 (3.4624e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
03-Mar-22 09:33:16 - Epoch: [0][ 10/352]	Time  0.119 ( 0.163)	Data  0.002 ( 0.022)	Loss 2.8381e-01 (2.9310e-01)	Acc@1  91.41 ( 92.19)	Acc@5  99.22 ( 99.50)
03-Mar-22 09:34:35 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:34:35 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:34:35 - match all modules defined in bit_config: False
03-Mar-22 09:34:35 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:34:40 - Epoch: [0][  0/352]	Time  0.456 ( 0.456)	Data  0.252 ( 0.252)	Loss 2.7826e-01 (2.7826e-01)	Acc@1  94.53 ( 94.53)	Acc@5  99.22 ( 99.22)
03-Mar-22 09:34:41 - Epoch: [0][ 10/352]	Time  0.116 ( 0.155)	Data  0.002 ( 0.025)	Loss 1.9750e-01 (2.6128e-01)	Acc@1  96.09 ( 92.90)	Acc@5 100.00 ( 99.79)
03-Mar-22 09:34:42 - Epoch: [0][ 20/352]	Time  0.124 ( 0.140)	Data  0.002 ( 0.014)	Loss 2.1885e-01 (2.6429e-01)	Acc@1  92.97 ( 92.52)	Acc@5 100.00 ( 99.70)
03-Mar-22 09:34:43 - Epoch: [0][ 30/352]	Time  0.127 ( 0.131)	Data  0.003 ( 0.010)	Loss 2.0099e-01 (2.6117e-01)	Acc@1  94.53 ( 92.34)	Acc@5 100.00 ( 99.75)
03-Mar-22 09:34:45 - Epoch: [0][ 40/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.008)	Loss 2.7462e-01 (2.6284e-01)	Acc@1  91.41 ( 92.17)	Acc@5 100.00 ( 99.70)
03-Mar-22 09:34:46 - Epoch: [0][ 50/352]	Time  0.127 ( 0.129)	Data  0.003 ( 0.007)	Loss 2.0681e-01 (2.5539e-01)	Acc@1  94.53 ( 92.43)	Acc@5  99.22 ( 99.69)
03-Mar-22 09:34:47 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:34:47 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:34:47 - match all modules defined in bit_config: False
03-Mar-22 09:34:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:34:47 - Epoch: [0][ 60/352]	Time  0.130 ( 0.129)	Data  0.003 ( 0.006)	Loss 3.0912e-01 (2.4658e-01)	Acc@1  92.19 ( 92.65)	Acc@5  98.44 ( 99.72)
03-Mar-22 09:34:48 - Epoch: [0][ 70/352]	Time  0.136 ( 0.129)	Data  0.003 ( 0.006)	Loss 2.1660e-01 (2.4340e-01)	Acc@1  94.53 ( 92.75)	Acc@5 100.00 ( 99.71)
03-Mar-22 09:34:50 - Epoch: [0][ 80/352]	Time  0.155 ( 0.129)	Data  0.003 ( 0.005)	Loss 2.6344e-01 (2.4256e-01)	Acc@1  91.41 ( 92.72)	Acc@5 100.00 ( 99.74)
03-Mar-22 09:34:51 - Epoch: [0][ 90/352]	Time  0.103 ( 0.129)	Data  0.004 ( 0.005)	Loss 3.2199e-01 (2.4129e-01)	Acc@1  88.28 ( 92.73)	Acc@5 100.00 ( 99.75)
03-Mar-22 09:34:51 - Epoch: [0][  0/352]	Time  0.406 ( 0.406)	Data  0.219 ( 0.219)	Loss 2.5951e-01 (2.5951e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
03-Mar-22 09:34:52 - Epoch: [0][100/352]	Time  0.128 ( 0.129)	Data  0.003 ( 0.005)	Loss 2.5989e-01 (2.4187e-01)	Acc@1  90.62 ( 92.69)	Acc@5 100.00 ( 99.78)
03-Mar-22 09:34:52 - Epoch: [0][ 10/352]	Time  0.124 ( 0.139)	Data  0.002 ( 0.022)	Loss 3.6868e-01 (2.8737e-01)	Acc@1  85.94 ( 91.05)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:34:54 - Epoch: [0][110/352]	Time  0.119 ( 0.129)	Data  0.002 ( 0.005)	Loss 3.0044e-01 (2.4048e-01)	Acc@1  88.28 ( 92.72)	Acc@5 100.00 ( 99.78)
03-Mar-22 09:34:54 - Epoch: [0][ 20/352]	Time  0.107 ( 0.132)	Data  0.002 ( 0.013)	Loss 3.4879e-01 (2.8253e-01)	Acc@1  89.84 ( 91.48)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:34:55 - Epoch: [0][120/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.4756e-01 (2.3822e-01)	Acc@1  92.97 ( 92.80)	Acc@5 100.00 ( 99.79)
03-Mar-22 09:34:55 - Epoch: [0][ 30/352]	Time  0.126 ( 0.130)	Data  0.003 ( 0.009)	Loss 2.3900e-01 (2.7931e-01)	Acc@1  92.97 ( 91.51)	Acc@5  99.22 ( 99.82)
03-Mar-22 09:34:56 - Epoch: [0][130/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.8770e-01 (2.3730e-01)	Acc@1  92.97 ( 92.77)	Acc@5  99.22 ( 99.79)
03-Mar-22 09:34:56 - Epoch: [0][ 40/352]	Time  0.128 ( 0.130)	Data  0.003 ( 0.008)	Loss 1.9676e-01 (2.7383e-01)	Acc@1  95.31 ( 91.62)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:34:57 - Epoch: [0][140/352]	Time  0.129 ( 0.127)	Data  0.002 ( 0.004)	Loss 2.4497e-01 (2.3577e-01)	Acc@1  94.53 ( 92.87)	Acc@5 100.00 ( 99.78)
03-Mar-22 09:34:57 - Epoch: [0][ 50/352]	Time  0.128 ( 0.129)	Data  0.003 ( 0.007)	Loss 1.7405e-01 (2.7074e-01)	Acc@1  95.31 ( 91.71)	Acc@5 100.00 ( 99.80)
03-Mar-22 09:34:59 - Epoch: [0][150/352]	Time  0.124 ( 0.128)	Data  0.003 ( 0.004)	Loss 2.6332e-01 (2.3332e-01)	Acc@1  92.19 ( 92.94)	Acc@5  99.22 ( 99.79)
03-Mar-22 09:34:59 - Epoch: [0][ 60/352]	Time  0.127 ( 0.129)	Data  0.002 ( 0.006)	Loss 1.8156e-01 (2.6358e-01)	Acc@1  95.31 ( 91.97)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:35:00 - Epoch: [0][160/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.5320e-01 (2.3071e-01)	Acc@1  96.88 ( 93.03)	Acc@5 100.00 ( 99.80)
03-Mar-22 09:35:00 - Epoch: [0][ 70/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.4427e-01 (2.5729e-01)	Acc@1  95.31 ( 92.07)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:01 - Epoch: [0][170/352]	Time  0.098 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.4294e-01 (2.2866e-01)	Acc@1  95.31 ( 93.08)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:35:01 - Epoch: [0][ 80/352]	Time  0.116 ( 0.128)	Data  0.002 ( 0.005)	Loss 2.3166e-01 (2.4893e-01)	Acc@1  92.97 ( 92.37)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:02 - Epoch: [0][180/352]	Time  0.132 ( 0.126)	Data  0.003 ( 0.004)	Loss 1.8212e-01 (2.2700e-01)	Acc@1  95.31 ( 93.13)	Acc@5  99.22 ( 99.81)
03-Mar-22 09:35:02 - Epoch: [0][ 90/352]	Time  0.115 ( 0.128)	Data  0.002 ( 0.005)	Loss 3.2173e-01 (2.4647e-01)	Acc@1  89.84 ( 92.48)	Acc@5  99.22 ( 99.83)
03-Mar-22 09:35:03 - Epoch: [0][190/352]	Time  0.156 ( 0.126)	Data  0.003 ( 0.004)	Loss 2.7061e-01 (2.2670e-01)	Acc@1  89.84 ( 93.10)	Acc@5  99.22 ( 99.81)
03-Mar-22 09:35:04 - Epoch: [0][100/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.005)	Loss 1.9928e-01 (2.4109e-01)	Acc@1  94.53 ( 92.65)	Acc@5  99.22 ( 99.82)
03-Mar-22 09:35:05 - Epoch: [0][200/352]	Time  0.113 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.4097e-01 (2.2576e-01)	Acc@1  96.88 ( 93.13)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:35:05 - Epoch: [0][110/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.004)	Loss 2.0982e-01 (2.3944e-01)	Acc@1  92.97 ( 92.71)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:06 - Epoch: [0][210/352]	Time  0.132 ( 0.126)	Data  0.003 ( 0.004)	Loss 2.6606e-01 (2.2607e-01)	Acc@1  91.41 ( 93.08)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:35:06 - Epoch: [0][120/352]	Time  0.130 ( 0.128)	Data  0.002 ( 0.004)	Loss 2.0398e-01 (2.3489e-01)	Acc@1  92.97 ( 92.87)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:07 - Epoch: [0][220/352]	Time  0.127 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.6473e-01 (2.2471e-01)	Acc@1  96.09 ( 93.14)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:35:07 - Epoch: [0][130/352]	Time  0.136 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.6994e-01 (2.3245e-01)	Acc@1  94.53 ( 92.93)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:08 - Epoch: [0][230/352]	Time  0.131 ( 0.126)	Data  0.003 ( 0.003)	Loss 1.9906e-01 (2.2431e-01)	Acc@1  93.75 ( 93.15)	Acc@5  99.22 ( 99.81)
03-Mar-22 09:35:09 - Epoch: [0][140/352]	Time  0.128 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.9147e-01 (2.3201e-01)	Acc@1  95.31 ( 92.94)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:10 - Epoch: [0][240/352]	Time  0.127 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.1192e-01 (2.2366e-01)	Acc@1  97.66 ( 93.16)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:10 - Epoch: [0][150/352]	Time  0.128 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.9188e-01 (2.3112e-01)	Acc@1  92.19 ( 92.92)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:11 - Epoch: [0][250/352]	Time  0.102 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.4844e-01 (2.2181e-01)	Acc@1  98.44 ( 93.25)	Acc@5  99.22 ( 99.82)
03-Mar-22 09:35:11 - Epoch: [0][160/352]	Time  0.133 ( 0.128)	Data  0.003 ( 0.004)	Loss 1.7374e-01 (2.2876e-01)	Acc@1  93.75 ( 93.01)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:12 - Epoch: [0][260/352]	Time  0.128 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.2560e-01 (2.2071e-01)	Acc@1  98.44 ( 93.25)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:13 - Epoch: [0][170/352]	Time  0.135 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.5004e-01 (2.2581e-01)	Acc@1  96.09 ( 93.09)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:13 - Epoch: [0][270/352]	Time  0.123 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.0959e-01 (2.2002e-01)	Acc@1  96.09 ( 93.25)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:14 - Epoch: [0][180/352]	Time  0.110 ( 0.127)	Data  0.003 ( 0.004)	Loss 1.6422e-01 (2.2439e-01)	Acc@1  96.09 ( 93.13)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:15 - Epoch: [0][280/352]	Time  0.129 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.3436e-01 (2.2040e-01)	Acc@1  91.41 ( 93.24)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:15 - Epoch: [0][190/352]	Time  0.129 ( 0.127)	Data  0.002 ( 0.004)	Loss 2.8992e-01 (2.2297e-01)	Acc@1  89.84 ( 93.17)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:16 - Epoch: [0][290/352]	Time  0.128 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.2926e-01 (2.1899e-01)	Acc@1  96.88 ( 93.28)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:16 - Epoch: [0][200/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.6853e-01 (2.2238e-01)	Acc@1  95.31 ( 93.19)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:17 - Epoch: [0][300/352]	Time  0.131 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.3662e-01 (2.1815e-01)	Acc@1  91.41 ( 93.29)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:18 - Epoch: [0][210/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.8862e-01 (2.2136e-01)	Acc@1  95.31 ( 93.19)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:18 - Epoch: [0][310/352]	Time  0.143 ( 0.126)	Data  0.003 ( 0.003)	Loss 1.5673e-01 (2.1700e-01)	Acc@1  94.53 ( 93.33)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:19 - Epoch: [0][220/352]	Time  0.129 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.6918e-01 (2.2167e-01)	Acc@1  90.62 ( 93.16)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:20 - Epoch: [0][320/352]	Time  0.133 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.1022e-01 (2.1698e-01)	Acc@1  93.75 ( 93.32)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:20 - Epoch: [0][230/352]	Time  0.130 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.6418e-01 (2.2040e-01)	Acc@1  95.31 ( 93.20)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:21 - Epoch: [0][330/352]	Time  0.119 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.4490e-01 (2.1614e-01)	Acc@1  92.97 ( 93.36)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:21 - Epoch: [0][240/352]	Time  0.128 ( 0.127)	Data  0.003 ( 0.003)	Loss 1.3041e-01 (2.1855e-01)	Acc@1  96.88 ( 93.24)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:22 - Epoch: [0][340/352]	Time  0.116 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.6803e-01 (2.1575e-01)	Acc@1  95.31 ( 93.34)	Acc@5  99.22 ( 99.83)
03-Mar-22 09:35:23 - Epoch: [0][250/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.3051e-01 (2.1840e-01)	Acc@1  90.62 ( 93.25)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:24 - Epoch: [0][350/352]	Time  0.132 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.4933e-01 (2.1539e-01)	Acc@1  90.62 ( 93.34)	Acc@5  99.22 ( 99.82)
03-Mar-22 09:35:24 - Epoch: [0][260/352]	Time  0.109 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.2865e-01 (2.1781e-01)	Acc@1  93.75 ( 93.27)	Acc@5  99.22 ( 99.84)
03-Mar-22 09:35:24 - Test: [ 0/20]	Time  0.376 ( 0.376)	Loss 1.7135e-01 (1.7135e-01)	Acc@1  94.92 ( 94.92)	Acc@5 100.00 (100.00)
03-Mar-22 09:35:25 - Test: [10/20]	Time  0.073 ( 0.104)	Loss 1.6158e-01 (1.7530e-01)	Acc@1  96.09 ( 94.74)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:35:25 - Epoch: [0][270/352]	Time  0.111 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.2405e-01 (2.1748e-01)	Acc@1  98.44 ( 93.25)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:26 -  * Acc@1 94.220 Acc@5 99.880
03-Mar-22 09:35:26 - Best acc at epoch 0: 94.22000122070312
03-Mar-22 09:35:26 - Epoch: [1][  0/352]	Time  0.374 ( 0.374)	Data  0.259 ( 0.259)	Loss 2.0222e-01 (2.0222e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:35:26 - Epoch: [0][280/352]	Time  0.113 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.9184e-01 (2.1703e-01)	Acc@1  89.06 ( 93.26)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:27 - Epoch: [1][ 10/352]	Time  0.106 ( 0.140)	Data  0.002 ( 0.025)	Loss 2.8557e-01 (2.0250e-01)	Acc@1  89.84 ( 93.61)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:35:27 - Epoch: [0][290/352]	Time  0.109 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.9697e-01 (2.1623e-01)	Acc@1  92.97 ( 93.28)	Acc@5  99.22 ( 99.83)
03-Mar-22 09:35:29 - Epoch: [1][ 20/352]	Time  0.135 ( 0.134)	Data  0.003 ( 0.014)	Loss 1.3317e-01 (1.9681e-01)	Acc@1  94.53 ( 93.49)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:35:29 - Epoch: [0][300/352]	Time  0.128 ( 0.126)	Data  0.003 ( 0.003)	Loss 2.0645e-01 (2.1578e-01)	Acc@1  94.53 ( 93.28)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:30 - Epoch: [1][ 30/352]	Time  0.117 ( 0.130)	Data  0.002 ( 0.010)	Loss 1.7041e-01 (1.8667e-01)	Acc@1  96.09 ( 93.93)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:35:30 - Epoch: [0][310/352]	Time  0.130 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.8824e-01 (2.1538e-01)	Acc@1  95.31 ( 93.28)	Acc@5  99.22 ( 99.83)
03-Mar-22 09:35:31 - Epoch: [1][ 40/352]	Time  0.136 ( 0.130)	Data  0.002 ( 0.008)	Loss 1.4980e-01 (1.8526e-01)	Acc@1  96.88 ( 94.04)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:35:31 - Epoch: [0][320/352]	Time  0.104 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.9011e-01 (2.1510e-01)	Acc@1  91.41 ( 93.27)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:32 - Epoch: [1][ 50/352]	Time  0.127 ( 0.130)	Data  0.002 ( 0.007)	Loss 1.3970e-01 (1.8798e-01)	Acc@1  96.88 ( 93.93)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:35:32 - Epoch: [0][330/352]	Time  0.127 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.7506e-01 (2.1456e-01)	Acc@1  93.75 ( 93.27)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:34 - Epoch: [0][340/352]	Time  0.102 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.0557e-01 (2.1342e-01)	Acc@1  92.19 ( 93.31)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:34 - Epoch: [1][ 60/352]	Time  0.112 ( 0.129)	Data  0.002 ( 0.006)	Loss 1.1275e-01 (1.8655e-01)	Acc@1  98.44 ( 94.06)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:35:35 - Epoch: [0][350/352]	Time  0.124 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.3390e-01 (2.1277e-01)	Acc@1  90.62 ( 93.32)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:35 - Epoch: [1][ 70/352]	Time  0.115 ( 0.129)	Data  0.002 ( 0.006)	Loss 1.5056e-01 (1.8449e-01)	Acc@1  94.53 ( 94.07)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:36 - Test: [ 0/20]	Time  0.383 ( 0.383)	Loss 1.6732e-01 (1.6732e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:35:36 - Epoch: [1][ 80/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.005)	Loss 1.8011e-01 (1.8460e-01)	Acc@1  94.53 ( 94.03)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:36 - Test: [10/20]	Time  0.085 ( 0.109)	Loss 1.3199e-01 (1.7087e-01)	Acc@1  96.88 ( 94.85)	Acc@5  99.61 ( 99.79)
03-Mar-22 09:35:37 -  * Acc@1 94.520 Acc@5 99.820
03-Mar-22 09:35:37 - Best acc at epoch 0: 94.5199966430664
03-Mar-22 09:35:37 - Epoch: [1][ 90/352]	Time  0.100 ( 0.127)	Data  0.002 ( 0.005)	Loss 2.5077e-01 (1.8549e-01)	Acc@1  89.84 ( 93.98)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:35:37 - Epoch: [1][  0/352]	Time  0.336 ( 0.336)	Data  0.218 ( 0.218)	Loss 1.9114e-01 (1.9114e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:35:39 - Epoch: [1][100/352]	Time  0.133 ( 0.127)	Data  0.003 ( 0.005)	Loss 1.3415e-01 (1.8580e-01)	Acc@1  96.09 ( 93.97)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:35:39 - Epoch: [1][ 10/352]	Time  0.128 ( 0.137)	Data  0.002 ( 0.021)	Loss 2.0399e-01 (1.8671e-01)	Acc@1  92.19 ( 93.54)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:35:40 - Epoch: [1][110/352]	Time  0.106 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.6270e-01 (1.8648e-01)	Acc@1  94.53 ( 93.91)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:35:40 - Epoch: [1][ 20/352]	Time  0.125 ( 0.128)	Data  0.002 ( 0.012)	Loss 1.8194e-01 (1.8819e-01)	Acc@1  94.53 ( 93.75)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:35:41 - Epoch: [1][120/352]	Time  0.112 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.5624e-01 (1.9029e-01)	Acc@1  96.09 ( 93.77)	Acc@5  99.22 ( 99.85)
03-Mar-22 09:35:41 - Epoch: [1][ 30/352]	Time  0.103 ( 0.126)	Data  0.001 ( 0.009)	Loss 1.6378e-01 (1.9646e-01)	Acc@1  96.09 ( 93.60)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:42 - Epoch: [1][130/352]	Time  0.110 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.9212e-01 (1.8843e-01)	Acc@1  93.75 ( 93.85)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:35:42 - Epoch: [1][ 40/352]	Time  0.104 ( 0.124)	Data  0.002 ( 0.007)	Loss 1.8031e-01 (1.8888e-01)	Acc@1  94.53 ( 93.90)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:35:43 - Epoch: [1][ 50/352]	Time  0.110 ( 0.121)	Data  0.003 ( 0.006)	Loss 2.2284e-01 (1.8615e-01)	Acc@1  92.19 ( 94.00)	Acc@5  99.22 ( 99.85)
03-Mar-22 09:35:43 - Epoch: [1][140/352]	Time  0.128 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.8906e-01 (1.8945e-01)	Acc@1  96.09 ( 93.86)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:35:44 - Epoch: [1][ 60/352]	Time  0.118 ( 0.119)	Data  0.002 ( 0.005)	Loss 1.9681e-01 (1.8656e-01)	Acc@1  93.75 ( 93.97)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:45 - Epoch: [1][150/352]	Time  0.131 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.4730e-01 (1.8777e-01)	Acc@1  96.88 ( 93.94)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:46 - Epoch: [1][ 70/352]	Time  0.105 ( 0.119)	Data  0.002 ( 0.005)	Loss 2.1392e-01 (1.8666e-01)	Acc@1  91.41 ( 93.93)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:46 - Epoch: [1][160/352]	Time  0.123 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.9135e-01 (1.8789e-01)	Acc@1  93.75 ( 93.95)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:47 - Epoch: [1][ 80/352]	Time  0.112 ( 0.119)	Data  0.002 ( 0.005)	Loss 1.8131e-01 (1.8777e-01)	Acc@1  96.09 ( 93.89)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:47 - Epoch: [1][170/352]	Time  0.136 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.5259e-01 (1.8769e-01)	Acc@1  96.09 ( 93.97)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:48 - Epoch: [1][ 90/352]	Time  0.131 ( 0.120)	Data  0.002 ( 0.004)	Loss 1.6149e-01 (1.8654e-01)	Acc@1  96.88 ( 94.02)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:48 - Epoch: [1][180/352]	Time  0.133 ( 0.125)	Data  0.003 ( 0.004)	Loss 1.7630e-01 (1.8783e-01)	Acc@1  93.75 ( 93.94)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:49 - Epoch: [1][100/352]	Time  0.115 ( 0.120)	Data  0.002 ( 0.004)	Loss 2.2304e-01 (1.8896e-01)	Acc@1  92.19 ( 93.93)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:50 - Epoch: [1][190/352]	Time  0.130 ( 0.125)	Data  0.002 ( 0.004)	Loss 2.5379e-01 (1.8738e-01)	Acc@1  95.31 ( 93.98)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:51 - Epoch: [1][110/352]	Time  0.106 ( 0.120)	Data  0.002 ( 0.004)	Loss 1.5288e-01 (1.9012e-01)	Acc@1  95.31 ( 93.93)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:51 - Epoch: [1][200/352]	Time  0.124 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.1079e-01 (1.8784e-01)	Acc@1  93.75 ( 93.96)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:52 - Epoch: [1][120/352]	Time  0.111 ( 0.120)	Data  0.002 ( 0.004)	Loss 1.8448e-01 (1.9129e-01)	Acc@1  95.31 ( 93.90)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:52 - Epoch: [1][210/352]	Time  0.113 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.2494e-01 (1.8622e-01)	Acc@1  96.88 ( 94.03)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:53 - Epoch: [1][130/352]	Time  0.127 ( 0.120)	Data  0.002 ( 0.004)	Loss 1.3262e-01 (1.9130e-01)	Acc@1  95.31 ( 93.91)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:53 - Epoch: [1][220/352]	Time  0.128 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.4970e-01 (1.8613e-01)	Acc@1  97.66 ( 94.04)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:54 - Epoch: [1][140/352]	Time  0.108 ( 0.119)	Data  0.002 ( 0.004)	Loss 2.3119e-01 (1.9043e-01)	Acc@1  90.62 ( 93.97)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:55 - Epoch: [1][230/352]	Time  0.130 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.6306e-01 (1.8576e-01)	Acc@1  94.53 ( 94.06)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:55 - Epoch: [1][150/352]	Time  0.115 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.5076e-01 (1.9094e-01)	Acc@1  96.88 ( 93.95)	Acc@5  99.22 ( 99.83)
03-Mar-22 09:35:56 - Epoch: [1][240/352]	Time  0.131 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.8908e-01 (1.8593e-01)	Acc@1  92.97 ( 94.04)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:35:56 - Epoch: [1][160/352]	Time  0.125 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.0930e-01 (1.9151e-01)	Acc@1  96.88 ( 93.91)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:57 - Epoch: [1][250/352]	Time  0.128 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.5239e-01 (1.8590e-01)	Acc@1  96.88 ( 94.02)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:58 - Epoch: [1][170/352]	Time  0.126 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.5563e-01 (1.9089e-01)	Acc@1  95.31 ( 93.92)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:58 - Epoch: [1][260/352]	Time  0.115 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.4590e-01 (1.8573e-01)	Acc@1  95.31 ( 94.04)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:35:59 - Epoch: [1][180/352]	Time  0.153 ( 0.120)	Data  0.002 ( 0.003)	Loss 1.9112e-01 (1.9037e-01)	Acc@1  93.75 ( 93.94)	Acc@5  99.22 ( 99.81)
03-Mar-22 09:36:00 - Epoch: [1][270/352]	Time  0.130 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.8497e-01 (1.8523e-01)	Acc@1  92.97 ( 94.06)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:00 - Epoch: [1][190/352]	Time  0.125 ( 0.120)	Data  0.002 ( 0.003)	Loss 1.6685e-01 (1.8887e-01)	Acc@1  95.31 ( 93.98)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:36:01 - Epoch: [1][280/352]	Time  0.109 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.0686e-01 (1.8560e-01)	Acc@1  94.53 ( 94.05)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:01 - Epoch: [1][200/352]	Time  0.130 ( 0.120)	Data  0.002 ( 0.003)	Loss 7.9481e-02 (1.8761e-01)	Acc@1  98.44 ( 94.05)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:36:02 - Epoch: [1][290/352]	Time  0.133 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.2825e-01 (1.8596e-01)	Acc@1  94.53 ( 94.05)	Acc@5  99.22 ( 99.86)
03-Mar-22 09:36:03 - Epoch: [1][210/352]	Time  0.128 ( 0.120)	Data  0.002 ( 0.003)	Loss 1.7633e-01 (1.8717e-01)	Acc@1  94.53 ( 94.05)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:04 - Epoch: [1][300/352]	Time  0.132 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.2411e-01 (1.8533e-01)	Acc@1  94.53 ( 94.09)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:04 - Epoch: [1][220/352]	Time  0.105 ( 0.120)	Data  0.002 ( 0.003)	Loss 2.6872e-01 (1.8824e-01)	Acc@1  90.62 ( 94.00)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:05 - Epoch: [1][310/352]	Time  0.130 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.0622e-01 (1.8490e-01)	Acc@1  98.44 ( 94.12)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:05 - Epoch: [1][230/352]	Time  0.125 ( 0.120)	Data  0.002 ( 0.003)	Loss 2.0169e-01 (1.8827e-01)	Acc@1  95.31 ( 94.02)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:06 - Epoch: [1][320/352]	Time  0.130 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.3324e-01 (1.8482e-01)	Acc@1  90.62 ( 94.11)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:06 - Epoch: [1][240/352]	Time  0.110 ( 0.120)	Data  0.002 ( 0.003)	Loss 2.0931e-01 (1.8735e-01)	Acc@1  92.19 ( 94.03)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:07 - Epoch: [1][330/352]	Time  0.131 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.3872e-01 (1.8409e-01)	Acc@1  95.31 ( 94.13)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:07 - Epoch: [1][250/352]	Time  0.118 ( 0.121)	Data  0.002 ( 0.003)	Loss 1.5512e-01 (1.8690e-01)	Acc@1  95.31 ( 94.05)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:08 - Epoch: [1][340/352]	Time  0.129 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.7831e-01 (1.8382e-01)	Acc@1  93.75 ( 94.13)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:09 - Epoch: [1][260/352]	Time  0.127 ( 0.121)	Data  0.002 ( 0.003)	Loss 2.9248e-01 (1.8675e-01)	Acc@1  92.19 ( 94.04)	Acc@5  99.22 ( 99.84)
03-Mar-22 09:36:10 - Epoch: [1][350/352]	Time  0.132 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.2992e-01 (1.8360e-01)	Acc@1  95.31 ( 94.16)	Acc@5  99.22 ( 99.86)
03-Mar-22 09:36:10 - Epoch: [1][270/352]	Time  0.127 ( 0.121)	Data  0.002 ( 0.003)	Loss 2.6084e-01 (1.8725e-01)	Acc@1  89.84 ( 94.01)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:10 - Test: [ 0/20]	Time  0.336 ( 0.336)	Loss 1.6500e-01 (1.6500e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.61 ( 99.61)
03-Mar-22 09:36:11 - Epoch: [1][280/352]	Time  0.094 ( 0.120)	Data  0.001 ( 0.003)	Loss 2.2780e-01 (1.8678e-01)	Acc@1  92.97 ( 94.04)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:11 - Test: [10/20]	Time  0.070 ( 0.098)	Loss 1.4541e-01 (1.7036e-01)	Acc@1  94.53 ( 94.32)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:36:12 -  * Acc@1 94.100 Acc@5 99.900
03-Mar-22 09:36:12 - Best acc at epoch 1: 94.22000122070312
03-Mar-22 09:36:12 - Epoch: [1][290/352]	Time  0.106 ( 0.120)	Data  0.002 ( 0.003)	Loss 1.2788e-01 (1.8583e-01)	Acc@1  96.09 ( 94.07)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:12 - Epoch: [2][  0/352]	Time  0.347 ( 0.347)	Data  0.247 ( 0.247)	Loss 1.6319e-01 (1.6319e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:36:13 - Epoch: [1][300/352]	Time  0.123 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.5321e-01 (1.8627e-01)	Acc@1  96.09 ( 94.07)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:13 - Epoch: [2][ 10/352]	Time  0.128 ( 0.148)	Data  0.002 ( 0.025)	Loss 1.5791e-01 (1.6075e-01)	Acc@1  93.75 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:36:14 - Epoch: [1][310/352]	Time  0.096 ( 0.119)	Data  0.002 ( 0.003)	Loss 2.8420e-01 (1.8608e-01)	Acc@1  89.84 ( 94.09)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:15 - Epoch: [2][ 20/352]	Time  0.127 ( 0.138)	Data  0.002 ( 0.014)	Loss 1.9580e-01 (1.7161e-01)	Acc@1  93.75 ( 94.42)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:36:15 - Epoch: [1][320/352]	Time  0.127 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.8760e-01 (1.8637e-01)	Acc@1  93.75 ( 94.07)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:16 - Epoch: [2][ 30/352]	Time  0.129 ( 0.135)	Data  0.002 ( 0.010)	Loss 1.6972e-01 (1.7942e-01)	Acc@1  94.53 ( 94.23)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:36:16 - Epoch: [1][330/352]	Time  0.126 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.7158e-01 (1.8652e-01)	Acc@1  94.53 ( 94.06)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:36:17 - Epoch: [2][ 40/352]	Time  0.111 ( 0.133)	Data  0.002 ( 0.008)	Loss 1.9196e-01 (1.7592e-01)	Acc@1  95.31 ( 94.49)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:18 - Epoch: [1][340/352]	Time  0.122 ( 0.119)	Data  0.002 ( 0.003)	Loss 2.7443e-01 (1.8592e-01)	Acc@1  90.62 ( 94.07)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:36:19 - Epoch: [2][ 50/352]	Time  0.128 ( 0.133)	Data  0.002 ( 0.007)	Loss 2.3959e-01 (1.7317e-01)	Acc@1  90.62 ( 94.50)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:19 - Epoch: [1][350/352]	Time  0.158 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.8315e-01 (1.8533e-01)	Acc@1  94.53 ( 94.08)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:36:19 - Test: [ 0/20]	Time  0.305 ( 0.305)	Loss 1.3811e-01 (1.3811e-01)	Acc@1  95.70 ( 95.70)	Acc@5 100.00 (100.00)
03-Mar-22 09:36:20 - Epoch: [2][ 60/352]	Time  0.132 ( 0.132)	Data  0.002 ( 0.006)	Loss 1.4772e-01 (1.7457e-01)	Acc@1  94.53 ( 94.48)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:20 - Test: [10/20]	Time  0.070 ( 0.094)	Loss 1.7333e-01 (1.6132e-01)	Acc@1  94.14 ( 94.82)	Acc@5  99.61 ( 99.86)
03-Mar-22 09:36:21 -  * Acc@1 94.520 Acc@5 99.880
03-Mar-22 09:36:21 - Best acc at epoch 1: 94.5199966430664
03-Mar-22 09:36:21 - Epoch: [2][ 70/352]	Time  0.099 ( 0.130)	Data  0.002 ( 0.006)	Loss 1.4175e-01 (1.7729e-01)	Acc@1  96.88 ( 94.43)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:21 - Epoch: [2][  0/352]	Time  0.343 ( 0.343)	Data  0.228 ( 0.228)	Loss 1.0681e-01 (1.0681e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
03-Mar-22 09:36:22 - Epoch: [2][ 10/352]	Time  0.096 ( 0.139)	Data  0.002 ( 0.023)	Loss 1.4397e-01 (1.5639e-01)	Acc@1  95.31 ( 94.46)	Acc@5 100.00 (100.00)
03-Mar-22 09:36:22 - Epoch: [2][ 80/352]	Time  0.139 ( 0.130)	Data  0.002 ( 0.005)	Loss 2.6705e-01 (1.7757e-01)	Acc@1  89.06 ( 94.30)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:24 - Epoch: [2][ 20/352]	Time  0.108 ( 0.129)	Data  0.002 ( 0.013)	Loss 2.1768e-01 (1.5995e-01)	Acc@1  94.53 ( 94.79)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:36:24 - Epoch: [2][ 90/352]	Time  0.135 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.5866e-01 (1.7747e-01)	Acc@1  94.53 ( 94.30)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:25 - Epoch: [2][ 30/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.009)	Loss 1.7946e-01 (1.6818e-01)	Acc@1  93.75 ( 94.43)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:36:25 - Epoch: [2][100/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.005)	Loss 2.0339e-01 (1.7798e-01)	Acc@1  93.75 ( 94.34)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:26 - Epoch: [2][ 40/352]	Time  0.129 ( 0.128)	Data  0.003 ( 0.008)	Loss 1.2893e-01 (1.6421e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:36:26 - Epoch: [2][110/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.5288e-01 (1.8038e-01)	Acc@1  91.41 ( 94.24)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:27 - Epoch: [2][ 50/352]	Time  0.129 ( 0.129)	Data  0.003 ( 0.007)	Loss 1.7044e-01 (1.6414e-01)	Acc@1  94.53 ( 94.78)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:36:27 - Epoch: [2][120/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.3456e-01 (1.7977e-01)	Acc@1  97.66 ( 94.29)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:29 - Epoch: [2][ 60/352]	Time  0.127 ( 0.128)	Data  0.002 ( 0.006)	Loss 1.9967e-01 (1.6815e-01)	Acc@1  94.53 ( 94.60)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:36:29 - Epoch: [2][130/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.2644e-01 (1.7945e-01)	Acc@1  95.31 ( 94.31)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:30 - Epoch: [2][ 70/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.005)	Loss 1.7677e-01 (1.6777e-01)	Acc@1  94.53 ( 94.63)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:36:30 - Epoch: [2][140/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.2623e-01 (1.7858e-01)	Acc@1  93.75 ( 94.33)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:31 - Epoch: [2][ 80/352]	Time  0.127 ( 0.127)	Data  0.003 ( 0.005)	Loss 1.3589e-01 (1.6940e-01)	Acc@1  95.31 ( 94.58)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:31 - Epoch: [2][150/352]	Time  0.139 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.9972e-01 (1.7738e-01)	Acc@1  94.53 ( 94.39)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:32 - Epoch: [2][ 90/352]	Time  0.129 ( 0.126)	Data  0.002 ( 0.005)	Loss 1.5632e-01 (1.7326e-01)	Acc@1  95.31 ( 94.45)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:33 - Epoch: [2][160/352]	Time  0.111 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.2722e-01 (1.7786e-01)	Acc@1  89.84 ( 94.34)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:33 - Epoch: [2][100/352]	Time  0.106 ( 0.126)	Data  0.002 ( 0.004)	Loss 2.5250e-01 (1.7457e-01)	Acc@1  92.97 ( 94.40)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:34 - Epoch: [2][170/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.4392e-01 (1.7570e-01)	Acc@1  94.53 ( 94.41)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:35 - Epoch: [2][110/352]	Time  0.123 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.8492e-01 (1.7332e-01)	Acc@1  92.97 ( 94.42)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:35 - Epoch: [2][180/352]	Time  0.116 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.1564e-01 (1.7694e-01)	Acc@1  92.97 ( 94.35)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:36 - Epoch: [2][120/352]	Time  0.126 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.1107e-01 (1.7254e-01)	Acc@1  96.09 ( 94.42)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:36 - Epoch: [2][190/352]	Time  0.133 ( 0.128)	Data  0.002 ( 0.004)	Loss 2.6246e-01 (1.7772e-01)	Acc@1  91.41 ( 94.33)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:37 - Epoch: [2][130/352]	Time  0.126 ( 0.126)	Data  0.002 ( 0.004)	Loss 2.5117e-01 (1.6955e-01)	Acc@1  92.19 ( 94.53)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:38 - Epoch: [2][200/352]	Time  0.142 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6984e-01 (1.7806e-01)	Acc@1  94.53 ( 94.33)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:38 - Epoch: [2][140/352]	Time  0.124 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.6219e-01 (1.7191e-01)	Acc@1  93.75 ( 94.40)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:39 - Epoch: [2][210/352]	Time  0.133 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.5118e-01 (1.7755e-01)	Acc@1  92.97 ( 94.33)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:40 - Epoch: [2][150/352]	Time  0.127 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.5133e-01 (1.7122e-01)	Acc@1  95.31 ( 94.44)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:40 - Epoch: [2][220/352]	Time  0.106 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.9439e-01 (1.7699e-01)	Acc@1  94.53 ( 94.35)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:41 - Epoch: [2][160/352]	Time  0.123 ( 0.125)	Data  0.003 ( 0.004)	Loss 1.8407e-01 (1.7256e-01)	Acc@1  94.53 ( 94.42)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:41 - Epoch: [2][230/352]	Time  0.133 ( 0.128)	Data  0.003 ( 0.003)	Loss 2.1462e-01 (1.7749e-01)	Acc@1  90.62 ( 94.31)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:42 - Epoch: [2][170/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.9183e-01 (1.7322e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:43 - Epoch: [2][240/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.8171e-01 (1.7794e-01)	Acc@1  93.75 ( 94.28)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:43 - Epoch: [2][180/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6721e-01 (1.7178e-01)	Acc@1  93.75 ( 94.45)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:44 - Epoch: [2][250/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.3250e-01 (1.7832e-01)	Acc@1  91.41 ( 94.28)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:44 - Epoch: [2][190/352]	Time  0.122 ( 0.124)	Data  0.002 ( 0.003)	Loss 2.9448e-01 (1.7202e-01)	Acc@1  91.41 ( 94.45)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:45 - Epoch: [2][260/352]	Time  0.111 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.2541e-01 (1.7820e-01)	Acc@1  92.19 ( 94.27)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:46 - Epoch: [2][200/352]	Time  0.109 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6637e-01 (1.7258e-01)	Acc@1  96.09 ( 94.42)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:46 - Epoch: [2][270/352]	Time  0.130 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.0332e-01 (1.7773e-01)	Acc@1  96.09 ( 94.28)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:47 - Epoch: [2][210/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.003)	Loss 2.2250e-01 (1.7251e-01)	Acc@1  89.84 ( 94.40)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:48 - Epoch: [2][280/352]	Time  0.132 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.8748e-01 (1.7657e-01)	Acc@1  93.75 ( 94.31)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:48 - Epoch: [2][220/352]	Time  0.129 ( 0.124)	Data  0.003 ( 0.003)	Loss 2.0760e-01 (1.7250e-01)	Acc@1  92.97 ( 94.39)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:36:49 - Epoch: [2][290/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.3660e-01 (1.7583e-01)	Acc@1  95.31 ( 94.33)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:49 - Epoch: [2][230/352]	Time  0.154 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.0453e-01 (1.7187e-01)	Acc@1  96.88 ( 94.42)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:50 - Epoch: [2][300/352]	Time  0.132 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.3280e-01 (1.7632e-01)	Acc@1  96.88 ( 94.31)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:51 - Epoch: [2][240/352]	Time  0.150 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.3203e-01 (1.7294e-01)	Acc@1  96.09 ( 94.38)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:51 - Epoch: [2][310/352]	Time  0.133 ( 0.127)	Data  0.002 ( 0.003)	Loss 8.2781e-02 (1.7594e-01)	Acc@1  98.44 ( 94.33)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:52 - Epoch: [2][250/352]	Time  0.124 ( 0.124)	Data  0.003 ( 0.003)	Loss 1.4926e-01 (1.7306e-01)	Acc@1  94.53 ( 94.37)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:53 - Epoch: [2][320/352]	Time  0.108 ( 0.127)	Data  0.002 ( 0.003)	Loss 8.2724e-02 (1.7516e-01)	Acc@1  96.88 ( 94.34)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:53 - Epoch: [2][260/352]	Time  0.127 ( 0.123)	Data  0.002 ( 0.003)	Loss 1.8306e-01 (1.7313e-01)	Acc@1  94.53 ( 94.38)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:54 - Epoch: [2][330/352]	Time  0.132 ( 0.127)	Data  0.003 ( 0.003)	Loss 1.1504e-01 (1.7424e-01)	Acc@1  95.31 ( 94.38)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:54 - Epoch: [2][270/352]	Time  0.121 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.3786e-01 (1.7250e-01)	Acc@1  94.53 ( 94.41)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:55 - Epoch: [2][340/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.003)	Loss 9.5746e-02 (1.7396e-01)	Acc@1  97.66 ( 94.40)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:36:56 - Epoch: [2][280/352]	Time  0.103 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.2755e-01 (1.7248e-01)	Acc@1  96.09 ( 94.41)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:57 - Epoch: [2][350/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.6857e-01 (1.7389e-01)	Acc@1  91.41 ( 94.39)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:36:57 - Epoch: [2][290/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.003)	Loss 2.6282e-01 (1.7255e-01)	Acc@1  90.62 ( 94.40)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:57 - Test: [ 0/20]	Time  0.350 ( 0.350)	Loss 1.4981e-01 (1.4981e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:36:58 - Epoch: [2][300/352]	Time  0.123 ( 0.123)	Data  0.002 ( 0.003)	Loss 1.3547e-01 (1.7180e-01)	Acc@1  96.88 ( 94.42)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:58 - Test: [10/20]	Time  0.071 ( 0.102)	Loss 1.4596e-01 (1.6966e-01)	Acc@1  94.92 ( 94.35)	Acc@5 100.00 ( 99.79)
03-Mar-22 09:36:59 -  * Acc@1 94.420 Acc@5 99.820
03-Mar-22 09:36:59 - Best acc at epoch 2: 94.41999816894531
03-Mar-22 09:36:59 - Epoch: [2][310/352]	Time  0.097 ( 0.123)	Data  0.002 ( 0.003)	Loss 2.5862e-01 (1.7244e-01)	Acc@1  90.62 ( 94.40)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:59 - Epoch: [3][  0/352]	Time  0.408 ( 0.408)	Data  0.277 ( 0.277)	Loss 2.6469e-01 (2.6469e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
03-Mar-22 09:37:00 - Epoch: [2][320/352]	Time  0.093 ( 0.122)	Data  0.002 ( 0.003)	Loss 2.2493e-01 (1.7326e-01)	Acc@1  91.41 ( 94.38)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:37:00 - Epoch: [3][ 10/352]	Time  0.127 ( 0.153)	Data  0.002 ( 0.027)	Loss 1.9362e-01 (1.6538e-01)	Acc@1  92.97 ( 94.67)	Acc@5  99.22 ( 99.79)
03-Mar-22 09:37:01 - Epoch: [2][330/352]	Time  0.092 ( 0.121)	Data  0.002 ( 0.003)	Loss 2.3921e-01 (1.7351e-01)	Acc@1  92.19 ( 94.37)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:02 - Epoch: [3][ 20/352]	Time  0.133 ( 0.141)	Data  0.002 ( 0.015)	Loss 1.9685e-01 (1.7229e-01)	Acc@1  94.53 ( 94.64)	Acc@5  99.22 ( 99.78)
03-Mar-22 09:37:02 - Epoch: [2][340/352]	Time  0.128 ( 0.121)	Data  0.002 ( 0.003)	Loss 1.9667e-01 (1.7361e-01)	Acc@1  95.31 ( 94.36)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:03 - Epoch: [3][ 30/352]	Time  0.130 ( 0.139)	Data  0.002 ( 0.011)	Loss 1.1427e-01 (1.7034e-01)	Acc@1  96.88 ( 94.63)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:37:03 - Epoch: [2][350/352]	Time  0.109 ( 0.121)	Data  0.002 ( 0.003)	Loss 1.6177e-01 (1.7291e-01)	Acc@1  96.09 ( 94.40)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:04 - Test: [ 0/20]	Time  0.327 ( 0.327)	Loss 1.6191e-01 (1.6191e-01)	Acc@1  95.31 ( 95.31)	Acc@5  99.61 ( 99.61)
03-Mar-22 09:37:04 - Epoch: [3][ 40/352]	Time  0.128 ( 0.136)	Data  0.002 ( 0.009)	Loss 2.0995e-01 (1.7261e-01)	Acc@1  92.97 ( 94.44)	Acc@5  99.22 ( 99.83)
03-Mar-22 09:37:05 - Test: [10/20]	Time  0.091 ( 0.098)	Loss 1.4004e-01 (1.7232e-01)	Acc@1  96.09 ( 94.21)	Acc@5 100.00 ( 99.75)
03-Mar-22 09:37:05 -  * Acc@1 94.200 Acc@5 99.800
03-Mar-22 09:37:05 - Best acc at epoch 2: 94.5199966430664
03-Mar-22 09:37:06 - Epoch: [3][ 50/352]	Time  0.113 ( 0.134)	Data  0.003 ( 0.008)	Loss 2.1274e-01 (1.7319e-01)	Acc@1  92.19 ( 94.47)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:37:06 - Epoch: [3][  0/352]	Time  0.350 ( 0.350)	Data  0.240 ( 0.240)	Loss 9.5441e-02 (9.5441e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
03-Mar-22 09:37:07 - Epoch: [3][ 60/352]	Time  0.117 ( 0.131)	Data  0.002 ( 0.007)	Loss 1.3446e-01 (1.7303e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:37:07 - Epoch: [3][ 10/352]	Time  0.154 ( 0.142)	Data  0.002 ( 0.024)	Loss 1.3266e-01 (1.5552e-01)	Acc@1  94.53 ( 94.67)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:37:08 - Epoch: [3][ 70/352]	Time  0.142 ( 0.131)	Data  0.002 ( 0.006)	Loss 9.5662e-02 (1.7006e-01)	Acc@1  96.09 ( 94.49)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:37:08 - Epoch: [3][ 20/352]	Time  0.127 ( 0.135)	Data  0.002 ( 0.013)	Loss 9.4512e-02 (1.6919e-01)	Acc@1  96.88 ( 94.53)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:09 - Epoch: [3][ 80/352]	Time  0.132 ( 0.131)	Data  0.003 ( 0.006)	Loss 2.3449e-01 (1.7168e-01)	Acc@1  91.41 ( 94.43)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:37:10 - Epoch: [3][ 30/352]	Time  0.127 ( 0.134)	Data  0.002 ( 0.010)	Loss 1.6414e-01 (1.6754e-01)	Acc@1  92.97 ( 94.71)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:11 - Epoch: [3][ 90/352]	Time  0.130 ( 0.131)	Data  0.002 ( 0.005)	Loss 9.6528e-02 (1.6855e-01)	Acc@1  98.44 ( 94.56)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:11 - Epoch: [3][ 40/352]	Time  0.130 ( 0.132)	Data  0.002 ( 0.008)	Loss 2.0341e-01 (1.6456e-01)	Acc@1  91.41 ( 94.72)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:12 - Epoch: [3][100/352]	Time  0.144 ( 0.131)	Data  0.003 ( 0.005)	Loss 2.0880e-01 (1.6693e-01)	Acc@1  92.19 ( 94.63)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:12 - Epoch: [3][ 50/352]	Time  0.131 ( 0.131)	Data  0.002 ( 0.007)	Loss 1.9912e-01 (1.7056e-01)	Acc@1  93.75 ( 94.42)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:13 - Epoch: [3][110/352]	Time  0.142 ( 0.131)	Data  0.002 ( 0.005)	Loss 1.8849e-01 (1.6569e-01)	Acc@1  96.88 ( 94.67)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:37:13 - Epoch: [3][ 60/352]	Time  0.133 ( 0.131)	Data  0.002 ( 0.006)	Loss 2.4888e-01 (1.6915e-01)	Acc@1  90.62 ( 94.43)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:15 - Epoch: [3][120/352]	Time  0.132 ( 0.131)	Data  0.003 ( 0.005)	Loss 1.4943e-01 (1.6410e-01)	Acc@1  92.97 ( 94.77)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:15 - Epoch: [3][ 70/352]	Time  0.111 ( 0.130)	Data  0.002 ( 0.006)	Loss 1.4429e-01 (1.6762e-01)	Acc@1  94.53 ( 94.54)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:37:16 - Epoch: [3][ 80/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.005)	Loss 8.3712e-02 (1.6549e-01)	Acc@1  98.44 ( 94.67)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:16 - Epoch: [3][130/352]	Time  0.109 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.0984e-01 (1.6281e-01)	Acc@1  92.97 ( 94.82)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:17 - Epoch: [3][ 90/352]	Time  0.128 ( 0.128)	Data  0.002 ( 0.005)	Loss 1.5166e-01 (1.6579e-01)	Acc@1  93.75 ( 94.60)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:17 - Epoch: [3][140/352]	Time  0.141 ( 0.130)	Data  0.002 ( 0.004)	Loss 3.0223e-01 (1.6557e-01)	Acc@1  89.84 ( 94.72)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:37:18 - Epoch: [3][100/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.005)	Loss 1.3858e-01 (1.6422e-01)	Acc@1  93.75 ( 94.64)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:18 - Epoch: [3][150/352]	Time  0.133 ( 0.130)	Data  0.003 ( 0.004)	Loss 2.1390e-01 (1.6487e-01)	Acc@1  93.75 ( 94.72)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:37:19 - Epoch: [3][110/352]	Time  0.128 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.6268e-01 (1.6447e-01)	Acc@1  93.75 ( 94.62)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:37:20 - Epoch: [3][160/352]	Time  0.148 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.2855e-01 (1.6422e-01)	Acc@1  96.09 ( 94.76)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:37:21 - Epoch: [3][120/352]	Time  0.129 ( 0.126)	Data  0.002 ( 0.004)	Loss 2.0216e-01 (1.6530e-01)	Acc@1  93.75 ( 94.60)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:37:21 - Epoch: [3][170/352]	Time  0.130 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.6061e-01 (1.6374e-01)	Acc@1  95.31 ( 94.77)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:22 - Epoch: [3][130/352]	Time  0.103 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.5736e-01 (1.6439e-01)	Acc@1  95.31 ( 94.62)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:37:22 - Epoch: [3][180/352]	Time  0.128 ( 0.130)	Data  0.001 ( 0.004)	Loss 2.3081e-01 (1.6444e-01)	Acc@1  91.41 ( 94.75)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:23 - Epoch: [3][140/352]	Time  0.129 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.0750e-01 (1.6349e-01)	Acc@1  97.66 ( 94.68)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:24 - Epoch: [3][190/352]	Time  0.109 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.3443e-01 (1.6573e-01)	Acc@1  96.88 ( 94.70)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:24 - Epoch: [3][150/352]	Time  0.131 ( 0.126)	Data  0.003 ( 0.004)	Loss 1.9928e-01 (1.6396e-01)	Acc@1  92.97 ( 94.62)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:25 - Epoch: [3][200/352]	Time  0.125 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.5230e-01 (1.6721e-01)	Acc@1  93.75 ( 94.62)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:26 - Epoch: [3][160/352]	Time  0.128 ( 0.126)	Data  0.003 ( 0.004)	Loss 2.0077e-01 (1.6452e-01)	Acc@1  92.97 ( 94.62)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:26 - Epoch: [3][210/352]	Time  0.130 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.5352e-01 (1.6768e-01)	Acc@1  94.53 ( 94.60)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:27 - Epoch: [3][170/352]	Time  0.117 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.8692e-01 (1.6481e-01)	Acc@1  92.97 ( 94.63)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:27 - Epoch: [3][220/352]	Time  0.126 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.3877e-01 (1.6686e-01)	Acc@1  96.88 ( 94.64)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:28 - Epoch: [3][180/352]	Time  0.127 ( 0.126)	Data  0.002 ( 0.004)	Loss 2.2957e-01 (1.6503e-01)	Acc@1  93.75 ( 94.60)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:29 - Epoch: [3][230/352]	Time  0.108 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.8450e-01 (1.6670e-01)	Acc@1  94.53 ( 94.64)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:29 - Epoch: [3][190/352]	Time  0.142 ( 0.126)	Data  0.003 ( 0.004)	Loss 1.1824e-01 (1.6434e-01)	Acc@1  96.88 ( 94.63)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:30 - Epoch: [3][240/352]	Time  0.122 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.3108e-01 (1.6580e-01)	Acc@1  96.09 ( 94.68)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:31 - Epoch: [3][200/352]	Time  0.128 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.3805e-01 (1.6458e-01)	Acc@1  96.88 ( 94.64)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:31 - Epoch: [3][250/352]	Time  0.123 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.6985e-01 (1.6581e-01)	Acc@1  92.97 ( 94.67)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:32 - Epoch: [3][210/352]	Time  0.132 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.1352e-01 (1.6463e-01)	Acc@1  96.09 ( 94.64)	Acc@5  98.44 ( 99.88)
03-Mar-22 09:37:33 - Epoch: [3][260/352]	Time  0.124 ( 0.130)	Data  0.003 ( 0.003)	Loss 1.6665e-01 (1.6581e-01)	Acc@1  95.31 ( 94.68)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:33 - Epoch: [3][220/352]	Time  0.109 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.6724e-01 (1.6399e-01)	Acc@1  94.53 ( 94.65)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:37:34 - Epoch: [3][270/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1338e-01 (1.6519e-01)	Acc@1  95.31 ( 94.69)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:35 - Epoch: [3][230/352]	Time  0.137 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.4411e-01 (1.6464e-01)	Acc@1  93.75 ( 94.63)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:35 - Epoch: [3][280/352]	Time  0.135 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4548e-01 (1.6390e-01)	Acc@1  96.09 ( 94.74)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:36 - Epoch: [3][240/352]	Time  0.136 ( 0.126)	Data  0.003 ( 0.003)	Loss 1.6627e-01 (1.6508e-01)	Acc@1  93.75 ( 94.61)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:36 - Epoch: [3][290/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7909e-01 (1.6368e-01)	Acc@1  93.75 ( 94.75)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:37 - Epoch: [3][250/352]	Time  0.142 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.3612e-01 (1.6571e-01)	Acc@1  96.88 ( 94.59)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:38 - Epoch: [3][300/352]	Time  0.111 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.0070e-01 (1.6411e-01)	Acc@1  93.75 ( 94.75)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:38 - Epoch: [3][260/352]	Time  0.127 ( 0.127)	Data  0.003 ( 0.003)	Loss 1.9608e-01 (1.6549e-01)	Acc@1  93.75 ( 94.59)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:37:39 - Epoch: [3][310/352]	Time  0.114 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.6400e-01 (1.6419e-01)	Acc@1  95.31 ( 94.74)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:40 - Epoch: [3][270/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.0011e-01 (1.6652e-01)	Acc@1  92.19 ( 94.55)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:37:40 - Epoch: [3][320/352]	Time  0.109 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3187e-01 (1.6355e-01)	Acc@1  94.53 ( 94.77)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:41 - Epoch: [3][280/352]	Time  0.136 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.9062e-01 (1.6686e-01)	Acc@1  94.53 ( 94.51)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:41 - Epoch: [3][330/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4351e-01 (1.6359e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:42 - Epoch: [3][290/352]	Time  0.120 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.8649e-01 (1.6700e-01)	Acc@1  92.97 ( 94.52)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:43 - Epoch: [3][340/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.6666e-01 (1.6406e-01)	Acc@1  96.09 ( 94.71)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:43 - Epoch: [3][300/352]	Time  0.111 ( 0.126)	Data  0.002 ( 0.003)	Loss 8.9872e-02 (1.6662e-01)	Acc@1  99.22 ( 94.54)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:44 - Epoch: [3][350/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0296e-01 (1.6387e-01)	Acc@1  97.66 ( 94.71)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:44 - Test: [ 0/20]	Time  0.341 ( 0.341)	Loss 1.5193e-01 (1.5193e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:37:45 - Epoch: [3][310/352]	Time  0.122 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.7018e-01 (1.6716e-01)	Acc@1  93.75 ( 94.51)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:45 - Test: [10/20]	Time  0.092 ( 0.103)	Loss 1.6250e-01 (1.6769e-01)	Acc@1  94.92 ( 94.21)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:37:46 -  * Acc@1 94.400 Acc@5 99.840
03-Mar-22 09:37:46 - Best acc at epoch 3: 94.41999816894531
03-Mar-22 09:37:46 - Epoch: [3][320/352]	Time  0.148 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.3533e-01 (1.6734e-01)	Acc@1  91.41 ( 94.50)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:46 - Epoch: [4][  0/352]	Time  0.356 ( 0.356)	Data  0.245 ( 0.245)	Loss 1.1441e-01 (1.1441e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
03-Mar-22 09:37:47 - Epoch: [3][330/352]	Time  0.105 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.0172e-01 (1.6662e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:48 - Epoch: [4][ 10/352]	Time  0.130 ( 0.150)	Data  0.003 ( 0.024)	Loss 2.3802e-01 (1.7684e-01)	Acc@1  91.41 ( 94.03)	Acc@5  99.22 ( 99.72)
03-Mar-22 09:37:48 - Epoch: [3][340/352]	Time  0.127 ( 0.126)	Data  0.003 ( 0.003)	Loss 1.8797e-01 (1.6638e-01)	Acc@1  96.09 ( 94.56)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:37:49 - Epoch: [4][ 20/352]	Time  0.125 ( 0.141)	Data  0.002 ( 0.014)	Loss 1.4966e-01 (1.6138e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:37:50 - Epoch: [3][350/352]	Time  0.123 ( 0.126)	Data  0.003 ( 0.003)	Loss 1.4421e-01 (1.6641e-01)	Acc@1  93.75 ( 94.55)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:50 - Test: [ 0/20]	Time  0.333 ( 0.333)	Loss 1.7690e-01 (1.7690e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:37:50 - Epoch: [4][ 30/352]	Time  0.109 ( 0.136)	Data  0.002 ( 0.010)	Loss 1.7271e-01 (1.5623e-01)	Acc@1  96.09 ( 95.06)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:37:51 - Test: [10/20]	Time  0.089 ( 0.102)	Loss 1.3970e-01 (1.7114e-01)	Acc@1  94.53 ( 94.42)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:37:52 - Epoch: [4][ 40/352]	Time  0.127 ( 0.134)	Data  0.002 ( 0.008)	Loss 1.1740e-01 (1.4989e-01)	Acc@1  96.88 ( 95.26)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:52 -  * Acc@1 94.380 Acc@5 99.900
03-Mar-22 09:37:52 - Best acc at epoch 3: 94.5199966430664
03-Mar-22 09:37:52 - Epoch: [4][  0/352]	Time  0.349 ( 0.349)	Data  0.224 ( 0.224)	Loss 1.1853e-01 (1.1853e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
03-Mar-22 09:37:53 - Epoch: [4][ 50/352]	Time  0.126 ( 0.131)	Data  0.002 ( 0.007)	Loss 1.6225e-01 (1.5286e-01)	Acc@1  94.53 ( 95.21)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:53 - Epoch: [4][ 10/352]	Time  0.131 ( 0.141)	Data  0.002 ( 0.022)	Loss 1.4596e-01 (1.5076e-01)	Acc@1  96.09 ( 94.74)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:37:54 - Epoch: [4][ 60/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.006)	Loss 1.8388e-01 (1.5369e-01)	Acc@1  95.31 ( 95.17)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:55 - Epoch: [4][ 20/352]	Time  0.126 ( 0.131)	Data  0.003 ( 0.013)	Loss 1.0174e-01 (1.5433e-01)	Acc@1  96.88 ( 94.75)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:55 - Epoch: [4][ 70/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.006)	Loss 2.6094e-01 (1.5545e-01)	Acc@1  92.97 ( 95.16)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:56 - Epoch: [4][ 30/352]	Time  0.130 ( 0.128)	Data  0.002 ( 0.009)	Loss 1.4971e-01 (1.5481e-01)	Acc@1  96.09 ( 94.71)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:37:57 - Epoch: [4][ 80/352]	Time  0.138 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.0344e-01 (1.5617e-01)	Acc@1  96.09 ( 95.11)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:57 - Epoch: [4][ 40/352]	Time  0.130 ( 0.125)	Data  0.002 ( 0.008)	Loss 2.0691e-01 (1.5666e-01)	Acc@1  92.97 ( 94.74)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:58 - Epoch: [4][ 90/352]	Time  0.115 ( 0.129)	Data  0.002 ( 0.005)	Loss 7.6910e-02 (1.5508e-01)	Acc@1  97.66 ( 95.06)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:58 - Epoch: [4][ 50/352]	Time  0.125 ( 0.123)	Data  0.003 ( 0.007)	Loss 1.1835e-01 (1.5709e-01)	Acc@1  96.09 ( 94.76)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:37:59 - Epoch: [4][100/352]	Time  0.138 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.4493e-01 (1.5349e-01)	Acc@1  93.75 ( 95.07)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:59 - Epoch: [4][ 60/352]	Time  0.110 ( 0.123)	Data  0.003 ( 0.006)	Loss 2.3014e-01 (1.5800e-01)	Acc@1  91.41 ( 94.81)	Acc@5  99.22 ( 99.86)
03-Mar-22 09:38:00 - Epoch: [4][110/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.7750e-01 (1.5312e-01)	Acc@1  92.97 ( 95.08)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:01 - Epoch: [4][ 70/352]	Time  0.109 ( 0.122)	Data  0.002 ( 0.005)	Loss 1.3837e-01 (1.5845e-01)	Acc@1  94.53 ( 94.71)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:02 - Epoch: [4][120/352]	Time  0.135 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.1537e-01 (1.5368e-01)	Acc@1  96.88 ( 95.07)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:02 - Epoch: [4][ 80/352]	Time  0.131 ( 0.122)	Data  0.002 ( 0.005)	Loss 1.7887e-01 (1.5794e-01)	Acc@1  96.09 ( 94.83)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:03 - Epoch: [4][130/352]	Time  0.132 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.7363e-01 (1.5442e-01)	Acc@1  92.97 ( 95.01)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:03 - Epoch: [4][ 90/352]	Time  0.129 ( 0.123)	Data  0.003 ( 0.005)	Loss 1.6384e-01 (1.5694e-01)	Acc@1  94.53 ( 94.82)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:04 - Epoch: [4][140/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.0562e-01 (1.5541e-01)	Acc@1  97.66 ( 94.96)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:04 - Epoch: [4][100/352]	Time  0.128 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.4020e-01 (1.5670e-01)	Acc@1  93.75 ( 94.82)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:05 - Epoch: [4][110/352]	Time  0.100 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.9687e-01 (1.5691e-01)	Acc@1  92.19 ( 94.81)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:06 - Epoch: [4][150/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.7073e-01 (1.5587e-01)	Acc@1  93.75 ( 94.92)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:07 - Epoch: [4][120/352]	Time  0.141 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.3085e-01 (1.6041e-01)	Acc@1  95.31 ( 94.65)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:07 - Epoch: [4][160/352]	Time  0.128 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.3363e-01 (1.5594e-01)	Acc@1  91.41 ( 94.95)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:38:08 - Epoch: [4][130/352]	Time  0.130 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.3715e-01 (1.6037e-01)	Acc@1  95.31 ( 94.64)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:08 - Epoch: [4][170/352]	Time  0.131 ( 0.130)	Data  0.003 ( 0.004)	Loss 7.3720e-02 (1.5555e-01)	Acc@1  96.88 ( 94.93)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:09 - Epoch: [4][140/352]	Time  0.121 ( 0.123)	Data  0.002 ( 0.004)	Loss 2.0560e-01 (1.6147e-01)	Acc@1  94.53 ( 94.62)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:38:10 - Epoch: [4][180/352]	Time  0.109 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.1279e-01 (1.5664e-01)	Acc@1  96.88 ( 94.92)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:10 - Epoch: [4][150/352]	Time  0.111 ( 0.123)	Data  0.002 ( 0.004)	Loss 2.1568e-01 (1.6229e-01)	Acc@1  91.41 ( 94.61)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:38:11 - Epoch: [4][190/352]	Time  0.128 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.4320e-01 (1.5792e-01)	Acc@1  93.75 ( 94.87)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:12 - Epoch: [4][160/352]	Time  0.137 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.5992e-01 (1.6235e-01)	Acc@1  96.09 ( 94.62)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:38:12 - Epoch: [4][200/352]	Time  0.130 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.6239e-01 (1.5815e-01)	Acc@1  95.31 ( 94.89)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:13 - Epoch: [4][170/352]	Time  0.113 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.2743e-01 (1.6268e-01)	Acc@1  96.09 ( 94.59)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:13 - Epoch: [4][210/352]	Time  0.122 ( 0.130)	Data  0.003 ( 0.003)	Loss 1.5960e-01 (1.5800e-01)	Acc@1  93.75 ( 94.86)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:14 - Epoch: [4][180/352]	Time  0.136 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6419e-01 (1.6366e-01)	Acc@1  95.31 ( 94.57)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:15 - Epoch: [4][220/352]	Time  0.132 ( 0.130)	Data  0.003 ( 0.003)	Loss 2.7856e-01 (1.5969e-01)	Acc@1  90.62 ( 94.82)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:38:16 - Epoch: [4][190/352]	Time  0.140 ( 0.125)	Data  0.003 ( 0.003)	Loss 2.4513e-01 (1.6419e-01)	Acc@1  93.75 ( 94.58)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:38:16 - Epoch: [4][230/352]	Time  0.097 ( 0.130)	Data  0.001 ( 0.003)	Loss 1.5102e-01 (1.6090e-01)	Acc@1  94.53 ( 94.80)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:17 - Epoch: [4][200/352]	Time  0.136 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.2304e-01 (1.6356e-01)	Acc@1  96.88 ( 94.61)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:17 - Epoch: [4][240/352]	Time  0.136 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3037e-01 (1.6200e-01)	Acc@1  95.31 ( 94.77)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:18 - Epoch: [4][210/352]	Time  0.121 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.1378e-01 (1.6347e-01)	Acc@1  93.75 ( 94.60)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:19 - Epoch: [4][250/352]	Time  0.109 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4775e-01 (1.6225e-01)	Acc@1  93.75 ( 94.75)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:19 - Epoch: [4][220/352]	Time  0.106 ( 0.124)	Data  0.002 ( 0.003)	Loss 2.0019e-01 (1.6318e-01)	Acc@1  91.41 ( 94.62)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:20 - Epoch: [4][260/352]	Time  0.135 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1366e-01 (1.6249e-01)	Acc@1  95.31 ( 94.70)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:21 - Epoch: [4][230/352]	Time  0.123 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.7565e-01 (1.6278e-01)	Acc@1  94.53 ( 94.64)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:21 - Epoch: [4][270/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0423e-01 (1.6178e-01)	Acc@1  96.88 ( 94.72)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:22 - Epoch: [4][240/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.003)	Loss 8.4067e-02 (1.6221e-01)	Acc@1  99.22 ( 94.67)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:22 - Epoch: [4][280/352]	Time  0.135 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.6038e-01 (1.6178e-01)	Acc@1  93.75 ( 94.70)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:23 - Epoch: [4][250/352]	Time  0.116 ( 0.124)	Data  0.002 ( 0.003)	Loss 8.2782e-02 (1.6199e-01)	Acc@1  96.88 ( 94.67)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:24 - Epoch: [4][290/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1411e-01 (1.6168e-01)	Acc@1  96.09 ( 94.69)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:24 - Epoch: [4][260/352]	Time  0.102 ( 0.124)	Data  0.001 ( 0.003)	Loss 2.9601e-01 (1.6229e-01)	Acc@1  89.84 ( 94.64)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:25 - Epoch: [4][300/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5984e-01 (1.6252e-01)	Acc@1  94.53 ( 94.67)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:25 - Epoch: [4][270/352]	Time  0.103 ( 0.123)	Data  0.002 ( 0.003)	Loss 1.3394e-01 (1.6250e-01)	Acc@1  96.09 ( 94.63)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:26 - Epoch: [4][310/352]	Time  0.145 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.2600e-01 (1.6263e-01)	Acc@1  96.88 ( 94.66)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:26 - Epoch: [4][280/352]	Time  0.107 ( 0.123)	Data  0.002 ( 0.003)	Loss 1.3377e-01 (1.6208e-01)	Acc@1  96.09 ( 94.63)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:28 - Epoch: [4][320/352]	Time  0.102 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0725e-01 (1.6245e-01)	Acc@1  96.88 ( 94.67)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:28 - Epoch: [4][290/352]	Time  0.115 ( 0.123)	Data  0.002 ( 0.003)	Loss 1.6450e-01 (1.6282e-01)	Acc@1  94.53 ( 94.59)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:29 - Epoch: [4][330/352]	Time  0.115 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3044e-01 (1.6256e-01)	Acc@1  96.88 ( 94.66)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:29 - Epoch: [4][300/352]	Time  0.103 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.1775e-01 (1.6369e-01)	Acc@1  96.88 ( 94.57)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:30 - Epoch: [4][340/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5708e-01 (1.6216e-01)	Acc@1  93.75 ( 94.67)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:30 - Epoch: [4][310/352]	Time  0.124 ( 0.123)	Data  0.002 ( 0.003)	Loss 1.6178e-01 (1.6351e-01)	Acc@1  93.75 ( 94.56)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:31 - Epoch: [4][320/352]	Time  0.123 ( 0.123)	Data  0.002 ( 0.003)	Loss 5.9187e-02 (1.6322e-01)	Acc@1  99.22 ( 94.60)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:31 - Epoch: [4][350/352]	Time  0.108 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5690e-01 (1.6237e-01)	Acc@1  96.09 ( 94.67)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:32 - Test: [ 0/20]	Time  0.356 ( 0.356)	Loss 1.7736e-01 (1.7736e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:38:32 - Epoch: [4][330/352]	Time  0.092 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.9389e-01 (1.6320e-01)	Acc@1  93.75 ( 94.60)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:33 - Test: [10/20]	Time  0.071 ( 0.102)	Loss 1.2909e-01 (1.6111e-01)	Acc@1  96.48 ( 94.99)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:38:33 - Epoch: [4][340/352]	Time  0.092 ( 0.121)	Data  0.002 ( 0.003)	Loss 1.2349e-01 (1.6307e-01)	Acc@1  95.31 ( 94.61)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:33 -  * Acc@1 94.780 Acc@5 99.860
03-Mar-22 09:38:33 - Best acc at epoch 4: 94.77999877929688
03-Mar-22 09:38:34 - Epoch: [5][  0/352]	Time  0.340 ( 0.340)	Data  0.226 ( 0.226)	Loss 1.4162e-01 (1.4162e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:38:34 - Epoch: [4][350/352]	Time  0.101 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.3037e-01 (1.6231e-01)	Acc@1  94.53 ( 94.64)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:35 - Test: [ 0/20]	Time  0.322 ( 0.322)	Loss 1.7307e-01 (1.7307e-01)	Acc@1  93.36 ( 93.36)	Acc@5 100.00 (100.00)
03-Mar-22 09:38:35 - Epoch: [5][ 10/352]	Time  0.130 ( 0.138)	Data  0.002 ( 0.022)	Loss 1.0685e-01 (1.6494e-01)	Acc@1  98.44 ( 94.60)	Acc@5  99.22 ( 99.72)
03-Mar-22 09:38:36 - Test: [10/20]	Time  0.087 ( 0.098)	Loss 1.6222e-01 (1.6356e-01)	Acc@1  92.97 ( 94.39)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:36 - Epoch: [5][ 20/352]	Time  0.127 ( 0.133)	Data  0.002 ( 0.013)	Loss 1.6888e-01 (1.7281e-01)	Acc@1  94.53 ( 94.20)	Acc@5 100.00 ( 99.78)
03-Mar-22 09:38:36 -  * Acc@1 94.340 Acc@5 99.920
03-Mar-22 09:38:36 - Best acc at epoch 4: 94.5199966430664
03-Mar-22 09:38:37 - Epoch: [5][  0/352]	Time  0.354 ( 0.354)	Data  0.237 ( 0.237)	Loss 1.7563e-01 (1.7563e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
03-Mar-22 09:38:37 - Epoch: [5][ 30/352]	Time  0.121 ( 0.128)	Data  0.002 ( 0.009)	Loss 1.0905e-01 (1.6675e-01)	Acc@1  97.66 ( 94.41)	Acc@5  99.22 ( 99.82)
03-Mar-22 09:38:38 - Epoch: [5][ 10/352]	Time  0.128 ( 0.144)	Data  0.002 ( 0.023)	Loss 1.4214e-01 (1.5393e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:38:39 - Epoch: [5][ 40/352]	Time  0.149 ( 0.129)	Data  0.002 ( 0.008)	Loss 1.0632e-01 (1.6089e-01)	Acc@1  96.88 ( 94.74)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:38:39 - Epoch: [5][ 20/352]	Time  0.136 ( 0.137)	Data  0.002 ( 0.013)	Loss 2.6260e-01 (1.6609e-01)	Acc@1  90.62 ( 94.23)	Acc@5  99.22 ( 99.81)
03-Mar-22 09:38:40 - Epoch: [5][ 50/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.007)	Loss 1.1993e-01 (1.5808e-01)	Acc@1  96.88 ( 94.94)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:41 - Epoch: [5][ 30/352]	Time  0.140 ( 0.134)	Data  0.003 ( 0.010)	Loss 1.6171e-01 (1.5898e-01)	Acc@1  94.53 ( 94.78)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:38:41 - Epoch: [5][ 60/352]	Time  0.108 ( 0.129)	Data  0.002 ( 0.006)	Loss 1.1397e-01 (1.5845e-01)	Acc@1  96.88 ( 94.94)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:38:42 - Epoch: [5][ 40/352]	Time  0.135 ( 0.132)	Data  0.002 ( 0.008)	Loss 7.8852e-02 (1.5519e-01)	Acc@1  98.44 ( 94.80)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:38:43 - Epoch: [5][ 70/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.005)	Loss 2.7760e-01 (1.6008e-01)	Acc@1  90.62 ( 94.85)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:43 - Epoch: [5][ 50/352]	Time  0.136 ( 0.133)	Data  0.002 ( 0.007)	Loss 1.8932e-01 (1.5908e-01)	Acc@1  94.53 ( 94.81)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:38:44 - Epoch: [5][ 80/352]	Time  0.152 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.5681e-01 (1.6114e-01)	Acc@1  94.53 ( 94.76)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:44 - Epoch: [5][ 60/352]	Time  0.124 ( 0.132)	Data  0.002 ( 0.006)	Loss 1.4079e-01 (1.6082e-01)	Acc@1  96.09 ( 94.70)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:38:45 - Epoch: [5][ 90/352]	Time  0.132 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.4539e-01 (1.5956e-01)	Acc@1  93.75 ( 94.73)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:46 - Epoch: [5][ 70/352]	Time  0.136 ( 0.132)	Data  0.002 ( 0.006)	Loss 1.8405e-01 (1.6176e-01)	Acc@1  92.19 ( 94.66)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:47 - Epoch: [5][100/352]	Time  0.110 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.0520e-01 (1.5992e-01)	Acc@1  96.09 ( 94.70)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:47 - Epoch: [5][ 80/352]	Time  0.140 ( 0.132)	Data  0.002 ( 0.005)	Loss 2.1151e-01 (1.5989e-01)	Acc@1  92.19 ( 94.68)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:48 - Epoch: [5][110/352]	Time  0.127 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.7520e-01 (1.5966e-01)	Acc@1  93.75 ( 94.68)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:48 - Epoch: [5][ 90/352]	Time  0.137 ( 0.132)	Data  0.002 ( 0.005)	Loss 8.3122e-02 (1.5833e-01)	Acc@1  96.88 ( 94.81)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:38:49 - Epoch: [5][120/352]	Time  0.126 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.2916e-01 (1.6034e-01)	Acc@1  95.31 ( 94.69)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:50 - Epoch: [5][100/352]	Time  0.149 ( 0.131)	Data  0.002 ( 0.005)	Loss 1.0695e-01 (1.5783e-01)	Acc@1  96.88 ( 94.82)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:50 - Epoch: [5][130/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.3240e-01 (1.5784e-01)	Acc@1  97.66 ( 94.85)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:38:51 - Epoch: [5][110/352]	Time  0.137 ( 0.131)	Data  0.003 ( 0.004)	Loss 1.3801e-01 (1.5964e-01)	Acc@1  96.88 ( 94.75)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:52 - Epoch: [5][140/352]	Time  0.130 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.6372e-01 (1.5984e-01)	Acc@1  90.62 ( 94.73)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:52 - Epoch: [5][120/352]	Time  0.119 ( 0.132)	Data  0.002 ( 0.004)	Loss 2.5389e-01 (1.6050e-01)	Acc@1  92.97 ( 94.73)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:53 - Epoch: [5][150/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.1975e-01 (1.5854e-01)	Acc@1  96.09 ( 94.77)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:54 - Epoch: [5][130/352]	Time  0.125 ( 0.131)	Data  0.002 ( 0.004)	Loss 1.0196e-01 (1.6065e-01)	Acc@1  96.09 ( 94.72)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:54 - Epoch: [5][160/352]	Time  0.137 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.9128e-01 (1.5947e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:55 - Epoch: [5][140/352]	Time  0.149 ( 0.131)	Data  0.002 ( 0.004)	Loss 2.3729e-01 (1.6198e-01)	Acc@1  92.19 ( 94.68)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:56 - Epoch: [5][170/352]	Time  0.115 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.2061e-01 (1.6046e-01)	Acc@1  92.19 ( 94.76)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:56 - Epoch: [5][150/352]	Time  0.135 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.6519e-01 (1.6126e-01)	Acc@1  94.53 ( 94.72)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:57 - Epoch: [5][180/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.3254e-01 (1.6062e-01)	Acc@1  92.97 ( 94.75)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:57 - Epoch: [5][160/352]	Time  0.114 ( 0.131)	Data  0.002 ( 0.004)	Loss 2.0088e-01 (1.6120e-01)	Acc@1  92.19 ( 94.71)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:58 - Epoch: [5][190/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4632e-01 (1.6090e-01)	Acc@1  96.88 ( 94.72)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:59 - Epoch: [5][170/352]	Time  0.135 ( 0.131)	Data  0.003 ( 0.004)	Loss 1.1669e-01 (1.6100e-01)	Acc@1  98.44 ( 94.74)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:59 - Epoch: [5][200/352]	Time  0.130 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.4442e-01 (1.6058e-01)	Acc@1  96.09 ( 94.71)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:00 - Epoch: [5][180/352]	Time  0.114 ( 0.131)	Data  0.002 ( 0.004)	Loss 1.9763e-01 (1.6050e-01)	Acc@1  92.19 ( 94.74)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:01 - Epoch: [5][210/352]	Time  0.109 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4196e-01 (1.5988e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:01 - Epoch: [5][190/352]	Time  0.136 ( 0.131)	Data  0.003 ( 0.003)	Loss 2.1552e-01 (1.6082e-01)	Acc@1  94.53 ( 94.72)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:39:02 - Epoch: [5][220/352]	Time  0.128 ( 0.128)	Data  0.003 ( 0.003)	Loss 2.7248e-01 (1.6071e-01)	Acc@1  91.41 ( 94.72)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:03 - Epoch: [5][200/352]	Time  0.116 ( 0.130)	Data  0.001 ( 0.003)	Loss 6.6137e-02 (1.6014e-01)	Acc@1  98.44 ( 94.75)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:03 - Epoch: [5][230/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.5925e-01 (1.6122e-01)	Acc@1  91.41 ( 94.69)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:39:04 - Epoch: [5][210/352]	Time  0.139 ( 0.130)	Data  0.003 ( 0.003)	Loss 1.8849e-01 (1.5987e-01)	Acc@1  92.97 ( 94.75)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:05 - Epoch: [5][240/352]	Time  0.142 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.6180e-01 (1.6080e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:05 - Epoch: [5][220/352]	Time  0.140 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.2135e-01 (1.6015e-01)	Acc@1  96.88 ( 94.74)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:06 - Epoch: [5][250/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7257e-01 (1.6101e-01)	Acc@1  94.53 ( 94.69)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:06 - Epoch: [5][230/352]	Time  0.148 ( 0.130)	Data  0.002 ( 0.003)	Loss 2.3851e-01 (1.6016e-01)	Acc@1  91.41 ( 94.75)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:07 - Epoch: [5][260/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.9428e-01 (1.6099e-01)	Acc@1  92.97 ( 94.69)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:08 - Epoch: [5][240/352]	Time  0.154 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.3747e-01 (1.5932e-01)	Acc@1  96.09 ( 94.76)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:08 - Epoch: [5][270/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7321e-01 (1.6065e-01)	Acc@1  93.75 ( 94.71)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:39:09 - Epoch: [5][250/352]	Time  0.140 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.4822e-01 (1.5953e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:10 - Epoch: [5][280/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0699e-01 (1.6027e-01)	Acc@1  96.09 ( 94.72)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:10 - Epoch: [5][260/352]	Time  0.112 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5452e-01 (1.5836e-01)	Acc@1  94.53 ( 94.82)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:11 - Epoch: [5][290/352]	Time  0.127 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.6636e-01 (1.6057e-01)	Acc@1  94.53 ( 94.71)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:11 - Epoch: [5][270/352]	Time  0.137 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1703e-01 (1.5859e-01)	Acc@1  97.66 ( 94.84)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:12 - Epoch: [5][300/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7311e-01 (1.6073e-01)	Acc@1  92.97 ( 94.70)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:39:13 - Epoch: [5][280/352]	Time  0.139 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.2128e-01 (1.5920e-01)	Acc@1  92.97 ( 94.78)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:14 - Epoch: [5][310/352]	Time  0.134 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3878e-01 (1.6038e-01)	Acc@1  94.53 ( 94.69)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:14 - Epoch: [5][290/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0590e-01 (1.5921e-01)	Acc@1  96.88 ( 94.75)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:15 - Epoch: [5][320/352]	Time  0.139 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3506e-01 (1.6054e-01)	Acc@1  97.66 ( 94.70)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:15 - Epoch: [5][300/352]	Time  0.136 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4131e-01 (1.5952e-01)	Acc@1  95.31 ( 94.77)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:16 - Epoch: [5][330/352]	Time  0.104 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.3008e-01 (1.6075e-01)	Acc@1  91.41 ( 94.69)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:17 - Epoch: [5][310/352]	Time  0.138 ( 0.130)	Data  0.003 ( 0.003)	Loss 1.7321e-01 (1.6085e-01)	Acc@1  93.75 ( 94.73)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:17 - Epoch: [5][340/352]	Time  0.140 ( 0.129)	Data  0.001 ( 0.003)	Loss 1.8369e-01 (1.6120e-01)	Acc@1  93.75 ( 94.68)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:18 - Epoch: [5][320/352]	Time  0.139 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7632e-01 (1.6082e-01)	Acc@1  93.75 ( 94.73)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:19 - Epoch: [5][350/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4019e-01 (1.6050e-01)	Acc@1  94.53 ( 94.69)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:19 - Epoch: [5][330/352]	Time  0.125 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.0108e-01 (1.6134e-01)	Acc@1  96.88 ( 94.72)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:19 - Test: [ 0/20]	Time  0.323 ( 0.323)	Loss 1.7837e-01 (1.7837e-01)	Acc@1  94.53 ( 94.53)	Acc@5  99.61 ( 99.61)
03-Mar-22 09:39:20 - Test: [10/20]	Time  0.072 ( 0.098)	Loss 1.7533e-01 (1.6922e-01)	Acc@1  94.92 ( 94.64)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:20 - Epoch: [5][340/352]	Time  0.161 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1426e-01 (1.6131e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:21 -  * Acc@1 94.660 Acc@5 99.880
03-Mar-22 09:39:21 - Best acc at epoch 5: 94.77999877929688
03-Mar-22 09:39:21 - Epoch: [6][  0/352]	Time  0.341 ( 0.341)	Data  0.241 ( 0.241)	Loss 1.5468e-01 (1.5468e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:39:22 - Epoch: [5][350/352]	Time  0.100 ( 0.129)	Data  0.001 ( 0.003)	Loss 1.9660e-01 (1.6146e-01)	Acc@1  92.97 ( 94.74)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:22 - Test: [ 0/20]	Time  0.309 ( 0.309)	Loss 1.3605e-01 (1.3605e-01)	Acc@1  94.92 ( 94.92)	Acc@5 100.00 (100.00)
03-Mar-22 09:39:22 - Epoch: [6][ 10/352]	Time  0.134 ( 0.144)	Data  0.003 ( 0.024)	Loss 1.6643e-01 (1.7830e-01)	Acc@1  95.31 ( 94.39)	Acc@5  99.22 ( 99.86)
03-Mar-22 09:39:23 - Test: [10/20]	Time  0.069 ( 0.098)	Loss 1.4809e-01 (1.6415e-01)	Acc@1  94.53 ( 94.42)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:24 -  * Acc@1 94.300 Acc@5 99.880
03-Mar-22 09:39:24 - Best acc at epoch 5: 94.5199966430664
03-Mar-22 09:39:24 - Epoch: [6][ 20/352]	Time  0.137 ( 0.134)	Data  0.002 ( 0.014)	Loss 1.2918e-01 (1.6959e-01)	Acc@1  96.09 ( 94.79)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:24 - Epoch: [6][  0/352]	Time  0.371 ( 0.371)	Data  0.241 ( 0.241)	Loss 1.3188e-01 (1.3188e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:39:25 - Epoch: [6][ 30/352]	Time  0.133 ( 0.134)	Data  0.002 ( 0.010)	Loss 2.0074e-01 (1.6299e-01)	Acc@1  92.97 ( 94.91)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:39:25 - Epoch: [6][ 10/352]	Time  0.138 ( 0.153)	Data  0.002 ( 0.024)	Loss 1.3683e-01 (1.4527e-01)	Acc@1  96.09 ( 95.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:39:26 - Epoch: [6][ 40/352]	Time  0.145 ( 0.130)	Data  0.002 ( 0.008)	Loss 1.1852e-01 (1.6422e-01)	Acc@1  96.88 ( 94.66)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:39:27 - Epoch: [6][ 20/352]	Time  0.127 ( 0.141)	Data  0.002 ( 0.014)	Loss 1.4570e-01 (1.4319e-01)	Acc@1  95.31 ( 95.46)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:39:28 - Epoch: [6][ 50/352]	Time  0.145 ( 0.132)	Data  0.003 ( 0.007)	Loss 2.1169e-01 (1.6347e-01)	Acc@1  93.75 ( 94.55)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:39:28 - Epoch: [6][ 30/352]	Time  0.145 ( 0.139)	Data  0.002 ( 0.010)	Loss 1.2816e-01 (1.3922e-01)	Acc@1  95.31 ( 95.49)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:39:29 - Epoch: [6][ 60/352]	Time  0.131 ( 0.131)	Data  0.002 ( 0.006)	Loss 2.5048e-01 (1.6166e-01)	Acc@1  90.62 ( 94.60)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:29 - Epoch: [6][ 40/352]	Time  0.152 ( 0.138)	Data  0.003 ( 0.008)	Loss 2.0290e-01 (1.4947e-01)	Acc@1  92.97 ( 95.06)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:30 - Epoch: [6][ 70/352]	Time  0.141 ( 0.131)	Data  0.002 ( 0.006)	Loss 2.1056e-01 (1.5865e-01)	Acc@1  92.19 ( 94.67)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:31 - Epoch: [6][ 50/352]	Time  0.128 ( 0.136)	Data  0.002 ( 0.007)	Loss 1.8019e-01 (1.5336e-01)	Acc@1  94.53 ( 94.88)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:39:31 - Epoch: [6][ 80/352]	Time  0.110 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.5500e-01 (1.5853e-01)	Acc@1  93.75 ( 94.63)	Acc@5  99.22 ( 99.94)
03-Mar-22 09:39:32 - Epoch: [6][ 60/352]	Time  0.103 ( 0.133)	Data  0.002 ( 0.006)	Loss 1.4540e-01 (1.5446e-01)	Acc@1  95.31 ( 94.84)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:39:33 - Epoch: [6][ 90/352]	Time  0.124 ( 0.130)	Data  0.001 ( 0.005)	Loss 1.7124e-01 (1.5908e-01)	Acc@1  94.53 ( 94.65)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:33 - Epoch: [6][ 70/352]	Time  0.096 ( 0.131)	Data  0.002 ( 0.006)	Loss 1.7363e-01 (1.5419e-01)	Acc@1  93.75 ( 94.85)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:39:34 - Epoch: [6][100/352]	Time  0.112 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.1415e-01 (1.5798e-01)	Acc@1  94.53 ( 94.67)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:34 - Epoch: [6][ 80/352]	Time  0.112 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.4799e-01 (1.5479e-01)	Acc@1  96.88 ( 94.83)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:39:35 - Epoch: [6][110/352]	Time  0.141 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.1940e-01 (1.5777e-01)	Acc@1  94.53 ( 94.62)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:35 - Epoch: [6][ 90/352]	Time  0.124 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.6681e-01 (1.5537e-01)	Acc@1  93.75 ( 94.81)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:39:37 - Epoch: [6][120/352]	Time  0.144 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.3022e-01 (1.5649e-01)	Acc@1  94.53 ( 94.72)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:37 - Epoch: [6][100/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.005)	Loss 2.5584e-01 (1.5867e-01)	Acc@1  91.41 ( 94.72)	Acc@5  99.22 ( 99.86)
03-Mar-22 09:39:38 - Epoch: [6][130/352]	Time  0.148 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.6851e-01 (1.5783e-01)	Acc@1  95.31 ( 94.66)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:38 - Epoch: [6][110/352]	Time  0.105 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.0346e-01 (1.5891e-01)	Acc@1  96.88 ( 94.70)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:39:39 - Epoch: [6][120/352]	Time  0.127 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.8285e-01 (1.5875e-01)	Acc@1  92.97 ( 94.71)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:39:39 - Epoch: [6][140/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.1690e-01 (1.5779e-01)	Acc@1  96.88 ( 94.67)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:40 - Epoch: [6][130/352]	Time  0.127 ( 0.128)	Data  0.002 ( 0.004)	Loss 2.2728e-01 (1.5884e-01)	Acc@1  95.31 ( 94.73)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:39:41 - Epoch: [6][150/352]	Time  0.141 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.3443e-01 (1.5694e-01)	Acc@1  96.09 ( 94.71)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:42 - Epoch: [6][140/352]	Time  0.127 ( 0.128)	Data  0.003 ( 0.004)	Loss 1.3976e-01 (1.5810e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:42 - Epoch: [6][160/352]	Time  0.139 ( 0.130)	Data  0.003 ( 0.004)	Loss 2.4405e-01 (1.5752e-01)	Acc@1  93.75 ( 94.68)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:43 - Epoch: [6][150/352]	Time  0.128 ( 0.128)	Data  0.002 ( 0.004)	Loss 8.9253e-02 (1.5815e-01)	Acc@1  96.09 ( 94.73)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:39:43 - Epoch: [6][170/352]	Time  0.142 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.2309e-01 (1.5703e-01)	Acc@1  92.19 ( 94.72)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:39:44 - Epoch: [6][160/352]	Time  0.118 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.6292e-01 (1.5768e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:39:44 - Epoch: [6][180/352]	Time  0.132 ( 0.131)	Data  0.003 ( 0.004)	Loss 1.8122e-01 (1.5833e-01)	Acc@1  92.97 ( 94.66)	Acc@5  99.22 ( 99.94)
03-Mar-22 09:39:45 - Epoch: [6][170/352]	Time  0.124 ( 0.127)	Data  0.002 ( 0.004)	Loss 9.3485e-02 (1.5576e-01)	Acc@1  97.66 ( 94.84)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:39:46 - Epoch: [6][190/352]	Time  0.137 ( 0.131)	Data  0.002 ( 0.004)	Loss 1.3114e-01 (1.5668e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:47 - Epoch: [6][180/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.2450e-01 (1.5470e-01)	Acc@1  96.09 ( 94.86)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:39:47 - Epoch: [6][200/352]	Time  0.101 ( 0.131)	Data  0.002 ( 0.004)	Loss 1.7893e-01 (1.5557e-01)	Acc@1  94.53 ( 94.75)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:39:48 - Epoch: [6][190/352]	Time  0.108 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.3045e-01 (1.5442e-01)	Acc@1  93.75 ( 94.88)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:48 - Epoch: [6][210/352]	Time  0.132 ( 0.131)	Data  0.002 ( 0.003)	Loss 2.0293e-01 (1.5408e-01)	Acc@1  92.19 ( 94.83)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:49 - Epoch: [6][200/352]	Time  0.104 ( 0.127)	Data  0.001 ( 0.003)	Loss 1.9295e-01 (1.5492e-01)	Acc@1  92.97 ( 94.87)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:50 - Epoch: [6][220/352]	Time  0.112 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.3835e-01 (1.5473e-01)	Acc@1  96.88 ( 94.82)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:50 - Epoch: [6][210/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.6500e-01 (1.5568e-01)	Acc@1  94.53 ( 94.87)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:51 - Epoch: [6][230/352]	Time  0.110 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.1775e-01 (1.5508e-01)	Acc@1  95.31 ( 94.80)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:52 - Epoch: [6][220/352]	Time  0.110 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.4895e-01 (1.5636e-01)	Acc@1  91.41 ( 94.84)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:52 - Epoch: [6][240/352]	Time  0.135 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.7564e-01 (1.5653e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:53 - Epoch: [6][230/352]	Time  0.132 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.3050e-01 (1.5616e-01)	Acc@1  96.88 ( 94.84)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:54 - Epoch: [6][250/352]	Time  0.129 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.8256e-01 (1.5688e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:54 - Epoch: [6][240/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.4140e-01 (1.5602e-01)	Acc@1  92.19 ( 94.85)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:55 - Epoch: [6][260/352]	Time  0.134 ( 0.131)	Data  0.003 ( 0.003)	Loss 1.5259e-01 (1.5657e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:55 - Epoch: [6][250/352]	Time  0.105 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.6777e-01 (1.5599e-01)	Acc@1  93.75 ( 94.87)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:56 - Epoch: [6][270/352]	Time  0.131 ( 0.131)	Data  0.002 ( 0.003)	Loss 2.2077e-01 (1.5623e-01)	Acc@1  93.75 ( 94.78)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:57 - Epoch: [6][260/352]	Time  0.128 ( 0.126)	Data  0.001 ( 0.003)	Loss 2.5071e-01 (1.5597e-01)	Acc@1  91.41 ( 94.87)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:39:58 - Epoch: [6][280/352]	Time  0.112 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.5435e-01 (1.5572e-01)	Acc@1  92.19 ( 94.78)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:58 - Epoch: [6][270/352]	Time  0.110 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.2660e-01 (1.5664e-01)	Acc@1  96.09 ( 94.86)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:59 - Epoch: [6][290/352]	Time  0.141 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.7686e-01 (1.5613e-01)	Acc@1  94.53 ( 94.78)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:59 - Epoch: [6][280/352]	Time  0.107 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.5520e-01 (1.5756e-01)	Acc@1  95.31 ( 94.84)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:40:00 - Epoch: [6][300/352]	Time  0.123 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.0020e-01 (1.5532e-01)	Acc@1  97.66 ( 94.81)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:00 - Epoch: [6][290/352]	Time  0.128 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.3880e-01 (1.5728e-01)	Acc@1  95.31 ( 94.86)	Acc@5  98.44 ( 99.89)
03-Mar-22 09:40:02 - Epoch: [6][310/352]	Time  0.136 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.4182e-01 (1.5584e-01)	Acc@1  96.09 ( 94.79)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:02 - Epoch: [6][300/352]	Time  0.125 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.4779e-01 (1.5789e-01)	Acc@1  95.31 ( 94.82)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:40:03 - Epoch: [6][320/352]	Time  0.124 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.6396e-01 (1.5634e-01)	Acc@1  94.53 ( 94.78)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:03 - Epoch: [6][310/352]	Time  0.132 ( 0.126)	Data  0.003 ( 0.003)	Loss 1.2582e-01 (1.5853e-01)	Acc@1  97.66 ( 94.82)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:40:04 - Epoch: [6][330/352]	Time  0.145 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.6013e-01 (1.5674e-01)	Acc@1  94.53 ( 94.75)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:04 - Epoch: [6][320/352]	Time  0.127 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.0761e-01 (1.5754e-01)	Acc@1  92.97 ( 94.84)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:40:05 - Epoch: [6][330/352]	Time  0.133 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.0460e-01 (1.5810e-01)	Acc@1  92.19 ( 94.80)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:40:05 - Epoch: [6][340/352]	Time  0.133 ( 0.131)	Data  0.003 ( 0.003)	Loss 1.3074e-01 (1.5752e-01)	Acc@1  96.09 ( 94.73)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:07 - Epoch: [6][350/352]	Time  0.132 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.9634e-01 (1.5789e-01)	Acc@1  92.97 ( 94.74)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:07 - Epoch: [6][340/352]	Time  0.130 ( 0.126)	Data  0.003 ( 0.003)	Loss 2.6831e-01 (1.5812e-01)	Acc@1  89.06 ( 94.79)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:40:07 - Test: [ 0/20]	Time  0.342 ( 0.342)	Loss 1.4421e-01 (1.4421e-01)	Acc@1  95.70 ( 95.70)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:08 - Epoch: [6][350/352]	Time  0.146 ( 0.126)	Data  0.003 ( 0.003)	Loss 2.0355e-01 (1.5800e-01)	Acc@1  94.53 ( 94.79)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:40:08 - Test: [10/20]	Time  0.071 ( 0.095)	Loss 1.5601e-01 (1.7426e-01)	Acc@1  94.92 ( 94.28)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:40:08 - Test: [ 0/20]	Time  0.295 ( 0.295)	Loss 1.6076e-01 (1.6076e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:09 -  * Acc@1 94.480 Acc@5 99.840
03-Mar-22 09:40:09 - Best acc at epoch 6: 94.77999877929688
03-Mar-22 09:40:09 - Test: [10/20]	Time  0.073 ( 0.095)	Loss 1.4419e-01 (1.7500e-01)	Acc@1  94.92 ( 94.07)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:40:09 - Epoch: [7][  0/352]	Time  0.336 ( 0.336)	Data  0.237 ( 0.237)	Loss 1.5831e-01 (1.5831e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:10 -  * Acc@1 93.900 Acc@5 99.860
03-Mar-22 09:40:10 - Best acc at epoch 6: 94.5199966430664
03-Mar-22 09:40:10 - Epoch: [7][  0/352]	Time  0.354 ( 0.354)	Data  0.247 ( 0.247)	Loss 2.1995e-01 (2.1995e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:10 - Epoch: [7][ 10/352]	Time  0.125 ( 0.140)	Data  0.002 ( 0.024)	Loss 1.9116e-01 (1.6715e-01)	Acc@1  90.62 ( 94.82)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:11 - Epoch: [7][ 10/352]	Time  0.109 ( 0.148)	Data  0.002 ( 0.024)	Loss 1.4878e-01 (1.6818e-01)	Acc@1  94.53 ( 93.96)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:12 - Epoch: [7][ 20/352]	Time  0.133 ( 0.135)	Data  0.002 ( 0.014)	Loss 1.5970e-01 (1.6300e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:13 - Epoch: [7][ 20/352]	Time  0.132 ( 0.140)	Data  0.002 ( 0.014)	Loss 2.4325e-01 (1.6589e-01)	Acc@1  95.31 ( 94.61)	Acc@5  99.22 ( 99.96)
03-Mar-22 09:40:13 - Epoch: [7][ 30/352]	Time  0.133 ( 0.134)	Data  0.002 ( 0.010)	Loss 2.3479e-01 (1.6076e-01)	Acc@1  92.97 ( 94.88)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:14 - Epoch: [7][ 30/352]	Time  0.139 ( 0.138)	Data  0.002 ( 0.010)	Loss 1.5528e-01 (1.6471e-01)	Acc@1  93.75 ( 94.71)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:14 - Epoch: [7][ 40/352]	Time  0.126 ( 0.134)	Data  0.002 ( 0.008)	Loss 1.6376e-01 (1.5376e-01)	Acc@1  94.53 ( 95.14)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:40:15 - Epoch: [7][ 40/352]	Time  0.153 ( 0.138)	Data  0.002 ( 0.008)	Loss 2.3056e-01 (1.6611e-01)	Acc@1  90.62 ( 94.65)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:40:16 - Epoch: [7][ 50/352]	Time  0.128 ( 0.133)	Data  0.002 ( 0.007)	Loss 1.1713e-01 (1.5324e-01)	Acc@1  96.88 ( 95.10)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:40:17 - Epoch: [7][ 50/352]	Time  0.128 ( 0.136)	Data  0.002 ( 0.007)	Loss 1.1372e-01 (1.5963e-01)	Acc@1  97.66 ( 94.85)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:40:17 - Epoch: [7][ 60/352]	Time  0.129 ( 0.132)	Data  0.002 ( 0.006)	Loss 8.3344e-02 (1.5275e-01)	Acc@1  97.66 ( 95.09)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:40:18 - Epoch: [7][ 60/352]	Time  0.138 ( 0.135)	Data  0.003 ( 0.006)	Loss 1.5226e-01 (1.6315e-01)	Acc@1  95.31 ( 94.63)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:18 - Epoch: [7][ 70/352]	Time  0.132 ( 0.131)	Data  0.002 ( 0.006)	Loss 9.8223e-02 (1.5100e-01)	Acc@1  96.09 ( 95.14)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:40:19 - Epoch: [7][ 70/352]	Time  0.115 ( 0.133)	Data  0.002 ( 0.006)	Loss 1.6032e-01 (1.6147e-01)	Acc@1  96.09 ( 94.74)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:40:19 - Epoch: [7][ 80/352]	Time  0.129 ( 0.131)	Data  0.002 ( 0.005)	Loss 1.4579e-01 (1.5113e-01)	Acc@1  92.97 ( 95.05)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:21 - Epoch: [7][ 80/352]	Time  0.152 ( 0.135)	Data  0.002 ( 0.005)	Loss 1.3138e-01 (1.6078e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:21 - Epoch: [7][ 90/352]	Time  0.130 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.7410e-01 (1.5303e-01)	Acc@1  93.75 ( 95.04)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:22 - Epoch: [7][100/352]	Time  0.126 ( 0.130)	Data  0.002 ( 0.005)	Loss 2.0997e-01 (1.5451e-01)	Acc@1  92.97 ( 94.98)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:22 - Epoch: [7][ 90/352]	Time  0.136 ( 0.135)	Data  0.002 ( 0.005)	Loss 2.1819e-01 (1.6203e-01)	Acc@1  92.19 ( 94.64)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:23 - Epoch: [7][110/352]	Time  0.127 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.0785e-01 (1.5554e-01)	Acc@1  92.97 ( 94.91)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:23 - Epoch: [7][100/352]	Time  0.110 ( 0.134)	Data  0.002 ( 0.005)	Loss 1.2718e-01 (1.5917e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:25 - Epoch: [7][120/352]	Time  0.128 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.2228e-01 (1.5624e-01)	Acc@1  96.09 ( 94.92)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:40:25 - Epoch: [7][110/352]	Time  0.130 ( 0.133)	Data  0.002 ( 0.005)	Loss 1.0310e-01 (1.5802e-01)	Acc@1  97.66 ( 94.82)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:26 - Epoch: [7][130/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.2827e-01 (1.5771e-01)	Acc@1  92.19 ( 94.84)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:26 - Epoch: [7][120/352]	Time  0.115 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.5325e-01 (1.5797e-01)	Acc@1  96.88 ( 94.87)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:27 - Epoch: [7][140/352]	Time  0.127 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.1284e-01 (1.5696e-01)	Acc@1  96.88 ( 94.87)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:40:27 - Epoch: [7][130/352]	Time  0.129 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.7756e-01 (1.5808e-01)	Acc@1  93.75 ( 94.79)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:40:28 - Epoch: [7][150/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.9895e-01 (1.5717e-01)	Acc@1  92.19 ( 94.87)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:40:29 - Epoch: [7][140/352]	Time  0.126 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.5413e-01 (1.5705e-01)	Acc@1  92.97 ( 94.81)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:40:30 - Epoch: [7][160/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.3571e-01 (1.5590e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:30 - Epoch: [7][150/352]	Time  0.126 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.2641e-01 (1.5771e-01)	Acc@1  96.09 ( 94.78)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:31 - Epoch: [7][170/352]	Time  0.108 ( 0.129)	Data  0.001 ( 0.004)	Loss 1.9905e-01 (1.5577e-01)	Acc@1  95.31 ( 94.94)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:40:31 - Epoch: [7][160/352]	Time  0.123 ( 0.131)	Data  0.002 ( 0.004)	Loss 1.5811e-01 (1.5744e-01)	Acc@1  94.53 ( 94.79)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:32 - Epoch: [7][180/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.6735e-01 (1.5494e-01)	Acc@1  92.97 ( 94.93)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:32 - Epoch: [7][170/352]	Time  0.127 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.2565e-01 (1.5745e-01)	Acc@1  96.09 ( 94.82)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:40:33 - Epoch: [7][190/352]	Time  0.124 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.1855e-01 (1.5499e-01)	Acc@1  95.31 ( 94.92)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:33 - Epoch: [7][180/352]	Time  0.104 ( 0.130)	Data  0.002 ( 0.004)	Loss 6.1273e-02 (1.5678e-01)	Acc@1  97.66 ( 94.82)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:35 - Epoch: [7][200/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6299e-01 (1.5530e-01)	Acc@1  95.31 ( 94.90)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:40:35 - Epoch: [7][190/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.1534e-01 (1.5768e-01)	Acc@1  94.53 ( 94.80)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:40:36 - Epoch: [7][210/352]	Time  0.126 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6473e-01 (1.5461e-01)	Acc@1  95.31 ( 94.93)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:40:36 - Epoch: [7][200/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.4164e-01 (1.5713e-01)	Acc@1  92.19 ( 94.80)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:40:37 - Epoch: [7][210/352]	Time  0.114 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.9608e-01 (1.5711e-01)	Acc@1  90.62 ( 94.79)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:40:37 - Epoch: [7][220/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4517e-01 (1.5439e-01)	Acc@1  96.09 ( 94.92)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:40:38 - Epoch: [7][220/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0892e-01 (1.5658e-01)	Acc@1  96.88 ( 94.82)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:40:38 - Epoch: [7][230/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.5082e-01 (1.5428e-01)	Acc@1  96.88 ( 94.95)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:40 - Epoch: [7][230/352]	Time  0.121 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5889e-01 (1.5662e-01)	Acc@1  95.31 ( 94.82)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:40 - Epoch: [7][240/352]	Time  0.126 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.3333e-01 (1.5324e-01)	Acc@1  93.75 ( 95.00)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:41 - Epoch: [7][240/352]	Time  0.126 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6251e-01 (1.5626e-01)	Acc@1  94.53 ( 94.80)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:41 - Epoch: [7][250/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.3730e-01 (1.5445e-01)	Acc@1  89.84 ( 94.99)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:42 - Epoch: [7][250/352]	Time  0.133 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.1159e-01 (1.5554e-01)	Acc@1  96.09 ( 94.83)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:40:42 - Epoch: [7][260/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.0329e-01 (1.5449e-01)	Acc@1  96.88 ( 94.99)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:43 - Epoch: [7][260/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.8663e-01 (1.5587e-01)	Acc@1  96.09 ( 94.82)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:40:43 - Epoch: [7][270/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.3667e-01 (1.5518e-01)	Acc@1  96.09 ( 94.95)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:45 - Epoch: [7][270/352]	Time  0.106 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.7518e-01 (1.5505e-01)	Acc@1  93.75 ( 94.83)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:45 - Epoch: [7][280/352]	Time  0.143 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4704e-01 (1.5585e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:46 - Epoch: [7][280/352]	Time  0.130 ( 0.128)	Data  0.003 ( 0.003)	Loss 1.5273e-01 (1.5480e-01)	Acc@1  96.09 ( 94.86)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:46 - Epoch: [7][290/352]	Time  0.110 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.6229e-01 (1.5600e-01)	Acc@1  96.09 ( 94.94)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:47 - Epoch: [7][290/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4664e-01 (1.5487e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:47 - Epoch: [7][300/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.5762e-01 (1.5705e-01)	Acc@1  96.09 ( 94.90)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:48 - Epoch: [7][310/352]	Time  0.127 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.2929e-01 (1.5688e-01)	Acc@1  96.09 ( 94.90)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:49 - Epoch: [7][300/352]	Time  0.140 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.5756e-01 (1.5474e-01)	Acc@1  96.09 ( 94.88)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:50 - Epoch: [7][320/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.5318e-01 (1.5630e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:50 - Epoch: [7][310/352]	Time  0.127 ( 0.128)	Data  0.002 ( 0.003)	Loss 6.1593e-02 (1.5403e-01)	Acc@1  99.22 ( 94.91)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:51 - Epoch: [7][330/352]	Time  0.113 ( 0.127)	Data  0.002 ( 0.003)	Loss 9.3286e-02 (1.5618e-01)	Acc@1  97.66 ( 94.94)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:51 - Epoch: [7][320/352]	Time  0.125 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4167e-01 (1.5352e-01)	Acc@1  96.09 ( 94.95)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:52 - Epoch: [7][340/352]	Time  0.108 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.1378e-01 (1.5615e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:52 - Epoch: [7][330/352]	Time  0.116 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.0363e-01 (1.5339e-01)	Acc@1  92.97 ( 94.94)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:53 - Epoch: [7][350/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.2960e-01 (1.5609e-01)	Acc@1  93.75 ( 94.94)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:53 - Epoch: [7][340/352]	Time  0.128 ( 0.128)	Data  0.003 ( 0.003)	Loss 2.1479e-01 (1.5346e-01)	Acc@1  91.41 ( 94.94)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:54 - Test: [ 0/20]	Time  0.345 ( 0.345)	Loss 1.5364e-01 (1.5364e-01)	Acc@1  94.92 ( 94.92)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:55 - Epoch: [7][350/352]	Time  0.117 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.1007e-01 (1.5397e-01)	Acc@1  94.53 ( 94.92)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:40:55 - Test: [10/20]	Time  0.071 ( 0.098)	Loss 1.7148e-01 (1.7248e-01)	Acc@1  94.14 ( 94.07)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:55 - Test: [ 0/20]	Time  0.322 ( 0.322)	Loss 1.7999e-01 (1.7999e-01)	Acc@1  94.14 ( 94.14)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:55 -  * Acc@1 94.200 Acc@5 99.900
03-Mar-22 09:40:55 - Best acc at epoch 7: 94.77999877929688
03-Mar-22 09:40:56 - Test: [10/20]	Time  0.070 ( 0.096)	Loss 1.5121e-01 (1.6458e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:56 - Epoch: [8][  0/352]	Time  0.349 ( 0.349)	Data  0.249 ( 0.249)	Loss 1.6488e-01 (1.6488e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:57 -  * Acc@1 94.480 Acc@5 99.960
03-Mar-22 09:40:57 - Best acc at epoch 7: 94.5199966430664
03-Mar-22 09:40:57 - Epoch: [8][  0/352]	Time  0.337 ( 0.337)	Data  0.230 ( 0.230)	Loss 1.5276e-01 (1.5276e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:57 - Epoch: [8][ 10/352]	Time  0.131 ( 0.141)	Data  0.002 ( 0.025)	Loss 9.7751e-02 (1.5948e-01)	Acc@1  96.88 ( 94.46)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:40:58 - Epoch: [8][ 10/352]	Time  0.138 ( 0.145)	Data  0.002 ( 0.023)	Loss 1.5515e-01 (1.5038e-01)	Acc@1  94.53 ( 94.96)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:58 - Epoch: [8][ 20/352]	Time  0.128 ( 0.135)	Data  0.002 ( 0.014)	Loss 1.1713e-01 (1.6624e-01)	Acc@1  96.09 ( 94.35)	Acc@5 100.00 ( 99.78)
03-Mar-22 09:41:00 - Epoch: [8][ 20/352]	Time  0.117 ( 0.139)	Data  0.002 ( 0.013)	Loss 1.7396e-01 (1.5420e-01)	Acc@1  93.75 ( 94.94)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:00 - Epoch: [8][ 30/352]	Time  0.128 ( 0.134)	Data  0.002 ( 0.010)	Loss 1.0268e-01 (1.5572e-01)	Acc@1  97.66 ( 94.81)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:41:01 - Epoch: [8][ 30/352]	Time  0.140 ( 0.133)	Data  0.002 ( 0.009)	Loss 1.4628e-01 (1.6303e-01)	Acc@1  94.53 ( 94.48)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:01 - Epoch: [8][ 40/352]	Time  0.137 ( 0.133)	Data  0.002 ( 0.008)	Loss 1.5672e-01 (1.5219e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:41:02 - Epoch: [8][ 40/352]	Time  0.145 ( 0.133)	Data  0.002 ( 0.008)	Loss 1.1976e-01 (1.5693e-01)	Acc@1  96.88 ( 94.72)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:41:02 - Epoch: [8][ 50/352]	Time  0.132 ( 0.133)	Data  0.002 ( 0.007)	Loss 1.3992e-01 (1.5775e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:41:03 - Epoch: [8][ 50/352]	Time  0.137 ( 0.133)	Data  0.003 ( 0.007)	Loss 1.1796e-01 (1.5372e-01)	Acc@1  96.09 ( 94.94)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:04 - Epoch: [8][ 60/352]	Time  0.130 ( 0.132)	Data  0.003 ( 0.006)	Loss 1.4517e-01 (1.5800e-01)	Acc@1  93.75 ( 94.66)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:41:05 - Epoch: [8][ 60/352]	Time  0.136 ( 0.133)	Data  0.002 ( 0.006)	Loss 1.6797e-01 (1.5271e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:05 - Epoch: [8][ 70/352]	Time  0.142 ( 0.132)	Data  0.002 ( 0.006)	Loss 9.2937e-02 (1.5360e-01)	Acc@1  96.09 ( 94.82)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:06 - Epoch: [8][ 70/352]	Time  0.137 ( 0.133)	Data  0.003 ( 0.005)	Loss 9.2528e-02 (1.5272e-01)	Acc@1  96.88 ( 94.97)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:06 - Epoch: [8][ 80/352]	Time  0.133 ( 0.132)	Data  0.002 ( 0.005)	Loss 1.2340e-01 (1.5268e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:07 - Epoch: [8][ 80/352]	Time  0.137 ( 0.134)	Data  0.002 ( 0.005)	Loss 2.2690e-01 (1.5355e-01)	Acc@1  92.97 ( 94.93)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:08 - Epoch: [8][ 90/352]	Time  0.143 ( 0.132)	Data  0.002 ( 0.005)	Loss 1.4193e-01 (1.5206e-01)	Acc@1  94.53 ( 94.92)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:09 - Epoch: [8][ 90/352]	Time  0.139 ( 0.134)	Data  0.003 ( 0.005)	Loss 1.4307e-01 (1.5467e-01)	Acc@1  95.31 ( 94.88)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:09 - Epoch: [8][100/352]	Time  0.133 ( 0.132)	Data  0.002 ( 0.005)	Loss 9.3774e-02 (1.4953e-01)	Acc@1  97.66 ( 95.03)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:10 - Epoch: [8][100/352]	Time  0.121 ( 0.134)	Data  0.002 ( 0.005)	Loss 1.1374e-01 (1.5424e-01)	Acc@1  96.88 ( 94.93)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:10 - Epoch: [8][110/352]	Time  0.129 ( 0.132)	Data  0.003 ( 0.005)	Loss 2.6759e-01 (1.5135e-01)	Acc@1  92.19 ( 94.99)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:41:11 - Epoch: [8][110/352]	Time  0.141 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.0806e-01 (1.5320e-01)	Acc@1  96.88 ( 95.02)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:12 - Epoch: [8][120/352]	Time  0.142 ( 0.132)	Data  0.003 ( 0.004)	Loss 1.4263e-01 (1.5171e-01)	Acc@1  97.66 ( 95.00)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:41:13 - Epoch: [8][120/352]	Time  0.135 ( 0.133)	Data  0.002 ( 0.004)	Loss 2.2024e-01 (1.5349e-01)	Acc@1  92.97 ( 94.99)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:13 - Epoch: [8][130/352]	Time  0.144 ( 0.132)	Data  0.003 ( 0.004)	Loss 1.4609e-01 (1.5198e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:14 - Epoch: [8][130/352]	Time  0.141 ( 0.133)	Data  0.002 ( 0.004)	Loss 8.6783e-02 (1.5439e-01)	Acc@1  98.44 ( 94.95)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:14 - Epoch: [8][140/352]	Time  0.131 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.3784e-01 (1.5258e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:41:15 - Epoch: [8][140/352]	Time  0.139 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.6990e-01 (1.5317e-01)	Acc@1  93.75 ( 95.00)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:16 - Epoch: [8][150/352]	Time  0.134 ( 0.132)	Data  0.003 ( 0.004)	Loss 2.3429e-01 (1.5251e-01)	Acc@1  92.19 ( 94.93)	Acc@5  98.44 ( 99.88)
03-Mar-22 09:41:17 - Epoch: [8][150/352]	Time  0.139 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.7908e-01 (1.5422e-01)	Acc@1  94.53 ( 94.97)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:17 - Epoch: [8][160/352]	Time  0.131 ( 0.133)	Data  0.003 ( 0.004)	Loss 1.5870e-01 (1.5245e-01)	Acc@1  94.53 ( 94.94)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:41:18 - Epoch: [8][160/352]	Time  0.138 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.6920e-01 (1.5225e-01)	Acc@1  94.53 ( 95.01)	Acc@5  99.22 ( 99.95)
03-Mar-22 09:41:18 - Epoch: [8][170/352]	Time  0.133 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.4814e-01 (1.5164e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:41:19 - Epoch: [8][170/352]	Time  0.142 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.4511e-01 (1.5118e-01)	Acc@1  95.31 ( 95.03)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:19 - Epoch: [8][180/352]	Time  0.125 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.9104e-01 (1.5143e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:41:21 - Epoch: [8][180/352]	Time  0.138 ( 0.133)	Data  0.003 ( 0.004)	Loss 1.7146e-01 (1.5338e-01)	Acc@1  93.75 ( 94.94)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:21 - Epoch: [8][190/352]	Time  0.119 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.7493e-01 (1.5147e-01)	Acc@1  92.97 ( 95.01)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:22 - Epoch: [8][190/352]	Time  0.145 ( 0.133)	Data  0.003 ( 0.004)	Loss 2.1272e-01 (1.5311e-01)	Acc@1  91.41 ( 94.97)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:22 - Epoch: [8][200/352]	Time  0.133 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.9054e-01 (1.5157e-01)	Acc@1  94.53 ( 95.05)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:23 - Epoch: [8][200/352]	Time  0.137 ( 0.133)	Data  0.003 ( 0.003)	Loss 1.6728e-01 (1.5204e-01)	Acc@1  92.97 ( 95.01)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:23 - Epoch: [8][210/352]	Time  0.136 ( 0.132)	Data  0.003 ( 0.004)	Loss 1.2427e-01 (1.5112e-01)	Acc@1  95.31 ( 95.05)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:24 - Epoch: [8][210/352]	Time  0.136 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.4450e-01 (1.5228e-01)	Acc@1  94.53 ( 95.00)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:25 - Epoch: [8][220/352]	Time  0.132 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.1368e-01 (1.5009e-01)	Acc@1  96.09 ( 95.09)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:26 - Epoch: [8][220/352]	Time  0.131 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.3492e-01 (1.5213e-01)	Acc@1  96.09 ( 95.02)	Acc@5  99.22 ( 99.94)
03-Mar-22 09:41:26 - Epoch: [8][230/352]	Time  0.101 ( 0.132)	Data  0.002 ( 0.003)	Loss 2.0255e-01 (1.4955e-01)	Acc@1  94.53 ( 95.12)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:27 - Epoch: [8][230/352]	Time  0.136 ( 0.132)	Data  0.002 ( 0.003)	Loss 2.1576e-01 (1.5242e-01)	Acc@1  91.41 ( 94.97)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:27 - Epoch: [8][240/352]	Time  0.100 ( 0.131)	Data  0.001 ( 0.003)	Loss 1.3986e-01 (1.4948e-01)	Acc@1  95.31 ( 95.13)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:28 - Epoch: [8][250/352]	Time  0.097 ( 0.131)	Data  0.002 ( 0.003)	Loss 2.3550e-01 (1.5074e-01)	Acc@1  93.75 ( 95.09)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:28 - Epoch: [8][240/352]	Time  0.131 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.1560e-01 (1.5270e-01)	Acc@1  97.66 ( 94.95)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:41:30 - Epoch: [8][260/352]	Time  0.134 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.0896e-01 (1.5055e-01)	Acc@1  96.88 ( 95.09)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:30 - Epoch: [8][250/352]	Time  0.116 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.4239e-01 (1.5233e-01)	Acc@1  96.88 ( 94.96)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:31 - Epoch: [8][270/352]	Time  0.139 ( 0.130)	Data  0.003 ( 0.003)	Loss 1.8741e-01 (1.5099e-01)	Acc@1  92.97 ( 95.05)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:31 - Epoch: [8][260/352]	Time  0.159 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.6368e-01 (1.5233e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:32 - Epoch: [8][280/352]	Time  0.129 ( 0.130)	Data  0.003 ( 0.003)	Loss 2.2938e-01 (1.5215e-01)	Acc@1  89.84 ( 95.01)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:32 - Epoch: [8][270/352]	Time  0.136 ( 0.132)	Data  0.003 ( 0.003)	Loss 1.8779e-01 (1.5278e-01)	Acc@1  92.97 ( 94.96)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:33 - Epoch: [8][290/352]	Time  0.132 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.1903e-01 (1.5276e-01)	Acc@1  96.88 ( 94.97)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:34 - Epoch: [8][280/352]	Time  0.129 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.2419e-01 (1.5206e-01)	Acc@1  96.09 ( 94.97)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:35 - Epoch: [8][300/352]	Time  0.114 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.4499e-01 (1.5342e-01)	Acc@1  95.31 ( 94.94)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:35 - Epoch: [8][290/352]	Time  0.116 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.4782e-01 (1.5144e-01)	Acc@1  96.09 ( 95.00)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:36 - Epoch: [8][310/352]	Time  0.132 ( 0.130)	Data  0.002 ( 0.003)	Loss 2.7774e-01 (1.5419e-01)	Acc@1  92.19 ( 94.93)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:41:36 - Epoch: [8][300/352]	Time  0.141 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.3371e-01 (1.5208e-01)	Acc@1  96.09 ( 95.00)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:37 - Epoch: [8][320/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.003)	Loss 2.0967e-01 (1.5406e-01)	Acc@1  92.97 ( 94.93)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:38 - Epoch: [8][310/352]	Time  0.143 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.8102e-01 (1.5209e-01)	Acc@1  92.97 ( 94.99)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:39 - Epoch: [8][330/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.3152e-01 (1.5454e-01)	Acc@1  96.09 ( 94.89)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:39 - Epoch: [8][320/352]	Time  0.140 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.4624e-01 (1.5194e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:40 - Epoch: [8][340/352]	Time  0.134 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.5202e-01 (1.5515e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:40 - Epoch: [8][330/352]	Time  0.142 ( 0.132)	Data  0.003 ( 0.003)	Loss 1.8924e-01 (1.5239e-01)	Acc@1  94.53 ( 94.96)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:41:41 - Epoch: [8][350/352]	Time  0.129 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.0199e-01 (1.5581e-01)	Acc@1  96.88 ( 94.83)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:42 - Epoch: [8][340/352]	Time  0.106 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.2792e-01 (1.5301e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:42 - Test: [ 0/20]	Time  0.338 ( 0.338)	Loss 1.3805e-01 (1.3805e-01)	Acc@1  95.70 ( 95.70)	Acc@5 100.00 (100.00)
03-Mar-22 09:41:43 - Test: [10/20]	Time  0.079 ( 0.100)	Loss 1.6201e-01 (1.6393e-01)	Acc@1  94.14 ( 94.46)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:43 - Epoch: [8][350/352]	Time  0.152 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.4009e-01 (1.5329e-01)	Acc@1  94.53 ( 94.91)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:43 -  * Acc@1 94.620 Acc@5 99.920
03-Mar-22 09:41:43 - Best acc at epoch 8: 94.77999877929688
03-Mar-22 09:41:44 - Test: [ 0/20]	Time  0.318 ( 0.318)	Loss 1.7306e-01 (1.7306e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.61 ( 99.61)
03-Mar-22 09:41:44 - Epoch: [9][  0/352]	Time  0.355 ( 0.355)	Data  0.227 ( 0.227)	Loss 2.2384e-01 (2.2384e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
03-Mar-22 09:41:44 - Test: [10/20]	Time  0.069 ( 0.094)	Loss 1.6276e-01 (1.8128e-01)	Acc@1  94.92 ( 93.93)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:45 - Epoch: [9][ 10/352]	Time  0.126 ( 0.142)	Data  0.002 ( 0.022)	Loss 7.9740e-02 (1.6088e-01)	Acc@1  96.88 ( 94.60)	Acc@5 100.00 (100.00)
03-Mar-22 09:41:45 -  * Acc@1 94.220 Acc@5 99.840
03-Mar-22 09:41:45 - Best acc at epoch 8: 94.5199966430664
03-Mar-22 09:41:45 - Epoch: [9][  0/352]	Time  0.358 ( 0.358)	Data  0.245 ( 0.245)	Loss 1.5315e-01 (1.5315e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:41:46 - Epoch: [9][ 20/352]	Time  0.139 ( 0.134)	Data  0.002 ( 0.013)	Loss 1.2968e-01 (1.5046e-01)	Acc@1  96.88 ( 95.05)	Acc@5 100.00 (100.00)
03-Mar-22 09:41:47 - Epoch: [9][ 10/352]	Time  0.123 ( 0.138)	Data  0.002 ( 0.024)	Loss 2.2673e-01 (1.3913e-01)	Acc@1  93.75 ( 95.81)	Acc@5 100.00 (100.00)
03-Mar-22 09:41:47 - Epoch: [9][ 30/352]	Time  0.130 ( 0.131)	Data  0.002 ( 0.010)	Loss 1.4005e-01 (1.4984e-01)	Acc@1  96.09 ( 95.11)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:41:48 - Epoch: [9][ 20/352]	Time  0.121 ( 0.131)	Data  0.002 ( 0.014)	Loss 1.7300e-01 (1.5592e-01)	Acc@1  94.53 ( 94.79)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:41:49 - Epoch: [9][ 40/352]	Time  0.142 ( 0.129)	Data  0.002 ( 0.008)	Loss 1.5003e-01 (1.6018e-01)	Acc@1  93.75 ( 94.70)	Acc@5 100.00 ( 99.98)
03-Mar-22 09:41:49 - Epoch: [9][ 30/352]	Time  0.111 ( 0.129)	Data  0.002 ( 0.010)	Loss 1.6326e-01 (1.6076e-01)	Acc@1  94.53 ( 94.68)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:50 - Epoch: [9][ 50/352]	Time  0.110 ( 0.129)	Data  0.002 ( 0.007)	Loss 1.9384e-01 (1.6494e-01)	Acc@1  94.53 ( 94.49)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:41:50 - Epoch: [9][ 40/352]	Time  0.104 ( 0.128)	Data  0.002 ( 0.008)	Loss 1.7911e-01 (1.5715e-01)	Acc@1  95.31 ( 94.91)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:51 - Epoch: [9][ 60/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.006)	Loss 1.7455e-01 (1.6787e-01)	Acc@1  93.75 ( 94.40)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:41:51 - Epoch: [9][ 50/352]	Time  0.105 ( 0.126)	Data  0.002 ( 0.007)	Loss 1.6713e-01 (1.5246e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:53 - Epoch: [9][ 70/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.1061e-01 (1.6448e-01)	Acc@1  94.53 ( 94.54)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:41:53 - Epoch: [9][ 60/352]	Time  0.127 ( 0.125)	Data  0.003 ( 0.006)	Loss 1.2110e-01 (1.4921e-01)	Acc@1  96.09 ( 94.95)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:54 - Epoch: [9][ 70/352]	Time  0.125 ( 0.123)	Data  0.002 ( 0.006)	Loss 1.9256e-01 (1.4950e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:54 - Epoch: [9][ 80/352]	Time  0.138 ( 0.130)	Data  0.002 ( 0.005)	Loss 2.1794e-01 (1.6785e-01)	Acc@1  92.19 ( 94.43)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:55 - Epoch: [9][ 80/352]	Time  0.127 ( 0.122)	Data  0.002 ( 0.005)	Loss 8.3054e-02 (1.4886e-01)	Acc@1  96.88 ( 94.97)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:55 - Epoch: [9][ 90/352]	Time  0.127 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.6364e-01 (1.6351e-01)	Acc@1  94.53 ( 94.56)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:56 - Epoch: [9][ 90/352]	Time  0.110 ( 0.121)	Data  0.002 ( 0.005)	Loss 1.2034e-01 (1.4930e-01)	Acc@1  94.53 ( 94.96)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:56 - Epoch: [9][100/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.005)	Loss 9.1220e-02 (1.6249e-01)	Acc@1  97.66 ( 94.57)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:57 - Epoch: [9][100/352]	Time  0.108 ( 0.120)	Data  0.002 ( 0.005)	Loss 2.2425e-01 (1.4976e-01)	Acc@1  90.62 ( 94.96)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:58 - Epoch: [9][110/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.1918e-01 (1.5775e-01)	Acc@1  95.31 ( 94.74)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:58 - Epoch: [9][110/352]	Time  0.103 ( 0.119)	Data  0.002 ( 0.004)	Loss 1.7433e-01 (1.5288e-01)	Acc@1  93.75 ( 94.80)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:59 - Epoch: [9][120/352]	Time  0.142 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.0634e-01 (1.5513e-01)	Acc@1  96.09 ( 94.80)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:59 - Epoch: [9][120/352]	Time  0.125 ( 0.119)	Data  0.002 ( 0.004)	Loss 1.5416e-01 (1.5156e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:00 - Epoch: [9][130/352]	Time  0.132 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.5264e-01 (1.5568e-01)	Acc@1  95.31 ( 94.80)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:42:01 - Epoch: [9][130/352]	Time  0.127 ( 0.119)	Data  0.002 ( 0.004)	Loss 1.2391e-01 (1.4989e-01)	Acc@1  94.53 ( 94.92)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:02 - Epoch: [9][140/352]	Time  0.142 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.1283e-01 (1.5835e-01)	Acc@1  95.31 ( 94.66)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:42:02 - Epoch: [9][140/352]	Time  0.123 ( 0.119)	Data  0.002 ( 0.004)	Loss 1.8291e-01 (1.5110e-01)	Acc@1  92.19 ( 94.86)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:03 - Epoch: [9][150/352]	Time  0.113 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.6418e-01 (1.5747e-01)	Acc@1  94.53 ( 94.67)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:42:03 - Epoch: [9][150/352]	Time  0.122 ( 0.119)	Data  0.002 ( 0.004)	Loss 7.3123e-02 (1.5092e-01)	Acc@1 100.00 ( 94.90)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:04 - Epoch: [9][160/352]	Time  0.103 ( 0.118)	Data  0.002 ( 0.004)	Loss 1.9607e-01 (1.5121e-01)	Acc@1  92.97 ( 94.92)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:42:04 - Epoch: [9][160/352]	Time  0.132 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.6019e-01 (1.5663e-01)	Acc@1  95.31 ( 94.70)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:42:05 - Epoch: [9][170/352]	Time  0.104 ( 0.118)	Data  0.002 ( 0.004)	Loss 1.5160e-01 (1.5075e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:05 - Epoch: [9][170/352]	Time  0.125 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.0528e-01 (1.5647e-01)	Acc@1  96.09 ( 94.74)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:42:06 - Epoch: [9][180/352]	Time  0.105 ( 0.118)	Data  0.002 ( 0.003)	Loss 8.9636e-02 (1.5113e-01)	Acc@1  98.44 ( 95.01)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:07 - Epoch: [9][180/352]	Time  0.126 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.3478e-01 (1.5693e-01)	Acc@1  89.06 ( 94.71)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:42:08 - Epoch: [9][190/352]	Time  0.124 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.1667e-01 (1.5104e-01)	Acc@1  96.09 ( 95.02)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:08 - Epoch: [9][190/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.004)	Loss 7.6267e-02 (1.5603e-01)	Acc@1  98.44 ( 94.78)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:42:09 - Epoch: [9][200/352]	Time  0.103 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.1045e-01 (1.5016e-01)	Acc@1  96.88 ( 95.05)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:09 - Epoch: [9][200/352]	Time  0.117 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4573e-01 (1.5538e-01)	Acc@1  94.53 ( 94.82)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:10 - Epoch: [9][210/352]	Time  0.115 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.5270e-01 (1.5107e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:11 - Epoch: [9][210/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4393e-01 (1.5503e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:11 - Epoch: [9][220/352]	Time  0.127 ( 0.117)	Data  0.002 ( 0.003)	Loss 2.5567e-01 (1.5222e-01)	Acc@1  93.75 ( 94.97)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:42:12 - Epoch: [9][220/352]	Time  0.128 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.2196e-01 (1.5465e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:12 - Epoch: [9][230/352]	Time  0.128 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.1196e-01 (1.5203e-01)	Acc@1  96.88 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:13 - Epoch: [9][230/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4853e-01 (1.5503e-01)	Acc@1  95.31 ( 94.84)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:13 - Epoch: [9][240/352]	Time  0.117 ( 0.117)	Data  0.002 ( 0.003)	Loss 1.3118e-01 (1.5202e-01)	Acc@1  95.31 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:14 - Epoch: [9][240/352]	Time  0.126 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.8271e-01 (1.5479e-01)	Acc@1  92.19 ( 94.83)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:14 - Epoch: [9][250/352]	Time  0.103 ( 0.117)	Data  0.002 ( 0.003)	Loss 1.2271e-01 (1.5228e-01)	Acc@1  95.31 ( 94.95)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:16 - Epoch: [9][260/352]	Time  0.105 ( 0.117)	Data  0.002 ( 0.003)	Loss 1.3456e-01 (1.5221e-01)	Acc@1  95.31 ( 94.94)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:16 - Epoch: [9][250/352]	Time  0.124 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.3085e-01 (1.5424e-01)	Acc@1  94.53 ( 94.86)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:42:17 - Epoch: [9][270/352]	Time  0.127 ( 0.117)	Data  0.002 ( 0.003)	Loss 1.0578e-01 (1.5205e-01)	Acc@1  95.31 ( 94.94)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:17 - Epoch: [9][260/352]	Time  0.128 ( 0.128)	Data  0.002 ( 0.003)	Loss 8.4686e-02 (1.5369e-01)	Acc@1  96.09 ( 94.88)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:18 - Epoch: [9][280/352]	Time  0.127 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.5161e-01 (1.5234e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:18 - Epoch: [9][270/352]	Time  0.138 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6271e-01 (1.5313e-01)	Acc@1  94.53 ( 94.92)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:42:19 - Epoch: [9][290/352]	Time  0.123 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.1830e-01 (1.5205e-01)	Acc@1  94.53 ( 94.94)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:19 - Epoch: [9][280/352]	Time  0.125 ( 0.128)	Data  0.002 ( 0.003)	Loss 9.5841e-02 (1.5285e-01)	Acc@1  96.88 ( 94.92)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:21 - Epoch: [9][300/352]	Time  0.126 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.5257e-01 (1.5171e-01)	Acc@1  94.53 ( 94.98)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:21 - Epoch: [9][290/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.003)	Loss 9.3955e-02 (1.5338e-01)	Acc@1  97.66 ( 94.91)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:22 - Epoch: [9][310/352]	Time  0.127 ( 0.118)	Data  0.002 ( 0.003)	Loss 2.0719e-01 (1.5138e-01)	Acc@1  91.41 ( 94.98)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:22 - Epoch: [9][300/352]	Time  0.121 ( 0.128)	Data  0.003 ( 0.003)	Loss 1.1554e-01 (1.5407e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:23 - Epoch: [9][320/352]	Time  0.127 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.3782e-01 (1.5110e-01)	Acc@1  95.31 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:23 - Epoch: [9][310/352]	Time  0.110 ( 0.128)	Data  0.002 ( 0.003)	Loss 7.7226e-02 (1.5371e-01)	Acc@1  99.22 ( 94.89)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:24 - Epoch: [9][330/352]	Time  0.105 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.3822e-01 (1.5147e-01)	Acc@1  96.09 ( 94.97)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:24 - Epoch: [9][320/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.5142e-01 (1.5361e-01)	Acc@1  95.31 ( 94.89)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:25 - Epoch: [9][340/352]	Time  0.127 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.3797e-01 (1.5201e-01)	Acc@1  96.09 ( 94.95)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:26 - Epoch: [9][330/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4000e-01 (1.5328e-01)	Acc@1  96.09 ( 94.92)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:27 - Epoch: [9][350/352]	Time  0.149 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.5381e-01 (1.5229e-01)	Acc@1  95.31 ( 94.92)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:27 - Epoch: [9][340/352]	Time  0.124 ( 0.128)	Data  0.003 ( 0.003)	Loss 1.2485e-01 (1.5345e-01)	Acc@1  96.09 ( 94.92)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:27 - Test: [ 0/20]	Time  0.332 ( 0.332)	Loss 1.1332e-01 (1.1332e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:42:28 - Epoch: [9][350/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.5902e-01 (1.5255e-01)	Acc@1  94.53 ( 94.95)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:28 - Test: [10/20]	Time  0.069 ( 0.094)	Loss 1.5917e-01 (1.6144e-01)	Acc@1  94.53 ( 94.71)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:42:29 - Test: [ 0/20]	Time  0.331 ( 0.331)	Loss 1.6685e-01 (1.6685e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:42:29 -  * Acc@1 94.620 Acc@5 99.880
03-Mar-22 09:42:29 - Best acc at epoch 9: 94.6199951171875
03-Mar-22 09:42:29 - Epoch: [10][  0/352]	Time  0.326 ( 0.326)	Data  0.228 ( 0.228)	Loss 1.1477e-01 (1.1477e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
03-Mar-22 09:42:29 - Test: [10/20]	Time  0.070 ( 0.097)	Loss 1.9133e-01 (1.7629e-01)	Acc@1  92.58 ( 94.18)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:42:30 -  * Acc@1 93.940 Acc@5 99.880
03-Mar-22 09:42:30 - Best acc at epoch 9: 94.77999877929688
03-Mar-22 09:42:30 - Epoch: [10][ 10/352]	Time  0.108 ( 0.134)	Data  0.002 ( 0.022)	Loss 1.2837e-01 (1.3623e-01)	Acc@1  95.31 ( 95.67)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:42:31 - Epoch: [10][  0/352]	Time  0.343 ( 0.343)	Data  0.235 ( 0.235)	Loss 8.4682e-02 (8.4682e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
03-Mar-22 09:42:31 - Epoch: [10][ 20/352]	Time  0.130 ( 0.127)	Data  0.002 ( 0.013)	Loss 2.2446e-01 (1.4876e-01)	Acc@1  94.53 ( 95.35)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:32 - Epoch: [10][ 10/352]	Time  0.125 ( 0.148)	Data  0.003 ( 0.024)	Loss 1.3123e-01 (1.4427e-01)	Acc@1  96.09 ( 95.67)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:42:33 - Epoch: [10][ 30/352]	Time  0.137 ( 0.125)	Data  0.002 ( 0.009)	Loss 1.5981e-01 (1.4915e-01)	Acc@1  93.75 ( 95.11)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:42:33 - Epoch: [10][ 20/352]	Time  0.132 ( 0.140)	Data  0.004 ( 0.014)	Loss 1.4014e-01 (1.4224e-01)	Acc@1  94.53 ( 95.28)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:42:34 - Epoch: [10][ 40/352]	Time  0.142 ( 0.127)	Data  0.002 ( 0.008)	Loss 1.2497e-01 (1.4972e-01)	Acc@1  96.88 ( 95.14)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:34 - Epoch: [10][ 30/352]	Time  0.111 ( 0.136)	Data  0.003 ( 0.010)	Loss 9.1864e-02 (1.4215e-01)	Acc@1  97.66 ( 95.21)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:35 - Epoch: [10][ 50/352]	Time  0.129 ( 0.126)	Data  0.002 ( 0.007)	Loss 2.1644e-01 (1.5059e-01)	Acc@1  92.97 ( 95.21)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:42:36 - Epoch: [10][ 40/352]	Time  0.116 ( 0.133)	Data  0.002 ( 0.008)	Loss 1.5603e-01 (1.4463e-01)	Acc@1  95.31 ( 95.29)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:42:37 - Epoch: [10][ 60/352]	Time  0.132 ( 0.127)	Data  0.002 ( 0.006)	Loss 1.5058e-01 (1.5159e-01)	Acc@1  94.53 ( 95.07)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:37 - Epoch: [10][ 50/352]	Time  0.114 ( 0.132)	Data  0.002 ( 0.007)	Loss 1.5602e-01 (1.4902e-01)	Acc@1  93.75 ( 95.10)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:42:38 - Epoch: [10][ 70/352]	Time  0.131 ( 0.126)	Data  0.002 ( 0.005)	Loss 1.1792e-01 (1.4900e-01)	Acc@1  95.31 ( 95.08)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:42:38 - Epoch: [10][ 60/352]	Time  0.131 ( 0.131)	Data  0.002 ( 0.006)	Loss 1.9703e-01 (1.5171e-01)	Acc@1  93.75 ( 95.06)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:42:39 - Epoch: [10][ 80/352]	Time  0.122 ( 0.126)	Data  0.002 ( 0.005)	Loss 8.8985e-02 (1.5090e-01)	Acc@1  96.88 ( 95.01)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:39 - Epoch: [10][ 70/352]	Time  0.134 ( 0.131)	Data  0.002 ( 0.006)	Loss 1.2132e-01 (1.4995e-01)	Acc@1  95.31 ( 95.09)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:42:40 - Epoch: [10][ 90/352]	Time  0.144 ( 0.127)	Data  0.002 ( 0.005)	Loss 1.4111e-01 (1.5021e-01)	Acc@1  94.53 ( 95.08)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:41 - Epoch: [10][ 80/352]	Time  0.134 ( 0.131)	Data  0.004 ( 0.005)	Loss 1.0344e-01 (1.5130e-01)	Acc@1  96.88 ( 95.01)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:42:42 - Epoch: [10][100/352]	Time  0.136 ( 0.127)	Data  0.003 ( 0.004)	Loss 1.3552e-01 (1.5111e-01)	Acc@1  95.31 ( 95.06)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:42 - Epoch: [10][ 90/352]	Time  0.129 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.5740e-01 (1.5005e-01)	Acc@1  95.31 ( 95.08)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:42:43 - Epoch: [10][110/352]	Time  0.124 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.1075e-01 (1.5038e-01)	Acc@1  96.09 ( 95.06)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:43 - Epoch: [10][100/352]	Time  0.117 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.5835e-01 (1.4961e-01)	Acc@1  96.09 ( 95.10)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:44 - Epoch: [10][120/352]	Time  0.108 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.7899e-01 (1.4930e-01)	Acc@1  91.41 ( 95.07)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:45 - Epoch: [10][110/352]	Time  0.130 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.8629e-01 (1.4797e-01)	Acc@1  92.97 ( 95.11)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:45 - Epoch: [10][130/352]	Time  0.137 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.9018e-01 (1.4925e-01)	Acc@1  92.19 ( 95.07)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:46 - Epoch: [10][120/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.8505e-01 (1.5089e-01)	Acc@1  93.75 ( 95.03)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:47 - Epoch: [10][140/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.4565e-01 (1.4980e-01)	Acc@1  95.31 ( 95.04)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:47 - Epoch: [10][130/352]	Time  0.126 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.5432e-01 (1.5172e-01)	Acc@1  94.53 ( 95.03)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:48 - Epoch: [10][150/352]	Time  0.101 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.2006e-01 (1.5014e-01)	Acc@1  96.09 ( 95.00)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:49 - Epoch: [10][140/352]	Time  0.127 ( 0.130)	Data  0.002 ( 0.004)	Loss 6.1017e-02 (1.5158e-01)	Acc@1  99.22 ( 95.07)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:49 - Epoch: [10][160/352]	Time  0.130 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.4591e-01 (1.4794e-01)	Acc@1  93.75 ( 95.07)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:42:50 - Epoch: [10][150/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.9306e-01 (1.5280e-01)	Acc@1  92.97 ( 95.02)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:42:50 - Epoch: [10][170/352]	Time  0.130 ( 0.126)	Data  0.003 ( 0.004)	Loss 1.0526e-01 (1.4927e-01)	Acc@1  97.66 ( 95.01)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:51 - Epoch: [10][160/352]	Time  0.109 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.6166e-01 (1.5385e-01)	Acc@1  92.19 ( 94.94)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:52 - Epoch: [10][180/352]	Time  0.141 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.6871e-01 (1.4908e-01)	Acc@1  92.97 ( 95.00)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:52 - Epoch: [10][170/352]	Time  0.130 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.2343e-01 (1.5438e-01)	Acc@1  94.53 ( 94.90)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:53 - Epoch: [10][190/352]	Time  0.128 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.5503e-01 (1.4922e-01)	Acc@1  91.41 ( 95.01)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:54 - Epoch: [10][180/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.7679e-01 (1.5536e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:42:54 - Epoch: [10][200/352]	Time  0.141 ( 0.127)	Data  0.003 ( 0.003)	Loss 1.7589e-01 (1.4974e-01)	Acc@1  93.75 ( 95.02)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:55 - Epoch: [10][190/352]	Time  0.128 ( 0.129)	Data  0.003 ( 0.004)	Loss 2.6120e-01 (1.5623e-01)	Acc@1  90.62 ( 94.80)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:42:56 - Epoch: [10][210/352]	Time  0.125 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.8064e-01 (1.5043e-01)	Acc@1  94.53 ( 95.00)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:56 - Epoch: [10][200/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.1607e-01 (1.5544e-01)	Acc@1  97.66 ( 94.83)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:42:57 - Epoch: [10][220/352]	Time  0.141 ( 0.127)	Data  0.002 ( 0.003)	Loss 7.1261e-02 (1.5039e-01)	Acc@1  98.44 ( 95.02)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:57 - Epoch: [10][210/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.9048e-01 (1.5566e-01)	Acc@1  93.75 ( 94.82)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:42:58 - Epoch: [10][230/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.9764e-01 (1.4948e-01)	Acc@1  92.97 ( 95.04)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:59 - Epoch: [10][220/352]	Time  0.114 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3528e-01 (1.5505e-01)	Acc@1  96.09 ( 94.85)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:43:00 - Epoch: [10][240/352]	Time  0.125 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.7168e-01 (1.4905e-01)	Acc@1  93.75 ( 95.03)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:00 - Epoch: [10][230/352]	Time  0.126 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5452e-01 (1.5483e-01)	Acc@1  95.31 ( 94.88)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:01 - Epoch: [10][250/352]	Time  0.156 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.3575e-01 (1.4922e-01)	Acc@1  94.53 ( 95.04)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:43:01 - Epoch: [10][240/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7155e-01 (1.5397e-01)	Acc@1  96.09 ( 94.92)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:43:02 - Epoch: [10][260/352]	Time  0.140 ( 0.128)	Data  0.002 ( 0.003)	Loss 3.2478e-01 (1.4911e-01)	Acc@1  87.50 ( 95.04)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:03 - Epoch: [10][250/352]	Time  0.144 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3723e-01 (1.5289e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:43:03 - Epoch: [10][270/352]	Time  0.127 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.2002e-01 (1.4842e-01)	Acc@1  94.53 ( 95.09)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:43:04 - Epoch: [10][260/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3291e-01 (1.5329e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:05 - Epoch: [10][280/352]	Time  0.128 ( 0.127)	Data  0.003 ( 0.003)	Loss 1.4643e-01 (1.4843e-01)	Acc@1  93.75 ( 95.08)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:05 - Epoch: [10][270/352]	Time  0.137 ( 0.129)	Data  0.002 ( 0.003)	Loss 8.4052e-02 (1.5296e-01)	Acc@1  96.88 ( 94.97)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:43:06 - Epoch: [10][290/352]	Time  0.126 ( 0.127)	Data  0.003 ( 0.003)	Loss 1.2858e-01 (1.4799e-01)	Acc@1  97.66 ( 95.10)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:06 - Epoch: [10][280/352]	Time  0.130 ( 0.129)	Data  0.003 ( 0.003)	Loss 2.6253e-01 (1.5354e-01)	Acc@1  90.62 ( 94.94)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:43:07 - Epoch: [10][300/352]	Time  0.127 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.1753e-01 (1.4835e-01)	Acc@1  96.88 ( 95.08)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:08 - Epoch: [10][290/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5063e-01 (1.5351e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:43:08 - Epoch: [10][310/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.8426e-01 (1.4900e-01)	Acc@1  94.53 ( 95.05)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:43:09 - Epoch: [10][300/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0541e-01 (1.5287e-01)	Acc@1  96.88 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:10 - Epoch: [10][320/352]	Time  0.122 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.5618e-01 (1.4970e-01)	Acc@1  92.97 ( 95.02)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:10 - Epoch: [10][310/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0239e-01 (1.5272e-01)	Acc@1  97.66 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:11 - Epoch: [10][330/352]	Time  0.103 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.5327e-01 (1.4933e-01)	Acc@1  94.53 ( 95.02)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:12 - Epoch: [10][320/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4866e-01 (1.5313e-01)	Acc@1  96.09 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:12 - Epoch: [10][340/352]	Time  0.129 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.3848e-01 (1.4911e-01)	Acc@1  94.53 ( 95.02)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:43:13 - Epoch: [10][330/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0386e-01 (1.5265e-01)	Acc@1  97.66 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:13 - Epoch: [10][350/352]	Time  0.159 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.7695e-01 (1.4824e-01)	Acc@1  95.31 ( 95.05)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:14 - Test: [ 0/20]	Time  0.353 ( 0.353)	Loss 1.5037e-01 (1.5037e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:43:14 - Epoch: [10][340/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0049e-01 (1.5209e-01)	Acc@1  97.66 ( 94.99)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:15 - Test: [10/20]	Time  0.068 ( 0.097)	Loss 1.5574e-01 (1.7737e-01)	Acc@1  95.31 ( 94.32)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:15 - Epoch: [10][350/352]	Time  0.126 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.1924e-01 (1.5239e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:15 -  * Acc@1 94.420 Acc@5 99.880
03-Mar-22 09:43:15 - Best acc at epoch 10: 94.6199951171875
03-Mar-22 09:43:16 - Epoch: [11][  0/352]	Time  0.333 ( 0.333)	Data  0.234 ( 0.234)	Loss 1.7163e-01 (1.7163e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.22 ( 99.22)
03-Mar-22 09:43:16 - Test: [ 0/20]	Time  0.334 ( 0.334)	Loss 1.3412e-01 (1.3412e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:43:17 - Test: [10/20]	Time  0.088 ( 0.102)	Loss 1.7238e-01 (1.6821e-01)	Acc@1  93.36 ( 93.96)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:43:17 - Epoch: [11][ 10/352]	Time  0.103 ( 0.128)	Data  0.002 ( 0.023)	Loss 1.3694e-01 (1.5379e-01)	Acc@1  95.31 ( 94.53)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:43:18 -  * Acc@1 94.040 Acc@5 99.900
03-Mar-22 09:43:18 - Best acc at epoch 10: 94.77999877929688
03-Mar-22 09:43:18 - Epoch: [11][  0/352]	Time  0.356 ( 0.356)	Data  0.246 ( 0.246)	Loss 1.3721e-01 (1.3721e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:43:18 - Epoch: [11][ 20/352]	Time  0.094 ( 0.126)	Data  0.002 ( 0.013)	Loss 1.5191e-01 (1.4870e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:43:19 - Epoch: [11][ 10/352]	Time  0.127 ( 0.140)	Data  0.002 ( 0.024)	Loss 9.0639e-02 (1.3136e-01)	Acc@1  97.66 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:43:19 - Epoch: [11][ 30/352]	Time  0.124 ( 0.121)	Data  0.002 ( 0.010)	Loss 1.7543e-01 (1.4433e-01)	Acc@1  91.41 ( 94.93)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:20 - Epoch: [11][ 20/352]	Time  0.106 ( 0.130)	Data  0.002 ( 0.014)	Loss 1.0570e-01 (1.3835e-01)	Acc@1  96.09 ( 95.39)	Acc@5 100.00 (100.00)
03-Mar-22 09:43:20 - Epoch: [11][ 40/352]	Time  0.125 ( 0.120)	Data  0.002 ( 0.008)	Loss 1.4082e-01 (1.4374e-01)	Acc@1  94.53 ( 95.05)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:43:22 - Epoch: [11][ 30/352]	Time  0.149 ( 0.129)	Data  0.003 ( 0.010)	Loss 1.0840e-01 (1.3012e-01)	Acc@1  95.31 ( 95.77)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:43:22 - Epoch: [11][ 50/352]	Time  0.128 ( 0.121)	Data  0.003 ( 0.007)	Loss 1.6912e-01 (1.4842e-01)	Acc@1  93.75 ( 94.99)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:43:23 - Epoch: [11][ 60/352]	Time  0.122 ( 0.122)	Data  0.002 ( 0.006)	Loss 1.1308e-01 (1.4622e-01)	Acc@1  96.09 ( 95.06)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:43:23 - Epoch: [11][ 40/352]	Time  0.138 ( 0.130)	Data  0.004 ( 0.008)	Loss 1.3318e-01 (1.4082e-01)	Acc@1  96.09 ( 95.43)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:43:24 - Epoch: [11][ 70/352]	Time  0.115 ( 0.123)	Data  0.002 ( 0.005)	Loss 1.2445e-01 (1.4660e-01)	Acc@1  96.09 ( 95.02)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:43:24 - Epoch: [11][ 50/352]	Time  0.142 ( 0.130)	Data  0.003 ( 0.007)	Loss 6.1197e-02 (1.4446e-01)	Acc@1  98.44 ( 95.34)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:25 - Epoch: [11][ 80/352]	Time  0.111 ( 0.123)	Data  0.002 ( 0.005)	Loss 1.4638e-01 (1.4398e-01)	Acc@1  95.31 ( 95.04)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:43:26 - Epoch: [11][ 60/352]	Time  0.129 ( 0.130)	Data  0.002 ( 0.006)	Loss 1.0244e-01 (1.4237e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:27 - Epoch: [11][ 90/352]	Time  0.125 ( 0.123)	Data  0.002 ( 0.005)	Loss 9.6768e-02 (1.4234e-01)	Acc@1  98.44 ( 95.11)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:27 - Epoch: [11][ 70/352]	Time  0.140 ( 0.130)	Data  0.002 ( 0.006)	Loss 1.3266e-01 (1.4216e-01)	Acc@1  96.09 ( 95.38)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:28 - Epoch: [11][100/352]	Time  0.126 ( 0.123)	Data  0.002 ( 0.005)	Loss 1.9074e-01 (1.4338e-01)	Acc@1  92.97 ( 95.10)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:28 - Epoch: [11][ 80/352]	Time  0.119 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.1076e-01 (1.4225e-01)	Acc@1  96.88 ( 95.37)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:43:29 - Epoch: [11][110/352]	Time  0.126 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.6680e-01 (1.4718e-01)	Acc@1  96.09 ( 94.97)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:29 - Epoch: [11][ 90/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.4392e-01 (1.4478e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:30 - Epoch: [11][120/352]	Time  0.131 ( 0.123)	Data  0.003 ( 0.004)	Loss 9.8470e-02 (1.4944e-01)	Acc@1  96.88 ( 94.89)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:31 - Epoch: [11][100/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.7233e-01 (1.4588e-01)	Acc@1  95.31 ( 95.28)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:43:32 - Epoch: [11][130/352]	Time  0.126 ( 0.123)	Data  0.002 ( 0.004)	Loss 6.9633e-02 (1.4631e-01)	Acc@1  98.44 ( 95.06)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:43:32 - Epoch: [11][110/352]	Time  0.109 ( 0.129)	Data  0.002 ( 0.005)	Loss 6.9310e-02 (1.4526e-01)	Acc@1  99.22 ( 95.28)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:43:33 - Epoch: [11][140/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.4939e-01 (1.4781e-01)	Acc@1  96.09 ( 95.02)	Acc@5  99.22 ( 99.95)
03-Mar-22 09:43:33 - Epoch: [11][120/352]	Time  0.107 ( 0.128)	Data  0.002 ( 0.004)	Loss 2.5590e-01 (1.4792e-01)	Acc@1  93.75 ( 95.21)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:43:34 - Epoch: [11][150/352]	Time  0.112 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.2410e-01 (1.4799e-01)	Acc@1  94.53 ( 95.00)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:34 - Epoch: [11][130/352]	Time  0.109 ( 0.128)	Data  0.002 ( 0.004)	Loss 2.4101e-01 (1.4807e-01)	Acc@1  90.62 ( 95.16)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:35 - Epoch: [11][160/352]	Time  0.129 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.7892e-01 (1.4820e-01)	Acc@1  93.75 ( 94.99)	Acc@5  99.22 ( 99.95)
03-Mar-22 09:43:36 - Epoch: [11][140/352]	Time  0.130 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.3282e-01 (1.4829e-01)	Acc@1  94.53 ( 95.11)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:37 - Epoch: [11][170/352]	Time  0.127 ( 0.124)	Data  0.003 ( 0.004)	Loss 6.3098e-02 (1.4858e-01)	Acc@1  99.22 ( 94.98)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:37 - Epoch: [11][150/352]	Time  0.132 ( 0.128)	Data  0.003 ( 0.004)	Loss 7.8503e-02 (1.4934e-01)	Acc@1  97.66 ( 95.06)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:43:38 - Epoch: [11][180/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.2175e-01 (1.4849e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:38 - Epoch: [11][160/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.9056e-01 (1.4920e-01)	Acc@1  92.97 ( 95.06)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:43:39 - Epoch: [11][190/352]	Time  0.129 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.0667e-01 (1.4862e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:40 - Epoch: [11][170/352]	Time  0.143 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.7584e-01 (1.5021e-01)	Acc@1  92.97 ( 95.01)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:40 - Epoch: [11][200/352]	Time  0.133 ( 0.124)	Data  0.003 ( 0.003)	Loss 8.0840e-02 (1.4875e-01)	Acc@1  96.09 ( 95.01)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:41 - Epoch: [11][180/352]	Time  0.124 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.6290e-01 (1.5087e-01)	Acc@1  96.09 ( 94.98)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:43:42 - Epoch: [11][210/352]	Time  0.108 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.8356e-01 (1.4879e-01)	Acc@1  94.53 ( 95.01)	Acc@5  99.22 ( 99.95)
03-Mar-22 09:43:42 - Epoch: [11][190/352]	Time  0.129 ( 0.128)	Data  0.003 ( 0.004)	Loss 2.1165e-01 (1.5136e-01)	Acc@1  94.53 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:43 - Epoch: [11][220/352]	Time  0.128 ( 0.124)	Data  0.003 ( 0.003)	Loss 8.3641e-02 (1.4746e-01)	Acc@1  98.44 ( 95.07)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:43 - Epoch: [11][200/352]	Time  0.134 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.9149e-01 (1.5195e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:44 - Epoch: [11][230/352]	Time  0.129 ( 0.124)	Data  0.003 ( 0.003)	Loss 1.6247e-01 (1.4747e-01)	Acc@1  93.75 ( 95.07)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:45 - Epoch: [11][210/352]	Time  0.134 ( 0.128)	Data  0.003 ( 0.004)	Loss 1.1288e-01 (1.5177e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:45 - Epoch: [11][240/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.003)	Loss 2.1056e-01 (1.4776e-01)	Acc@1  94.53 ( 95.05)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:46 - Epoch: [11][220/352]	Time  0.149 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.0617e-01 (1.5232e-01)	Acc@1  97.66 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:47 - Epoch: [11][250/352]	Time  0.125 ( 0.124)	Data  0.002 ( 0.003)	Loss 3.0318e-01 (1.4789e-01)	Acc@1  90.62 ( 95.07)	Acc@5  99.22 ( 99.95)
03-Mar-22 09:43:47 - Epoch: [11][230/352]	Time  0.147 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1849e-01 (1.5295e-01)	Acc@1  96.88 ( 94.94)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:43:48 - Epoch: [11][260/352]	Time  0.151 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.3820e-01 (1.4738e-01)	Acc@1  95.31 ( 95.09)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:49 - Epoch: [11][240/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4927e-01 (1.5296e-01)	Acc@1  96.09 ( 94.94)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:43:49 - Epoch: [11][270/352]	Time  0.129 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6661e-01 (1.4835e-01)	Acc@1  93.75 ( 95.05)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:50 - Epoch: [11][250/352]	Time  0.128 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.2253e-01 (1.5152e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:50 - Epoch: [11][280/352]	Time  0.122 ( 0.124)	Data  0.002 ( 0.003)	Loss 9.4226e-02 (1.4824e-01)	Acc@1  96.88 ( 95.05)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:51 - Epoch: [11][260/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3257e-01 (1.5189e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:52 - Epoch: [11][290/352]	Time  0.128 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.5730e-01 (1.4885e-01)	Acc@1  96.09 ( 95.04)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:53 - Epoch: [11][270/352]	Time  0.148 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.3982e-01 (1.5166e-01)	Acc@1  96.09 ( 94.98)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:53 - Epoch: [11][300/352]	Time  0.128 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.1108e-01 (1.4929e-01)	Acc@1  92.19 ( 95.03)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:43:54 - Epoch: [11][280/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0867e-01 (1.5210e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:54 - Epoch: [11][310/352]	Time  0.125 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.3814e-01 (1.5007e-01)	Acc@1  95.31 ( 95.00)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:55 - Epoch: [11][290/352]	Time  0.106 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1276e-01 (1.5153e-01)	Acc@1  95.31 ( 94.99)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:56 - Epoch: [11][320/352]	Time  0.132 ( 0.125)	Data  0.003 ( 0.003)	Loss 9.1789e-02 (1.5051e-01)	Acc@1  97.66 ( 94.97)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:57 - Epoch: [11][300/352]	Time  0.116 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.1983e-01 (1.5194e-01)	Acc@1  92.19 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:57 - Epoch: [11][330/352]	Time  0.127 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.3197e-01 (1.5100e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:58 - Epoch: [11][310/352]	Time  0.132 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.9708e-01 (1.5258e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:43:58 - Epoch: [11][340/352]	Time  0.130 ( 0.125)	Data  0.003 ( 0.003)	Loss 1.1738e-01 (1.5089e-01)	Acc@1  95.31 ( 94.96)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:59 - Epoch: [11][320/352]	Time  0.137 ( 0.129)	Data  0.003 ( 0.003)	Loss 9.9619e-02 (1.5230e-01)	Acc@1  96.88 ( 94.96)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:43:59 - Epoch: [11][350/352]	Time  0.129 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.2608e-01 (1.5132e-01)	Acc@1  94.53 ( 94.96)	Acc@5  98.44 ( 99.94)
03-Mar-22 09:44:00 - Test: [ 0/20]	Time  0.331 ( 0.331)	Loss 1.3612e-01 (1.3612e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:00 - Epoch: [11][330/352]	Time  0.118 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7027e-01 (1.5183e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:01 - Test: [10/20]	Time  0.071 ( 0.105)	Loss 1.2440e-01 (1.5514e-01)	Acc@1  94.14 ( 95.03)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:44:01 -  * Acc@1 94.660 Acc@5 99.920
03-Mar-22 09:44:01 - Best acc at epoch 11: 94.65999603271484
03-Mar-22 09:44:02 - Epoch: [11][340/352]	Time  0.108 ( 0.129)	Data  0.003 ( 0.003)	Loss 2.0298e-01 (1.5168e-01)	Acc@1  92.97 ( 94.98)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:44:02 - Epoch: [12][  0/352]	Time  0.328 ( 0.328)	Data  0.221 ( 0.221)	Loss 1.0938e-01 (1.0938e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:03 - Epoch: [11][350/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0353e-01 (1.5201e-01)	Acc@1  97.66 ( 94.98)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:03 - Epoch: [12][ 10/352]	Time  0.128 ( 0.136)	Data  0.002 ( 0.022)	Loss 1.6716e-01 (1.4033e-01)	Acc@1  95.31 ( 95.10)	Acc@5  99.22 ( 99.86)
03-Mar-22 09:44:03 - Test: [ 0/20]	Time  0.349 ( 0.349)	Loss 1.4611e-01 (1.4611e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:04 - Epoch: [12][ 20/352]	Time  0.097 ( 0.123)	Data  0.002 ( 0.013)	Loss 2.2583e-01 (1.3867e-01)	Acc@1  94.53 ( 95.50)	Acc@5  99.22 ( 99.85)
03-Mar-22 09:44:04 - Test: [10/20]	Time  0.078 ( 0.101)	Loss 1.6991e-01 (1.6162e-01)	Acc@1  92.97 ( 94.64)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:44:05 -  * Acc@1 94.500 Acc@5 99.900
03-Mar-22 09:44:05 - Best acc at epoch 11: 94.77999877929688
03-Mar-22 09:44:05 - Epoch: [12][ 30/352]	Time  0.121 ( 0.121)	Data  0.003 ( 0.009)	Loss 2.1118e-01 (1.4796e-01)	Acc@1  92.19 ( 95.19)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:44:05 - Epoch: [12][  0/352]	Time  0.340 ( 0.340)	Data  0.231 ( 0.231)	Loss 1.6780e-01 (1.6780e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:06 - Epoch: [12][ 40/352]	Time  0.120 ( 0.117)	Data  0.002 ( 0.007)	Loss 1.3134e-01 (1.4987e-01)	Acc@1  93.75 ( 94.99)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:06 - Epoch: [12][ 10/352]	Time  0.125 ( 0.136)	Data  0.002 ( 0.023)	Loss 2.0767e-01 (1.5788e-01)	Acc@1  92.19 ( 94.32)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:07 - Epoch: [12][ 50/352]	Time  0.128 ( 0.118)	Data  0.002 ( 0.006)	Loss 9.5632e-02 (1.4316e-01)	Acc@1  96.09 ( 95.28)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:08 - Epoch: [12][ 20/352]	Time  0.114 ( 0.127)	Data  0.002 ( 0.013)	Loss 2.0473e-01 (1.5935e-01)	Acc@1  91.41 ( 94.20)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:44:09 - Epoch: [12][ 60/352]	Time  0.107 ( 0.119)	Data  0.002 ( 0.006)	Loss 1.4154e-01 (1.4240e-01)	Acc@1  94.53 ( 95.26)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:09 - Epoch: [12][ 30/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.009)	Loss 1.9344e-01 (1.6111e-01)	Acc@1  93.75 ( 94.23)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:10 - Epoch: [12][ 70/352]	Time  0.131 ( 0.121)	Data  0.002 ( 0.005)	Loss 1.3719e-01 (1.4302e-01)	Acc@1  95.31 ( 95.20)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:10 - Epoch: [12][ 40/352]	Time  0.153 ( 0.128)	Data  0.002 ( 0.008)	Loss 2.4222e-01 (1.5839e-01)	Acc@1  92.19 ( 94.40)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:44:11 - Epoch: [12][ 80/352]	Time  0.126 ( 0.121)	Data  0.002 ( 0.005)	Loss 6.7027e-02 (1.4133e-01)	Acc@1  96.88 ( 95.24)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:12 - Epoch: [12][ 50/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.007)	Loss 1.8901e-01 (1.5791e-01)	Acc@1  94.53 ( 94.50)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:44:12 - Epoch: [12][ 90/352]	Time  0.124 ( 0.121)	Data  0.002 ( 0.005)	Loss 1.4954e-01 (1.4203e-01)	Acc@1  95.31 ( 95.18)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:13 - Epoch: [12][ 60/352]	Time  0.137 ( 0.128)	Data  0.002 ( 0.006)	Loss 1.9855e-01 (1.5725e-01)	Acc@1  94.53 ( 94.56)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:44:14 - Epoch: [12][100/352]	Time  0.151 ( 0.122)	Data  0.003 ( 0.004)	Loss 1.6579e-01 (1.4403e-01)	Acc@1  95.31 ( 95.17)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:44:14 - Epoch: [12][ 70/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.005)	Loss 1.1220e-01 (1.5817e-01)	Acc@1  95.31 ( 94.53)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:15 - Epoch: [12][110/352]	Time  0.141 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.4597e-01 (1.4479e-01)	Acc@1  95.31 ( 95.12)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:15 - Epoch: [12][ 80/352]	Time  0.119 ( 0.126)	Data  0.003 ( 0.005)	Loss 1.3545e-01 (1.5771e-01)	Acc@1  97.66 ( 94.55)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:16 - Epoch: [12][120/352]	Time  0.128 ( 0.124)	Data  0.003 ( 0.004)	Loss 1.1981e-01 (1.4431e-01)	Acc@1  96.88 ( 95.11)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:17 - Epoch: [12][ 90/352]	Time  0.109 ( 0.126)	Data  0.002 ( 0.005)	Loss 1.7535e-01 (1.5905e-01)	Acc@1  94.53 ( 94.58)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:18 - Epoch: [12][130/352]	Time  0.125 ( 0.124)	Data  0.002 ( 0.004)	Loss 2.2420e-01 (1.4604e-01)	Acc@1  92.19 ( 95.07)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:18 - Epoch: [12][100/352]	Time  0.156 ( 0.127)	Data  0.002 ( 0.005)	Loss 1.1889e-01 (1.5710e-01)	Acc@1  96.88 ( 94.66)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:19 - Epoch: [12][140/352]	Time  0.121 ( 0.124)	Data  0.003 ( 0.004)	Loss 1.3186e-01 (1.4680e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:19 - Epoch: [12][110/352]	Time  0.133 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.5918e-01 (1.5625e-01)	Acc@1  96.88 ( 94.81)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:44:20 - Epoch: [12][150/352]	Time  0.109 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.4736e-01 (1.4666e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:20 - Epoch: [12][120/352]	Time  0.143 ( 0.127)	Data  0.002 ( 0.004)	Loss 2.3096e-01 (1.5723e-01)	Acc@1  89.84 ( 94.81)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:44:21 - Epoch: [12][160/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.004)	Loss 2.1645e-01 (1.4699e-01)	Acc@1  93.75 ( 95.00)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:22 - Epoch: [12][130/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.7466e-01 (1.5707e-01)	Acc@1  95.31 ( 94.79)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:44:23 - Epoch: [12][170/352]	Time  0.106 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.0857e-01 (1.4718e-01)	Acc@1  97.66 ( 95.00)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:44:23 - Epoch: [12][140/352]	Time  0.113 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.2018e-01 (1.5778e-01)	Acc@1  94.53 ( 94.76)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:44:24 - Epoch: [12][180/352]	Time  0.125 ( 0.124)	Data  0.003 ( 0.004)	Loss 6.5958e-02 (1.4613e-01)	Acc@1  98.44 ( 95.05)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:44:24 - Epoch: [12][150/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.6655e-01 (1.5564e-01)	Acc@1  92.19 ( 94.81)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:44:25 - Epoch: [12][190/352]	Time  0.126 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.3192e-01 (1.4630e-01)	Acc@1  96.09 ( 95.06)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:26 - Epoch: [12][160/352]	Time  0.133 ( 0.128)	Data  0.003 ( 0.004)	Loss 7.2617e-02 (1.5549e-01)	Acc@1  98.44 ( 94.75)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:44:26 - Epoch: [12][200/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.9582e-01 (1.4729e-01)	Acc@1  92.19 ( 95.02)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:27 - Epoch: [12][170/352]	Time  0.120 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.2052e-01 (1.5523e-01)	Acc@1  94.53 ( 94.74)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:44:27 - Epoch: [12][210/352]	Time  0.111 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.9338e-01 (1.4665e-01)	Acc@1  94.53 ( 95.05)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:28 - Epoch: [12][180/352]	Time  0.134 ( 0.128)	Data  0.002 ( 0.004)	Loss 8.7277e-02 (1.5369e-01)	Acc@1  97.66 ( 94.79)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:44:29 - Epoch: [12][220/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.3648e-01 (1.4754e-01)	Acc@1  96.88 ( 95.05)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:29 - Epoch: [12][190/352]	Time  0.128 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.4561e-01 (1.5288e-01)	Acc@1  94.53 ( 94.82)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:30 - Epoch: [12][230/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.4543e-01 (1.4752e-01)	Acc@1  93.75 ( 95.02)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:31 - Epoch: [12][200/352]	Time  0.139 ( 0.128)	Data  0.003 ( 0.003)	Loss 1.0548e-01 (1.5192e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:31 - Epoch: [12][240/352]	Time  0.105 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6424e-01 (1.4798e-01)	Acc@1  93.75 ( 95.02)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:44:32 - Epoch: [12][210/352]	Time  0.127 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4013e-01 (1.5114e-01)	Acc@1  94.53 ( 94.91)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:32 - Epoch: [12][250/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.0305e-01 (1.4733e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:33 - Epoch: [12][220/352]	Time  0.126 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.1332e-01 (1.5112e-01)	Acc@1  97.66 ( 94.93)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:34 - Epoch: [12][260/352]	Time  0.135 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.5143e-01 (1.4824e-01)	Acc@1  95.31 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:35 - Epoch: [12][230/352]	Time  0.113 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.1206e-01 (1.5063e-01)	Acc@1  95.31 ( 94.95)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:35 - Epoch: [12][270/352]	Time  0.122 ( 0.124)	Data  0.003 ( 0.003)	Loss 1.3216e-01 (1.4806e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:36 - Epoch: [12][240/352]	Time  0.133 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.9580e-01 (1.5085e-01)	Acc@1  93.75 ( 94.95)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:36 - Epoch: [12][280/352]	Time  0.132 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.3020e-01 (1.4726e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:37 - Epoch: [12][250/352]	Time  0.110 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4629e-01 (1.5163e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:37 - Epoch: [12][290/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.003)	Loss 8.0525e-02 (1.4799e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:38 - Epoch: [12][260/352]	Time  0.144 ( 0.128)	Data  0.003 ( 0.003)	Loss 9.1379e-02 (1.5050e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:39 - Epoch: [12][300/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.003)	Loss 5.6630e-02 (1.4760e-01)	Acc@1  99.22 ( 94.99)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:40 - Epoch: [12][270/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6827e-01 (1.5096e-01)	Acc@1  94.53 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:40 - Epoch: [12][310/352]	Time  0.113 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.7138e-01 (1.4844e-01)	Acc@1  93.75 ( 94.98)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:41 - Epoch: [12][280/352]	Time  0.109 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6814e-01 (1.5054e-01)	Acc@1  92.97 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:41 - Epoch: [12][320/352]	Time  0.108 ( 0.124)	Data  0.002 ( 0.003)	Loss 2.2055e-01 (1.4880e-01)	Acc@1  93.75 ( 94.97)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:42 - Epoch: [12][330/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.1684e-01 (1.4831e-01)	Acc@1  96.88 ( 95.00)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:44:42 - Epoch: [12][290/352]	Time  0.135 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5484e-01 (1.5030e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:44 - Epoch: [12][340/352]	Time  0.123 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.7939e-01 (1.4818e-01)	Acc@1  93.75 ( 95.00)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:44 - Epoch: [12][300/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.2814e-01 (1.5006e-01)	Acc@1  96.88 ( 94.99)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:45 - Epoch: [12][350/352]	Time  0.132 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.4952e-01 (1.4824e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:45 - Epoch: [12][310/352]	Time  0.132 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.8616e-01 (1.5045e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:46 - Test: [ 0/20]	Time  0.329 ( 0.329)	Loss 1.3573e-01 (1.3573e-01)	Acc@1  95.70 ( 95.70)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:46 - Epoch: [12][320/352]	Time  0.098 ( 0.128)	Data  0.001 ( 0.003)	Loss 2.2409e-01 (1.5096e-01)	Acc@1  92.97 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:46 - Test: [10/20]	Time  0.070 ( 0.098)	Loss 1.6061e-01 (1.6195e-01)	Acc@1  93.36 ( 94.74)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:44:47 -  * Acc@1 94.540 Acc@5 99.920
03-Mar-22 09:44:47 - Best acc at epoch 12: 94.65999603271484
03-Mar-22 09:44:47 - Epoch: [12][330/352]	Time  0.100 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.0859e-01 (1.5062e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:47 - Epoch: [13][  0/352]	Time  0.333 ( 0.333)	Data  0.236 ( 0.236)	Loss 1.0573e-01 (1.0573e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:49 - Epoch: [13][ 10/352]	Time  0.136 ( 0.139)	Data  0.002 ( 0.023)	Loss 1.5792e-01 (1.3916e-01)	Acc@1  96.09 ( 95.45)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:49 - Epoch: [12][340/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.2585e-01 (1.5058e-01)	Acc@1  95.31 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:50 - Epoch: [13][ 20/352]	Time  0.103 ( 0.130)	Data  0.002 ( 0.013)	Loss 1.4589e-01 (1.4216e-01)	Acc@1  95.31 ( 95.28)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:44:50 - Epoch: [12][350/352]	Time  0.130 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.3477e-01 (1.5095e-01)	Acc@1  94.53 ( 94.96)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:51 - Test: [ 0/20]	Time  0.347 ( 0.347)	Loss 1.2547e-01 (1.2547e-01)	Acc@1  94.14 ( 94.14)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:51 - Epoch: [13][ 30/352]	Time  0.109 ( 0.125)	Data  0.002 ( 0.010)	Loss 1.5794e-01 (1.4820e-01)	Acc@1  94.53 ( 95.19)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:44:51 - Test: [10/20]	Time  0.072 ( 0.105)	Loss 1.8625e-01 (1.6511e-01)	Acc@1  94.14 ( 94.57)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:44:52 -  * Acc@1 94.540 Acc@5 99.860
03-Mar-22 09:44:52 - Best acc at epoch 12: 94.77999877929688
03-Mar-22 09:44:52 - Epoch: [13][ 40/352]	Time  0.125 ( 0.123)	Data  0.002 ( 0.008)	Loss 1.2796e-01 (1.4474e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:44:53 - Epoch: [13][  0/352]	Time  0.406 ( 0.406)	Data  0.267 ( 0.267)	Loss 2.2211e-01 (2.2211e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:53 - Epoch: [13][ 50/352]	Time  0.111 ( 0.121)	Data  0.002 ( 0.007)	Loss 1.3452e-01 (1.4462e-01)	Acc@1  96.88 ( 95.40)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:44:54 - Epoch: [13][ 10/352]	Time  0.130 ( 0.147)	Data  0.002 ( 0.026)	Loss 1.8006e-01 (1.6067e-01)	Acc@1  93.75 ( 94.53)	Acc@5 100.00 ( 99.79)
03-Mar-22 09:44:54 - Epoch: [13][ 60/352]	Time  0.101 ( 0.121)	Data  0.002 ( 0.006)	Loss 1.8143e-01 (1.4544e-01)	Acc@1  93.75 ( 95.36)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:44:55 - Epoch: [13][ 20/352]	Time  0.136 ( 0.140)	Data  0.002 ( 0.015)	Loss 9.7454e-02 (1.4708e-01)	Acc@1  96.09 ( 95.05)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:44:56 - Epoch: [13][ 70/352]	Time  0.142 ( 0.120)	Data  0.002 ( 0.005)	Loss 1.2262e-01 (1.4682e-01)	Acc@1  95.31 ( 95.35)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:44:56 - Epoch: [13][ 30/352]	Time  0.130 ( 0.137)	Data  0.002 ( 0.011)	Loss 1.0354e-01 (1.4332e-01)	Acc@1  97.66 ( 95.06)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:44:57 - Epoch: [13][ 80/352]	Time  0.127 ( 0.120)	Data  0.002 ( 0.005)	Loss 1.4486e-01 (1.4721e-01)	Acc@1  93.75 ( 95.28)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:44:58 - Epoch: [13][ 40/352]	Time  0.143 ( 0.137)	Data  0.003 ( 0.009)	Loss 7.9813e-02 (1.3876e-01)	Acc@1  96.09 ( 95.16)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:58 - Epoch: [13][ 90/352]	Time  0.139 ( 0.120)	Data  0.002 ( 0.005)	Loss 2.0733e-01 (1.4918e-01)	Acc@1  90.62 ( 95.14)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:44:59 - Epoch: [13][ 50/352]	Time  0.135 ( 0.136)	Data  0.002 ( 0.008)	Loss 1.2063e-01 (1.3654e-01)	Acc@1  96.09 ( 95.39)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:59 - Epoch: [13][100/352]	Time  0.128 ( 0.121)	Data  0.002 ( 0.004)	Loss 2.0513e-01 (1.5238e-01)	Acc@1  91.41 ( 95.05)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:00 - Epoch: [13][ 60/352]	Time  0.141 ( 0.136)	Data  0.003 ( 0.007)	Loss 1.8613e-01 (1.3743e-01)	Acc@1  91.41 ( 95.31)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:45:01 - Epoch: [13][110/352]	Time  0.133 ( 0.121)	Data  0.003 ( 0.004)	Loss 1.9849e-01 (1.5247e-01)	Acc@1  95.31 ( 95.06)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:02 - Epoch: [13][ 70/352]	Time  0.133 ( 0.136)	Data  0.003 ( 0.006)	Loss 7.6453e-02 (1.3826e-01)	Acc@1  98.44 ( 95.29)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:45:02 - Epoch: [13][120/352]	Time  0.112 ( 0.122)	Data  0.002 ( 0.004)	Loss 1.1018e-01 (1.5071e-01)	Acc@1  96.09 ( 95.12)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:03 - Epoch: [13][130/352]	Time  0.110 ( 0.122)	Data  0.002 ( 0.004)	Loss 1.6073e-01 (1.4851e-01)	Acc@1  93.75 ( 95.18)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:03 - Epoch: [13][ 80/352]	Time  0.138 ( 0.136)	Data  0.002 ( 0.006)	Loss 8.3173e-02 (1.3714e-01)	Acc@1  97.66 ( 95.33)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:45:04 - Epoch: [13][140/352]	Time  0.131 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.0499e-01 (1.4758e-01)	Acc@1  96.88 ( 95.21)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:05 - Epoch: [13][ 90/352]	Time  0.133 ( 0.136)	Data  0.002 ( 0.006)	Loss 9.1183e-02 (1.3409e-01)	Acc@1  97.66 ( 95.42)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:45:06 - Epoch: [13][150/352]	Time  0.109 ( 0.123)	Data  0.002 ( 0.004)	Loss 5.8049e-02 (1.4585e-01)	Acc@1  98.44 ( 95.29)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:06 - Epoch: [13][100/352]	Time  0.113 ( 0.136)	Data  0.002 ( 0.005)	Loss 7.8762e-02 (1.3422e-01)	Acc@1  97.66 ( 95.46)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:45:07 - Epoch: [13][160/352]	Time  0.129 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.2532e-01 (1.4602e-01)	Acc@1  96.09 ( 95.25)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:07 - Epoch: [13][110/352]	Time  0.134 ( 0.136)	Data  0.003 ( 0.005)	Loss 1.1961e-01 (1.3520e-01)	Acc@1  97.66 ( 95.47)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:08 - Epoch: [13][170/352]	Time  0.126 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.4642e-01 (1.4688e-01)	Acc@1  96.88 ( 95.22)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:09 - Epoch: [13][120/352]	Time  0.099 ( 0.136)	Data  0.002 ( 0.005)	Loss 1.8634e-01 (1.3648e-01)	Acc@1  92.19 ( 95.44)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:45:09 - Epoch: [13][180/352]	Time  0.138 ( 0.124)	Data  0.003 ( 0.004)	Loss 1.0392e-01 (1.4677e-01)	Acc@1  96.88 ( 95.24)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:10 - Epoch: [13][130/352]	Time  0.132 ( 0.135)	Data  0.003 ( 0.005)	Loss 1.3691e-01 (1.3759e-01)	Acc@1  94.53 ( 95.35)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:11 - Epoch: [13][190/352]	Time  0.104 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.9772e-01 (1.4622e-01)	Acc@1  96.09 ( 95.28)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:11 - Epoch: [13][140/352]	Time  0.136 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.1027e-01 (1.3773e-01)	Acc@1  96.09 ( 95.32)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:12 - Epoch: [13][200/352]	Time  0.131 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.0162e-01 (1.4538e-01)	Acc@1  98.44 ( 95.28)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:13 - Epoch: [13][150/352]	Time  0.132 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.5934e-01 (1.3985e-01)	Acc@1  92.97 ( 95.22)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:13 - Epoch: [13][210/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.0539e-01 (1.4544e-01)	Acc@1  97.66 ( 95.26)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:14 - Epoch: [13][160/352]	Time  0.134 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.5652e-01 (1.4071e-01)	Acc@1  94.53 ( 95.20)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:15 - Epoch: [13][220/352]	Time  0.134 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6909e-01 (1.4602e-01)	Acc@1  94.53 ( 95.25)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:15 - Epoch: [13][170/352]	Time  0.123 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.1294e-01 (1.4042e-01)	Acc@1  95.31 ( 95.23)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:45:16 - Epoch: [13][230/352]	Time  0.153 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6212e-01 (1.4609e-01)	Acc@1  94.53 ( 95.27)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:17 - Epoch: [13][180/352]	Time  0.130 ( 0.135)	Data  0.003 ( 0.004)	Loss 8.9137e-02 (1.4008e-01)	Acc@1  97.66 ( 95.24)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:17 - Epoch: [13][240/352]	Time  0.146 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.3866e-01 (1.4569e-01)	Acc@1  95.31 ( 95.30)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:18 - Epoch: [13][190/352]	Time  0.138 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.5772e-01 (1.4023e-01)	Acc@1  94.53 ( 95.25)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:18 - Epoch: [13][250/352]	Time  0.137 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.4778e-01 (1.4610e-01)	Acc@1  92.97 ( 95.25)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:19 - Epoch: [13][200/352]	Time  0.143 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.4173e-01 (1.4122e-01)	Acc@1  94.53 ( 95.22)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:20 - Epoch: [13][260/352]	Time  0.126 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.3378e-01 (1.4645e-01)	Acc@1  94.53 ( 95.24)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:21 - Epoch: [13][210/352]	Time  0.140 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.2084e-01 (1.4111e-01)	Acc@1  96.88 ( 95.25)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:21 - Epoch: [13][270/352]	Time  0.125 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.0705e-01 (1.4613e-01)	Acc@1  96.88 ( 95.26)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:22 - Epoch: [13][220/352]	Time  0.127 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.1863e-01 (1.4174e-01)	Acc@1  96.09 ( 95.21)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:22 - Epoch: [13][280/352]	Time  0.125 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.3913e-01 (1.4554e-01)	Acc@1  95.31 ( 95.26)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:23 - Epoch: [13][230/352]	Time  0.136 ( 0.134)	Data  0.002 ( 0.004)	Loss 1.6058e-01 (1.4242e-01)	Acc@1  94.53 ( 95.21)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:23 - Epoch: [13][290/352]	Time  0.125 ( 0.124)	Data  0.003 ( 0.003)	Loss 1.5633e-01 (1.4548e-01)	Acc@1  93.75 ( 95.27)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:24 - Epoch: [13][300/352]	Time  0.110 ( 0.124)	Data  0.002 ( 0.003)	Loss 3.4839e-01 (1.4700e-01)	Acc@1  86.72 ( 95.20)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:25 - Epoch: [13][240/352]	Time  0.110 ( 0.134)	Data  0.002 ( 0.004)	Loss 1.7794e-01 (1.4347e-01)	Acc@1  93.75 ( 95.16)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:26 - Epoch: [13][310/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6145e-01 (1.4697e-01)	Acc@1  94.53 ( 95.18)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:26 - Epoch: [13][250/352]	Time  0.128 ( 0.134)	Data  0.002 ( 0.004)	Loss 1.4358e-01 (1.4343e-01)	Acc@1  96.09 ( 95.18)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:27 - Epoch: [13][320/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.1425e-01 (1.4639e-01)	Acc@1  96.88 ( 95.22)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:27 - Epoch: [13][260/352]	Time  0.124 ( 0.134)	Data  0.002 ( 0.004)	Loss 1.5606e-01 (1.4237e-01)	Acc@1  96.09 ( 95.24)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:28 - Epoch: [13][330/352]	Time  0.125 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.0787e-01 (1.4517e-01)	Acc@1  96.88 ( 95.25)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:45:28 - Epoch: [13][270/352]	Time  0.144 ( 0.134)	Data  0.002 ( 0.004)	Loss 8.4358e-02 (1.4360e-01)	Acc@1  97.66 ( 95.20)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:29 - Epoch: [13][340/352]	Time  0.130 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.2351e-01 (1.4510e-01)	Acc@1  96.09 ( 95.23)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:30 - Epoch: [13][280/352]	Time  0.132 ( 0.134)	Data  0.003 ( 0.003)	Loss 6.6440e-02 (1.4354e-01)	Acc@1  99.22 ( 95.21)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:31 - Epoch: [13][350/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.1877e-01 (1.4513e-01)	Acc@1  97.66 ( 95.24)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:45:31 - Epoch: [13][290/352]	Time  0.099 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.3647e-01 (1.4360e-01)	Acc@1  95.31 ( 95.19)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:45:31 - Test: [ 0/20]	Time  0.323 ( 0.323)	Loss 1.3446e-01 (1.3446e-01)	Acc@1  94.92 ( 94.92)	Acc@5 100.00 (100.00)
03-Mar-22 09:45:32 - Test: [10/20]	Time  0.069 ( 0.093)	Loss 1.5294e-01 (1.6112e-01)	Acc@1  94.92 ( 94.78)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:32 - Epoch: [13][300/352]	Time  0.134 ( 0.133)	Data  0.002 ( 0.003)	Loss 1.3692e-01 (1.4411e-01)	Acc@1  97.66 ( 95.20)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:32 -  * Acc@1 94.640 Acc@5 99.940
03-Mar-22 09:45:32 - Best acc at epoch 13: 94.65999603271484
03-Mar-22 09:45:33 - Epoch: [14][  0/352]	Time  0.333 ( 0.333)	Data  0.226 ( 0.226)	Loss 1.8403e-01 (1.8403e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:45:33 - Epoch: [13][310/352]	Time  0.127 ( 0.133)	Data  0.002 ( 0.003)	Loss 1.5723e-01 (1.4395e-01)	Acc@1  93.75 ( 95.22)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:34 - Epoch: [14][ 10/352]	Time  0.126 ( 0.145)	Data  0.002 ( 0.023)	Loss 1.5553e-01 (1.3901e-01)	Acc@1  94.53 ( 95.24)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:45:35 - Epoch: [13][320/352]	Time  0.114 ( 0.132)	Data  0.003 ( 0.003)	Loss 9.5389e-02 (1.4427e-01)	Acc@1  98.44 ( 95.21)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:35 - Epoch: [14][ 20/352]	Time  0.145 ( 0.136)	Data  0.003 ( 0.013)	Loss 7.7351e-02 (1.3279e-01)	Acc@1  96.88 ( 95.65)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:36 - Epoch: [13][330/352]	Time  0.132 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.3914e-01 (1.4470e-01)	Acc@1  96.09 ( 95.21)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:37 - Epoch: [14][ 30/352]	Time  0.144 ( 0.136)	Data  0.003 ( 0.010)	Loss 1.0730e-01 (1.3389e-01)	Acc@1  97.66 ( 95.74)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:45:37 - Epoch: [13][340/352]	Time  0.129 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.6165e-01 (1.4551e-01)	Acc@1  94.53 ( 95.19)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:38 - Epoch: [14][ 40/352]	Time  0.131 ( 0.134)	Data  0.002 ( 0.008)	Loss 1.3767e-01 (1.3144e-01)	Acc@1  96.09 ( 95.77)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:38 - Epoch: [13][350/352]	Time  0.130 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.5476e-01 (1.4576e-01)	Acc@1  96.09 ( 95.18)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:39 - Test: [ 0/20]	Time  0.319 ( 0.319)	Loss 1.6447e-01 (1.6447e-01)	Acc@1  94.53 ( 94.53)	Acc@5  99.61 ( 99.61)
03-Mar-22 09:45:39 - Epoch: [14][ 50/352]	Time  0.124 ( 0.133)	Data  0.002 ( 0.007)	Loss 8.7521e-02 (1.3006e-01)	Acc@1  98.44 ( 95.86)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:40 - Test: [10/20]	Time  0.086 ( 0.099)	Loss 1.4485e-01 (1.6678e-01)	Acc@1  95.70 ( 94.39)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:45:41 - Epoch: [14][ 60/352]	Time  0.123 ( 0.132)	Data  0.002 ( 0.006)	Loss 1.0945e-01 (1.3070e-01)	Acc@1  96.09 ( 95.77)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:41 -  * Acc@1 94.280 Acc@5 99.920
03-Mar-22 09:45:41 - Best acc at epoch 13: 94.77999877929688
03-Mar-22 09:45:41 - Epoch: [14][  0/352]	Time  0.350 ( 0.350)	Data  0.241 ( 0.241)	Loss 2.5042e-01 (2.5042e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
03-Mar-22 09:45:42 - Epoch: [14][ 70/352]	Time  0.135 ( 0.131)	Data  0.003 ( 0.006)	Loss 2.2352e-01 (1.3322e-01)	Acc@1  92.19 ( 95.70)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:45:42 - Epoch: [14][ 10/352]	Time  0.129 ( 0.142)	Data  0.002 ( 0.024)	Loss 1.8261e-01 (1.6064e-01)	Acc@1  93.75 ( 93.89)	Acc@5 100.00 (100.00)
03-Mar-22 09:45:43 - Epoch: [14][ 80/352]	Time  0.131 ( 0.131)	Data  0.002 ( 0.005)	Loss 1.1817e-01 (1.3555e-01)	Acc@1  96.09 ( 95.57)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:44 - Epoch: [14][ 20/352]	Time  0.129 ( 0.136)	Data  0.002 ( 0.014)	Loss 1.0143e-01 (1.5310e-01)	Acc@1  96.88 ( 94.61)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:45:44 - Epoch: [14][ 90/352]	Time  0.130 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.4078e-01 (1.3645e-01)	Acc@1  94.53 ( 95.52)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:45:45 - Epoch: [14][ 30/352]	Time  0.132 ( 0.134)	Data  0.002 ( 0.010)	Loss 1.6798e-01 (1.5329e-01)	Acc@1  94.53 ( 94.68)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:45:46 - Epoch: [14][100/352]	Time  0.110 ( 0.130)	Data  0.003 ( 0.005)	Loss 1.4707e-01 (1.3875e-01)	Acc@1  93.75 ( 95.45)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:46 - Epoch: [14][ 40/352]	Time  0.113 ( 0.132)	Data  0.002 ( 0.008)	Loss 1.3882e-01 (1.5044e-01)	Acc@1  96.09 ( 94.87)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:45:47 - Epoch: [14][110/352]	Time  0.137 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.3584e-01 (1.3982e-01)	Acc@1  96.09 ( 95.43)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:47 - Epoch: [14][ 50/352]	Time  0.113 ( 0.131)	Data  0.002 ( 0.007)	Loss 1.7995e-01 (1.5192e-01)	Acc@1  93.75 ( 94.76)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:45:48 - Epoch: [14][120/352]	Time  0.135 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.3009e-01 (1.3995e-01)	Acc@1  92.97 ( 95.34)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:49 - Epoch: [14][ 60/352]	Time  0.127 ( 0.130)	Data  0.003 ( 0.006)	Loss 1.5515e-01 (1.5322e-01)	Acc@1  94.53 ( 94.79)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:45:49 - Epoch: [14][130/352]	Time  0.124 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.3094e-01 (1.4042e-01)	Acc@1  94.53 ( 95.36)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:50 - Epoch: [14][ 70/352]	Time  0.127 ( 0.129)	Data  0.002 ( 0.006)	Loss 2.4879e-01 (1.5639e-01)	Acc@1  92.19 ( 94.69)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:45:51 - Epoch: [14][140/352]	Time  0.103 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.5372e-01 (1.4267e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:51 - Epoch: [14][ 80/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.5672e-01 (1.5388e-01)	Acc@1  95.31 ( 94.81)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:45:52 - Epoch: [14][150/352]	Time  0.150 ( 0.129)	Data  0.002 ( 0.004)	Loss 9.5474e-02 (1.4343e-01)	Acc@1  96.09 ( 95.31)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:52 - Epoch: [14][ 90/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.005)	Loss 2.0206e-01 (1.5224e-01)	Acc@1  95.31 ( 94.88)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:45:53 - Epoch: [14][160/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.004)	Loss 9.7943e-02 (1.4376e-01)	Acc@1  96.09 ( 95.27)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:54 - Epoch: [14][100/352]	Time  0.133 ( 0.130)	Data  0.003 ( 0.005)	Loss 1.2250e-01 (1.5194e-01)	Acc@1  96.09 ( 94.87)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:55 - Epoch: [14][170/352]	Time  0.107 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.2769e-01 (1.4345e-01)	Acc@1  97.66 ( 95.33)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:45:55 - Epoch: [14][110/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.3899e-01 (1.5202e-01)	Acc@1  95.31 ( 94.88)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:56 - Epoch: [14][180/352]	Time  0.132 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.4099e-01 (1.4408e-01)	Acc@1  96.09 ( 95.30)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:56 - Epoch: [14][120/352]	Time  0.110 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.7462e-01 (1.5442e-01)	Acc@1  91.41 ( 94.81)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:57 - Epoch: [14][190/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.3407e-01 (1.4487e-01)	Acc@1  96.09 ( 95.27)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:58 - Epoch: [14][130/352]	Time  0.132 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.4872e-01 (1.5322e-01)	Acc@1  93.75 ( 94.85)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:58 - Epoch: [14][200/352]	Time  0.113 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.8424e-01 (1.4515e-01)	Acc@1  96.09 ( 95.28)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:45:59 - Epoch: [14][140/352]	Time  0.130 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.3384e-01 (1.5321e-01)	Acc@1  92.97 ( 94.86)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:00 - Epoch: [14][210/352]	Time  0.116 ( 0.129)	Data  0.002 ( 0.003)	Loss 8.7557e-02 (1.4517e-01)	Acc@1  98.44 ( 95.24)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:00 - Epoch: [14][150/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.2640e-01 (1.5163e-01)	Acc@1  96.09 ( 94.90)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:01 - Epoch: [14][220/352]	Time  0.127 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7774e-01 (1.4490e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:01 - Epoch: [14][160/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.004)	Loss 9.6842e-02 (1.5028e-01)	Acc@1  97.66 ( 94.99)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:02 - Epoch: [14][230/352]	Time  0.131 ( 0.129)	Data  0.003 ( 0.003)	Loss 7.3600e-02 (1.4541e-01)	Acc@1  96.88 ( 95.19)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:03 - Epoch: [14][170/352]	Time  0.134 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.7247e-01 (1.4943e-01)	Acc@1  94.53 ( 94.99)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:04 - Epoch: [14][240/352]	Time  0.129 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.8026e-01 (1.4696e-01)	Acc@1  93.75 ( 95.12)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:46:04 - Epoch: [14][180/352]	Time  0.134 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.7086e-01 (1.4970e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:05 - Epoch: [14][250/352]	Time  0.142 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7417e-01 (1.4828e-01)	Acc@1  94.53 ( 95.08)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:46:05 - Epoch: [14][190/352]	Time  0.134 ( 0.129)	Data  0.003 ( 0.004)	Loss 8.5629e-02 (1.4879e-01)	Acc@1  96.88 ( 95.04)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:06 - Epoch: [14][260/352]	Time  0.122 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.6216e-01 (1.4856e-01)	Acc@1  96.09 ( 95.07)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:07 - Epoch: [14][200/352]	Time  0.126 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.6554e-01 (1.5035e-01)	Acc@1  92.97 ( 94.95)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:46:08 - Epoch: [14][270/352]	Time  0.124 ( 0.129)	Data  0.003 ( 0.003)	Loss 2.2099e-01 (1.4951e-01)	Acc@1  92.97 ( 95.02)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:08 - Epoch: [14][210/352]	Time  0.132 ( 0.129)	Data  0.003 ( 0.004)	Loss 2.5951e-01 (1.5046e-01)	Acc@1  92.97 ( 94.96)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:09 - Epoch: [14][280/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0046e-01 (1.4864e-01)	Acc@1  96.09 ( 95.05)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:09 - Epoch: [14][220/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.6640e-01 (1.5034e-01)	Acc@1  91.41 ( 94.94)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:10 - Epoch: [14][290/352]	Time  0.138 ( 0.129)	Data  0.004 ( 0.003)	Loss 1.9902e-01 (1.4869e-01)	Acc@1  92.97 ( 95.02)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:10 - Epoch: [14][230/352]	Time  0.142 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.2616e-01 (1.4930e-01)	Acc@1  96.09 ( 95.00)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:11 - Epoch: [14][300/352]	Time  0.129 ( 0.129)	Data  0.004 ( 0.003)	Loss 2.2263e-01 (1.4901e-01)	Acc@1  92.97 ( 95.01)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:12 - Epoch: [14][240/352]	Time  0.133 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.3959e-01 (1.4960e-01)	Acc@1  96.88 ( 95.01)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:13 - Epoch: [14][310/352]	Time  0.134 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4783e-01 (1.4846e-01)	Acc@1  96.88 ( 95.06)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:13 - Epoch: [14][250/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7150e-01 (1.4926e-01)	Acc@1  94.53 ( 95.02)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:14 - Epoch: [14][320/352]	Time  0.135 ( 0.129)	Data  0.004 ( 0.003)	Loss 1.9826e-01 (1.4825e-01)	Acc@1  93.75 ( 95.07)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:14 - Epoch: [14][260/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1942e-01 (1.4883e-01)	Acc@1  95.31 ( 95.04)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:15 - Epoch: [14][330/352]	Time  0.140 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1651e-01 (1.4771e-01)	Acc@1  97.66 ( 95.09)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:16 - Epoch: [14][270/352]	Time  0.112 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5455e-01 (1.4870e-01)	Acc@1  92.19 ( 95.02)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:17 - Epoch: [14][340/352]	Time  0.160 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5691e-01 (1.4736e-01)	Acc@1  93.75 ( 95.10)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:17 - Epoch: [14][280/352]	Time  0.146 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1240e-01 (1.4877e-01)	Acc@1  97.66 ( 95.03)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:18 - Epoch: [14][350/352]	Time  0.137 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.3958e-01 (1.4750e-01)	Acc@1  93.75 ( 95.09)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:18 - Epoch: [14][290/352]	Time  0.102 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7203e-01 (1.4883e-01)	Acc@1  93.75 ( 95.03)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:18 - Test: [ 0/20]	Time  0.333 ( 0.333)	Loss 1.1318e-01 (1.1318e-01)	Acc@1  96.48 ( 96.48)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:19 - Test: [10/20]	Time  0.071 ( 0.101)	Loss 1.6651e-01 (1.6571e-01)	Acc@1  92.58 ( 94.35)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:46:19 - Epoch: [14][300/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.0098e-01 (1.4874e-01)	Acc@1  89.84 ( 95.02)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:20 -  * Acc@1 94.600 Acc@5 99.940
03-Mar-22 09:46:20 - Best acc at epoch 14: 94.65999603271484
03-Mar-22 09:46:20 - Epoch: [15][  0/352]	Time  0.341 ( 0.341)	Data  0.240 ( 0.240)	Loss 2.1297e-01 (2.1297e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:21 - Epoch: [14][310/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.8862e-01 (1.4869e-01)	Acc@1  91.41 ( 95.02)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:21 - Epoch: [15][ 10/352]	Time  0.124 ( 0.141)	Data  0.002 ( 0.023)	Loss 2.1727e-01 (1.6731e-01)	Acc@1  94.53 ( 94.82)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:22 - Epoch: [14][320/352]	Time  0.112 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.0619e-01 (1.4830e-01)	Acc@1  94.53 ( 95.04)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:23 - Epoch: [15][ 20/352]	Time  0.155 ( 0.131)	Data  0.003 ( 0.013)	Loss 9.8965e-02 (1.4734e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:23 - Epoch: [14][330/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.1946e-01 (1.4815e-01)	Acc@1  97.66 ( 95.06)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:24 - Epoch: [15][ 30/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.010)	Loss 9.7826e-02 (1.3930e-01)	Acc@1  97.66 ( 95.39)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:24 - Epoch: [14][340/352]	Time  0.128 ( 0.128)	Data  0.003 ( 0.003)	Loss 1.3094e-01 (1.4806e-01)	Acc@1  93.75 ( 95.03)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:25 - Epoch: [15][ 40/352]	Time  0.108 ( 0.127)	Data  0.002 ( 0.008)	Loss 2.4232e-01 (1.4162e-01)	Acc@1  93.75 ( 95.31)	Acc@5 100.00 ( 99.98)
03-Mar-22 09:46:26 - Epoch: [14][350/352]	Time  0.142 ( 0.128)	Data  0.003 ( 0.003)	Loss 1.3590e-01 (1.4788e-01)	Acc@1  96.88 ( 95.04)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:26 - Test: [ 0/20]	Time  0.339 ( 0.339)	Loss 1.4521e-01 (1.4521e-01)	Acc@1  94.92 ( 94.92)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:26 - Epoch: [15][ 50/352]	Time  0.104 ( 0.126)	Data  0.002 ( 0.007)	Loss 1.7027e-01 (1.4486e-01)	Acc@1  94.53 ( 95.14)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:27 - Test: [10/20]	Time  0.073 ( 0.105)	Loss 1.3383e-01 (1.5746e-01)	Acc@1  94.53 ( 94.85)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:28 - Epoch: [15][ 60/352]	Time  0.138 ( 0.125)	Data  0.002 ( 0.006)	Loss 1.3529e-01 (1.3993e-01)	Acc@1  95.31 ( 95.30)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:28 -  * Acc@1 94.760 Acc@5 99.900
03-Mar-22 09:46:28 - Best acc at epoch 14: 94.77999877929688
03-Mar-22 09:46:28 - Epoch: [15][  0/352]	Time  0.351 ( 0.351)	Data  0.238 ( 0.238)	Loss 1.2946e-01 (1.2946e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:29 - Epoch: [15][ 70/352]	Time  0.128 ( 0.125)	Data  0.002 ( 0.006)	Loss 1.9672e-01 (1.4037e-01)	Acc@1  93.75 ( 95.29)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:30 - Epoch: [15][ 10/352]	Time  0.125 ( 0.144)	Data  0.002 ( 0.024)	Loss 1.5223e-01 (1.3903e-01)	Acc@1  94.53 ( 95.38)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:46:30 - Epoch: [15][ 80/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.005)	Loss 1.9520e-01 (1.4001e-01)	Acc@1  91.41 ( 95.19)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:31 - Epoch: [15][ 20/352]	Time  0.143 ( 0.139)	Data  0.002 ( 0.014)	Loss 9.4987e-02 (1.3179e-01)	Acc@1  96.09 ( 95.35)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:46:31 - Epoch: [15][ 90/352]	Time  0.143 ( 0.125)	Data  0.003 ( 0.005)	Loss 2.1949e-01 (1.4127e-01)	Acc@1  92.97 ( 95.15)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:32 - Epoch: [15][ 30/352]	Time  0.131 ( 0.138)	Data  0.002 ( 0.010)	Loss 1.4941e-01 (1.3567e-01)	Acc@1  93.75 ( 95.31)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:33 - Epoch: [15][100/352]	Time  0.131 ( 0.125)	Data  0.002 ( 0.005)	Loss 2.0563e-01 (1.4123e-01)	Acc@1  94.53 ( 95.20)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:34 - Epoch: [15][ 40/352]	Time  0.139 ( 0.136)	Data  0.003 ( 0.008)	Loss 1.5644e-01 (1.3253e-01)	Acc@1  92.97 ( 95.46)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:34 - Epoch: [15][110/352]	Time  0.126 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.6100e-01 (1.4176e-01)	Acc@1  95.31 ( 95.21)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:35 - Epoch: [15][ 50/352]	Time  0.144 ( 0.135)	Data  0.003 ( 0.007)	Loss 1.0314e-01 (1.3522e-01)	Acc@1  96.88 ( 95.47)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:46:35 - Epoch: [15][120/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.004)	Loss 2.0103e-01 (1.4129e-01)	Acc@1  92.97 ( 95.24)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:36 - Epoch: [15][ 60/352]	Time  0.144 ( 0.134)	Data  0.003 ( 0.006)	Loss 1.7438e-01 (1.3696e-01)	Acc@1  93.75 ( 95.36)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:46:36 - Epoch: [15][130/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.1408e-01 (1.4101e-01)	Acc@1  96.09 ( 95.25)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:37 - Epoch: [15][140/352]	Time  0.131 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.1016e-01 (1.4023e-01)	Acc@1  94.53 ( 95.30)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:38 - Epoch: [15][ 70/352]	Time  0.142 ( 0.135)	Data  0.002 ( 0.006)	Loss 8.9351e-02 (1.3687e-01)	Acc@1  96.88 ( 95.35)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:46:39 - Epoch: [15][150/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.3730e-01 (1.4205e-01)	Acc@1  97.66 ( 95.24)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:39 - Epoch: [15][ 80/352]	Time  0.142 ( 0.135)	Data  0.002 ( 0.005)	Loss 8.6794e-02 (1.3651e-01)	Acc@1  97.66 ( 95.31)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:40 - Epoch: [15][160/352]	Time  0.125 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.1145e-01 (1.4216e-01)	Acc@1  96.09 ( 95.24)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:40 - Epoch: [15][ 90/352]	Time  0.142 ( 0.136)	Data  0.003 ( 0.005)	Loss 1.0515e-01 (1.3764e-01)	Acc@1  96.88 ( 95.31)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:41 - Epoch: [15][170/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.3024e-01 (1.4137e-01)	Acc@1  96.09 ( 95.27)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:42 - Epoch: [15][100/352]	Time  0.136 ( 0.136)	Data  0.003 ( 0.005)	Loss 9.5189e-02 (1.3779e-01)	Acc@1  96.09 ( 95.29)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:42 - Epoch: [15][180/352]	Time  0.129 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.8926e-01 (1.4289e-01)	Acc@1  94.53 ( 95.24)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:43 - Epoch: [15][110/352]	Time  0.130 ( 0.137)	Data  0.003 ( 0.005)	Loss 2.1993e-01 (1.3796e-01)	Acc@1  89.06 ( 95.24)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:44 - Epoch: [15][190/352]	Time  0.129 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.5044e-01 (1.4265e-01)	Acc@1  95.31 ( 95.24)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:45 - Epoch: [15][120/352]	Time  0.128 ( 0.137)	Data  0.002 ( 0.004)	Loss 1.3320e-01 (1.3759e-01)	Acc@1  94.53 ( 95.22)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:45 - Epoch: [15][200/352]	Time  0.124 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.2279e-01 (1.4212e-01)	Acc@1  94.53 ( 95.25)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:46 - Epoch: [15][130/352]	Time  0.133 ( 0.136)	Data  0.002 ( 0.004)	Loss 1.5781e-01 (1.3884e-01)	Acc@1  94.53 ( 95.21)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:46 - Epoch: [15][210/352]	Time  0.117 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.4426e-01 (1.4206e-01)	Acc@1  96.88 ( 95.23)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:47 - Epoch: [15][140/352]	Time  0.139 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.6292e-01 (1.3896e-01)	Acc@1  92.97 ( 95.20)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:47 - Epoch: [15][220/352]	Time  0.132 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.2118e-01 (1.4153e-01)	Acc@1  93.75 ( 95.26)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:48 - Epoch: [15][150/352]	Time  0.127 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.4676e-01 (1.4106e-01)	Acc@1  96.09 ( 95.17)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:49 - Epoch: [15][230/352]	Time  0.111 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.4702e-01 (1.4164e-01)	Acc@1  95.31 ( 95.26)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:46:50 - Epoch: [15][160/352]	Time  0.147 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.4718e-01 (1.4068e-01)	Acc@1  95.31 ( 95.20)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:50 - Epoch: [15][240/352]	Time  0.139 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.4776e-01 (1.4179e-01)	Acc@1  94.53 ( 95.24)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:51 - Epoch: [15][170/352]	Time  0.134 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.3360e-01 (1.4010e-01)	Acc@1  95.31 ( 95.18)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:51 - Epoch: [15][250/352]	Time  0.136 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.2835e-01 (1.4203e-01)	Acc@1  97.66 ( 95.25)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:52 - Epoch: [15][180/352]	Time  0.144 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.5855e-01 (1.3961e-01)	Acc@1  95.31 ( 95.22)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:53 - Epoch: [15][260/352]	Time  0.146 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.4791e-01 (1.4278e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:54 - Epoch: [15][190/352]	Time  0.132 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.4916e-01 (1.3792e-01)	Acc@1  93.75 ( 95.27)	Acc@5 100.00 ( 99.94)
03-Mar-22 10:01:14 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:01:14 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:04:39 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:04:39 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:04:39 - match all modules defined in bit_config: False
03-Mar-22 10:04:39 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:13:06 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:13:06 - Use GPU: 0 for training
03-Mar-22 10:13:06 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:15:02 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:15:02 - Use GPU: 0 for training
03-Mar-22 10:15:02 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:15:06 - match all modules defined in bit_config: False
03-Mar-22 10:15:06 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:15:27 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:15:27 - Use GPU: 0 for training
03-Mar-22 10:15:27 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:15:31 - match all modules defined in bit_config: False
03-Mar-22 10:15:31 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:17:14 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:17:14 - Use GPU: 0 for training
03-Mar-22 10:17:14 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:17:18 - match all modules defined in bit_config: False
03-Mar-22 10:17:18 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:19:22 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:19:22 - Use GPU: 0 for training
03-Mar-22 10:19:22 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:19:26 - match all modules defined in bit_config: False
03-Mar-22 10:19:26 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:41:56 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:41:56 - Use GPU: 0 for training
03-Mar-22 10:41:56 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:42:00 - match all modules defined in bit_config: False
03-Mar-22 10:42:00 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:43:16 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:43:16 - Use GPU: 0 for training
03-Mar-22 10:43:16 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 10:43:20 - match all modules defined in bit_config: True
03-Mar-22 10:43:20 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:43:20 - Epoch: [0][  0/352]	Time  0.413 ( 0.413)	Data  0.237 ( 0.237)	Loss 3.0795e-01 (3.0795e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
03-Mar-22 10:43:22 - Epoch: [0][ 10/352]	Time  0.098 ( 0.142)	Data  0.001 ( 0.023)	Loss 1.9220e-01 (1.7236e-01)	Acc@1  96.09 ( 94.32)	Acc@5 100.00 (100.00)
03-Mar-22 10:43:23 - Epoch: [0][ 20/352]	Time  0.097 ( 0.121)	Data  0.001 ( 0.013)	Loss 9.7979e-02 (1.4542e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 (100.00)
03-Mar-22 10:43:23 - Epoch: [0][ 30/352]	Time  0.097 ( 0.114)	Data  0.001 ( 0.009)	Loss 1.0826e-01 (1.4135e-01)	Acc@1  96.88 ( 95.09)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:24 - Epoch: [0][ 40/352]	Time  0.096 ( 0.110)	Data  0.001 ( 0.007)	Loss 8.8699e-02 (1.3833e-01)	Acc@1  96.88 ( 95.18)	Acc@5  99.22 ( 99.94)
03-Mar-22 10:43:25 - Epoch: [0][ 50/352]	Time  0.096 ( 0.108)	Data  0.001 ( 0.006)	Loss 1.0920e-01 (1.3333e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.95)
03-Mar-22 10:43:27 - Epoch: [0][ 60/352]	Time  0.131 ( 0.109)	Data  0.002 ( 0.005)	Loss 1.4759e-01 (1.3037e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 ( 99.96)
03-Mar-22 10:43:28 - Epoch: [0][ 70/352]	Time  0.133 ( 0.112)	Data  0.002 ( 0.005)	Loss 9.4575e-02 (1.2611e-01)	Acc@1  96.09 ( 95.35)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:29 - Epoch: [0][ 80/352]	Time  0.131 ( 0.115)	Data  0.002 ( 0.005)	Loss 1.0132e-01 (1.2644e-01)	Acc@1  96.09 ( 95.37)	Acc@5 100.00 ( 99.96)
03-Mar-22 10:43:31 - Epoch: [0][ 90/352]	Time  0.096 ( 0.116)	Data  0.002 ( 0.004)	Loss 8.2958e-02 (1.2348e-01)	Acc@1  95.31 ( 95.52)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:31 - Epoch: [0][100/352]	Time  0.096 ( 0.114)	Data  0.001 ( 0.004)	Loss 5.7821e-02 (1.2140e-01)	Acc@1  98.44 ( 95.60)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:32 - Epoch: [0][110/352]	Time  0.098 ( 0.113)	Data  0.002 ( 0.004)	Loss 1.1381e-01 (1.1959e-01)	Acc@1  94.53 ( 95.69)	Acc@5 100.00 ( 99.96)
03-Mar-22 10:43:33 - Epoch: [0][120/352]	Time  0.096 ( 0.111)	Data  0.001 ( 0.004)	Loss 1.7338e-01 (1.1988e-01)	Acc@1  93.75 ( 95.69)	Acc@5 100.00 ( 99.96)
03-Mar-22 10:43:34 - Epoch: [0][130/352]	Time  0.097 ( 0.110)	Data  0.002 ( 0.004)	Loss 1.2692e-01 (1.1928e-01)	Acc@1  94.53 ( 95.71)	Acc@5 100.00 ( 99.96)
03-Mar-22 10:43:35 - Epoch: [0][140/352]	Time  0.096 ( 0.109)	Data  0.001 ( 0.003)	Loss 7.6918e-02 (1.1740e-01)	Acc@1  96.88 ( 95.80)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:36 - Epoch: [0][150/352]	Time  0.097 ( 0.108)	Data  0.002 ( 0.003)	Loss 9.9368e-02 (1.1762e-01)	Acc@1  95.31 ( 95.76)	Acc@5 100.00 ( 99.96)
03-Mar-22 10:43:37 - Epoch: [0][160/352]	Time  0.097 ( 0.108)	Data  0.001 ( 0.003)	Loss 3.1986e-02 (1.1649e-01)	Acc@1 100.00 ( 95.80)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:38 - Epoch: [0][170/352]	Time  0.096 ( 0.107)	Data  0.002 ( 0.003)	Loss 1.2716e-01 (1.1669e-01)	Acc@1  95.31 ( 95.76)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:39 - Epoch: [0][180/352]	Time  0.096 ( 0.107)	Data  0.001 ( 0.003)	Loss 1.0362e-01 (1.1563e-01)	Acc@1  96.88 ( 95.81)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:40 - Epoch: [0][190/352]	Time  0.097 ( 0.106)	Data  0.001 ( 0.003)	Loss 8.7430e-02 (1.1528e-01)	Acc@1  97.66 ( 95.84)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:41 - Epoch: [0][200/352]	Time  0.096 ( 0.106)	Data  0.001 ( 0.003)	Loss 1.3658e-01 (1.1441e-01)	Acc@1  95.31 ( 95.88)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:42 - Epoch: [0][210/352]	Time  0.096 ( 0.105)	Data  0.002 ( 0.003)	Loss 1.3520e-01 (1.1345e-01)	Acc@1  96.09 ( 95.93)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:43 - Epoch: [0][220/352]	Time  0.096 ( 0.105)	Data  0.001 ( 0.003)	Loss 9.1622e-02 (1.1400e-01)	Acc@1  96.88 ( 95.91)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:44 - Epoch: [0][230/352]	Time  0.097 ( 0.105)	Data  0.001 ( 0.003)	Loss 1.0555e-01 (1.1345e-01)	Acc@1  95.31 ( 95.90)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:45 - Epoch: [0][240/352]	Time  0.101 ( 0.105)	Data  0.001 ( 0.003)	Loss 5.0035e-02 (1.1285e-01)	Acc@1  98.44 ( 95.93)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:46 - Epoch: [0][250/352]	Time  0.097 ( 0.104)	Data  0.002 ( 0.003)	Loss 2.1798e-01 (1.1270e-01)	Acc@1  90.62 ( 95.92)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:47 - Epoch: [0][260/352]	Time  0.097 ( 0.104)	Data  0.001 ( 0.003)	Loss 7.1866e-02 (1.1198e-01)	Acc@1  98.44 ( 95.97)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:48 - Epoch: [0][270/352]	Time  0.100 ( 0.104)	Data  0.002 ( 0.003)	Loss 4.5325e-02 (1.1209e-01)	Acc@1  97.66 ( 95.96)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:49 - Epoch: [0][280/352]	Time  0.100 ( 0.104)	Data  0.002 ( 0.002)	Loss 8.9781e-02 (1.1243e-01)	Acc@1  96.09 ( 95.96)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:50 - Epoch: [0][290/352]	Time  0.097 ( 0.104)	Data  0.002 ( 0.002)	Loss 1.3641e-01 (1.1194e-01)	Acc@1  92.19 ( 95.96)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:51 - Epoch: [0][300/352]	Time  0.094 ( 0.104)	Data  0.001 ( 0.002)	Loss 1.5616e-01 (1.1220e-01)	Acc@1  94.53 ( 95.95)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:52 - Epoch: [0][310/352]	Time  0.097 ( 0.104)	Data  0.001 ( 0.002)	Loss 1.3005e-01 (1.1210e-01)	Acc@1  94.53 ( 95.96)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:53 - Epoch: [0][320/352]	Time  0.096 ( 0.104)	Data  0.001 ( 0.002)	Loss 3.3182e-02 (1.1114e-01)	Acc@1  99.22 ( 96.00)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:54 - Epoch: [0][330/352]	Time  0.097 ( 0.104)	Data  0.001 ( 0.002)	Loss 1.9141e-01 (1.1168e-01)	Acc@1  92.19 ( 95.98)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:55 - Epoch: [0][340/352]	Time  0.096 ( 0.103)	Data  0.001 ( 0.002)	Loss 4.6548e-02 (1.1105e-01)	Acc@1  98.44 ( 96.01)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:56 - Epoch: [0][350/352]	Time  0.096 ( 0.103)	Data  0.001 ( 0.002)	Loss 5.1613e-02 (1.1129e-01)	Acc@1  98.44 ( 96.01)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:57 - Test: [ 0/20]	Time  0.416 ( 0.416)	Loss 3.7956e-01 (3.7956e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
03-Mar-22 10:43:58 - Test: [10/20]	Time  0.068 ( 0.104)	Loss 4.1133e-01 (3.6462e-01)	Acc@1  89.45 ( 90.52)	Acc@5  99.22 ( 99.57)
03-Mar-22 10:43:58 -  * Acc@1 90.580 Acc@5 99.560
03-Mar-22 10:43:58 - Best acc at epoch 0: 90.57999420166016
03-Mar-22 10:43:59 - Epoch: [1][  0/352]	Time  0.360 ( 0.360)	Data  0.229 ( 0.229)	Loss 1.1012e-01 (1.1012e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:00 - Epoch: [1][ 10/352]	Time  0.096 ( 0.123)	Data  0.001 ( 0.022)	Loss 6.5734e-02 (9.9683e-02)	Acc@1  97.66 ( 96.38)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:01 - Epoch: [1][ 20/352]	Time  0.096 ( 0.110)	Data  0.001 ( 0.012)	Loss 7.6517e-02 (9.0303e-02)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:02 - Epoch: [1][ 30/352]	Time  0.097 ( 0.110)	Data  0.001 ( 0.009)	Loss 3.4132e-02 (9.5411e-02)	Acc@1  99.22 ( 96.52)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:03 - Epoch: [1][ 40/352]	Time  0.098 ( 0.107)	Data  0.001 ( 0.007)	Loss 1.2667e-01 (9.8297e-02)	Acc@1  96.09 ( 96.46)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:04 - Epoch: [1][ 50/352]	Time  0.100 ( 0.107)	Data  0.002 ( 0.006)	Loss 5.1800e-02 (9.6819e-02)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:05 - Epoch: [1][ 60/352]	Time  0.097 ( 0.107)	Data  0.001 ( 0.005)	Loss 1.0185e-01 (9.9054e-02)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:06 - Epoch: [1][ 70/352]	Time  0.097 ( 0.106)	Data  0.001 ( 0.005)	Loss 9.0441e-02 (1.0038e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:07 - Epoch: [1][ 80/352]	Time  0.096 ( 0.105)	Data  0.001 ( 0.004)	Loss 1.0128e-01 (9.8681e-02)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:08 - Epoch: [1][ 90/352]	Time  0.097 ( 0.104)	Data  0.001 ( 0.004)	Loss 1.0746e-01 (9.8275e-02)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:09 - Epoch: [1][100/352]	Time  0.096 ( 0.103)	Data  0.001 ( 0.004)	Loss 5.0433e-02 (9.7333e-02)	Acc@1  99.22 ( 96.56)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:10 - Epoch: [1][110/352]	Time  0.097 ( 0.103)	Data  0.001 ( 0.004)	Loss 6.5939e-02 (9.6042e-02)	Acc@1  96.09 ( 96.60)	Acc@5 100.00 ( 99.99)
03-Mar-22 10:44:11 - Epoch: [1][120/352]	Time  0.096 ( 0.103)	Data  0.001 ( 0.003)	Loss 1.3300e-01 (9.7863e-02)	Acc@1  95.31 ( 96.55)	Acc@5  99.22 ( 99.99)
03-Mar-22 10:44:12 - Epoch: [1][130/352]	Time  0.097 ( 0.102)	Data  0.001 ( 0.003)	Loss 5.1890e-02 (9.7773e-02)	Acc@1  98.44 ( 96.57)	Acc@5 100.00 ( 99.99)
03-Mar-22 10:51:35 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:51:35 - Use GPU: 0 for training
03-Mar-22 10:51:35 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 10:51:39 - match all modules defined in bit_config: True
03-Mar-22 10:51:39 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:51:40 - Epoch: [0][  0/352]	Time  0.408 ( 0.408)	Data  0.226 ( 0.226)	Loss 1.1617e-01 (1.1617e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 10:51:41 - Epoch: [0][ 10/352]	Time  0.095 ( 0.131)	Data  0.001 ( 0.022)	Loss 1.0751e-01 (1.5330e-01)	Acc@1  95.31 ( 94.32)	Acc@5 100.00 (100.00)
03-Mar-22 10:51:54 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:51:54 - Use GPU: 0 for training
03-Mar-22 10:51:54 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:51:58 - match all modules defined in bit_config: False
03-Mar-22 10:51:58 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:03:39 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:03:39 - Use GPU: 0 for training
03-Mar-22 11:03:39 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:03:43 - match all modules defined in bit_config: False
03-Mar-22 11:05:14 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:05:14 - Use GPU: 0 for training
03-Mar-22 11:05:14 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:05:18 - match all modules defined in bit_config: False
03-Mar-22 11:05:18 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=None, groups=1, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:07:59 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:07:59 - Use GPU: 0 for training
03-Mar-22 11:07:59 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:08:03 - match all modules defined in bit_config: True
03-Mar-22 11:08:03 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:23:10 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:23:10 - Use GPU: 0 for training
03-Mar-22 11:23:10 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:23:14 - match all modules defined in bit_config: True
03-Mar-22 11:23:14 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:24:49 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:24:49 - Use GPU: 0 for training
03-Mar-22 11:24:49 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:24:54 - match all modules defined in bit_config: True
03-Mar-22 11:24:54 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:25:46 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:25:46 - Use GPU: 0 for training
03-Mar-22 11:25:46 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:25:50 - match all modules defined in bit_config: True
03-Mar-22 11:25:50 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:27:33 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:27:33 - Use GPU: 0 for training
03-Mar-22 11:27:33 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:27:37 - match all modules defined in bit_config: True
03-Mar-22 11:27:37 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:27:54 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:27:54 - Use GPU: 0 for training
03-Mar-22 11:27:54 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:27:58 - match all modules defined in bit_config: True
03-Mar-22 11:27:58 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:14:19 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:14:19 - Use GPU: 0 for training
03-Mar-22 13:14:19 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:14:23 - match all modules defined in bit_config: True
03-Mar-22 13:14:23 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:15:04 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:15:04 - Use GPU: 0 for training
03-Mar-22 13:15:04 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:15:08 - match all modules defined in bit_config: True
03-Mar-22 13:15:08 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:15:53 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:15:53 - Use GPU: 0 for training
03-Mar-22 13:15:53 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:15:57 - match all modules defined in bit_config: True
03-Mar-22 13:15:57 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:18:43 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:18:43 - Use GPU: 0 for training
03-Mar-22 13:18:43 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:18:47 - match all modules defined in bit_config: True
03-Mar-22 13:18:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:20:50 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:20:50 - Use GPU: 0 for training
03-Mar-22 13:20:50 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:20:54 - match all modules defined in bit_config: True
03-Mar-22 13:20:54 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:23:51 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:23:51 - Use GPU: 0 for training
03-Mar-22 13:23:51 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:23:55 - match all modules defined in bit_config: True
03-Mar-22 13:23:55 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:25:03 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:25:03 - Use GPU: 0 for training
03-Mar-22 13:25:03 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:25:07 - match all modules defined in bit_config: True
03-Mar-22 13:25:07 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:25:22 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:25:22 - Use GPU: 0 for training
03-Mar-22 13:25:22 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:25:26 - match all modules defined in bit_config: True
03-Mar-22 13:25:26 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:26:08 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:26:08 - Use GPU: 0 for training
03-Mar-22 13:26:08 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:26:12 - match all modules defined in bit_config: True
03-Mar-22 13:26:12 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:26:36 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:26:36 - Use GPU: 0 for training
03-Mar-22 13:26:36 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:26:41 - match all modules defined in bit_config: True
03-Mar-22 13:26:41 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:27:17 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:27:17 - Use GPU: 0 for training
03-Mar-22 13:27:17 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:27:21 - match all modules defined in bit_config: True
03-Mar-22 13:27:21 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:27:22 - Epoch: [0][  0/352]	Time  0.395 ( 0.395)	Data  0.223 ( 0.223)	Loss 2.3707e+00 (2.3707e+00)	Acc@1   5.47 (  5.47)	Acc@5  50.78 ( 50.78)
03-Mar-22 13:27:23 - Epoch: [0][ 10/352]	Time  0.102 ( 0.141)	Data  0.001 ( 0.022)	Loss 2.3235e+00 (2.3473e+00)	Acc@1  10.16 (  9.87)	Acc@5  53.91 ( 49.79)
03-Mar-22 13:27:24 - Epoch: [0][ 20/352]	Time  0.102 ( 0.123)	Data  0.002 ( 0.012)	Loss 2.3226e+00 (2.3478e+00)	Acc@1  14.84 ( 10.27)	Acc@5  51.56 ( 48.74)
03-Mar-22 13:27:25 - Epoch: [0][ 30/352]	Time  0.102 ( 0.123)	Data  0.002 ( 0.009)	Loss 2.3227e+00 (2.3505e+00)	Acc@1  13.28 ( 10.26)	Acc@5  52.34 ( 48.71)
03-Mar-22 13:27:26 - Epoch: [0][ 40/352]	Time  0.127 ( 0.121)	Data  0.002 ( 0.007)	Loss 2.3111e+00 (2.3472e+00)	Acc@1  11.72 ( 10.04)	Acc@5  57.03 ( 49.58)
03-Mar-22 13:27:28 - Epoch: [0][ 50/352]	Time  0.102 ( 0.119)	Data  0.002 ( 0.006)	Loss 2.3633e+00 (2.3476e+00)	Acc@1   4.69 ( 10.16)	Acc@5  49.22 ( 49.59)
03-Mar-22 13:27:29 - Epoch: [0][ 60/352]	Time  0.112 ( 0.117)	Data  0.002 ( 0.005)	Loss 2.3277e+00 (2.3458e+00)	Acc@1  10.94 ( 10.03)	Acc@5  49.22 ( 49.72)
03-Mar-22 13:27:30 - Epoch: [0][ 70/352]	Time  0.102 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.3262e+00 (2.3441e+00)	Acc@1  11.72 ( 10.09)	Acc@5  48.44 ( 49.89)
03-Mar-22 13:27:31 - Epoch: [0][ 80/352]	Time  0.132 ( 0.116)	Data  0.002 ( 0.005)	Loss 2.3191e+00 (2.3429e+00)	Acc@1  14.06 ( 10.20)	Acc@5  50.78 ( 49.87)
03-Mar-22 13:27:32 - Epoch: [0][ 90/352]	Time  0.127 ( 0.117)	Data  0.002 ( 0.004)	Loss 2.3357e+00 (2.3419e+00)	Acc@1  12.50 ( 10.22)	Acc@5  46.88 ( 49.99)
03-Mar-22 13:27:33 - Epoch: [0][100/352]	Time  0.102 ( 0.117)	Data  0.001 ( 0.004)	Loss 2.3543e+00 (2.3416e+00)	Acc@1  10.16 ( 10.09)	Acc@5  47.66 ( 49.98)
03-Mar-22 13:27:34 - Epoch: [0][110/352]	Time  0.104 ( 0.116)	Data  0.001 ( 0.004)	Loss 2.3537e+00 (2.3410e+00)	Acc@1  10.16 ( 10.14)	Acc@5  48.44 ( 50.08)
03-Mar-22 13:27:35 - Epoch: [0][120/352]	Time  0.101 ( 0.116)	Data  0.001 ( 0.004)	Loss 2.3709e+00 (2.3407e+00)	Acc@1  10.94 ( 10.12)	Acc@5  46.09 ( 50.15)
03-Mar-22 13:27:37 - Epoch: [0][130/352]	Time  0.130 ( 0.117)	Data  0.002 ( 0.004)	Loss 2.3600e+00 (2.3405e+00)	Acc@1   7.03 ( 10.08)	Acc@5  47.66 ( 50.16)
03-Mar-22 13:27:38 - Epoch: [0][140/352]	Time  0.102 ( 0.117)	Data  0.001 ( 0.003)	Loss 2.3343e+00 (2.3390e+00)	Acc@1  11.72 ( 10.17)	Acc@5  50.78 ( 50.27)
03-Mar-22 13:27:39 - Epoch: [0][150/352]	Time  0.109 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3481e+00 (2.3389e+00)	Acc@1   9.38 ( 10.14)	Acc@5  47.66 ( 50.24)
03-Mar-22 13:27:40 - Epoch: [0][160/352]	Time  0.101 ( 0.116)	Data  0.001 ( 0.003)	Loss 2.3410e+00 (2.3386e+00)	Acc@1   9.38 ( 10.09)	Acc@5  47.66 ( 50.21)
03-Mar-22 13:27:41 - Epoch: [0][170/352]	Time  0.124 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3263e+00 (2.3382e+00)	Acc@1   7.03 ( 10.02)	Acc@5  52.34 ( 50.25)
03-Mar-22 13:27:42 - Epoch: [0][180/352]	Time  0.100 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3456e+00 (2.3382e+00)	Acc@1   7.81 (  9.91)	Acc@5  51.56 ( 50.27)
03-Mar-22 13:27:44 - Epoch: [0][190/352]	Time  0.130 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3518e+00 (2.3382e+00)	Acc@1  11.72 (  9.95)	Acc@5  43.75 ( 50.19)
03-Mar-22 13:27:45 - Epoch: [0][200/352]	Time  0.129 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3316e+00 (2.3379e+00)	Acc@1  14.06 (  9.93)	Acc@5  52.34 ( 50.19)
03-Mar-22 13:27:46 - Epoch: [0][210/352]	Time  0.104 ( 0.116)	Data  0.001 ( 0.003)	Loss 2.3510e+00 (2.3374e+00)	Acc@1   6.25 (  9.90)	Acc@5  45.31 ( 50.20)
03-Mar-22 13:27:47 - Epoch: [0][220/352]	Time  0.128 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3171e+00 (2.3365e+00)	Acc@1   6.25 (  9.93)	Acc@5  53.91 ( 50.27)
03-Mar-22 13:27:48 - Epoch: [0][230/352]	Time  0.127 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3786e+00 (2.3363e+00)	Acc@1   2.34 (  9.90)	Acc@5  42.19 ( 50.18)
03-Mar-22 13:27:49 - Epoch: [0][240/352]	Time  0.100 ( 0.116)	Data  0.001 ( 0.003)	Loss 2.3132e+00 (2.3361e+00)	Acc@1   7.03 (  9.87)	Acc@5  52.34 ( 50.15)
03-Mar-22 13:27:50 - Epoch: [0][250/352]	Time  0.104 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3590e+00 (2.3360e+00)	Acc@1   7.03 (  9.86)	Acc@5  46.88 ( 50.14)
03-Mar-22 13:27:52 - Epoch: [0][260/352]	Time  0.104 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3174e+00 (2.3355e+00)	Acc@1  14.06 (  9.88)	Acc@5  52.34 ( 50.16)
03-Mar-22 13:27:53 - Epoch: [0][270/352]	Time  0.123 ( 0.115)	Data  0.002 ( 0.003)	Loss 2.3414e+00 (2.3352e+00)	Acc@1   8.59 (  9.86)	Acc@5  50.00 ( 50.17)
03-Mar-22 13:27:54 - Epoch: [0][280/352]	Time  0.105 ( 0.115)	Data  0.002 ( 0.003)	Loss 2.3237e+00 (2.3349e+00)	Acc@1  10.94 (  9.82)	Acc@5  50.78 ( 50.23)
03-Mar-22 13:27:55 - Epoch: [0][290/352]	Time  0.101 ( 0.115)	Data  0.001 ( 0.003)	Loss 2.3341e+00 (2.3344e+00)	Acc@1   8.59 (  9.87)	Acc@5  53.12 ( 50.25)
03-Mar-22 13:27:56 - Epoch: [0][300/352]	Time  0.104 ( 0.115)	Data  0.001 ( 0.003)	Loss 2.2942e+00 (2.3343e+00)	Acc@1  12.50 (  9.89)	Acc@5  57.03 ( 50.22)
03-Mar-22 13:27:57 - Epoch: [0][310/352]	Time  0.103 ( 0.115)	Data  0.002 ( 0.003)	Loss 2.3285e+00 (2.3339e+00)	Acc@1  16.41 (  9.93)	Acc@5  47.66 ( 50.22)
03-Mar-22 13:27:58 - Epoch: [0][320/352]	Time  0.104 ( 0.115)	Data  0.002 ( 0.003)	Loss 2.3121e+00 (2.3339e+00)	Acc@1  10.94 (  9.92)	Acc@5  50.78 ( 50.15)
03-Mar-22 13:28:00 - Epoch: [0][330/352]	Time  0.127 ( 0.115)	Data  0.002 ( 0.002)	Loss 2.3375e+00 (2.3334e+00)	Acc@1   8.59 (  9.94)	Acc@5  47.66 ( 50.19)
03-Mar-22 13:28:01 - Epoch: [0][340/352]	Time  0.104 ( 0.115)	Data  0.002 ( 0.002)	Loss 2.3282e+00 (2.3332e+00)	Acc@1  14.06 (  9.98)	Acc@5  48.44 ( 50.14)
03-Mar-22 13:28:02 - Epoch: [0][350/352]	Time  0.109 ( 0.115)	Data  0.002 ( 0.002)	Loss 2.2957e+00 (2.3331e+00)	Acc@1  14.84 (  9.96)	Acc@5  58.59 ( 50.11)
03-Mar-22 13:28:03 - Test: [ 0/20]	Time  0.360 ( 0.360)	Loss 2.3250e+00 (2.3250e+00)	Acc@1  10.16 ( 10.16)	Acc@5  50.78 ( 50.78)
03-Mar-22 13:28:03 - Test: [10/20]	Time  0.074 ( 0.100)	Loss 2.3363e+00 (2.3255e+00)	Acc@1   8.98 (  9.98)	Acc@5  46.48 ( 49.08)
03-Mar-22 13:28:04 -  * Acc@1 10.380 Acc@5 49.080
03-Mar-22 13:28:04 - Best acc at epoch 0: 10.380000114440918
03-Mar-22 13:28:05 - Epoch: [1][  0/352]	Time  0.335 ( 0.335)	Data  0.224 ( 0.224)	Loss 2.3433e+00 (2.3433e+00)	Acc@1  10.94 ( 10.94)	Acc@5  42.97 ( 42.97)
03-Mar-22 13:28:06 - Epoch: [1][ 10/352]	Time  0.100 ( 0.122)	Data  0.001 ( 0.022)	Loss 2.3487e+00 (2.3295e+00)	Acc@1   9.38 ( 10.16)	Acc@5  41.41 ( 48.08)
03-Mar-22 13:28:07 - Epoch: [1][ 20/352]	Time  0.101 ( 0.118)	Data  0.001 ( 0.012)	Loss 2.3151e+00 (2.3216e+00)	Acc@1   9.38 (  9.82)	Acc@5  51.56 ( 50.11)
03-Mar-22 13:28:08 - Epoch: [1][ 30/352]	Time  0.105 ( 0.113)	Data  0.002 ( 0.009)	Loss 2.3099e+00 (2.3217e+00)	Acc@1  11.72 ( 10.26)	Acc@5  53.12 ( 49.85)
03-Mar-22 13:28:09 - Epoch: [1][ 40/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.007)	Loss 2.3251e+00 (2.3225e+00)	Acc@1   5.47 ( 10.00)	Acc@5  48.44 ( 49.47)
03-Mar-22 13:28:10 - Epoch: [1][ 50/352]	Time  0.127 ( 0.112)	Data  0.002 ( 0.006)	Loss 2.2889e+00 (2.3223e+00)	Acc@1  11.72 (  9.74)	Acc@5  53.91 ( 49.69)
03-Mar-22 13:28:11 - Epoch: [1][ 60/352]	Time  0.101 ( 0.111)	Data  0.002 ( 0.005)	Loss 2.3354e+00 (2.3233e+00)	Acc@1  10.94 (  9.73)	Acc@5  46.88 ( 49.68)
03-Mar-22 13:28:12 - Epoch: [1][ 70/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.005)	Loss 2.3159e+00 (2.3231e+00)	Acc@1  10.16 (  9.69)	Acc@5  49.22 ( 49.70)
03-Mar-22 13:28:13 - Epoch: [1][ 80/352]	Time  0.104 ( 0.111)	Data  0.002 ( 0.004)	Loss 2.3309e+00 (2.3231e+00)	Acc@1  10.16 (  9.73)	Acc@5  46.88 ( 49.62)
03-Mar-22 13:28:14 - Epoch: [1][ 90/352]	Time  0.104 ( 0.110)	Data  0.002 ( 0.004)	Loss 2.3225e+00 (2.3224e+00)	Acc@1  13.28 (  9.86)	Acc@5  50.00 ( 49.73)
03-Mar-22 13:28:16 - Epoch: [1][100/352]	Time  0.134 ( 0.111)	Data  0.002 ( 0.004)	Loss 2.3198e+00 (2.3220e+00)	Acc@1  10.94 (  9.91)	Acc@5  50.78 ( 49.83)
03-Mar-22 13:28:17 - Epoch: [1][110/352]	Time  0.101 ( 0.111)	Data  0.001 ( 0.004)	Loss 2.3364e+00 (2.3215e+00)	Acc@1   8.59 (  9.91)	Acc@5  46.09 ( 49.96)
03-Mar-22 13:28:18 - Epoch: [1][120/352]	Time  0.124 ( 0.112)	Data  0.002 ( 0.004)	Loss 2.3011e+00 (2.3215e+00)	Acc@1  13.28 (  9.91)	Acc@5  54.69 ( 49.91)
03-Mar-22 13:28:19 - Epoch: [1][130/352]	Time  0.101 ( 0.111)	Data  0.001 ( 0.003)	Loss 2.3121e+00 (2.3213e+00)	Acc@1   9.38 (  9.86)	Acc@5  51.56 ( 49.93)
03-Mar-22 13:28:20 - Epoch: [1][140/352]	Time  0.101 ( 0.111)	Data  0.001 ( 0.003)	Loss 2.3233e+00 (2.3217e+00)	Acc@1  14.06 (  9.88)	Acc@5  44.53 ( 49.73)
03-Mar-22 13:28:21 - Epoch: [1][150/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.2967e+00 (2.3211e+00)	Acc@1  14.84 (  9.85)	Acc@5  49.22 ( 49.82)
03-Mar-22 13:28:22 - Epoch: [1][160/352]	Time  0.124 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.2841e+00 (2.3204e+00)	Acc@1  15.62 (  9.90)	Acc@5  57.81 ( 49.98)
03-Mar-22 13:28:23 - Epoch: [1][170/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3086e+00 (2.3206e+00)	Acc@1  12.50 (  9.90)	Acc@5  47.66 ( 49.86)
03-Mar-22 13:28:24 - Epoch: [1][180/352]	Time  0.104 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3270e+00 (2.3208e+00)	Acc@1   5.47 (  9.91)	Acc@5  46.09 ( 49.83)
03-Mar-22 13:28:25 - Epoch: [1][190/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3303e+00 (2.3206e+00)	Acc@1   6.25 (  9.77)	Acc@5  47.66 ( 49.85)
03-Mar-22 13:28:26 - Epoch: [1][200/352]	Time  0.120 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3357e+00 (2.3205e+00)	Acc@1  10.16 (  9.78)	Acc@5  46.09 ( 49.86)
03-Mar-22 13:28:27 - Epoch: [1][210/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3356e+00 (2.3202e+00)	Acc@1   9.38 (  9.78)	Acc@5  42.19 ( 49.82)
03-Mar-22 13:28:29 - Epoch: [1][220/352]	Time  0.124 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3154e+00 (2.3198e+00)	Acc@1  10.94 (  9.83)	Acc@5  49.22 ( 49.89)
03-Mar-22 13:28:30 - Epoch: [1][230/352]	Time  0.124 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3050e+00 (2.3197e+00)	Acc@1  10.16 (  9.82)	Acc@5  51.56 ( 49.87)
03-Mar-22 13:28:31 - Epoch: [1][240/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3281e+00 (2.3196e+00)	Acc@1  10.94 (  9.81)	Acc@5  41.41 ( 49.85)
03-Mar-22 13:28:32 - Epoch: [1][250/352]	Time  0.106 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.3030e+00 (2.3192e+00)	Acc@1  14.06 (  9.84)	Acc@5  51.56 ( 49.88)
03-Mar-22 13:28:33 - Epoch: [1][260/352]	Time  0.105 ( 0.109)	Data  0.002 ( 0.002)	Loss 2.2995e+00 (2.3193e+00)	Acc@1  10.16 (  9.77)	Acc@5  56.25 ( 49.85)
03-Mar-22 13:28:34 - Epoch: [1][270/352]	Time  0.100 ( 0.109)	Data  0.001 ( 0.002)	Loss 2.3398e+00 (2.3191e+00)	Acc@1  10.16 (  9.79)	Acc@5  40.62 ( 49.85)
03-Mar-22 13:28:35 - Epoch: [1][280/352]	Time  0.127 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3055e+00 (2.3189e+00)	Acc@1  12.50 (  9.83)	Acc@5  50.78 ( 49.89)
03-Mar-22 13:28:36 - Epoch: [1][290/352]	Time  0.128 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3016e+00 (2.3185e+00)	Acc@1  11.72 (  9.86)	Acc@5  53.12 ( 49.94)
03-Mar-22 13:28:37 - Epoch: [1][300/352]	Time  0.105 ( 0.110)	Data  0.001 ( 0.002)	Loss 2.2939e+00 (2.3182e+00)	Acc@1  10.94 (  9.88)	Acc@5  58.59 ( 50.01)
03-Mar-22 13:28:38 - Epoch: [1][310/352]	Time  0.124 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3059e+00 (2.3179e+00)	Acc@1   8.59 (  9.88)	Acc@5  53.12 ( 50.06)
03-Mar-22 13:28:40 - Epoch: [1][320/352]	Time  0.123 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3207e+00 (2.3177e+00)	Acc@1  12.50 (  9.90)	Acc@5  46.09 ( 50.07)
03-Mar-22 13:28:41 - Epoch: [1][330/352]	Time  0.103 ( 0.110)	Data  0.001 ( 0.002)	Loss 2.3355e+00 (2.3175e+00)	Acc@1  10.16 (  9.92)	Acc@5  42.97 ( 50.09)
03-Mar-22 13:28:42 - Epoch: [1][340/352]	Time  0.106 ( 0.110)	Data  0.001 ( 0.002)	Loss 2.3094e+00 (2.3173e+00)	Acc@1   7.81 (  9.95)	Acc@5  50.00 ( 50.12)
03-Mar-22 13:28:43 - Epoch: [1][350/352]	Time  0.124 ( 0.110)	Data  0.001 ( 0.002)	Loss 2.3120e+00 (2.3171e+00)	Acc@1   7.03 (  9.96)	Acc@5  48.44 ( 50.12)
03-Mar-22 13:28:44 - Test: [ 0/20]	Time  0.350 ( 0.350)	Loss 2.3138e+00 (2.3138e+00)	Acc@1  10.16 ( 10.16)	Acc@5  50.78 ( 50.78)
03-Mar-22 13:28:44 - Test: [10/20]	Time  0.076 ( 0.103)	Loss 2.3216e+00 (2.3140e+00)	Acc@1   8.98 (  9.98)	Acc@5  46.48 ( 49.08)
03-Mar-22 13:28:45 -  * Acc@1 10.380 Acc@5 49.080
03-Mar-22 13:28:45 - Best acc at epoch 1: 10.380000114440918
03-Mar-22 13:28:46 - Epoch: [2][  0/352]	Time  0.369 ( 0.369)	Data  0.244 ( 0.244)	Loss 2.3204e+00 (2.3204e+00)	Acc@1   7.81 (  7.81)	Acc@5  46.09 ( 46.09)
03-Mar-22 13:28:47 - Epoch: [2][ 10/352]	Time  0.124 ( 0.136)	Data  0.001 ( 0.024)	Loss 2.3131e+00 (2.3099e+00)	Acc@1   8.59 ( 10.65)	Acc@5  50.78 ( 50.00)
03-Mar-22 13:28:48 - Epoch: [2][ 20/352]	Time  0.100 ( 0.119)	Data  0.001 ( 0.013)	Loss 2.3098e+00 (2.3083e+00)	Acc@1   8.59 ( 10.60)	Acc@5  50.00 ( 50.93)
03-Mar-22 13:28:49 - Epoch: [2][ 30/352]	Time  0.109 ( 0.114)	Data  0.002 ( 0.009)	Loss 2.3017e+00 (2.3088e+00)	Acc@1  10.94 ( 10.38)	Acc@5  51.56 ( 50.68)
03-Mar-22 13:28:50 - Epoch: [2][ 40/352]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.3115e+00 (2.3092e+00)	Acc@1  12.50 ( 10.08)	Acc@5  51.56 ( 50.57)
03-Mar-22 13:28:51 - Epoch: [2][ 50/352]	Time  0.151 ( 0.111)	Data  0.002 ( 0.006)	Loss 2.3316e+00 (2.3107e+00)	Acc@1   7.03 (  9.79)	Acc@5  46.09 ( 50.15)
03-Mar-22 13:28:52 - Epoch: [2][ 60/352]	Time  0.100 ( 0.112)	Data  0.001 ( 0.006)	Loss 2.3223e+00 (2.3119e+00)	Acc@1   8.59 (  9.73)	Acc@5  44.53 ( 49.83)
03-Mar-22 13:28:53 - Epoch: [2][ 70/352]	Time  0.100 ( 0.111)	Data  0.002 ( 0.005)	Loss 2.3126e+00 (2.3114e+00)	Acc@1  10.16 (  9.79)	Acc@5  50.00 ( 49.94)
03-Mar-22 13:28:54 - Epoch: [2][ 80/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.005)	Loss 2.3114e+00 (2.3120e+00)	Acc@1   5.47 (  9.76)	Acc@5  50.00 ( 49.81)
03-Mar-22 13:28:55 - Epoch: [2][ 90/352]	Time  0.100 ( 0.109)	Data  0.001 ( 0.004)	Loss 2.3012e+00 (2.3118e+00)	Acc@1   8.59 (  9.78)	Acc@5  53.91 ( 49.93)
03-Mar-22 13:28:56 - Epoch: [2][100/352]	Time  0.101 ( 0.108)	Data  0.001 ( 0.004)	Loss 2.3012e+00 (2.3111e+00)	Acc@1  12.50 (  9.92)	Acc@5  56.25 ( 50.25)
03-Mar-22 13:28:57 - Epoch: [2][110/352]	Time  0.103 ( 0.109)	Data  0.002 ( 0.004)	Loss 2.3319e+00 (2.3107e+00)	Acc@1   7.81 (  9.99)	Acc@5  42.19 ( 50.39)
03-Mar-22 13:28:58 - Epoch: [2][120/352]	Time  0.100 ( 0.108)	Data  0.001 ( 0.004)	Loss 2.3158e+00 (2.3111e+00)	Acc@1   9.38 (  9.93)	Acc@5  47.66 ( 50.21)
03-Mar-22 13:28:59 - Epoch: [2][130/352]	Time  0.100 ( 0.108)	Data  0.001 ( 0.003)	Loss 2.2890e+00 (2.3109e+00)	Acc@1   9.38 (  9.94)	Acc@5  57.81 ( 50.26)
03-Mar-22 13:29:00 - Epoch: [2][140/352]	Time  0.124 ( 0.108)	Data  0.002 ( 0.003)	Loss 2.3121e+00 (2.3109e+00)	Acc@1  10.94 (  9.98)	Acc@5  43.75 ( 50.14)
03-Mar-22 13:29:02 - Epoch: [2][150/352]	Time  0.104 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.3107e+00 (2.3110e+00)	Acc@1   7.03 (  9.91)	Acc@5  52.34 ( 50.11)
03-Mar-22 13:29:03 - Epoch: [2][160/352]	Time  0.105 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.2912e+00 (2.3107e+00)	Acc@1  13.28 (  9.94)	Acc@5  57.03 ( 50.21)
03-Mar-22 13:29:04 - Epoch: [2][170/352]	Time  0.124 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.2974e+00 (2.3108e+00)	Acc@1   8.59 (  9.84)	Acc@5  56.25 ( 50.15)
03-Mar-22 13:29:05 - Epoch: [2][180/352]	Time  0.124 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3173e+00 (2.3105e+00)	Acc@1   7.03 (  9.87)	Acc@5  50.00 ( 50.22)
03-Mar-22 13:29:06 - Epoch: [2][190/352]	Time  0.123 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3103e+00 (2.3106e+00)	Acc@1   3.91 (  9.85)	Acc@5  53.12 ( 50.17)
03-Mar-22 13:29:07 - Epoch: [2][200/352]	Time  0.104 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3182e+00 (2.3108e+00)	Acc@1   4.69 (  9.79)	Acc@5  47.66 ( 50.02)
03-Mar-22 13:29:08 - Epoch: [2][210/352]	Time  0.105 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3160e+00 (2.3106e+00)	Acc@1   9.38 (  9.82)	Acc@5  49.22 ( 50.04)
03-Mar-22 13:29:10 - Epoch: [2][220/352]	Time  0.105 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3158e+00 (2.3105e+00)	Acc@1   9.38 (  9.79)	Acc@5  49.22 ( 50.07)
03-Mar-22 13:29:11 - Epoch: [2][230/352]	Time  0.128 ( 0.111)	Data  0.002 ( 0.003)	Loss 2.3336e+00 (2.3104e+00)	Acc@1   7.03 (  9.78)	Acc@5  38.28 ( 50.03)
03-Mar-22 13:29:12 - Epoch: [2][240/352]	Time  0.104 ( 0.111)	Data  0.001 ( 0.003)	Loss 2.3255e+00 (2.3104e+00)	Acc@1   7.81 (  9.81)	Acc@5  39.84 ( 49.99)
03-Mar-22 13:29:13 - Epoch: [2][250/352]	Time  0.123 ( 0.111)	Data  0.002 ( 0.003)	Loss 2.3069e+00 (2.3105e+00)	Acc@1   9.38 (  9.77)	Acc@5  52.34 ( 49.96)
03-Mar-22 13:29:14 - Epoch: [2][260/352]	Time  0.105 ( 0.112)	Data  0.002 ( 0.003)	Loss 2.3171e+00 (2.3106e+00)	Acc@1   7.03 (  9.76)	Acc@5  46.09 ( 49.85)
03-Mar-22 13:29:16 - Epoch: [2][270/352]	Time  0.105 ( 0.112)	Data  0.002 ( 0.003)	Loss 2.2888e+00 (2.3103e+00)	Acc@1  16.41 (  9.83)	Acc@5  54.69 ( 49.95)
03-Mar-22 13:29:17 - Epoch: [2][280/352]	Time  0.124 ( 0.112)	Data  0.002 ( 0.003)	Loss 2.2977e+00 (2.3101e+00)	Acc@1  11.72 (  9.85)	Acc@5  53.91 ( 50.00)
03-Mar-22 13:29:18 - Epoch: [2][290/352]	Time  0.124 ( 0.112)	Data  0.002 ( 0.003)	Loss 2.3020e+00 (2.3100e+00)	Acc@1  12.50 (  9.84)	Acc@5  49.22 ( 50.01)
03-Mar-22 13:29:19 - Epoch: [2][300/352]	Time  0.105 ( 0.112)	Data  0.002 ( 0.003)	Loss 2.2949e+00 (2.3098e+00)	Acc@1   7.81 (  9.83)	Acc@5  55.47 ( 50.08)
03-Mar-22 13:29:20 - Epoch: [2][310/352]	Time  0.105 ( 0.112)	Data  0.002 ( 0.002)	Loss 2.3012e+00 (2.3098e+00)	Acc@1   9.38 (  9.84)	Acc@5  57.03 ( 50.11)
03-Mar-22 13:29:21 - Epoch: [2][320/352]	Time  0.106 ( 0.112)	Data  0.002 ( 0.002)	Loss 2.3228e+00 (2.3097e+00)	Acc@1  10.16 (  9.87)	Acc@5  46.88 ( 50.13)
03-Mar-22 13:29:22 - Epoch: [2][330/352]	Time  0.104 ( 0.112)	Data  0.002 ( 0.002)	Loss 2.3176e+00 (2.3097e+00)	Acc@1  12.50 (  9.89)	Acc@5  46.09 ( 50.09)
03-Mar-22 13:29:24 - Epoch: [2][340/352]	Time  0.123 ( 0.112)	Data  0.002 ( 0.002)	Loss 2.3132e+00 (2.3096e+00)	Acc@1   8.59 (  9.92)	Acc@5  46.09 ( 50.04)
03-Mar-22 13:29:25 - Epoch: [2][350/352]	Time  0.100 ( 0.112)	Data  0.001 ( 0.002)	Loss 2.3029e+00 (2.3094e+00)	Acc@1   7.03 (  9.96)	Acc@5  51.56 ( 50.11)
03-Mar-22 13:29:25 - Test: [ 0/20]	Time  0.331 ( 0.331)	Loss 2.3081e+00 (2.3081e+00)	Acc@1  10.16 ( 10.16)	Acc@5  50.78 ( 50.78)
03-Mar-22 13:29:26 - Test: [10/20]	Time  0.073 ( 0.097)	Loss 2.3136e+00 (2.3082e+00)	Acc@1   8.98 (  9.98)	Acc@5  46.48 ( 49.08)
03-Mar-22 13:29:27 -  * Acc@1 10.380 Acc@5 49.080
03-Mar-22 13:29:27 - Best acc at epoch 2: 10.380000114440918
03-Mar-22 13:29:27 - Epoch: [3][  0/352]	Time  0.367 ( 0.367)	Data  0.245 ( 0.245)	Loss 2.3198e+00 (2.3198e+00)	Acc@1   7.03 (  7.03)	Acc@5  42.19 ( 42.19)
03-Mar-22 13:29:28 - Epoch: [3][ 10/352]	Time  0.101 ( 0.136)	Data  0.001 ( 0.024)	Loss 2.3098e+00 (2.3072e+00)	Acc@1  14.06 ( 10.01)	Acc@5  46.09 ( 50.43)
03-Mar-22 13:29:29 - Epoch: [3][ 20/352]	Time  0.101 ( 0.121)	Data  0.001 ( 0.013)	Loss 2.3093e+00 (2.3084e+00)	Acc@1   7.81 (  9.04)	Acc@5  48.44 ( 49.26)
03-Mar-22 13:29:30 - Epoch: [3][ 30/352]	Time  0.102 ( 0.121)	Data  0.002 ( 0.010)	Loss 2.2903e+00 (2.3074e+00)	Acc@1   9.38 (  9.10)	Acc@5  59.38 ( 49.72)
03-Mar-22 13:29:32 - Epoch: [3][ 40/352]	Time  0.119 ( 0.119)	Data  0.002 ( 0.008)	Loss 2.3086e+00 (2.3060e+00)	Acc@1   9.38 (  9.53)	Acc@5  50.00 ( 50.46)
03-Mar-22 13:29:33 - Epoch: [3][ 50/352]	Time  0.120 ( 0.116)	Data  0.002 ( 0.006)	Loss 2.3176e+00 (2.3067e+00)	Acc@1  10.16 (  9.36)	Acc@5  45.31 ( 50.20)
03-Mar-22 13:29:34 - Epoch: [3][ 60/352]	Time  0.101 ( 0.114)	Data  0.001 ( 0.006)	Loss 2.3067e+00 (2.3057e+00)	Acc@1  12.50 (  9.80)	Acc@5  50.78 ( 50.60)
03-Mar-22 13:29:35 - Epoch: [3][ 70/352]	Time  0.101 ( 0.112)	Data  0.001 ( 0.005)	Loss 2.3047e+00 (2.3064e+00)	Acc@1  11.72 (  9.66)	Acc@5  52.34 ( 50.25)
03-Mar-22 13:29:36 - Epoch: [3][ 80/352]	Time  0.101 ( 0.111)	Data  0.001 ( 0.005)	Loss 2.3159e+00 (2.3068e+00)	Acc@1  10.94 (  9.64)	Acc@5  45.31 ( 49.86)
03-Mar-22 13:29:37 - Epoch: [3][ 90/352]	Time  0.101 ( 0.110)	Data  0.001 ( 0.004)	Loss 2.3148e+00 (2.3068e+00)	Acc@1   8.59 (  9.65)	Acc@5  44.53 ( 49.97)
03-Mar-22 13:29:38 - Epoch: [3][100/352]	Time  0.103 ( 0.110)	Data  0.002 ( 0.004)	Loss 2.3092e+00 (2.3071e+00)	Acc@1   9.38 (  9.50)	Acc@5  48.44 ( 49.65)
03-Mar-22 13:29:39 - Epoch: [3][110/352]	Time  0.119 ( 0.110)	Data  0.002 ( 0.004)	Loss 2.3107e+00 (2.3073e+00)	Acc@1  12.50 (  9.54)	Acc@5  46.09 ( 49.65)
03-Mar-22 13:29:40 - Epoch: [3][120/352]	Time  0.122 ( 0.110)	Data  0.002 ( 0.004)	Loss 2.2961e+00 (2.3076e+00)	Acc@1   9.38 (  9.56)	Acc@5  57.81 ( 49.54)
03-Mar-22 13:29:41 - Epoch: [3][130/352]	Time  0.102 ( 0.109)	Data  0.001 ( 0.003)	Loss 2.3084e+00 (2.3074e+00)	Acc@1   7.81 (  9.55)	Acc@5  48.44 ( 49.65)
03-Mar-22 13:29:42 - Epoch: [3][140/352]	Time  0.101 ( 0.109)	Data  0.001 ( 0.003)	Loss 2.3080e+00 (2.3075e+00)	Acc@1  11.72 (  9.60)	Acc@5  50.00 ( 49.61)
03-Mar-22 13:29:43 - Epoch: [3][150/352]	Time  0.101 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.2967e+00 (2.3074e+00)	Acc@1  13.28 (  9.58)	Acc@5  55.47 ( 49.65)
03-Mar-22 13:29:44 - Epoch: [3][160/352]	Time  0.112 ( 0.108)	Data  0.002 ( 0.003)	Loss 2.3074e+00 (2.3073e+00)	Acc@1   9.38 (  9.56)	Acc@5  50.00 ( 49.58)
03-Mar-22 13:29:45 - Epoch: [3][170/352]	Time  0.122 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.3125e+00 (2.3070e+00)	Acc@1   7.81 (  9.57)	Acc@5  48.44 ( 49.74)
03-Mar-22 13:29:47 - Epoch: [3][180/352]	Time  0.104 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3140e+00 (2.3070e+00)	Acc@1   4.69 (  9.51)	Acc@5  46.88 ( 49.73)
03-Mar-22 13:29:48 - Epoch: [3][190/352]	Time  0.104 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3097e+00 (2.3071e+00)	Acc@1   7.81 (  9.58)	Acc@5  48.44 ( 49.63)
03-Mar-22 13:29:49 - Epoch: [3][200/352]	Time  0.101 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3034e+00 (2.3069e+00)	Acc@1  10.94 (  9.64)	Acc@5  48.44 ( 49.63)
03-Mar-22 13:29:50 - Epoch: [3][210/352]	Time  0.101 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.2973e+00 (2.3070e+00)	Acc@1   8.59 (  9.67)	Acc@5  59.38 ( 49.59)
03-Mar-22 13:29:51 - Epoch: [3][220/352]	Time  0.103 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.2955e+00 (2.3069e+00)	Acc@1  17.19 (  9.72)	Acc@5  57.03 ( 49.71)
03-Mar-22 13:29:52 - Epoch: [3][230/352]	Time  0.101 ( 0.109)	Data  0.001 ( 0.003)	Loss 2.3055e+00 (2.3068e+00)	Acc@1   6.25 (  9.71)	Acc@5  53.12 ( 49.73)
03-Mar-22 13:29:53 - Epoch: [3][240/352]	Time  0.123 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.3139e+00 (2.3065e+00)	Acc@1   7.81 (  9.76)	Acc@5  42.97 ( 49.85)
03-Mar-22 13:29:54 - Epoch: [3][250/352]	Time  0.101 ( 0.109)	Data  0.001 ( 0.003)	Loss 2.2938e+00 (2.3064e+00)	Acc@1  13.28 (  9.87)	Acc@5  54.69 ( 49.89)
03-Mar-22 13:29:55 - Epoch: [3][260/352]	Time  0.102 ( 0.109)	Data  0.001 ( 0.003)	Loss 2.2977e+00 (2.3063e+00)	Acc@1   9.38 (  9.92)	Acc@5  53.12 ( 49.91)
03-Mar-22 13:29:56 - Epoch: [3][270/352]	Time  0.124 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.2982e+00 (2.3062e+00)	Acc@1  12.50 (  9.92)	Acc@5  51.56 ( 49.90)
03-Mar-22 13:29:57 - Epoch: [3][280/352]	Time  0.128 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.3047e+00 (2.3061e+00)	Acc@1   7.03 (  9.92)	Acc@5  51.56 ( 49.98)
03-Mar-22 13:29:59 - Epoch: [3][290/352]	Time  0.107 ( 0.110)	Data  0.001 ( 0.002)	Loss 2.3020e+00 (2.3059e+00)	Acc@1  12.50 (  9.96)	Acc@5  50.78 ( 49.99)
03-Mar-22 13:30:00 - Epoch: [3][300/352]	Time  0.123 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3123e+00 (2.3059e+00)	Acc@1   7.81 (  9.94)	Acc@5  49.22 ( 50.01)
03-Mar-22 13:30:01 - Epoch: [3][310/352]	Time  0.101 ( 0.110)	Data  0.001 ( 0.002)	Loss 2.3133e+00 (2.3060e+00)	Acc@1   9.38 (  9.94)	Acc@5  47.66 ( 49.95)
03-Mar-22 13:30:02 - Epoch: [3][320/352]	Time  0.129 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3017e+00 (2.3058e+00)	Acc@1   6.25 (  9.93)	Acc@5  52.34 ( 50.02)
03-Mar-22 13:30:03 - Epoch: [3][330/352]	Time  0.129 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3023e+00 (2.3058e+00)	Acc@1  10.94 (  9.95)	Acc@5  52.34 ( 50.07)
03-Mar-22 13:30:04 - Epoch: [3][340/352]	Time  0.122 ( 0.111)	Data  0.002 ( 0.002)	Loss 2.3012e+00 (2.3058e+00)	Acc@1  13.28 (  9.97)	Acc@5  46.88 ( 50.06)
03-Mar-22 13:30:05 - Epoch: [3][350/352]	Time  0.104 ( 0.111)	Data  0.001 ( 0.002)	Loss 2.2938e+00 (2.3057e+00)	Acc@1  10.16 (  9.99)	Acc@5  58.59 ( 50.09)
03-Mar-22 13:30:06 - Test: [ 0/20]	Time  0.338 ( 0.338)	Loss 2.3052e+00 (2.3052e+00)	Acc@1  10.16 ( 10.16)	Acc@5  50.78 ( 50.78)
03-Mar-22 13:30:07 - Test: [10/20]	Time  0.076 ( 0.100)	Loss 2.3092e+00 (2.3053e+00)	Acc@1   8.98 ( 10.09)	Acc@5  46.48 ( 49.08)
03-Mar-22 13:30:08 -  * Acc@1 10.420 Acc@5 49.080
03-Mar-22 13:30:08 - Best acc at epoch 3: 10.420000076293945
03-Mar-22 13:30:08 - Epoch: [4][  0/352]	Time  0.336 ( 0.336)	Data  0.233 ( 0.233)	Loss 2.3134e+00 (2.3134e+00)	Acc@1   9.38 (  9.38)	Acc@5  42.19 ( 42.19)
03-Mar-22 13:30:09 - Epoch: [4][ 10/352]	Time  0.105 ( 0.134)	Data  0.002 ( 0.023)	Loss 2.3054e+00 (2.3067e+00)	Acc@1   7.81 (  8.59)	Acc@5  50.00 ( 47.66)
03-Mar-22 13:30:10 - Epoch: [4][ 20/352]	Time  0.126 ( 0.125)	Data  0.002 ( 0.013)	Loss 2.2986e+00 (2.3049e+00)	Acc@1   8.59 (  8.93)	Acc@5  57.81 ( 49.93)
03-Mar-22 13:30:11 - Epoch: [4][ 30/352]	Time  0.121 ( 0.123)	Data  0.002 ( 0.009)	Loss 2.3090e+00 (2.3043e+00)	Acc@1   7.03 (  9.35)	Acc@5  46.88 ( 50.15)
03-Mar-22 13:30:13 - Epoch: [4][ 40/352]	Time  0.124 ( 0.121)	Data  0.001 ( 0.007)	Loss 2.3090e+00 (2.3040e+00)	Acc@1   5.47 (  9.13)	Acc@5  48.44 ( 50.51)
03-Mar-22 13:30:14 - Epoch: [4][ 50/352]	Time  0.105 ( 0.120)	Data  0.002 ( 0.006)	Loss 2.3060e+00 (2.3051e+00)	Acc@1  10.16 (  9.38)	Acc@5  49.22 ( 49.60)
03-Mar-22 13:30:15 - Epoch: [4][ 60/352]	Time  0.128 ( 0.119)	Data  0.002 ( 0.006)	Loss 2.3138e+00 (2.3055e+00)	Acc@1  10.94 (  9.63)	Acc@5  42.97 ( 49.31)
03-Mar-22 13:30:16 - Epoch: [4][ 70/352]	Time  0.123 ( 0.119)	Data  0.002 ( 0.005)	Loss 2.3047e+00 (2.3056e+00)	Acc@1   8.59 (  9.47)	Acc@5  51.56 ( 49.27)
03-Mar-22 13:30:17 - Epoch: [4][ 80/352]	Time  0.127 ( 0.119)	Data  0.002 ( 0.005)	Loss 2.3079e+00 (2.3055e+00)	Acc@1  12.50 (  9.63)	Acc@5  48.44 ( 49.30)
03-Mar-22 13:30:18 - Epoch: [4][ 90/352]	Time  0.100 ( 0.117)	Data  0.001 ( 0.004)	Loss 2.3033e+00 (2.3053e+00)	Acc@1  10.94 (  9.61)	Acc@5  50.00 ( 49.45)
03-Mar-22 13:30:19 - Epoch: [4][100/352]	Time  0.125 ( 0.117)	Data  0.002 ( 0.004)	Loss 2.3008e+00 (2.3049e+00)	Acc@1   7.81 (  9.82)	Acc@5  54.69 ( 49.63)
03-Mar-22 13:30:21 - Epoch: [4][110/352]	Time  0.105 ( 0.117)	Data  0.002 ( 0.004)	Loss 2.3050e+00 (2.3049e+00)	Acc@1   9.38 (  9.87)	Acc@5  46.88 ( 49.66)
03-Mar-22 13:30:22 - Epoch: [4][120/352]	Time  0.100 ( 0.117)	Data  0.001 ( 0.004)	Loss 2.2973e+00 (2.3046e+00)	Acc@1  12.50 (  9.95)	Acc@5  54.69 ( 49.85)
03-Mar-22 13:30:23 - Epoch: [4][130/352]	Time  0.100 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3146e+00 (2.3047e+00)	Acc@1   7.81 ( 10.03)	Acc@5  42.19 ( 49.82)
03-Mar-22 13:30:24 - Epoch: [4][140/352]	Time  0.104 ( 0.114)	Data  0.001 ( 0.003)	Loss 2.3058e+00 (2.3046e+00)	Acc@1   7.81 (  9.88)	Acc@5  53.12 ( 49.96)
03-Mar-22 13:30:25 - Epoch: [4][150/352]	Time  0.116 ( 0.114)	Data  0.001 ( 0.003)	Loss 2.3010e+00 (2.3046e+00)	Acc@1  10.16 (  9.90)	Acc@5  47.66 ( 49.82)
03-Mar-22 13:30:26 - Epoch: [4][160/352]	Time  0.100 ( 0.114)	Data  0.001 ( 0.003)	Loss 2.3048e+00 (2.3043e+00)	Acc@1  12.50 ( 10.06)	Acc@5  49.22 ( 50.00)
03-Mar-22 13:30:27 - Epoch: [4][170/352]	Time  0.112 ( 0.113)	Data  0.001 ( 0.003)	Loss 2.3046e+00 (2.3045e+00)	Acc@1   7.81 (  9.91)	Acc@5  52.34 ( 49.85)
03-Mar-22 13:30:28 - Epoch: [4][180/352]	Time  0.116 ( 0.113)	Data  0.002 ( 0.003)	Loss 2.2976e+00 (2.3043e+00)	Acc@1   8.59 (  9.96)	Acc@5  49.22 ( 49.96)
03-Mar-22 13:30:29 - Epoch: [4][190/352]	Time  0.122 ( 0.112)	Data  0.002 ( 0.003)	Loss 2.2990e+00 (2.3043e+00)	Acc@1   9.38 (  9.94)	Acc@5  54.69 ( 49.95)
03-Mar-22 13:30:30 - Epoch: [4][200/352]	Time  0.105 ( 0.112)	Data  0.001 ( 0.003)	Loss 2.2987e+00 (2.3041e+00)	Acc@1  10.16 ( 10.02)	Acc@5  53.12 ( 50.08)
03-Mar-22 13:30:31 - Epoch: [4][210/352]	Time  0.105 ( 0.112)	Data  0.001 ( 0.003)	Loss 2.3026e+00 (2.3041e+00)	Acc@1  10.94 ( 10.02)	Acc@5  53.12 ( 50.13)
03-Mar-22 13:30:33 - Epoch: [4][220/352]	Time  0.128 ( 0.113)	Data  0.002 ( 0.003)	Loss 2.2990e+00 (2.3041e+00)	Acc@1  13.28 ( 10.11)	Acc@5  54.69 ( 50.09)
03-Mar-22 13:30:34 - Epoch: [4][230/352]	Time  0.104 ( 0.113)	Data  0.002 ( 0.003)	Loss 2.3010e+00 (2.3040e+00)	Acc@1  11.72 ( 10.16)	Acc@5  51.56 ( 50.15)
03-Mar-22 13:30:35 - Epoch: [4][240/352]	Time  0.103 ( 0.113)	Data  0.001 ( 0.003)	Loss 2.2984e+00 (2.3039e+00)	Acc@1  13.28 ( 10.19)	Acc@5  53.91 ( 50.20)
03-Mar-22 13:30:36 - Epoch: [4][250/352]	Time  0.104 ( 0.113)	Data  0.002 ( 0.003)	Loss 2.3041e+00 (2.3039e+00)	Acc@1   8.59 ( 10.21)	Acc@5  50.00 ( 50.18)
03-Mar-22 13:30:37 - Epoch: [4][260/352]	Time  0.124 ( 0.113)	Data  0.002 ( 0.003)	Loss 2.3061e+00 (2.3039e+00)	Acc@1  12.50 ( 10.25)	Acc@5  46.88 ( 50.16)
03-Mar-22 13:30:38 - Epoch: [4][270/352]	Time  0.100 ( 0.113)	Data  0.001 ( 0.003)	Loss 2.3029e+00 (2.3039e+00)	Acc@1  10.94 ( 10.25)	Acc@5  48.44 ( 50.13)
03-Mar-22 13:30:39 - Epoch: [4][280/352]	Time  0.100 ( 0.113)	Data  0.001 ( 0.003)	Loss 2.3068e+00 (2.3039e+00)	Acc@1  11.72 ( 10.30)	Acc@5  47.66 ( 50.15)
03-Mar-22 13:47:51 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:47:51 - Use GPU: 0 for training
03-Mar-22 13:47:51 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:47:55 - match all modules defined in bit_config: False
03-Mar-22 13:47:55 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:49:10 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:49:10 - Use GPU: 0 for training
03-Mar-22 13:49:10 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:49:14 - match all modules defined in bit_config: False
03-Mar-22 13:49:14 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:50:32 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:50:32 - Use GPU: 0 for training
03-Mar-22 13:50:32 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:50:36 - match all modules defined in bit_config: True
03-Mar-22 13:50:36 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:54:04 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:54:04 - Use GPU: 0 for training
03-Mar-22 13:54:04 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:54:09 - match all modules defined in bit_config: True
03-Mar-22 13:54:36 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:54:36 - Use GPU: 0 for training
03-Mar-22 13:54:36 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:54:41 - match all modules defined in bit_config: True
03-Mar-22 13:54:41 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:56:29 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:56:29 - Use GPU: 0 for training
03-Mar-22 13:56:29 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:56:33 - match all modules defined in bit_config: True
03-Mar-22 13:57:56 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:57:56 - Use GPU: 0 for training
03-Mar-22 13:57:56 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:58:00 - match all modules defined in bit_config: True
03-Mar-22 13:58:00 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:02:30 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:02:30 - Use GPU: 0 for training
03-Mar-22 14:02:30 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:02:34 - match all modules defined in bit_config: True
03-Mar-22 14:02:34 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:06:43 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:06:43 - Use GPU: 0 for training
03-Mar-22 14:06:43 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:06:47 - match all modules defined in bit_config: True
03-Mar-22 14:06:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:09:16 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:09:16 - Use GPU: 0 for training
03-Mar-22 14:09:16 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:09:20 - match all modules defined in bit_config: True
03-Mar-22 14:09:20 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:09:53 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:09:53 - Use GPU: 0 for training
03-Mar-22 14:09:53 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:09:57 - match all modules defined in bit_config: True
03-Mar-22 14:09:57 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:12:56 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:12:56 - Use GPU: 0 for training
03-Mar-22 14:12:56 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:13:00 - match all modules defined in bit_config: True
03-Mar-22 14:13:00 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:15:32 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:15:32 - Use GPU: 0 for training
03-Mar-22 14:15:32 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 14:15:36 - match all modules defined in bit_config: True
03-Mar-22 14:15:36 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:15:37 - Epoch: [0][  0/352]	Time  0.555 ( 0.555)	Data  0.216 ( 0.216)	Loss 2.1393e-01 (2.1393e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 14:15:56 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:15:56 - Use GPU: 0 for training
03-Mar-22 14:15:56 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 14:16:00 - match all modules defined in bit_config: True
03-Mar-22 14:16:00 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:20:13 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:20:13 - Use GPU: 0 for training
03-Mar-22 14:20:13 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 14:20:38 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:20:38 - Use GPU: 0 for training
03-Mar-22 14:20:38 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:20:42 - match all modules defined in bit_config: True
03-Mar-22 14:20:42 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:24:07 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:24:07 - Use GPU: 0 for training
03-Mar-22 14:24:07 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:24:11 - match all modules defined in bit_config: True
03-Mar-22 14:24:11 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:25:32 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:25:32 - Use GPU: 0 for training
03-Mar-22 14:25:32 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:25:36 - match all modules defined in bit_config: True
03-Mar-22 14:25:36 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:44:58 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:44:58 - Use GPU: 0 for training
03-Mar-22 14:44:58 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:45:02 - match all modules defined in bit_config: True
03-Mar-22 14:45:02 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:47:02 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:47:02 - Use GPU: 0 for training
03-Mar-22 14:47:02 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:47:06 - match all modules defined in bit_config: True
03-Mar-22 14:47:06 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:50:41 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:50:41 - Use GPU: 0 for training
03-Mar-22 14:50:41 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 14:50:45 - match all modules defined in bit_config: True
03-Mar-22 14:50:45 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:54:08 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:54:08 - Use GPU: 0 for training
03-Mar-22 14:54:08 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:54:12 - match all modules defined in bit_config: True
03-Mar-22 14:54:12 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:55:17 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:55:17 - Use GPU: 0 for training
03-Mar-22 14:55:17 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:55:21 - match all modules defined in bit_config: True
03-Mar-22 14:55:21 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:55:45 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:55:45 - Use GPU: 0 for training
03-Mar-22 14:55:45 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 14:55:49 - match all modules defined in bit_config: True
03-Mar-22 14:55:49 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:07:49 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:07:49 - Use GPU: 0 for training
03-Mar-22 15:07:49 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 15:07:53 - match all modules defined in bit_config: True
03-Mar-22 15:07:53 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:09:18 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:09:18 - Use GPU: 0 for training
03-Mar-22 15:09:18 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:09:22 - match all modules defined in bit_config: True
03-Mar-22 15:09:22 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:17:12 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:17:12 - Use GPU: 0 for training
03-Mar-22 15:17:12 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 15:17:16 - match all modules defined in bit_config: True
03-Mar-22 15:17:16 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:18:24 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:18:24 - Use GPU: 0 for training
03-Mar-22 15:18:24 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 15:18:28 - match all modules defined in bit_config: True
03-Mar-22 15:18:28 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:18:29 - Epoch: [0][  0/352]	Time  0.378 ( 0.378)	Data  0.217 ( 0.217)	Loss 1.5242e-01 (1.5242e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 15:18:30 - Epoch: [0][ 10/352]	Time  0.094 ( 0.124)	Data  0.001 ( 0.021)	Loss 1.3767e-01 (1.5941e-01)	Acc@1  93.75 ( 94.11)	Acc@5 100.00 (100.00)
03-Mar-22 15:20:01 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:20:01 - Use GPU: 0 for training
03-Mar-22 15:20:01 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:20:05 - match all modules defined in bit_config: True
03-Mar-22 15:20:05 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:20:40 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:20:40 - Use GPU: 0 for training
03-Mar-22 15:20:40 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:20:44 - match all modules defined in bit_config: True
03-Mar-22 15:20:44 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:21:30 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:21:30 - Use GPU: 0 for training
03-Mar-22 15:21:30 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:21:34 - match all modules defined in bit_config: True
03-Mar-22 15:21:34 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:22:20 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:22:20 - Use GPU: 0 for training
03-Mar-22 15:22:20 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:22:24 - match all modules defined in bit_config: True
03-Mar-22 15:22:24 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:26:18 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:26:18 - Use GPU: 0 for training
03-Mar-22 15:26:18 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:26:22 - match all modules defined in bit_config: True
03-Mar-22 15:26:22 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:32:42 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:32:42 - Use GPU: 0 for training
03-Mar-22 15:32:42 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:32:47 - match all modules defined in bit_config: True
03-Mar-22 15:32:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:58:45 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:58:45 - Use GPU: 0 for training
03-Mar-22 15:58:45 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 15:58:49 - match all modules defined in bit_config: True
03-Mar-22 15:58:49 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:58:50 - Epoch: [0][  0/352]	Time  0.420 ( 0.420)	Data  0.244 ( 0.244)	Loss 2.3715e-01 (2.3715e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.22 ( 99.22)
03-Mar-22 15:58:51 - Epoch: [0][ 10/352]	Time  0.098 ( 0.136)	Data  0.002 ( 0.024)	Loss 1.2559e-01 (1.4926e-01)	Acc@1  96.09 ( 94.82)	Acc@5 100.00 ( 99.93)
03-Mar-22 15:58:52 - Epoch: [0][ 20/352]	Time  0.117 ( 0.119)	Data  0.002 ( 0.013)	Loss 2.7271e-01 (1.5694e-01)	Acc@1  92.97 ( 94.79)	Acc@5 100.00 ( 99.96)
03-Mar-22 15:58:53 - Epoch: [0][ 30/352]	Time  0.093 ( 0.114)	Data  0.001 ( 0.010)	Loss 1.3964e-01 (1.4792e-01)	Acc@1  94.53 ( 94.88)	Acc@5 100.00 ( 99.97)
03-Mar-22 15:58:54 - Epoch: [0][ 40/352]	Time  0.117 ( 0.112)	Data  0.002 ( 0.008)	Loss 1.1712e-01 (1.4101e-01)	Acc@1  96.88 ( 95.08)	Acc@5 100.00 ( 99.96)
03-Mar-22 15:58:55 - Epoch: [0][ 50/352]	Time  0.118 ( 0.111)	Data  0.002 ( 0.007)	Loss 1.4252e-01 (1.3894e-01)	Acc@1  94.53 ( 95.13)	Acc@5 100.00 ( 99.95)
03-Mar-22 15:58:56 - Epoch: [0][ 60/352]	Time  0.117 ( 0.111)	Data  0.002 ( 0.006)	Loss 1.1157e-01 (1.3731e-01)	Acc@1  97.66 ( 95.22)	Acc@5 100.00 ( 99.96)
03-Mar-22 15:58:57 - Epoch: [0][ 70/352]	Time  0.094 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.5617e-02 (1.3291e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.97)
03-Mar-22 15:59:35 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:59:35 - Use GPU: 0 for training
03-Mar-22 15:59:35 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 15:59:39 - match all modules defined in bit_config: True
03-Mar-22 15:59:39 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:59:39 - Epoch: [0][  0/352]	Time  0.426 ( 0.426)	Data  0.236 ( 0.236)	Loss 3.0758e-01 (3.0758e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
03-Mar-22 15:59:41 - Epoch: [0][ 10/352]	Time  0.122 ( 0.137)	Data  0.002 ( 0.023)	Loss 1.3989e-01 (1.7335e-01)	Acc@1  94.53 ( 94.18)	Acc@5 100.00 ( 99.86)
03-Mar-22 15:59:42 - Epoch: [0][ 20/352]	Time  0.096 ( 0.121)	Data  0.002 ( 0.013)	Loss 1.1212e-01 (1.5339e-01)	Acc@1  96.88 ( 94.72)	Acc@5 100.00 ( 99.93)
03-Mar-22 15:59:43 - Epoch: [0][ 30/352]	Time  0.121 ( 0.118)	Data  0.002 ( 0.009)	Loss 6.7367e-02 (1.4284e-01)	Acc@1  96.88 ( 95.09)	Acc@5 100.00 ( 99.95)
03-Mar-22 15:59:44 - Epoch: [0][ 40/352]	Time  0.096 ( 0.113)	Data  0.002 ( 0.007)	Loss 1.3325e-01 (1.3959e-01)	Acc@1  97.66 ( 95.45)	Acc@5 100.00 ( 99.94)
03-Mar-22 16:00:40 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:00:40 - Use GPU: 0 for training
03-Mar-22 16:00:40 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:00:44 - match all modules defined in bit_config: True
03-Mar-22 16:00:44 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:01:27 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:01:27 - Use GPU: 0 for training
03-Mar-22 16:01:27 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:01:31 - match all modules defined in bit_config: True
03-Mar-22 16:01:31 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:23:03 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:23:03 - Use GPU: 0 for training
03-Mar-22 16:23:03 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 16:23:07 - match all modules defined in bit_config: True
03-Mar-22 16:23:07 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:29:43 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:29:43 - Use GPU: 0 for training
03-Mar-22 16:29:43 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 16:29:47 - match all modules defined in bit_config: True
03-Mar-22 16:29:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:37:24 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:37:24 - Use GPU: 0 for training
03-Mar-22 16:37:24 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:37:28 - match all modules defined in bit_config: True
03-Mar-22 16:37:28 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:37:28 - Epoch: [0][  0/352]	Time  0.417 ( 0.417)	Data  0.225 ( 0.225)	Loss 1.5334e-01 (1.5334e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 16:37:29 - Epoch: [0][ 10/352]	Time  0.121 ( 0.132)	Data  0.002 ( 0.022)	Loss 5.2840e-02 (1.3998e-01)	Acc@1  97.66 ( 95.10)	Acc@5 100.00 ( 99.93)
03-Mar-22 16:37:30 - Epoch: [0][ 20/352]	Time  0.096 ( 0.116)	Data  0.002 ( 0.012)	Loss 1.3331e-01 (1.4151e-01)	Acc@1  93.75 ( 95.09)	Acc@5 100.00 ( 99.93)
03-Mar-22 16:37:54 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:37:54 - Use GPU: 0 for training
03-Mar-22 16:37:54 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:37:58 - match all modules defined in bit_config: True
03-Mar-22 16:37:58 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:37:58 - Epoch: [0][  0/352]	Time  0.456 ( 0.456)	Data  0.259 ( 0.259)	Loss 9.4047e-02 (9.4047e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
03-Mar-22 16:38:00 - Epoch: [0][ 10/352]	Time  0.096 ( 0.136)	Data  0.002 ( 0.025)	Loss 1.5748e-01 (1.6084e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 (100.00)
03-Mar-22 16:38:01 - Epoch: [0][ 20/352]	Time  0.095 ( 0.120)	Data  0.002 ( 0.014)	Loss 1.1596e-01 (1.3742e-01)	Acc@1  95.31 ( 95.20)	Acc@5 100.00 ( 99.96)
03-Mar-22 16:38:02 - Epoch: [0][ 30/352]	Time  0.118 ( 0.115)	Data  0.002 ( 0.010)	Loss 1.1495e-01 (1.3294e-01)	Acc@1  95.31 ( 95.34)	Acc@5 100.00 ( 99.95)
03-Mar-22 16:38:03 - Epoch: [0][ 40/352]	Time  0.117 ( 0.114)	Data  0.002 ( 0.008)	Loss 1.6831e-01 (1.2695e-01)	Acc@1  93.75 ( 95.56)	Acc@5 100.00 ( 99.96)
03-Mar-22 16:39:18 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:39:18 - Use GPU: 0 for training
03-Mar-22 16:39:18 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:39:22 - match all modules defined in bit_config: True
03-Mar-22 16:39:22 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:39:23 - Epoch: [0][  0/352]	Time  0.397 ( 0.397)	Data  0.225 ( 0.225)	Loss 1.0740e-01 (1.0740e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 16:39:24 - Epoch: [0][ 10/352]	Time  0.097 ( 0.126)	Data  0.001 ( 0.022)	Loss 1.2862e-01 (1.1954e-01)	Acc@1  95.31 ( 95.45)	Acc@5 100.00 (100.00)
03-Mar-22 16:39:25 - Epoch: [0][ 20/352]	Time  0.097 ( 0.112)	Data  0.001 ( 0.012)	Loss 1.6124e-01 (1.1588e-01)	Acc@1  94.53 ( 95.76)	Acc@5 100.00 ( 99.96)
03-Mar-22 16:39:26 - Epoch: [0][ 30/352]	Time  0.126 ( 0.108)	Data  0.002 ( 0.009)	Loss 1.6326e-01 (1.1690e-01)	Acc@1  95.31 ( 95.87)	Acc@5 100.00 ( 99.97)
03-Mar-22 16:39:27 - Epoch: [0][ 40/352]	Time  0.096 ( 0.106)	Data  0.001 ( 0.007)	Loss 7.3781e-02 (1.1647e-01)	Acc@1  96.88 ( 95.81)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:28 - Epoch: [0][ 50/352]	Time  0.141 ( 0.111)	Data  0.002 ( 0.006)	Loss 1.0817e-01 (1.1825e-01)	Acc@1  96.88 ( 95.86)	Acc@5 100.00 ( 99.97)
03-Mar-22 16:39:29 - Epoch: [0][ 60/352]	Time  0.096 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2395e-02 (1.1464e-01)	Acc@1  96.88 ( 95.94)	Acc@5 100.00 ( 99.97)
03-Mar-22 16:39:30 - Epoch: [0][ 70/352]	Time  0.096 ( 0.107)	Data  0.001 ( 0.005)	Loss 1.2106e-01 (1.1419e-01)	Acc@1  95.31 ( 95.99)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:31 - Epoch: [0][ 80/352]	Time  0.096 ( 0.107)	Data  0.001 ( 0.004)	Loss 1.8384e-01 (1.1417e-01)	Acc@1  96.88 ( 96.02)	Acc@5  99.22 ( 99.97)
03-Mar-22 16:39:32 - Epoch: [0][ 90/352]	Time  0.096 ( 0.105)	Data  0.001 ( 0.004)	Loss 1.0539e-01 (1.1272e-01)	Acc@1  96.88 ( 96.04)	Acc@5 100.00 ( 99.97)
03-Mar-22 16:39:33 - Epoch: [0][100/352]	Time  0.101 ( 0.105)	Data  0.002 ( 0.004)	Loss 7.8795e-02 (1.1301e-01)	Acc@1  97.66 ( 96.02)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:34 - Epoch: [0][110/352]	Time  0.096 ( 0.104)	Data  0.001 ( 0.004)	Loss 7.0538e-02 (1.1230e-01)	Acc@1  97.66 ( 96.09)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:35 - Epoch: [0][120/352]	Time  0.142 ( 0.106)	Data  0.002 ( 0.003)	Loss 3.9086e-02 (1.1034e-01)	Acc@1  99.22 ( 96.18)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:36 - Epoch: [0][130/352]	Time  0.141 ( 0.108)	Data  0.002 ( 0.003)	Loss 1.1819e-01 (1.0862e-01)	Acc@1  96.88 ( 96.21)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:38 - Epoch: [0][140/352]	Time  0.142 ( 0.110)	Data  0.002 ( 0.003)	Loss 1.0071e-01 (1.0924e-01)	Acc@1  94.53 ( 96.17)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:39 - Epoch: [0][150/352]	Time  0.118 ( 0.111)	Data  0.002 ( 0.003)	Loss 5.0714e-02 (1.0858e-01)	Acc@1  98.44 ( 96.20)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:40 - Epoch: [0][160/352]	Time  0.146 ( 0.113)	Data  0.002 ( 0.003)	Loss 9.0117e-02 (1.0828e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:42 - Epoch: [0][170/352]	Time  0.118 ( 0.114)	Data  0.002 ( 0.003)	Loss 6.8622e-02 (1.0844e-01)	Acc@1  96.88 ( 96.21)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:43 - Epoch: [0][180/352]	Time  0.118 ( 0.115)	Data  0.002 ( 0.003)	Loss 9.0309e-02 (1.0771e-01)	Acc@1  98.44 ( 96.24)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:44 - Epoch: [0][190/352]	Time  0.145 ( 0.116)	Data  0.002 ( 0.003)	Loss 5.8610e-02 (1.0748e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:46 - Epoch: [0][200/352]	Time  0.144 ( 0.117)	Data  0.002 ( 0.003)	Loss 7.4623e-02 (1.0767e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:47 - Epoch: [0][210/352]	Time  0.118 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.5186e-01 (1.0756e-01)	Acc@1  92.97 ( 96.20)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:48 - Epoch: [0][220/352]	Time  0.142 ( 0.118)	Data  0.002 ( 0.003)	Loss 9.7963e-02 (1.0704e-01)	Acc@1  96.88 ( 96.21)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:49 - Epoch: [0][230/352]	Time  0.102 ( 0.118)	Data  0.002 ( 0.003)	Loss 6.3965e-02 (1.0807e-01)	Acc@1  96.09 ( 96.18)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:50 - Epoch: [0][240/352]	Time  0.098 ( 0.117)	Data  0.001 ( 0.003)	Loss 8.2589e-02 (1.0879e-01)	Acc@1  96.88 ( 96.16)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:51 - Epoch: [0][250/352]	Time  0.096 ( 0.116)	Data  0.001 ( 0.003)	Loss 1.9599e-01 (1.0877e-01)	Acc@1  91.41 ( 96.16)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:52 - Epoch: [0][260/352]	Time  0.096 ( 0.116)	Data  0.001 ( 0.003)	Loss 7.7076e-02 (1.0869e-01)	Acc@1  97.66 ( 96.17)	Acc@5 100.00 ( 99.99)
03-Mar-22 16:39:53 - Epoch: [0][270/352]	Time  0.096 ( 0.115)	Data  0.001 ( 0.003)	Loss 9.0574e-02 (1.0852e-01)	Acc@1  98.44 ( 96.18)	Acc@5 100.00 ( 99.99)
03-Mar-22 16:39:54 - Epoch: [0][280/352]	Time  0.096 ( 0.115)	Data  0.001 ( 0.003)	Loss 1.3253e-01 (1.0865e-01)	Acc@1  96.09 ( 96.17)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:55 - Epoch: [0][290/352]	Time  0.123 ( 0.114)	Data  0.002 ( 0.002)	Loss 9.8870e-02 (1.0906e-01)	Acc@1  95.31 ( 96.15)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:56 - Epoch: [0][300/352]	Time  0.122 ( 0.114)	Data  0.002 ( 0.002)	Loss 9.7022e-02 (1.0812e-01)	Acc@1  95.31 ( 96.19)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:57 - Epoch: [0][310/352]	Time  0.096 ( 0.113)	Data  0.001 ( 0.002)	Loss 1.0084e-01 (1.0753e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:58 - Epoch: [0][320/352]	Time  0.096 ( 0.113)	Data  0.001 ( 0.002)	Loss 1.2287e-01 (1.0766e-01)	Acc@1  94.53 ( 96.20)	Acc@5 100.00 ( 99.99)
03-Mar-22 16:39:59 - Epoch: [0][330/352]	Time  0.096 ( 0.112)	Data  0.001 ( 0.002)	Loss 8.0304e-02 (1.0743e-01)	Acc@1  98.44 ( 96.21)	Acc@5 100.00 ( 99.99)
03-Mar-22 16:40:00 - Epoch: [0][340/352]	Time  0.119 ( 0.112)	Data  0.002 ( 0.002)	Loss 8.2080e-02 (1.0729e-01)	Acc@1  96.09 ( 96.21)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:40:01 - Epoch: [0][350/352]	Time  0.095 ( 0.112)	Data  0.001 ( 0.002)	Loss 1.5755e-01 (1.0824e-01)	Acc@1  94.53 ( 96.18)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:40:02 - Test: [ 0/20]	Time  0.378 ( 0.378)	Loss 3.9787e-01 (3.9787e-01)	Acc@1  89.84 ( 89.84)	Acc@5  98.83 ( 98.83)
03-Mar-22 16:40:03 - Test: [10/20]	Time  0.081 ( 0.106)	Loss 4.6640e-01 (3.6020e-01)	Acc@1  88.28 ( 90.45)	Acc@5  99.61 ( 99.47)
03-Mar-22 16:40:04 -  * Acc@1 90.420 Acc@5 99.500
03-Mar-22 16:40:04 - Best acc at epoch 0: 90.41999816894531
03-Mar-22 16:40:04 - Epoch: [1][  0/352]	Time  0.328 ( 0.328)	Data  0.223 ( 0.223)	Loss 1.4667e-01 (1.4667e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 16:40:05 - Epoch: [1][ 10/352]	Time  0.096 ( 0.120)	Data  0.001 ( 0.022)	Loss 1.7281e-01 (1.0933e-01)	Acc@1  95.31 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 16:40:06 - Epoch: [1][ 20/352]	Time  0.096 ( 0.108)	Data  0.001 ( 0.012)	Loss 7.6673e-02 (1.0637e-01)	Acc@1  97.66 ( 96.39)	Acc@5 100.00 ( 99.96)
03-Mar-22 16:40:07 - Epoch: [1][ 30/352]	Time  0.096 ( 0.106)	Data  0.001 ( 0.009)	Loss 1.0099e-01 (1.0798e-01)	Acc@1  96.09 ( 96.19)	Acc@5 100.00 ( 99.97)
03-Mar-22 16:40:46 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:40:46 - Use GPU: 0 for training
03-Mar-22 16:40:46 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:40:50 - match all modules defined in bit_config: True
03-Mar-22 16:40:50 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:40:50 - Epoch: [0][  0/352]	Time  0.469 ( 0.469)	Data  0.258 ( 0.258)	Loss 1.7980e-01 (1.7980e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 16:40:51 - Epoch: [0][ 10/352]	Time  0.095 ( 0.137)	Data  0.001 ( 0.025)	Loss 1.1492e-01 (1.4237e-01)	Acc@1  96.09 ( 95.10)	Acc@5 100.00 ( 99.93)
03-Mar-22 16:40:52 - Epoch: [0][ 20/352]	Time  0.112 ( 0.123)	Data  0.002 ( 0.014)	Loss 1.4314e-01 (1.3299e-01)	Acc@1  92.19 ( 95.31)	Acc@5 100.00 ( 99.93)
03-Mar-22 16:40:53 - Epoch: [0][ 30/352]	Time  0.096 ( 0.114)	Data  0.001 ( 0.010)	Loss 1.4230e-01 (1.2686e-01)	Acc@1  96.09 ( 95.46)	Acc@5 100.00 ( 99.95)
03-Mar-22 16:40:54 - Epoch: [0][ 40/352]	Time  0.095 ( 0.110)	Data  0.002 ( 0.008)	Loss 1.3704e-01 (1.2093e-01)	Acc@1  94.53 ( 95.66)	Acc@5 100.00 ( 99.96)
03-Mar-22 16:42:43 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:42:43 - Use GPU: 0 for training
03-Mar-22 16:42:43 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:42:47 - match all modules defined in bit_config: True
03-Mar-22 16:42:47 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:43:48 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:43:48 - Use GPU: 0 for training
03-Mar-22 16:43:48 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:43:52 - match all modules defined in bit_config: True
03-Mar-22 16:43:52 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:14:02 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:14:02 - Use GPU: 0 for training
03-Mar-22 18:14:02 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:14:06 - match all modules defined in bit_config: True
03-Mar-22 18:14:06 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:15:47 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:15:47 - Use GPU: 0 for training
03-Mar-22 18:15:47 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:15:51 - match all modules defined in bit_config: True
03-Mar-22 18:15:51 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:23:08 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:23:08 - Use GPU: 0 for training
03-Mar-22 18:23:08 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:23:12 - match all modules defined in bit_config: True
03-Mar-22 18:23:12 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:29:13 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:29:13 - Use GPU: 0 for training
03-Mar-22 18:29:13 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:29:17 - match all modules defined in bit_config: True
03-Mar-22 18:29:17 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:32:28 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:32:28 - Use GPU: 0 for training
03-Mar-22 18:32:28 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:32:32 - match all modules defined in bit_config: True
03-Mar-22 18:32:32 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:46:44 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:46:44 - Use GPU: 0 for training
03-Mar-22 18:46:44 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:46:48 - match all modules defined in bit_config: True
03-Mar-22 18:46:48 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:47:30 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:47:30 - Use GPU: 0 for training
03-Mar-22 18:47:30 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:47:34 - match all modules defined in bit_config: True
03-Mar-22 18:47:34 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:48:05 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:48:05 - Use GPU: 0 for training
03-Mar-22 18:48:05 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:48:09 - match all modules defined in bit_config: True
03-Mar-22 18:48:09 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:54:44 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:54:44 - Use GPU: 0 for training
03-Mar-22 18:54:44 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:54:48 - match all modules defined in bit_config: True
03-Mar-22 18:54:48 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:55:12 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:55:12 - Use GPU: 0 for training
03-Mar-22 18:55:12 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:55:16 - match all modules defined in bit_config: True
03-Mar-22 18:55:16 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 19:00:10 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 19:00:10 - Use GPU: 0 for training
03-Mar-22 19:00:10 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 19:00:14 - match all modules defined in bit_config: True
03-Mar-22 19:00:14 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 19:02:36 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 19:02:36 - Use GPU: 0 for training
03-Mar-22 19:02:36 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 19:02:40 - match all modules defined in bit_config: True
03-Mar-22 19:02:40 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 19:04:34 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 19:04:34 - Use GPU: 0 for training
03-Mar-22 19:04:34 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 19:04:39 - match all modules defined in bit_config: True
03-Mar-22 19:04:39 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 19:05:48 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 19:05:48 - Use GPU: 0 for training
03-Mar-22 19:05:48 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 19:05:52 - match all modules defined in bit_config: True
03-Mar-22 19:05:52 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 19:09:24 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 19:09:24 - Use GPU: 0 for training
03-Mar-22 19:09:24 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 19:09:28 - match all modules defined in bit_config: True
03-Mar-22 19:09:28 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 19:09:28 - Epoch: [0][  0/352]	Time  0.426 ( 0.426)	Data  0.261 ( 0.261)	Loss 1.9919e-01 (1.9919e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
03-Mar-22 19:10:21 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 19:10:21 - Use GPU: 0 for training
03-Mar-22 19:10:21 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 19:10:26 - match all modules defined in bit_config: True
03-Mar-22 19:10:26 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 03:48:01 - Epoch: [0][  0/352]	Time 31054.939 (31054.939)	Data  0.219 ( 0.219)	Loss 2.4985e-01 (2.4985e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
04-Mar-22 03:48:10 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 03:48:10 - Use GPU: 0 for training
04-Mar-22 03:48:10 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
04-Mar-22 03:48:14 - match all modules defined in bit_config: True
04-Mar-22 03:48:14 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 03:48:29 - Epoch: [0][  0/352]	Time 15.282 (15.282)	Data  0.256 ( 0.256)	Loss 1.9154e-01 (1.9154e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
04-Mar-22 03:48:42 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 03:48:42 - Use GPU: 0 for training
04-Mar-22 03:48:42 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 03:48:47 - match all modules defined in bit_config: True
04-Mar-22 03:48:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 03:49:36 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 03:49:36 - Use GPU: 0 for training
04-Mar-22 03:49:36 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 03:49:40 - match all modules defined in bit_config: True
04-Mar-22 03:49:40 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 03:50:43 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 03:50:43 - Use GPU: 0 for training
04-Mar-22 03:50:43 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 03:50:47 - match all modules defined in bit_config: True
04-Mar-22 03:50:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 03:52:45 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 03:52:45 - Use GPU: 0 for training
04-Mar-22 03:52:45 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 03:52:49 - match all modules defined in bit_config: True
04-Mar-22 03:52:49 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 03:58:33 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 03:58:33 - Use GPU: 0 for training
04-Mar-22 03:58:33 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 03:58:37 - match all modules defined in bit_config: True
04-Mar-22 03:58:37 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:41:07 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:41:07 - Use GPU: 0 for training
04-Mar-22 05:41:07 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 05:41:11 - match all modules defined in bit_config: True
04-Mar-22 05:41:11 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:42:27 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:42:27 - Use GPU: 0 for training
04-Mar-22 05:42:27 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 05:42:31 - match all modules defined in bit_config: True
04-Mar-22 05:42:31 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:43:58 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:43:58 - Use GPU: 0 for training
04-Mar-22 05:43:58 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
04-Mar-22 05:44:02 - match all modules defined in bit_config: True
04-Mar-22 05:44:02 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:44:24 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:44:24 - Use GPU: 0 for training
04-Mar-22 05:44:24 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
04-Mar-22 05:44:28 - match all modules defined in bit_config: True
04-Mar-22 05:44:28 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:45:03 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:45:03 - Use GPU: 0 for training
04-Mar-22 05:45:03 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 05:45:07 - match all modules defined in bit_config: True
04-Mar-22 05:45:07 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:48:09 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:48:09 - Use GPU: 0 for training
04-Mar-22 05:48:09 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 05:48:13 - match all modules defined in bit_config: True
04-Mar-22 05:48:13 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:50:28 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:50:28 - Use GPU: 0 for training
04-Mar-22 05:50:28 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 05:50:32 - match all modules defined in bit_config: True
04-Mar-22 05:50:32 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:59:57 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:59:57 - Use GPU: 0 for training
04-Mar-22 05:59:57 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 06:00:01 - match all modules defined in bit_config: True
04-Mar-22 06:00:01 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 07:07:30 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 07:07:30 - Use GPU: 0 for training
04-Mar-22 07:07:30 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 07:07:34 - match all modules defined in bit_config: True
04-Mar-22 07:07:34 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 07:10:37 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 07:10:37 - Use GPU: 0 for training
04-Mar-22 07:10:37 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 07:10:41 - match all modules defined in bit_config: True
04-Mar-22 07:10:41 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_conv_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
07-Mar-22 02:40:35 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 02:40:35 - Use GPU: 0 for training
07-Mar-22 02:40:35 - => using pre-trained PyTorchCV model 'resnet20_unfold'
07-Mar-22 02:40:39 - match all modules defined in bit_config: True
07-Mar-22 02:40:39 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_conv_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
07-Mar-22 02:45:48 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 02:45:48 - Use GPU: 0 for training
07-Mar-22 02:45:48 - => using pre-trained PyTorchCV model 'resnet20_unfold'
07-Mar-22 02:45:52 - match all modules defined in bit_config: True
07-Mar-22 02:45:52 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_conv_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
07-Mar-22 02:45:53 - Epoch: [0][  0/352]	Time  0.490 ( 0.490)	Data  0.253 ( 0.253)	Loss 8.2595e-01 (8.2595e-01)	Acc@1  78.91 ( 78.91)	Acc@5  97.66 ( 97.66)
07-Mar-22 02:45:54 - Epoch: [0][ 10/352]	Time  0.142 ( 0.192)	Data  0.002 ( 0.025)	Loss 3.8731e-01 (4.6000e-01)	Acc@1  87.50 ( 85.65)	Acc@5  98.44 ( 99.22)
07-Mar-22 02:45:56 - Epoch: [0][ 20/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.014)	Loss 1.2888e-01 (3.7600e-01)	Acc@1  94.53 ( 87.69)	Acc@5 100.00 ( 99.52)
07-Mar-22 02:45:57 - Epoch: [0][ 30/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.010)	Loss 1.1737e-01 (3.2863e-01)	Acc@1  96.88 ( 89.34)	Acc@5 100.00 ( 99.55)
07-Mar-22 02:45:59 - Epoch: [0][ 40/352]	Time  0.164 ( 0.170)	Data  0.002 ( 0.008)	Loss 3.1755e-01 (2.9590e-01)	Acc@1  91.41 ( 90.28)	Acc@5 100.00 ( 99.64)
07-Mar-22 02:46:01 - Epoch: [0][ 50/352]	Time  0.164 ( 0.170)	Data  0.003 ( 0.007)	Loss 2.2173e-01 (2.7683e-01)	Acc@1  92.97 ( 90.85)	Acc@5 100.00 ( 99.66)
07-Mar-22 02:46:02 - Epoch: [0][ 60/352]	Time  0.163 ( 0.169)	Data  0.002 ( 0.006)	Loss 1.7123e-01 (2.6655e-01)	Acc@1  94.53 ( 91.10)	Acc@5 100.00 ( 99.68)
07-Mar-22 02:46:04 - Epoch: [0][ 70/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.006)	Loss 1.3919e-01 (2.5656e-01)	Acc@1  94.53 ( 91.35)	Acc@5 100.00 ( 99.71)
07-Mar-22 02:46:06 - Epoch: [0][ 80/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.005)	Loss 2.6992e-01 (2.5027e-01)	Acc@1  92.97 ( 91.64)	Acc@5  98.44 ( 99.72)
07-Mar-22 02:46:07 - Epoch: [0][ 90/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.2298e-01 (2.4145e-01)	Acc@1  96.88 ( 91.98)	Acc@5 100.00 ( 99.75)
07-Mar-22 02:46:09 - Epoch: [0][100/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.005)	Loss 2.0295e-01 (2.3432e-01)	Acc@1  93.75 ( 92.22)	Acc@5 100.00 ( 99.78)
07-Mar-22 02:46:11 - Epoch: [0][110/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2132e-01 (2.2823e-01)	Acc@1  94.53 ( 92.36)	Acc@5 100.00 ( 99.80)
07-Mar-22 02:46:13 - Epoch: [0][120/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 2.1244e-01 (2.2574e-01)	Acc@1  91.41 ( 92.39)	Acc@5 100.00 ( 99.79)
07-Mar-22 02:46:14 - Epoch: [0][130/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 2.2002e-01 (2.2222e-01)	Acc@1  92.97 ( 92.52)	Acc@5 100.00 ( 99.80)
07-Mar-22 02:46:16 - Epoch: [0][140/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.0033e-01 (2.1765e-01)	Acc@1  96.88 ( 92.68)	Acc@5 100.00 ( 99.82)
07-Mar-22 02:46:17 - Epoch: [0][150/352]	Time  0.164 ( 0.168)	Data  0.003 ( 0.004)	Loss 1.2052e-01 (2.1499e-01)	Acc@1  95.31 ( 92.80)	Acc@5 100.00 ( 99.82)
07-Mar-22 02:46:19 - Epoch: [0][160/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.5771e-02 (2.1216e-01)	Acc@1  95.31 ( 92.87)	Acc@5 100.00 ( 99.83)
07-Mar-22 02:46:21 - Epoch: [0][170/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.1442e-01 (2.0901e-01)	Acc@1  95.31 ( 92.98)	Acc@5 100.00 ( 99.83)
07-Mar-22 02:46:22 - Epoch: [0][180/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.7575e-01 (2.0692e-01)	Acc@1  92.19 ( 93.02)	Acc@5 100.00 ( 99.84)
07-Mar-22 02:46:24 - Epoch: [0][190/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 2.1886e-01 (2.0438e-01)	Acc@1  90.62 ( 93.10)	Acc@5 100.00 ( 99.84)
07-Mar-22 02:46:26 - Epoch: [0][200/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3268e-01 (2.0215e-01)	Acc@1  93.75 ( 93.15)	Acc@5 100.00 ( 99.85)
07-Mar-22 02:46:28 - Epoch: [0][210/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.5958e-01 (1.9984e-01)	Acc@1  92.19 ( 93.19)	Acc@5 100.00 ( 99.86)
07-Mar-22 02:46:29 - Epoch: [0][220/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.2911e-02 (1.9679e-01)	Acc@1  96.88 ( 93.28)	Acc@5 100.00 ( 99.87)
07-Mar-22 02:46:31 - Epoch: [0][230/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.8943e-01 (1.9418e-01)	Acc@1  93.75 ( 93.38)	Acc@5 100.00 ( 99.87)
07-Mar-22 02:46:32 - Epoch: [0][240/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3046e-01 (1.9172e-01)	Acc@1  95.31 ( 93.47)	Acc@5 100.00 ( 99.87)
07-Mar-22 02:46:34 - Epoch: [0][250/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 2.5677e-01 (1.9072e-01)	Acc@1  89.84 ( 93.49)	Acc@5 100.00 ( 99.87)
07-Mar-22 02:46:36 - Epoch: [0][260/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 2.8975e-01 (1.9055e-01)	Acc@1  92.19 ( 93.51)	Acc@5  99.22 ( 99.87)
07-Mar-22 02:46:38 - Epoch: [0][270/352]	Time  0.189 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.0808e-01 (1.8967e-01)	Acc@1  94.53 ( 93.55)	Acc@5 100.00 ( 99.88)
07-Mar-22 02:46:39 - Epoch: [0][280/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.4452e-01 (1.8854e-01)	Acc@1  92.19 ( 93.59)	Acc@5 100.00 ( 99.87)
07-Mar-22 02:46:41 - Epoch: [0][290/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0465e-01 (1.8684e-01)	Acc@1  96.09 ( 93.63)	Acc@5 100.00 ( 99.88)
07-Mar-22 02:46:43 - Epoch: [0][300/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.9037e-01 (1.8550e-01)	Acc@1  95.31 ( 93.70)	Acc@5 100.00 ( 99.88)
07-Mar-22 02:46:44 - Epoch: [0][310/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4708e-01 (1.8407e-01)	Acc@1  92.97 ( 93.72)	Acc@5 100.00 ( 99.89)
07-Mar-22 02:46:46 - Epoch: [0][320/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.6437e-01 (1.8361e-01)	Acc@1  95.31 ( 93.74)	Acc@5 100.00 ( 99.89)
07-Mar-22 02:46:48 - Epoch: [0][330/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1320e-01 (1.8237e-01)	Acc@1  93.75 ( 93.77)	Acc@5 100.00 ( 99.89)
07-Mar-22 02:46:49 - Epoch: [0][340/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.9593e-01 (1.8152e-01)	Acc@1  93.75 ( 93.81)	Acc@5  99.22 ( 99.89)
07-Mar-22 02:46:51 - Epoch: [0][350/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.7175e-02 (1.8029e-01)	Acc@1  96.88 ( 93.84)	Acc@5 100.00 ( 99.89)
07-Mar-22 02:46:52 - Test: [ 0/20]	Time  0.395 ( 0.395)	Loss 4.2695e-01 (4.2695e-01)	Acc@1  88.67 ( 88.67)	Acc@5  98.83 ( 98.83)
07-Mar-22 02:46:53 - Test: [10/20]	Time  0.098 ( 0.128)	Loss 4.3807e-01 (4.5339e-01)	Acc@1  87.11 ( 86.97)	Acc@5  99.22 ( 99.15)
07-Mar-22 02:46:54 -  * Acc@1 87.140 Acc@5 99.080
07-Mar-22 02:46:54 - Best acc at epoch 0: 87.13999938964844
07-Mar-22 02:46:54 - Epoch: [1][  0/352]	Time  0.384 ( 0.384)	Data  0.236 ( 0.236)	Loss 7.4646e-02 (7.4646e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 02:46:56 - Epoch: [1][ 10/352]	Time  0.143 ( 0.179)	Data  0.002 ( 0.023)	Loss 1.1887e-01 (1.3918e-01)	Acc@1  96.09 ( 95.60)	Acc@5  99.22 ( 99.93)
07-Mar-22 02:46:57 - Epoch: [1][ 20/352]	Time  0.143 ( 0.162)	Data  0.002 ( 0.013)	Loss 1.3256e-01 (1.3540e-01)	Acc@1  93.75 ( 95.39)	Acc@5 100.00 ( 99.93)
07-Mar-22 02:46:59 - Epoch: [1][ 30/352]	Time  0.143 ( 0.156)	Data  0.002 ( 0.009)	Loss 1.5539e-01 (1.4496e-01)	Acc@1  96.09 ( 95.09)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:00 - Epoch: [1][ 40/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.008)	Loss 1.5425e-01 (1.4440e-01)	Acc@1  94.53 ( 95.05)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:01 - Epoch: [1][ 50/352]	Time  0.143 ( 0.151)	Data  0.002 ( 0.006)	Loss 1.0162e-01 (1.4722e-01)	Acc@1  96.88 ( 94.85)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:47:03 - Epoch: [1][ 60/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.006)	Loss 1.3014e-01 (1.4332e-01)	Acc@1  95.31 ( 94.99)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:04 - Epoch: [1][ 70/352]	Time  0.144 ( 0.149)	Data  0.002 ( 0.005)	Loss 2.2432e-01 (1.4386e-01)	Acc@1  92.19 ( 94.91)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:06 - Epoch: [1][ 80/352]	Time  0.143 ( 0.148)	Data  0.002 ( 0.005)	Loss 1.7453e-01 (1.4575e-01)	Acc@1  95.31 ( 94.93)	Acc@5  99.22 ( 99.95)
07-Mar-22 02:47:07 - Epoch: [1][ 90/352]	Time  0.164 ( 0.150)	Data  0.002 ( 0.004)	Loss 6.8779e-02 (1.4585e-01)	Acc@1  96.88 ( 94.97)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:09 - Epoch: [1][100/352]	Time  0.163 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0185e-01 (1.4194e-01)	Acc@1  96.88 ( 95.10)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:11 - Epoch: [1][110/352]	Time  0.171 ( 0.153)	Data  0.001 ( 0.004)	Loss 9.7210e-02 (1.4237e-01)	Acc@1  98.44 ( 95.14)	Acc@5 100.00 ( 99.94)
07-Mar-22 02:47:12 - Epoch: [1][120/352]	Time  0.167 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.7816e-01 (1.4209e-01)	Acc@1  93.75 ( 95.13)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:14 - Epoch: [1][130/352]	Time  0.165 ( 0.155)	Data  0.002 ( 0.004)	Loss 1.2834e-01 (1.4211e-01)	Acc@1  94.53 ( 95.10)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:16 - Epoch: [1][140/352]	Time  0.166 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.1398e-01 (1.4075e-01)	Acc@1  94.53 ( 95.15)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:17 - Epoch: [1][150/352]	Time  0.167 ( 0.156)	Data  0.001 ( 0.003)	Loss 1.8824e-01 (1.4058e-01)	Acc@1  92.97 ( 95.14)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:19 - Epoch: [1][160/352]	Time  0.167 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.7880e-01 (1.3869e-01)	Acc@1  92.97 ( 95.18)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:21 - Epoch: [1][170/352]	Time  0.165 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.8233e-02 (1.3807e-01)	Acc@1  96.09 ( 95.20)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:22 - Epoch: [1][180/352]	Time  0.169 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.5022e-01 (1.3797e-01)	Acc@1  96.09 ( 95.19)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:24 - Epoch: [1][190/352]	Time  0.166 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1504e-01 (1.3723e-01)	Acc@1  96.09 ( 95.20)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:26 - Epoch: [1][200/352]	Time  0.168 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.8823e-01 (1.3572e-01)	Acc@1  93.75 ( 95.23)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:27 - Epoch: [1][210/352]	Time  0.170 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1502e-01 (1.3612e-01)	Acc@1  96.09 ( 95.21)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:29 - Epoch: [1][220/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.5820e-01 (1.3623e-01)	Acc@1  96.09 ( 95.22)	Acc@5  99.22 ( 99.96)
07-Mar-22 02:47:31 - Epoch: [1][230/352]	Time  0.167 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.0170e-01 (1.3502e-01)	Acc@1  98.44 ( 95.26)	Acc@5  99.22 ( 99.96)
07-Mar-22 02:47:32 - Epoch: [1][240/352]	Time  0.168 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.2557e-01 (1.3394e-01)	Acc@1  96.09 ( 95.30)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:34 - Epoch: [1][250/352]	Time  0.168 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.5474e-01 (1.3312e-01)	Acc@1  95.31 ( 95.33)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:36 - Epoch: [1][260/352]	Time  0.172 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.3487e-01 (1.3332e-01)	Acc@1  95.31 ( 95.32)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:37 - Epoch: [1][270/352]	Time  0.164 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.0459e-01 (1.3324e-01)	Acc@1  97.66 ( 95.32)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:39 - Epoch: [1][280/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.1504e-01 (1.3272e-01)	Acc@1  96.88 ( 95.34)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:41 - Epoch: [1][290/352]	Time  0.168 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.0638e-01 (1.3251e-01)	Acc@1  98.44 ( 95.37)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:42 - Epoch: [1][300/352]	Time  0.165 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.7803e-01 (1.3243e-01)	Acc@1  95.31 ( 95.37)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:44 - Epoch: [1][310/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.8575e-01 (1.3331e-01)	Acc@1  94.53 ( 95.35)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:46 - Epoch: [1][320/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.3620e-02 (1.3417e-01)	Acc@1  98.44 ( 95.32)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:47 - Epoch: [1][330/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.8247e-01 (1.3473e-01)	Acc@1  94.53 ( 95.28)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:49 - Epoch: [1][340/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.4683e-01 (1.3499e-01)	Acc@1  94.53 ( 95.28)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:51 - Epoch: [1][350/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.003)	Loss 6.5466e-02 (1.3409e-01)	Acc@1  97.66 ( 95.30)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:51 - Test: [ 0/20]	Time  0.401 ( 0.401)	Loss 4.2124e-01 (4.2124e-01)	Acc@1  87.11 ( 87.11)	Acc@5  98.83 ( 98.83)
07-Mar-22 02:47:52 - Test: [10/20]	Time  0.098 ( 0.127)	Loss 4.0017e-01 (4.2730e-01)	Acc@1  89.45 ( 87.39)	Acc@5  99.22 ( 99.15)
07-Mar-22 02:47:53 -  * Acc@1 87.600 Acc@5 99.220
07-Mar-22 02:47:53 - Best acc at epoch 1: 87.5999984741211
07-Mar-22 02:47:54 - Epoch: [2][  0/352]	Time  0.395 ( 0.395)	Data  0.232 ( 0.232)	Loss 3.2300e-01 (3.2300e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
07-Mar-22 02:47:56 - Epoch: [2][ 10/352]	Time  0.165 ( 0.187)	Data  0.002 ( 0.023)	Loss 1.9060e-01 (1.6503e-01)	Acc@1  92.97 ( 94.25)	Acc@5 100.00 (100.00)
07-Mar-22 02:47:57 - Epoch: [2][ 20/352]	Time  0.167 ( 0.180)	Data  0.002 ( 0.013)	Loss 1.1720e-01 (1.4785e-01)	Acc@1  97.66 ( 94.83)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:59 - Epoch: [2][ 30/352]	Time  0.168 ( 0.176)	Data  0.002 ( 0.009)	Loss 1.0647e-01 (1.3499e-01)	Acc@1  95.31 ( 95.26)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:01 - Epoch: [2][ 40/352]	Time  0.175 ( 0.174)	Data  0.003 ( 0.008)	Loss 7.0756e-02 (1.3165e-01)	Acc@1  97.66 ( 95.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:48:02 - Epoch: [2][ 50/352]	Time  0.151 ( 0.170)	Data  0.002 ( 0.007)	Loss 2.2828e-01 (1.3398e-01)	Acc@1  90.62 ( 95.33)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:04 - Epoch: [2][ 60/352]	Time  0.150 ( 0.167)	Data  0.002 ( 0.006)	Loss 9.4868e-02 (1.3063e-01)	Acc@1  98.44 ( 95.44)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:05 - Epoch: [2][ 70/352]	Time  0.157 ( 0.166)	Data  0.002 ( 0.005)	Loss 8.0674e-02 (1.2934e-01)	Acc@1  97.66 ( 95.53)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:07 - Epoch: [2][ 80/352]	Time  0.149 ( 0.164)	Data  0.002 ( 0.005)	Loss 1.2100e-01 (1.2851e-01)	Acc@1  95.31 ( 95.49)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:08 - Epoch: [2][ 90/352]	Time  0.146 ( 0.162)	Data  0.002 ( 0.004)	Loss 1.4664e-01 (1.2698e-01)	Acc@1  96.09 ( 95.54)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:10 - Epoch: [2][100/352]	Time  0.164 ( 0.162)	Data  0.002 ( 0.004)	Loss 8.7246e-02 (1.2454e-01)	Acc@1  96.88 ( 95.65)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:11 - Epoch: [2][110/352]	Time  0.141 ( 0.160)	Data  0.001 ( 0.004)	Loss 1.1280e-01 (1.2521e-01)	Acc@1  94.53 ( 95.61)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:13 - Epoch: [2][120/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.004)	Loss 5.6912e-02 (1.2425e-01)	Acc@1  99.22 ( 95.63)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:14 - Epoch: [2][130/352]	Time  0.156 ( 0.158)	Data  0.002 ( 0.004)	Loss 1.8033e-01 (1.2494e-01)	Acc@1  92.97 ( 95.59)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:16 - Epoch: [2][140/352]	Time  0.154 ( 0.158)	Data  0.001 ( 0.004)	Loss 1.2490e-01 (1.2573e-01)	Acc@1  93.75 ( 95.53)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:17 - Epoch: [2][150/352]	Time  0.143 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.3884e-02 (1.2680e-01)	Acc@1  97.66 ( 95.49)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:19 - Epoch: [2][160/352]	Time  0.154 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.2077e-01 (1.2800e-01)	Acc@1  95.31 ( 95.40)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:20 - Epoch: [2][170/352]	Time  0.151 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.1087e-01 (1.2784e-01)	Acc@1  97.66 ( 95.43)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:22 - Epoch: [2][180/352]	Time  0.157 ( 0.157)	Data  0.002 ( 0.003)	Loss 9.2817e-02 (1.2734e-01)	Acc@1  96.88 ( 95.47)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:23 - Epoch: [2][190/352]	Time  0.142 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.4302e-01 (1.2671e-01)	Acc@1  95.31 ( 95.50)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:25 - Epoch: [2][200/352]	Time  0.141 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.8687e-02 (1.2748e-01)	Acc@1  96.88 ( 95.48)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:26 - Epoch: [2][210/352]	Time  0.166 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.1863e-01 (1.2685e-01)	Acc@1  96.09 ( 95.50)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:28 - Epoch: [2][220/352]	Time  0.167 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.0256e-01 (1.2623e-01)	Acc@1  96.09 ( 95.52)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:30 - Epoch: [2][230/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.2761e-01 (1.2621e-01)	Acc@1  94.53 ( 95.50)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:31 - Epoch: [2][240/352]	Time  0.167 ( 0.158)	Data  0.002 ( 0.003)	Loss 6.9367e-02 (1.2613e-01)	Acc@1  98.44 ( 95.51)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:33 - Epoch: [2][250/352]	Time  0.160 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.6000e-01 (1.2641e-01)	Acc@1  93.75 ( 95.49)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:35 - Epoch: [2][260/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.003)	Loss 7.7305e-02 (1.2699e-01)	Acc@1  97.66 ( 95.47)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:36 - Epoch: [2][270/352]	Time  0.174 ( 0.159)	Data  0.002 ( 0.003)	Loss 2.1389e-01 (1.2706e-01)	Acc@1  90.62 ( 95.47)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:38 - Epoch: [2][280/352]	Time  0.144 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1561e-01 (1.2712e-01)	Acc@1  93.75 ( 95.47)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:40 - Epoch: [2][290/352]	Time  0.169 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.7515e-01 (1.2631e-01)	Acc@1  92.97 ( 95.50)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:41 - Epoch: [2][300/352]	Time  0.149 ( 0.159)	Data  0.002 ( 0.003)	Loss 2.1529e-01 (1.2642e-01)	Acc@1  92.19 ( 95.50)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:43 - Epoch: [2][310/352]	Time  0.164 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.8195e-01 (1.2626e-01)	Acc@1  93.75 ( 95.50)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:45 - Epoch: [2][320/352]	Time  0.165 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1046e-01 (1.2595e-01)	Acc@1  96.09 ( 95.50)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:46 - Epoch: [2][330/352]	Time  0.169 ( 0.160)	Data  0.001 ( 0.003)	Loss 1.5593e-01 (1.2611e-01)	Acc@1  94.53 ( 95.50)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:48 - Epoch: [2][340/352]	Time  0.165 ( 0.160)	Data  0.002 ( 0.003)	Loss 8.4949e-02 (1.2533e-01)	Acc@1  96.88 ( 95.52)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:50 - Epoch: [2][350/352]	Time  0.147 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.4725e-01 (1.2512e-01)	Acc@1  94.53 ( 95.53)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:50 - Test: [ 0/20]	Time  0.375 ( 0.375)	Loss 4.2507e-01 (4.2507e-01)	Acc@1  87.89 ( 87.89)	Acc@5  99.61 ( 99.61)
07-Mar-22 02:48:51 - Test: [10/20]	Time  0.100 ( 0.129)	Loss 4.6866e-01 (4.2135e-01)	Acc@1  89.06 ( 88.00)	Acc@5  98.83 ( 99.40)
07-Mar-22 02:48:52 -  * Acc@1 88.160 Acc@5 99.420
07-Mar-22 02:48:52 - Best acc at epoch 2: 88.15999603271484
07-Mar-22 02:48:53 - Epoch: [3][  0/352]	Time  0.393 ( 0.393)	Data  0.234 ( 0.234)	Loss 1.5994e-01 (1.5994e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
07-Mar-22 02:48:54 - Epoch: [3][ 10/352]	Time  0.166 ( 0.187)	Data  0.002 ( 0.023)	Loss 9.6748e-02 (1.3564e-01)	Acc@1  97.66 ( 95.24)	Acc@5 100.00 (100.00)
07-Mar-22 02:48:56 - Epoch: [3][ 20/352]	Time  0.167 ( 0.178)	Data  0.002 ( 0.013)	Loss 1.5928e-01 (1.3570e-01)	Acc@1  94.53 ( 95.24)	Acc@5 100.00 (100.00)
07-Mar-22 02:48:58 - Epoch: [3][ 30/352]	Time  0.173 ( 0.174)	Data  0.002 ( 0.010)	Loss 1.3196e-01 (1.3538e-01)	Acc@1  94.53 ( 95.24)	Acc@5  99.22 ( 99.97)
07-Mar-22 02:48:59 - Epoch: [3][ 40/352]	Time  0.166 ( 0.172)	Data  0.002 ( 0.008)	Loss 1.4632e-01 (1.3167e-01)	Acc@1  95.31 ( 95.45)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:01 - Epoch: [3][ 50/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.007)	Loss 5.0051e-02 (1.2825e-01)	Acc@1  98.44 ( 95.65)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:03 - Epoch: [3][ 60/352]	Time  0.165 ( 0.171)	Data  0.002 ( 0.006)	Loss 1.0913e-01 (1.2469e-01)	Acc@1  97.66 ( 95.72)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:49:04 - Epoch: [3][ 70/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.005)	Loss 9.0259e-02 (1.2355e-01)	Acc@1  97.66 ( 95.75)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:49:06 - Epoch: [3][ 80/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.5997e-01 (1.2403e-01)	Acc@1  92.19 ( 95.65)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:49:08 - Epoch: [3][ 90/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.005)	Loss 8.4894e-02 (1.2557e-01)	Acc@1  95.31 ( 95.69)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:09 - Epoch: [3][100/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.8765e-02 (1.2459e-01)	Acc@1  96.09 ( 95.72)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:11 - Epoch: [3][110/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.3848e-01 (1.2246e-01)	Acc@1  92.97 ( 95.82)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:13 - Epoch: [3][120/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.0810e-01 (1.2094e-01)	Acc@1  95.31 ( 95.87)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:14 - Epoch: [3][130/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.2857e-01 (1.2059e-01)	Acc@1  94.53 ( 95.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:16 - Epoch: [3][140/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.1966e-01 (1.2128e-01)	Acc@1  95.31 ( 95.86)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:18 - Epoch: [3][150/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.1828e-02 (1.2005e-01)	Acc@1  97.66 ( 95.89)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:19 - Epoch: [3][160/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.6793e-02 (1.1894e-01)	Acc@1  96.88 ( 95.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:21 - Epoch: [3][170/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0553e-01 (1.1822e-01)	Acc@1  95.31 ( 95.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:23 - Epoch: [3][180/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.9276e-02 (1.1865e-01)	Acc@1  96.09 ( 95.94)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:24 - Epoch: [3][190/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.2668e-02 (1.1933e-01)	Acc@1  98.44 ( 95.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:26 - Epoch: [3][200/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.3355e-01 (1.1907e-01)	Acc@1  96.09 ( 95.95)	Acc@5  99.22 ( 99.97)
07-Mar-22 02:49:28 - Epoch: [3][210/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.9084e-02 (1.1950e-01)	Acc@1  96.09 ( 95.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:29 - Epoch: [3][220/352]	Time  0.163 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0382e-01 (1.1881e-01)	Acc@1  96.88 ( 95.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:31 - Epoch: [3][230/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.9435e-01 (1.1988e-01)	Acc@1  92.19 ( 95.92)	Acc@5  99.22 ( 99.97)
07-Mar-22 02:49:33 - Epoch: [3][240/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.4277e-02 (1.1948e-01)	Acc@1  97.66 ( 95.95)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:34 - Epoch: [3][250/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1908e-01 (1.1908e-01)	Acc@1  94.53 ( 95.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:36 - Epoch: [3][260/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4072e-01 (1.1923e-01)	Acc@1  95.31 ( 95.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:38 - Epoch: [3][270/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2802e-01 (1.1994e-01)	Acc@1  93.75 ( 95.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:39 - Epoch: [3][280/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.6345e-02 (1.1965e-01)	Acc@1  96.09 ( 95.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:41 - Epoch: [3][290/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1281e-01 (1.1941e-01)	Acc@1  96.88 ( 95.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:43 - Epoch: [3][300/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4003e-01 (1.1883e-01)	Acc@1  92.97 ( 95.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:44 - Epoch: [3][310/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.7336e-02 (1.1855e-01)	Acc@1  99.22 ( 95.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:46 - Epoch: [3][320/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.5515e-01 (1.1927e-01)	Acc@1  96.09 ( 95.92)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:48 - Epoch: [3][330/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.4736e-02 (1.1919e-01)	Acc@1  97.66 ( 95.92)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:49 - Epoch: [3][340/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.7035e-01 (1.1968e-01)	Acc@1  93.75 ( 95.89)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:51 - Epoch: [3][350/352]	Time  0.168 ( 0.167)	Data  0.001 ( 0.003)	Loss 1.3807e-01 (1.1939e-01)	Acc@1  95.31 ( 95.90)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:52 - Test: [ 0/20]	Time  0.367 ( 0.367)	Loss 4.0775e-01 (4.0775e-01)	Acc@1  88.67 ( 88.67)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:49:53 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 3.8473e-01 (4.0269e-01)	Acc@1  88.67 ( 88.42)	Acc@5  99.61 ( 99.54)
07-Mar-22 02:49:54 -  * Acc@1 88.180 Acc@5 99.520
07-Mar-22 02:49:54 - Best acc at epoch 3: 88.18000030517578
07-Mar-22 02:49:54 - Epoch: [4][  0/352]	Time  0.428 ( 0.428)	Data  0.258 ( 0.258)	Loss 2.0419e-01 (2.0419e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
07-Mar-22 02:49:56 - Epoch: [4][ 10/352]	Time  0.167 ( 0.189)	Data  0.002 ( 0.025)	Loss 7.0146e-02 (1.1497e-01)	Acc@1  98.44 ( 96.16)	Acc@5 100.00 (100.00)
07-Mar-22 02:49:57 - Epoch: [4][ 20/352]	Time  0.170 ( 0.178)	Data  0.002 ( 0.014)	Loss 1.6990e-01 (1.2369e-01)	Acc@1  95.31 ( 95.83)	Acc@5 100.00 ( 99.89)
07-Mar-22 02:49:59 - Epoch: [4][ 30/352]	Time  0.167 ( 0.175)	Data  0.002 ( 0.010)	Loss 1.0979e-01 (1.2319e-01)	Acc@1  96.09 ( 95.72)	Acc@5 100.00 ( 99.90)
07-Mar-22 02:50:01 - Epoch: [4][ 40/352]	Time  0.171 ( 0.173)	Data  0.003 ( 0.008)	Loss 9.5551e-02 (1.2212e-01)	Acc@1  96.09 ( 95.79)	Acc@5 100.00 ( 99.92)
07-Mar-22 02:50:02 - Epoch: [4][ 50/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.007)	Loss 1.0782e-01 (1.2153e-01)	Acc@1  96.88 ( 95.89)	Acc@5 100.00 ( 99.94)
07-Mar-22 02:50:04 - Epoch: [4][ 60/352]	Time  0.170 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.4044e-01 (1.1739e-01)	Acc@1  94.53 ( 96.03)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:06 - Epoch: [4][ 70/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.006)	Loss 1.2002e-01 (1.1817e-01)	Acc@1  95.31 ( 95.98)	Acc@5 100.00 ( 99.93)
07-Mar-22 02:50:07 - Epoch: [4][ 80/352]	Time  0.161 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.4819e-01 (1.1713e-01)	Acc@1  95.31 ( 96.02)	Acc@5 100.00 ( 99.94)
07-Mar-22 02:50:09 - Epoch: [4][ 90/352]	Time  0.157 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.0273e-01 (1.1556e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:11 - Epoch: [4][100/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.2546e-01 (1.1361e-01)	Acc@1  96.88 ( 96.18)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:12 - Epoch: [4][110/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.4072e-02 (1.1339e-01)	Acc@1  97.66 ( 96.16)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:14 - Epoch: [4][120/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 9.5568e-02 (1.1332e-01)	Acc@1  95.31 ( 96.13)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:16 - Epoch: [4][130/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.9939e-02 (1.1411e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:17 - Epoch: [4][140/352]	Time  0.164 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.5096e-01 (1.1505e-01)	Acc@1  94.53 ( 95.99)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:19 - Epoch: [4][150/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.004)	Loss 2.0888e-01 (1.1639e-01)	Acc@1  94.53 ( 95.96)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:21 - Epoch: [4][160/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2104e-01 (1.1725e-01)	Acc@1  96.09 ( 95.89)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:22 - Epoch: [4][170/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.1903e-01 (1.1809e-01)	Acc@1  96.09 ( 95.88)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:24 - Epoch: [4][180/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2722e-01 (1.1741e-01)	Acc@1  92.97 ( 95.88)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:26 - Epoch: [4][190/352]	Time  0.157 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.8628e-02 (1.1745e-01)	Acc@1  97.66 ( 95.87)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:27 - Epoch: [4][200/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0051e-01 (1.1698e-01)	Acc@1  96.88 ( 95.91)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:29 - Epoch: [4][210/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.8544e-01 (1.1611e-01)	Acc@1  95.31 ( 95.97)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:31 - Epoch: [4][220/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.5709e-02 (1.1607e-01)	Acc@1  98.44 ( 95.99)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:32 - Epoch: [4][230/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.6037e-01 (1.1553e-01)	Acc@1  93.75 ( 95.99)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:34 - Epoch: [4][240/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0788e-01 (1.1475e-01)	Acc@1  95.31 ( 96.03)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:36 - Epoch: [4][250/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.0540e-02 (1.1398e-01)	Acc@1  96.88 ( 96.05)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:37 - Epoch: [4][260/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.9206e-02 (1.1371e-01)	Acc@1  96.88 ( 96.05)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:39 - Epoch: [4][270/352]	Time  0.142 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4243e-01 (1.1347e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:40 - Epoch: [4][280/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3534e-01 (1.1421e-01)	Acc@1  93.75 ( 96.00)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:42 - Epoch: [4][290/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4218e-01 (1.1471e-01)	Acc@1  96.09 ( 95.99)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:44 - Epoch: [4][300/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.1932e-02 (1.1489e-01)	Acc@1  98.44 ( 96.00)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:45 - Epoch: [4][310/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3555e-01 (1.1475e-01)	Acc@1  97.66 ( 96.01)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:47 - Epoch: [4][320/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3276e-01 (1.1363e-01)	Acc@1  94.53 ( 96.05)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:49 - Epoch: [4][330/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2236e-01 (1.1370e-01)	Acc@1  94.53 ( 96.04)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:51 - Epoch: [4][340/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.7064e-02 (1.1329e-01)	Acc@1  96.88 ( 96.04)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:50:52 - Epoch: [4][350/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4681e-01 (1.1311e-01)	Acc@1  94.53 ( 96.04)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:50:53 - Test: [ 0/20]	Time  0.400 ( 0.400)	Loss 4.3735e-01 (4.3735e-01)	Acc@1  87.11 ( 87.11)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:50:54 - Test: [10/20]	Time  0.098 ( 0.128)	Loss 3.7755e-01 (4.0494e-01)	Acc@1  89.06 ( 88.35)	Acc@5  99.61 ( 99.36)
07-Mar-22 02:50:55 -  * Acc@1 88.680 Acc@5 99.400
07-Mar-22 02:50:55 - Best acc at epoch 4: 88.68000030517578
07-Mar-22 02:50:55 - Epoch: [5][  0/352]	Time  0.415 ( 0.415)	Data  0.246 ( 0.246)	Loss 8.4952e-02 (8.4952e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 02:50:57 - Epoch: [5][ 10/352]	Time  0.165 ( 0.189)	Data  0.002 ( 0.024)	Loss 1.4818e-01 (1.2039e-01)	Acc@1  94.53 ( 95.81)	Acc@5 100.00 ( 99.93)
07-Mar-22 02:50:59 - Epoch: [5][ 20/352]	Time  0.165 ( 0.178)	Data  0.002 ( 0.014)	Loss 7.6700e-02 (1.1708e-01)	Acc@1  97.66 ( 95.91)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:51:00 - Epoch: [5][ 30/352]	Time  0.167 ( 0.174)	Data  0.002 ( 0.010)	Loss 1.4229e-01 (1.2111e-01)	Acc@1  95.31 ( 95.77)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:02 - Epoch: [5][ 40/352]	Time  0.165 ( 0.172)	Data  0.002 ( 0.008)	Loss 6.4886e-02 (1.2098e-01)	Acc@1  96.88 ( 95.75)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:04 - Epoch: [5][ 50/352]	Time  0.164 ( 0.171)	Data  0.002 ( 0.007)	Loss 8.0623e-02 (1.2001e-01)	Acc@1  96.88 ( 95.73)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:05 - Epoch: [5][ 60/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.006)	Loss 1.1802e-01 (1.1965e-01)	Acc@1  96.09 ( 95.76)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:51:07 - Epoch: [5][ 70/352]	Time  0.165 ( 0.170)	Data  0.002 ( 0.006)	Loss 6.1063e-02 (1.1582e-01)	Acc@1  97.66 ( 95.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:51:09 - Epoch: [5][ 80/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.1675e-01 (1.1587e-01)	Acc@1  96.88 ( 95.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:51:10 - Epoch: [5][ 90/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.1289e-01 (1.1654e-01)	Acc@1  96.09 ( 95.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:51:12 - Epoch: [5][100/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2118e-01 (1.1690e-01)	Acc@1  93.75 ( 95.82)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:14 - Epoch: [5][110/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.1503e-02 (1.1660e-01)	Acc@1  96.88 ( 95.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:15 - Epoch: [5][120/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.6912e-02 (1.1456e-01)	Acc@1  97.66 ( 95.95)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:17 - Epoch: [5][130/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.1486e-01 (1.1383e-01)	Acc@1  96.09 ( 96.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:19 - Epoch: [5][140/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2775e-01 (1.1267e-01)	Acc@1  94.53 ( 96.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:20 - Epoch: [5][150/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.2338e-02 (1.1315e-01)	Acc@1  96.88 ( 96.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:22 - Epoch: [5][160/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.6688e-01 (1.1408e-01)	Acc@1  96.88 ( 96.00)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:51:24 - Epoch: [5][170/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.1183e-02 (1.1393e-01)	Acc@1  97.66 ( 95.99)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:25 - Epoch: [5][180/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4594e-01 (1.1315e-01)	Acc@1  94.53 ( 96.05)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:27 - Epoch: [5][190/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.2235e-02 (1.1271e-01)	Acc@1  99.22 ( 96.08)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:29 - Epoch: [5][200/352]	Time  0.162 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0050e-01 (1.1339e-01)	Acc@1  96.88 ( 96.05)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:30 - Epoch: [5][210/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.3886e-02 (1.1425e-01)	Acc@1  96.88 ( 96.02)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:32 - Epoch: [5][220/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.6501e-01 (1.1320e-01)	Acc@1  93.75 ( 96.05)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:34 - Epoch: [5][230/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1262e-01 (1.1232e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:35 - Epoch: [5][240/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2859e-01 (1.1263e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:37 - Epoch: [5][250/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.2738e-02 (1.1230e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:39 - Epoch: [5][260/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1557e-01 (1.1174e-01)	Acc@1  97.66 ( 96.12)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:40 - Epoch: [5][270/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0445e-01 (1.1115e-01)	Acc@1  95.31 ( 96.14)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:42 - Epoch: [5][280/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1777e-01 (1.1104e-01)	Acc@1  96.09 ( 96.14)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:44 - Epoch: [5][290/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.5179e-01 (1.1095e-01)	Acc@1  96.09 ( 96.13)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:45 - Epoch: [5][300/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1407e-01 (1.1069e-01)	Acc@1  96.09 ( 96.12)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:47 - Epoch: [5][310/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2687e-01 (1.1061e-01)	Acc@1  93.75 ( 96.11)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:49 - Epoch: [5][320/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.1755e-02 (1.1108e-01)	Acc@1  97.66 ( 96.11)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:50 - Epoch: [5][330/352]	Time  0.171 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0923e-01 (1.1080e-01)	Acc@1  95.31 ( 96.12)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:52 - Epoch: [5][340/352]	Time  0.169 ( 0.167)	Data  0.001 ( 0.003)	Loss 2.0292e-01 (1.1096e-01)	Acc@1  93.75 ( 96.11)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:54 - Epoch: [5][350/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4626e-01 (1.1122e-01)	Acc@1  95.31 ( 96.08)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:54 - Test: [ 0/20]	Time  0.389 ( 0.389)	Loss 4.1934e-01 (4.1934e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:51:55 - Test: [10/20]	Time  0.097 ( 0.125)	Loss 3.2910e-01 (4.0358e-01)	Acc@1  90.62 ( 88.53)	Acc@5  99.22 ( 99.54)
07-Mar-22 02:51:56 -  * Acc@1 88.900 Acc@5 99.460
07-Mar-22 02:51:56 - Best acc at epoch 5: 88.9000015258789
07-Mar-22 02:51:57 - Epoch: [6][  0/352]	Time  0.375 ( 0.375)	Data  0.231 ( 0.231)	Loss 5.4320e-02 (5.4320e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 02:51:58 - Epoch: [6][ 10/352]	Time  0.161 ( 0.181)	Data  0.002 ( 0.023)	Loss 1.3950e-01 (9.9140e-02)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:00 - Epoch: [6][ 20/352]	Time  0.143 ( 0.164)	Data  0.002 ( 0.013)	Loss 1.1553e-01 (9.5985e-02)	Acc@1  95.31 ( 96.47)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:01 - Epoch: [6][ 30/352]	Time  0.157 ( 0.160)	Data  0.002 ( 0.009)	Loss 7.6308e-02 (9.7278e-02)	Acc@1  98.44 ( 96.65)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:03 - Epoch: [6][ 40/352]	Time  0.159 ( 0.159)	Data  0.003 ( 0.008)	Loss 1.2731e-01 (9.9956e-02)	Acc@1  92.97 ( 96.51)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:05 - Epoch: [6][ 50/352]	Time  0.157 ( 0.161)	Data  0.002 ( 0.007)	Loss 9.9399e-02 (1.0353e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:06 - Epoch: [6][ 60/352]	Time  0.142 ( 0.159)	Data  0.002 ( 0.006)	Loss 7.0821e-02 (1.0740e-01)	Acc@1  97.66 ( 96.32)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:08 - Epoch: [6][ 70/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.005)	Loss 1.7004e-01 (1.0669e-01)	Acc@1  93.75 ( 96.32)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:09 - Epoch: [6][ 80/352]	Time  0.142 ( 0.156)	Data  0.002 ( 0.005)	Loss 1.5059e-01 (1.0734e-01)	Acc@1  95.31 ( 96.34)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:11 - Epoch: [6][ 90/352]	Time  0.143 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.8997e-01 (1.0683e-01)	Acc@1  94.53 ( 96.36)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:12 - Epoch: [6][100/352]	Time  0.156 ( 0.156)	Data  0.001 ( 0.004)	Loss 1.3878e-01 (1.0749e-01)	Acc@1  95.31 ( 96.34)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:14 - Epoch: [6][110/352]	Time  0.156 ( 0.156)	Data  0.002 ( 0.004)	Loss 8.2528e-02 (1.0906e-01)	Acc@1  98.44 ( 96.30)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:52:15 - Epoch: [6][120/352]	Time  0.143 ( 0.155)	Data  0.002 ( 0.004)	Loss 1.0246e-01 (1.1147e-01)	Acc@1  95.31 ( 96.20)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:52:17 - Epoch: [6][130/352]	Time  0.168 ( 0.155)	Data  0.002 ( 0.004)	Loss 1.0513e-01 (1.1246e-01)	Acc@1  96.88 ( 96.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:52:18 - Epoch: [6][140/352]	Time  0.158 ( 0.155)	Data  0.002 ( 0.004)	Loss 1.1598e-01 (1.1132e-01)	Acc@1  96.88 ( 96.21)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:52:20 - Epoch: [6][150/352]	Time  0.157 ( 0.155)	Data  0.002 ( 0.003)	Loss 7.1115e-02 (1.1035e-01)	Acc@1  98.44 ( 96.22)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:52:22 - Epoch: [6][160/352]	Time  0.158 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.1972e-01 (1.1161e-01)	Acc@1  96.09 ( 96.17)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:23 - Epoch: [6][170/352]	Time  0.153 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.2362e-01 (1.1129e-01)	Acc@1  96.88 ( 96.20)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:25 - Epoch: [6][180/352]	Time  0.157 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.3027e-01 (1.1174e-01)	Acc@1  93.75 ( 96.18)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:26 - Epoch: [6][190/352]	Time  0.164 ( 0.156)	Data  0.002 ( 0.003)	Loss 4.4356e-02 (1.1118e-01)	Acc@1  98.44 ( 96.21)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:52:28 - Epoch: [6][200/352]	Time  0.149 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.4216e-02 (1.1029e-01)	Acc@1  96.88 ( 96.22)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:52:29 - Epoch: [6][210/352]	Time  0.158 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.7613e-02 (1.0981e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:52:31 - Epoch: [6][220/352]	Time  0.150 ( 0.156)	Data  0.002 ( 0.003)	Loss 4.7622e-02 (1.0928e-01)	Acc@1  99.22 ( 96.24)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:52:32 - Epoch: [6][230/352]	Time  0.143 ( 0.155)	Data  0.002 ( 0.003)	Loss 5.3372e-02 (1.0907e-01)	Acc@1  96.88 ( 96.24)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:34 - Epoch: [6][240/352]	Time  0.156 ( 0.155)	Data  0.002 ( 0.003)	Loss 7.9684e-02 (1.0842e-01)	Acc@1  97.66 ( 96.28)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:35 - Epoch: [6][250/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.6837e-01 (1.0857e-01)	Acc@1  94.53 ( 96.26)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:37 - Epoch: [6][260/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.1444e-01 (1.0857e-01)	Acc@1  94.53 ( 96.25)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:38 - Epoch: [6][270/352]	Time  0.143 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.2142e-01 (1.0903e-01)	Acc@1  96.09 ( 96.24)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:40 - Epoch: [6][280/352]	Time  0.166 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.0493e-01 (1.0894e-01)	Acc@1  97.66 ( 96.25)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:42 - Epoch: [6][290/352]	Time  0.168 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.2969e-01 (1.0935e-01)	Acc@1  94.53 ( 96.22)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:43 - Epoch: [6][300/352]	Time  0.166 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.5894e-02 (1.0919e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:45 - Epoch: [6][310/352]	Time  0.166 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.4270e-02 (1.0918e-01)	Acc@1  95.31 ( 96.20)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:47 - Epoch: [6][320/352]	Time  0.165 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.2074e-01 (1.0872e-01)	Acc@1  95.31 ( 96.22)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:48 - Epoch: [6][330/352]	Time  0.164 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.7344e-01 (1.0855e-01)	Acc@1  94.53 ( 96.22)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:50 - Epoch: [6][340/352]	Time  0.166 ( 0.158)	Data  0.002 ( 0.003)	Loss 2.1669e-01 (1.0890e-01)	Acc@1  95.31 ( 96.21)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:52 - Epoch: [6][350/352]	Time  0.166 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.0983e-01 (1.0878e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:52 - Test: [ 0/20]	Time  0.380 ( 0.380)	Loss 4.3238e-01 (4.3238e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.61 ( 99.61)
07-Mar-22 02:52:53 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 3.4207e-01 (4.1390e-01)	Acc@1  88.28 ( 87.89)	Acc@5  99.22 ( 99.36)
07-Mar-22 02:52:54 -  * Acc@1 88.180 Acc@5 99.360
07-Mar-22 02:52:54 - Best acc at epoch 6: 88.9000015258789
07-Mar-22 02:52:55 - Epoch: [7][  0/352]	Time  0.373 ( 0.373)	Data  0.224 ( 0.224)	Loss 7.3358e-02 (7.3358e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:56 - Epoch: [7][ 10/352]	Time  0.165 ( 0.183)	Data  0.002 ( 0.022)	Loss 1.5724e-01 (1.0043e-01)	Acc@1  94.53 ( 96.24)	Acc@5 100.00 ( 99.93)
07-Mar-22 02:52:58 - Epoch: [7][ 20/352]	Time  0.173 ( 0.176)	Data  0.003 ( 0.013)	Loss 1.1571e-01 (1.0021e-01)	Acc@1  96.09 ( 96.47)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:53:00 - Epoch: [7][ 30/352]	Time  0.171 ( 0.173)	Data  0.002 ( 0.009)	Loss 1.1291e-01 (1.0294e-01)	Acc@1  93.75 ( 96.37)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:01 - Epoch: [7][ 40/352]	Time  0.164 ( 0.172)	Data  0.002 ( 0.008)	Loss 3.1198e-01 (1.1180e-01)	Acc@1  88.28 ( 96.11)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:53:03 - Epoch: [7][ 50/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.006)	Loss 1.0085e-01 (1.1315e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:05 - Epoch: [7][ 60/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.006)	Loss 1.2628e-01 (1.1272e-01)	Acc@1  94.53 ( 95.99)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:06 - Epoch: [7][ 70/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.2149e-01 (1.1204e-01)	Acc@1  95.31 ( 95.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:08 - Epoch: [7][ 80/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.005)	Loss 8.0795e-02 (1.1339e-01)	Acc@1  96.88 ( 95.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:10 - Epoch: [7][ 90/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.0853e-01 (1.1269e-01)	Acc@1  96.09 ( 96.02)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:11 - Epoch: [7][100/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.3295e-01 (1.1396e-01)	Acc@1  94.53 ( 95.95)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:13 - Epoch: [7][110/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.2303e-02 (1.0990e-01)	Acc@1  98.44 ( 96.10)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:15 - Epoch: [7][120/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.8521e-02 (1.0993e-01)	Acc@1  98.44 ( 96.10)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:16 - Epoch: [7][130/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.8872e-01 (1.1102e-01)	Acc@1  94.53 ( 96.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:18 - Epoch: [7][140/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.7250e-02 (1.1106e-01)	Acc@1  97.66 ( 96.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:20 - Epoch: [7][150/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.8212e-01 (1.1066e-01)	Acc@1  93.75 ( 96.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:21 - Epoch: [7][160/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4782e-01 (1.1222e-01)	Acc@1  96.09 ( 96.05)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:23 - Epoch: [7][170/352]	Time  0.141 ( 0.168)	Data  0.001 ( 0.003)	Loss 9.9704e-02 (1.1087e-01)	Acc@1  95.31 ( 96.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:25 - Epoch: [7][180/352]	Time  0.141 ( 0.166)	Data  0.001 ( 0.003)	Loss 1.1502e-01 (1.1202e-01)	Acc@1  96.88 ( 96.09)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:26 - Epoch: [7][190/352]	Time  0.166 ( 0.166)	Data  0.001 ( 0.003)	Loss 6.5999e-02 (1.1042e-01)	Acc@1  96.88 ( 96.15)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:28 - Epoch: [7][200/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0571e-01 (1.1127e-01)	Acc@1  96.09 ( 96.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:30 - Epoch: [7][210/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.2386e-01 (1.1100e-01)	Acc@1  95.31 ( 96.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:31 - Epoch: [7][220/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.5613e-02 (1.1128e-01)	Acc@1  97.66 ( 96.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:33 - Epoch: [7][230/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.6952e-01 (1.1137e-01)	Acc@1  94.53 ( 96.13)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:35 - Epoch: [7][240/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.6700e-01 (1.1131e-01)	Acc@1  93.75 ( 96.11)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:53:36 - Epoch: [7][250/352]	Time  0.170 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.1506e-02 (1.1053e-01)	Acc@1  97.66 ( 96.13)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:38 - Epoch: [7][260/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.2395e-01 (1.1097e-01)	Acc@1  96.09 ( 96.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:39 - Epoch: [7][270/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.9438e-02 (1.1061e-01)	Acc@1  96.09 ( 96.15)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:41 - Epoch: [7][280/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.3471e-01 (1.1081e-01)	Acc@1  96.09 ( 96.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:43 - Epoch: [7][290/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0575e-01 (1.1078e-01)	Acc@1  96.09 ( 96.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:44 - Epoch: [7][300/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.6306e-01 (1.1085e-01)	Acc@1  94.53 ( 96.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:46 - Epoch: [7][310/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.1800e-02 (1.1074e-01)	Acc@1  98.44 ( 96.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:48 - Epoch: [7][320/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.4022e-01 (1.1115e-01)	Acc@1  93.75 ( 96.09)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:49 - Epoch: [7][330/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1870e-01 (1.1093e-01)	Acc@1  95.31 ( 96.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:51 - Epoch: [7][340/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 4.8221e-02 (1.1108e-01)	Acc@1  98.44 ( 96.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:53 - Epoch: [7][350/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1797e-01 (1.1114e-01)	Acc@1  97.66 ( 96.10)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:53:53 - Test: [ 0/20]	Time  0.498 ( 0.498)	Loss 3.8326e-01 (3.8326e-01)	Acc@1  89.06 ( 89.06)	Acc@5  98.83 ( 98.83)
07-Mar-22 02:53:54 - Test: [10/20]	Time  0.098 ( 0.136)	Loss 3.1705e-01 (4.0499e-01)	Acc@1  91.41 ( 88.46)	Acc@5  99.22 ( 99.43)
07-Mar-22 02:53:55 -  * Acc@1 88.840 Acc@5 99.400
07-Mar-22 02:53:55 - Best acc at epoch 7: 88.9000015258789
07-Mar-22 02:53:56 - Epoch: [8][  0/352]	Time  0.393 ( 0.393)	Data  0.234 ( 0.234)	Loss 7.5397e-02 (7.5397e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 02:53:58 - Epoch: [8][ 10/352]	Time  0.167 ( 0.188)	Data  0.002 ( 0.023)	Loss 1.3058e-01 (1.0984e-01)	Acc@1  92.97 ( 95.74)	Acc@5 100.00 (100.00)
07-Mar-22 02:53:59 - Epoch: [8][ 20/352]	Time  0.169 ( 0.178)	Data  0.002 ( 0.013)	Loss 8.5855e-02 (1.0854e-01)	Acc@1  97.66 ( 95.98)	Acc@5 100.00 (100.00)
07-Mar-22 02:54:01 - Epoch: [8][ 30/352]	Time  0.164 ( 0.175)	Data  0.002 ( 0.009)	Loss 7.7915e-02 (1.0506e-01)	Acc@1  97.66 ( 96.27)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:02 - Epoch: [8][ 40/352]	Time  0.146 ( 0.169)	Data  0.002 ( 0.008)	Loss 5.9924e-02 (1.0688e-01)	Acc@1  98.44 ( 96.27)	Acc@5 100.00 ( 99.94)
07-Mar-22 02:54:04 - Epoch: [8][ 50/352]	Time  0.141 ( 0.164)	Data  0.001 ( 0.006)	Loss 1.0735e-01 (1.0712e-01)	Acc@1  95.31 ( 96.29)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:54:05 - Epoch: [8][ 60/352]	Time  0.140 ( 0.160)	Data  0.001 ( 0.006)	Loss 8.1442e-02 (1.0458e-01)	Acc@1  95.31 ( 96.36)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:54:07 - Epoch: [8][ 70/352]	Time  0.140 ( 0.157)	Data  0.001 ( 0.005)	Loss 5.3759e-02 (1.0206e-01)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:08 - Epoch: [8][ 80/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.005)	Loss 1.3916e-01 (1.0299e-01)	Acc@1  93.75 ( 96.41)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:10 - Epoch: [8][ 90/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.004)	Loss 1.0246e-01 (1.0461e-01)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:12 - Epoch: [8][100/352]	Time  0.175 ( 0.159)	Data  0.002 ( 0.004)	Loss 1.2843e-01 (1.0374e-01)	Acc@1  95.31 ( 96.47)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:13 - Epoch: [8][110/352]	Time  0.165 ( 0.160)	Data  0.002 ( 0.004)	Loss 1.0292e-01 (1.0386e-01)	Acc@1  95.31 ( 96.43)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:54:15 - Epoch: [8][120/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.004)	Loss 1.2360e-01 (1.0573e-01)	Acc@1  94.53 ( 96.33)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:16 - Epoch: [8][130/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.004)	Loss 7.4471e-02 (1.0607e-01)	Acc@1  97.66 ( 96.30)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:18 - Epoch: [8][140/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.004)	Loss 9.8252e-02 (1.0553e-01)	Acc@1  96.09 ( 96.32)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:20 - Epoch: [8][150/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.003)	Loss 5.4466e-02 (1.0469e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:21 - Epoch: [8][160/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.7444e-02 (1.0413e-01)	Acc@1  98.44 ( 96.39)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:23 - Epoch: [8][170/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.5384e-01 (1.0425e-01)	Acc@1  93.75 ( 96.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:25 - Epoch: [8][180/352]	Time  0.168 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.3371e-02 (1.0426e-01)	Acc@1  98.44 ( 96.38)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:26 - Epoch: [8][190/352]	Time  0.141 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.9381e-02 (1.0435e-01)	Acc@1  98.44 ( 96.41)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:28 - Epoch: [8][200/352]	Time  0.140 ( 0.161)	Data  0.001 ( 0.003)	Loss 1.0037e-01 (1.0433e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:29 - Epoch: [8][210/352]	Time  0.141 ( 0.160)	Data  0.001 ( 0.003)	Loss 1.2895e-01 (1.0396e-01)	Acc@1  93.75 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:31 - Epoch: [8][220/352]	Time  0.141 ( 0.159)	Data  0.001 ( 0.003)	Loss 1.5891e-01 (1.0438e-01)	Acc@1  93.75 ( 96.37)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:32 - Epoch: [8][230/352]	Time  0.141 ( 0.158)	Data  0.001 ( 0.003)	Loss 1.3612e-01 (1.0540e-01)	Acc@1  96.88 ( 96.35)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:34 - Epoch: [8][240/352]	Time  0.167 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.0916e-02 (1.0477e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:35 - Epoch: [8][250/352]	Time  0.166 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.0406e-01 (1.0502e-01)	Acc@1  96.88 ( 96.34)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:37 - Epoch: [8][260/352]	Time  0.166 ( 0.159)	Data  0.002 ( 0.003)	Loss 6.9372e-02 (1.0458e-01)	Acc@1  98.44 ( 96.36)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:38 - Epoch: [8][270/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.003)	Loss 2.0958e-01 (1.0543e-01)	Acc@1  92.19 ( 96.34)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:40 - Epoch: [8][280/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.2355e-01 (1.0541e-01)	Acc@1  93.75 ( 96.33)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:41 - Epoch: [8][290/352]	Time  0.142 ( 0.157)	Data  0.002 ( 0.003)	Loss 9.4640e-02 (1.0541e-01)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:43 - Epoch: [8][300/352]	Time  0.166 ( 0.157)	Data  0.002 ( 0.003)	Loss 6.7193e-02 (1.0519e-01)	Acc@1  98.44 ( 96.34)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:44 - Epoch: [8][310/352]	Time  0.165 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.0103e-01 (1.0532e-01)	Acc@1  96.09 ( 96.34)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:46 - Epoch: [8][320/352]	Time  0.166 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.1527e-01 (1.0587e-01)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:48 - Epoch: [8][330/352]	Time  0.165 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.2113e-01 (1.0557e-01)	Acc@1  96.09 ( 96.33)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:49 - Epoch: [8][340/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.2760e-01 (1.0571e-01)	Acc@1  96.88 ( 96.34)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:51 - Epoch: [8][350/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.0827e-01 (1.0615e-01)	Acc@1  95.31 ( 96.32)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:52 - Test: [ 0/20]	Time  0.363 ( 0.363)	Loss 3.7319e-01 (3.7319e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.61 ( 99.61)
07-Mar-22 02:54:53 - Test: [10/20]	Time  0.098 ( 0.126)	Loss 3.9658e-01 (4.0719e-01)	Acc@1  87.89 ( 88.25)	Acc@5  98.83 ( 99.50)
07-Mar-22 02:54:54 -  * Acc@1 88.620 Acc@5 99.440
07-Mar-22 02:54:54 - Best acc at epoch 8: 88.9000015258789
07-Mar-22 02:54:54 - Epoch: [9][  0/352]	Time  0.396 ( 0.396)	Data  0.231 ( 0.231)	Loss 5.3302e-02 (5.3302e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
07-Mar-22 02:54:56 - Epoch: [9][ 10/352]	Time  0.171 ( 0.188)	Data  0.002 ( 0.023)	Loss 9.8116e-02 (1.1636e-01)	Acc@1  98.44 ( 96.45)	Acc@5  99.22 ( 99.93)
07-Mar-22 02:54:57 - Epoch: [9][ 20/352]	Time  0.142 ( 0.169)	Data  0.002 ( 0.013)	Loss 7.9407e-02 (1.0700e-01)	Acc@1  97.66 ( 96.32)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:54:59 - Epoch: [9][ 30/352]	Time  0.141 ( 0.160)	Data  0.002 ( 0.009)	Loss 7.3920e-02 (1.0108e-01)	Acc@1  96.88 ( 96.35)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:55:00 - Epoch: [9][ 40/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.007)	Loss 3.4497e-02 (9.8892e-02)	Acc@1 100.00 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:02 - Epoch: [9][ 50/352]	Time  0.143 ( 0.154)	Data  0.002 ( 0.006)	Loss 9.2972e-02 (9.6987e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:03 - Epoch: [9][ 60/352]	Time  0.142 ( 0.152)	Data  0.001 ( 0.006)	Loss 1.9594e-01 (9.8685e-02)	Acc@1  92.19 ( 96.44)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:04 - Epoch: [9][ 70/352]	Time  0.142 ( 0.150)	Data  0.002 ( 0.005)	Loss 1.0866e-01 (9.8423e-02)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:06 - Epoch: [9][ 80/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.005)	Loss 6.9205e-02 (9.8501e-02)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:07 - Epoch: [9][ 90/352]	Time  0.165 ( 0.149)	Data  0.002 ( 0.004)	Loss 2.0183e-01 (9.8657e-02)	Acc@1  90.62 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:09 - Epoch: [9][100/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.004)	Loss 1.1419e-01 (9.9817e-02)	Acc@1  96.09 ( 96.38)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:10 - Epoch: [9][110/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.0921e-02 (9.8142e-02)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:12 - Epoch: [9][120/352]	Time  0.143 ( 0.148)	Data  0.002 ( 0.004)	Loss 1.3029e-01 (9.8457e-02)	Acc@1  92.97 ( 96.41)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:13 - Epoch: [9][130/352]	Time  0.143 ( 0.148)	Data  0.002 ( 0.004)	Loss 1.7670e-01 (9.9408e-02)	Acc@1  92.97 ( 96.36)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:15 - Epoch: [9][140/352]	Time  0.143 ( 0.148)	Data  0.002 ( 0.003)	Loss 5.8302e-02 (1.0010e-01)	Acc@1  98.44 ( 96.34)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:16 - Epoch: [9][150/352]	Time  0.142 ( 0.147)	Data  0.002 ( 0.003)	Loss 1.1948e-01 (9.9305e-02)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:17 - Epoch: [9][160/352]	Time  0.142 ( 0.147)	Data  0.002 ( 0.003)	Loss 1.7919e-01 (9.9827e-02)	Acc@1  91.41 ( 96.37)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:19 - Epoch: [9][170/352]	Time  0.141 ( 0.147)	Data  0.001 ( 0.003)	Loss 7.8314e-02 (9.9611e-02)	Acc@1  97.66 ( 96.37)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:20 - Epoch: [9][180/352]	Time  0.144 ( 0.147)	Data  0.002 ( 0.003)	Loss 1.6091e-01 (1.0098e-01)	Acc@1  94.53 ( 96.29)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:55:22 - Epoch: [9][190/352]	Time  0.142 ( 0.147)	Data  0.002 ( 0.003)	Loss 9.6006e-02 (1.0122e-01)	Acc@1  95.31 ( 96.27)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:23 - Epoch: [9][200/352]	Time  0.143 ( 0.146)	Data  0.002 ( 0.003)	Loss 8.5899e-02 (1.0167e-01)	Acc@1  96.09 ( 96.23)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:25 - Epoch: [9][210/352]	Time  0.151 ( 0.147)	Data  0.002 ( 0.003)	Loss 7.2681e-02 (1.0047e-01)	Acc@1  97.66 ( 96.28)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:26 - Epoch: [9][220/352]	Time  0.141 ( 0.147)	Data  0.002 ( 0.003)	Loss 9.9178e-02 (1.0078e-01)	Acc@1  97.66 ( 96.28)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:55:27 - Epoch: [9][230/352]	Time  0.141 ( 0.146)	Data  0.002 ( 0.003)	Loss 9.0871e-02 (1.0104e-01)	Acc@1  97.66 ( 96.27)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:29 - Epoch: [9][240/352]	Time  0.165 ( 0.147)	Data  0.002 ( 0.003)	Loss 4.8228e-02 (1.0072e-01)	Acc@1  98.44 ( 96.29)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:31 - Epoch: [9][250/352]	Time  0.167 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.1890e-02 (1.0077e-01)	Acc@1  96.09 ( 96.30)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:32 - Epoch: [9][260/352]	Time  0.163 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1990e-01 (1.0082e-01)	Acc@1  96.09 ( 96.32)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:34 - Epoch: [9][270/352]	Time  0.165 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.6451e-02 (1.0096e-01)	Acc@1  97.66 ( 96.31)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:36 - Epoch: [9][280/352]	Time  0.187 ( 0.150)	Data  0.003 ( 0.003)	Loss 2.0322e-01 (1.0133e-01)	Acc@1  91.41 ( 96.29)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:37 - Epoch: [9][290/352]	Time  0.152 ( 0.150)	Data  0.002 ( 0.003)	Loss 2.4205e-01 (1.0221e-01)	Acc@1  93.75 ( 96.28)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:39 - Epoch: [9][300/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.003)	Loss 3.6004e-02 (1.0165e-01)	Acc@1  99.22 ( 96.30)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:40 - Epoch: [9][310/352]	Time  0.167 ( 0.150)	Data  0.002 ( 0.003)	Loss 5.3362e-02 (1.0122e-01)	Acc@1  98.44 ( 96.32)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:42 - Epoch: [9][320/352]	Time  0.168 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.1083e-01 (1.0171e-01)	Acc@1  96.09 ( 96.32)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:44 - Epoch: [9][330/352]	Time  0.171 ( 0.151)	Data  0.002 ( 0.003)	Loss 6.5085e-02 (1.0201e-01)	Acc@1  98.44 ( 96.29)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:45 - Epoch: [9][340/352]	Time  0.168 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.2376e-01 (1.0201e-01)	Acc@1  94.53 ( 96.28)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:47 - Epoch: [9][350/352]	Time  0.166 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.5783e-01 (1.0230e-01)	Acc@1  94.53 ( 96.27)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:48 - Test: [ 0/20]	Time  0.353 ( 0.353)	Loss 3.6346e-01 (3.6346e-01)	Acc@1  88.67 ( 88.67)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:55:49 - Test: [10/20]	Time  0.113 ( 0.126)	Loss 3.2340e-01 (3.9327e-01)	Acc@1  89.06 ( 88.07)	Acc@5  99.61 ( 99.43)
07-Mar-22 02:55:50 -  * Acc@1 88.580 Acc@5 99.360
07-Mar-22 02:55:50 - Best acc at epoch 9: 88.9000015258789
07-Mar-22 02:55:50 - Epoch: [10][  0/352]	Time  0.409 ( 0.409)	Data  0.264 ( 0.264)	Loss 1.8460e-01 (1.8460e-01)	Acc@1  96.09 ( 96.09)	Acc@5  98.44 ( 98.44)
07-Mar-22 02:55:52 - Epoch: [10][ 10/352]	Time  0.154 ( 0.182)	Data  0.002 ( 0.026)	Loss 1.0346e-01 (1.1432e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.86)
07-Mar-22 02:55:53 - Epoch: [10][ 20/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.014)	Loss 7.4532e-02 (1.0777e-01)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.93)
07-Mar-22 02:55:55 - Epoch: [10][ 30/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.011)	Loss 1.1520e-01 (1.0606e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:55:57 - Epoch: [10][ 40/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.009)	Loss 1.2154e-01 (1.0374e-01)	Acc@1  94.53 ( 96.42)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:55:59 - Epoch: [10][ 50/352]	Time  0.173 ( 0.168)	Data  0.002 ( 0.007)	Loss 1.2862e-01 (1.0729e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:00 - Epoch: [10][ 60/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.007)	Loss 7.9608e-02 (1.0620e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:56:02 - Epoch: [10][ 70/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.006)	Loss 8.8770e-02 (1.0778e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:04 - Epoch: [10][ 80/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.005)	Loss 5.4177e-02 (1.0606e-01)	Acc@1  98.44 ( 96.33)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:05 - Epoch: [10][ 90/352]	Time  0.130 ( 0.166)	Data  0.001 ( 0.005)	Loss 5.9837e-02 (1.0282e-01)	Acc@1  98.44 ( 96.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:07 - Epoch: [10][100/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.005)	Loss 8.4011e-02 (1.0254e-01)	Acc@1  97.66 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:08 - Epoch: [10][110/352]	Time  0.159 ( 0.165)	Data  0.002 ( 0.005)	Loss 1.4524e-01 (1.0223e-01)	Acc@1  95.31 ( 96.40)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:10 - Epoch: [10][120/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.004)	Loss 5.2823e-02 (1.0103e-01)	Acc@1  97.66 ( 96.46)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:12 - Epoch: [10][130/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.004)	Loss 7.8741e-02 (1.0075e-01)	Acc@1  96.09 ( 96.49)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:13 - Epoch: [10][140/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.004)	Loss 1.5074e-01 (1.0044e-01)	Acc@1  92.97 ( 96.45)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:15 - Epoch: [10][150/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.004)	Loss 7.2376e-02 (9.8800e-02)	Acc@1  96.88 ( 96.53)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:17 - Epoch: [10][160/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.004)	Loss 1.1622e-01 (1.0053e-01)	Acc@1  94.53 ( 96.47)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:18 - Epoch: [10][170/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.1722e-01 (1.0063e-01)	Acc@1  94.53 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:20 - Epoch: [10][180/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.4432e-01 (1.0009e-01)	Acc@1  94.53 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:22 - Epoch: [10][190/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.3151e-02 (9.9254e-02)	Acc@1  95.31 ( 96.44)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:23 - Epoch: [10][200/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1785e-01 (9.9918e-02)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:25 - Epoch: [10][210/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.1461e-02 (9.9782e-02)	Acc@1  98.44 ( 96.45)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:27 - Epoch: [10][220/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.4261e-02 (9.9850e-02)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:28 - Epoch: [10][230/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 2.1873e-02 (9.9905e-02)	Acc@1 100.00 ( 96.45)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:30 - Epoch: [10][240/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.4609e-01 (1.0056e-01)	Acc@1  92.97 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:32 - Epoch: [10][250/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 4.9207e-02 (1.0041e-01)	Acc@1  99.22 ( 96.45)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:33 - Epoch: [10][260/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.1891e-02 (1.0057e-01)	Acc@1  98.44 ( 96.44)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:35 - Epoch: [10][270/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.0152e-02 (1.0075e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:37 - Epoch: [10][280/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.8896e-02 (1.0074e-01)	Acc@1  96.09 ( 96.44)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:38 - Epoch: [10][290/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0009e-01 (1.0103e-01)	Acc@1  95.31 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:40 - Epoch: [10][300/352]	Time  0.170 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.5433e-01 (1.0169e-01)	Acc@1  95.31 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:42 - Epoch: [10][310/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.5748e-01 (1.0227e-01)	Acc@1  93.75 ( 96.38)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:43 - Epoch: [10][320/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.3670e-02 (1.0221e-01)	Acc@1  98.44 ( 96.37)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:45 - Epoch: [10][330/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.0965e-02 (1.0197e-01)	Acc@1  96.88 ( 96.37)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:47 - Epoch: [10][340/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.6692e-01 (1.0164e-01)	Acc@1  92.97 ( 96.38)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:48 - Epoch: [10][350/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.3699e-01 (1.0156e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:49 - Test: [ 0/20]	Time  0.384 ( 0.384)	Loss 3.5009e-01 (3.5009e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:56:50 - Test: [10/20]	Time  0.120 ( 0.128)	Loss 3.6599e-01 (3.9475e-01)	Acc@1  89.06 ( 88.67)	Acc@5  98.83 ( 99.47)
07-Mar-22 02:56:51 -  * Acc@1 89.000 Acc@5 99.360
07-Mar-22 02:56:51 - Best acc at epoch 10: 89.0
07-Mar-22 02:56:52 - Epoch: [11][  0/352]	Time  0.394 ( 0.394)	Data  0.227 ( 0.227)	Loss 9.4217e-02 (9.4217e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 02:56:53 - Epoch: [11][ 10/352]	Time  0.168 ( 0.186)	Data  0.002 ( 0.023)	Loss 1.4346e-01 (9.5861e-02)	Acc@1  93.75 ( 96.52)	Acc@5 100.00 (100.00)
07-Mar-22 02:56:55 - Epoch: [11][ 20/352]	Time  0.142 ( 0.173)	Data  0.002 ( 0.013)	Loss 1.4159e-01 (1.0546e-01)	Acc@1  94.53 ( 95.98)	Acc@5 100.00 (100.00)
07-Mar-22 02:56:56 - Epoch: [11][ 30/352]	Time  0.142 ( 0.163)	Data  0.001 ( 0.009)	Loss 1.4257e-01 (1.0704e-01)	Acc@1  93.75 ( 95.94)	Acc@5 100.00 (100.00)
07-Mar-22 02:56:58 - Epoch: [11][ 40/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.007)	Loss 7.3356e-02 (1.0501e-01)	Acc@1  98.44 ( 96.17)	Acc@5 100.00 (100.00)
07-Mar-22 02:56:59 - Epoch: [11][ 50/352]	Time  0.142 ( 0.155)	Data  0.002 ( 0.006)	Loss 1.5269e-01 (1.0538e-01)	Acc@1  93.75 ( 96.20)	Acc@5 100.00 (100.00)
07-Mar-22 02:57:00 - Epoch: [11][ 60/352]	Time  0.168 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.2490e-02 (1.0181e-01)	Acc@1  96.09 ( 96.34)	Acc@5 100.00 (100.00)
07-Mar-22 02:57:02 - Epoch: [11][ 70/352]	Time  0.168 ( 0.156)	Data  0.002 ( 0.005)	Loss 9.6058e-02 (1.0090e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:04 - Epoch: [11][ 80/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.005)	Loss 1.6795e-01 (1.0143e-01)	Acc@1  93.75 ( 96.33)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:06 - Epoch: [11][ 90/352]	Time  0.169 ( 0.158)	Data  0.002 ( 0.004)	Loss 2.4297e-01 (1.0296e-01)	Acc@1  93.75 ( 96.30)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:07 - Epoch: [11][100/352]	Time  0.168 ( 0.159)	Data  0.002 ( 0.004)	Loss 8.1657e-02 (1.0265e-01)	Acc@1  96.88 ( 96.27)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:09 - Epoch: [11][110/352]	Time  0.167 ( 0.160)	Data  0.002 ( 0.004)	Loss 5.8632e-02 (1.0173e-01)	Acc@1  97.66 ( 96.28)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:11 - Epoch: [11][120/352]	Time  0.167 ( 0.160)	Data  0.002 ( 0.004)	Loss 5.8809e-02 (1.0144e-01)	Acc@1  98.44 ( 96.33)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:12 - Epoch: [11][130/352]	Time  0.168 ( 0.161)	Data  0.002 ( 0.004)	Loss 7.8307e-02 (1.0036e-01)	Acc@1  97.66 ( 96.39)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:14 - Epoch: [11][140/352]	Time  0.165 ( 0.161)	Data  0.002 ( 0.004)	Loss 1.3226e-01 (1.0017e-01)	Acc@1  94.53 ( 96.41)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:16 - Epoch: [11][150/352]	Time  0.165 ( 0.162)	Data  0.002 ( 0.004)	Loss 7.9213e-02 (1.0106e-01)	Acc@1  96.88 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:17 - Epoch: [11][160/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 9.0444e-02 (9.9860e-02)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:19 - Epoch: [11][170/352]	Time  0.165 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.1578e-01 (9.9003e-02)	Acc@1  95.31 ( 96.45)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:57:21 - Epoch: [11][180/352]	Time  0.168 ( 0.163)	Data  0.002 ( 0.003)	Loss 5.8414e-02 (9.8164e-02)	Acc@1  98.44 ( 96.48)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:22 - Epoch: [11][190/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.003)	Loss 5.9125e-02 (9.7648e-02)	Acc@1  98.44 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:24 - Epoch: [11][200/352]	Time  0.168 ( 0.163)	Data  0.002 ( 0.003)	Loss 9.3210e-02 (9.8099e-02)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:26 - Epoch: [11][210/352]	Time  0.164 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.2753e-01 (9.8299e-02)	Acc@1  95.31 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:27 - Epoch: [11][220/352]	Time  0.167 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1653e-01 (9.7934e-02)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:29 - Epoch: [11][230/352]	Time  0.169 ( 0.164)	Data  0.002 ( 0.003)	Loss 4.7006e-02 (9.8180e-02)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:31 - Epoch: [11][240/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.0338e-01 (9.8645e-02)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:32 - Epoch: [11][250/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.2492e-01 (9.8715e-02)	Acc@1  95.31 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:34 - Epoch: [11][260/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.7705e-02 (9.8609e-02)	Acc@1  98.44 ( 96.52)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:36 - Epoch: [11][270/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.4007e-01 (9.9323e-02)	Acc@1  93.75 ( 96.48)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:37 - Epoch: [11][280/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.9260e-01 (9.9665e-02)	Acc@1  95.31 ( 96.49)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:39 - Epoch: [11][290/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.7223e-01 (9.9394e-02)	Acc@1  94.53 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:41 - Epoch: [11][300/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.4577e-01 (9.9915e-02)	Acc@1  96.88 ( 96.49)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:42 - Epoch: [11][310/352]	Time  0.165 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.1463e-01 (1.0030e-01)	Acc@1  96.09 ( 96.47)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:57:44 - Epoch: [11][320/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.5829e-01 (1.0057e-01)	Acc@1  93.75 ( 96.46)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:46 - Epoch: [11][330/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.1608e-01 (1.0130e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:47 - Epoch: [11][340/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.2565e-01 (1.0175e-01)	Acc@1  96.88 ( 96.43)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:57:49 - Epoch: [11][350/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.3666e-01 (1.0177e-01)	Acc@1  93.75 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:50 - Test: [ 0/20]	Time  0.366 ( 0.366)	Loss 3.3386e-01 (3.3386e-01)	Acc@1  91.02 ( 91.02)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:57:51 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 3.7670e-01 (4.0056e-01)	Acc@1  89.84 ( 88.39)	Acc@5  99.22 ( 99.50)
07-Mar-22 02:57:51 -  * Acc@1 89.020 Acc@5 99.460
07-Mar-22 02:57:52 - Best acc at epoch 11: 89.0199966430664
07-Mar-22 02:57:52 - Epoch: [12][  0/352]	Time  0.386 ( 0.386)	Data  0.241 ( 0.241)	Loss 1.0227e-01 (1.0227e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 02:57:54 - Epoch: [12][ 10/352]	Time  0.151 ( 0.177)	Data  0.002 ( 0.024)	Loss 6.1623e-02 (8.9903e-02)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 (100.00)
07-Mar-22 02:57:55 - Epoch: [12][ 20/352]	Time  0.161 ( 0.168)	Data  0.003 ( 0.013)	Loss 7.9204e-02 (9.4798e-02)	Acc@1  97.66 ( 96.73)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:57:57 - Epoch: [12][ 30/352]	Time  0.157 ( 0.168)	Data  0.002 ( 0.010)	Loss 1.0111e-01 (9.9129e-02)	Acc@1  95.31 ( 96.67)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:57:58 - Epoch: [12][ 40/352]	Time  0.162 ( 0.165)	Data  0.002 ( 0.008)	Loss 9.0243e-02 (9.8836e-02)	Acc@1  97.66 ( 96.65)	Acc@5 100.00 ( 99.94)
07-Mar-22 02:58:00 - Epoch: [12][ 50/352]	Time  0.141 ( 0.162)	Data  0.001 ( 0.007)	Loss 4.8575e-02 (1.0261e-01)	Acc@1  98.44 ( 96.32)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:58:01 - Epoch: [12][ 60/352]	Time  0.161 ( 0.160)	Data  0.002 ( 0.006)	Loss 1.1605e-01 (1.0419e-01)	Acc@1  96.09 ( 96.25)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:58:03 - Epoch: [12][ 70/352]	Time  0.155 ( 0.160)	Data  0.002 ( 0.005)	Loss 1.1519e-01 (1.0164e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:58:04 - Epoch: [12][ 80/352]	Time  0.147 ( 0.158)	Data  0.002 ( 0.005)	Loss 7.2888e-02 (1.0162e-01)	Acc@1  98.44 ( 96.36)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:58:06 - Epoch: [12][ 90/352]	Time  0.142 ( 0.157)	Data  0.002 ( 0.005)	Loss 1.2448e-01 (1.0139e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:07 - Epoch: [12][100/352]	Time  0.157 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.1526e-01 (1.0008e-01)	Acc@1  97.66 ( 96.53)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:09 - Epoch: [12][110/352]	Time  0.142 ( 0.155)	Data  0.002 ( 0.004)	Loss 9.9576e-02 (9.9085e-02)	Acc@1  94.53 ( 96.57)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:10 - Epoch: [12][120/352]	Time  0.163 ( 0.155)	Data  0.002 ( 0.004)	Loss 8.3046e-02 (9.9329e-02)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:12 - Epoch: [12][130/352]	Time  0.143 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.3748e-01 (9.8916e-02)	Acc@1  97.66 ( 96.61)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:58:13 - Epoch: [12][140/352]	Time  0.160 ( 0.154)	Data  0.002 ( 0.004)	Loss 5.7215e-02 (9.9210e-02)	Acc@1  99.22 ( 96.62)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:58:15 - Epoch: [12][150/352]	Time  0.142 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.2662e-01 (1.0033e-01)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:17 - Epoch: [12][160/352]	Time  0.166 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.4971e-01 (1.0152e-01)	Acc@1  94.53 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:58:18 - Epoch: [12][170/352]	Time  0.167 ( 0.156)	Data  0.002 ( 0.003)	Loss 2.1514e-01 (1.0208e-01)	Acc@1  91.41 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:58:20 - Epoch: [12][180/352]	Time  0.172 ( 0.156)	Data  0.002 ( 0.003)	Loss 2.0605e-01 (1.0296e-01)	Acc@1  95.31 ( 96.48)	Acc@5  99.22 ( 99.97)
07-Mar-22 02:58:22 - Epoch: [12][190/352]	Time  0.166 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.1958e-01 (1.0341e-01)	Acc@1  96.09 ( 96.44)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:23 - Epoch: [12][200/352]	Time  0.167 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.0771e-01 (1.0370e-01)	Acc@1  98.44 ( 96.45)	Acc@5  99.22 ( 99.97)
07-Mar-22 02:58:25 - Epoch: [12][210/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.003)	Loss 7.5023e-02 (1.0318e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:27 - Epoch: [12][220/352]	Time  0.165 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.3379e-02 (1.0316e-01)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:28 - Epoch: [12][230/352]	Time  0.166 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.5184e-01 (1.0306e-01)	Acc@1  92.19 ( 96.46)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:30 - Epoch: [12][240/352]	Time  0.167 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.4391e-01 (1.0373e-01)	Acc@1  93.75 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:32 - Epoch: [12][250/352]	Time  0.163 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.6596e-01 (1.0346e-01)	Acc@1  92.97 ( 96.42)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:33 - Epoch: [12][260/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.003)	Loss 5.5015e-02 (1.0324e-01)	Acc@1  96.88 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:35 - Epoch: [12][270/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.003)	Loss 9.1226e-02 (1.0322e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:37 - Epoch: [12][280/352]	Time  0.146 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.1634e-01 (1.0354e-01)	Acc@1  94.53 ( 96.41)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:38 - Epoch: [12][290/352]	Time  0.154 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.3895e-01 (1.0290e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:40 - Epoch: [12][300/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.003)	Loss 8.5865e-02 (1.0264e-01)	Acc@1  95.31 ( 96.42)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:41 - Epoch: [12][310/352]	Time  0.161 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.0521e-01 (1.0274e-01)	Acc@1  95.31 ( 96.41)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:43 - Epoch: [12][320/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.1273e-01 (1.0289e-01)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:45 - Epoch: [12][330/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.4843e-01 (1.0285e-01)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:46 - Epoch: [12][340/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.4682e-01 (1.0292e-01)	Acc@1  96.88 ( 96.41)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:48 - Epoch: [12][350/352]	Time  0.166 ( 0.161)	Data  0.001 ( 0.003)	Loss 9.8580e-02 (1.0279e-01)	Acc@1  96.88 ( 96.41)	Acc@5  99.22 ( 99.97)
07-Mar-22 02:58:49 - Test: [ 0/20]	Time  0.370 ( 0.370)	Loss 3.3922e-01 (3.3922e-01)	Acc@1  89.45 ( 89.45)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:58:50 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 3.8441e-01 (3.9572e-01)	Acc@1  87.89 ( 88.57)	Acc@5  99.22 ( 99.57)
07-Mar-22 02:58:51 -  * Acc@1 88.980 Acc@5 99.540
07-Mar-22 02:58:51 - Best acc at epoch 12: 89.0199966430664
07-Mar-22 02:58:51 - Epoch: [13][  0/352]	Time  0.392 ( 0.392)	Data  0.234 ( 0.234)	Loss 9.3384e-02 (9.3384e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 02:58:53 - Epoch: [13][ 10/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.023)	Loss 8.9570e-02 (1.0610e-01)	Acc@1  95.31 ( 96.24)	Acc@5 100.00 (100.00)
07-Mar-22 02:58:54 - Epoch: [13][ 20/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.013)	Loss 1.6067e-01 (1.0660e-01)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:58:56 - Epoch: [13][ 30/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.009)	Loss 1.7424e-01 (1.0685e-01)	Acc@1  93.75 ( 96.32)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:58 - Epoch: [13][ 40/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.008)	Loss 9.2040e-02 (1.0055e-01)	Acc@1  96.09 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:58:59 - Epoch: [13][ 50/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.007)	Loss 6.3230e-02 (1.0051e-01)	Acc@1  98.44 ( 96.46)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:01 - Epoch: [13][ 60/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.006)	Loss 7.8282e-02 (1.0266e-01)	Acc@1  96.88 ( 96.34)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:59:03 - Epoch: [13][ 70/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.1156e-01 (1.0196e-01)	Acc@1  95.31 ( 96.35)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:04 - Epoch: [13][ 80/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.005)	Loss 8.0387e-02 (1.0141e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:59:06 - Epoch: [13][ 90/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.0775e-01 (1.0025e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:59:08 - Epoch: [13][100/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.3825e-02 (1.0278e-01)	Acc@1  96.09 ( 96.31)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:59:09 - Epoch: [13][110/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.5845e-02 (1.0262e-01)	Acc@1  98.44 ( 96.35)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:59:11 - Epoch: [13][120/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.1033e-01 (1.0237e-01)	Acc@1  97.66 ( 96.38)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:59:13 - Epoch: [13][130/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.4007e-01 (1.0206e-01)	Acc@1  95.31 ( 96.42)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:59:14 - Epoch: [13][140/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.0638e-01 (1.0163e-01)	Acc@1  95.31 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:16 - Epoch: [13][150/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.2936e-01 (1.0179e-01)	Acc@1  94.53 ( 96.44)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:59:18 - Epoch: [13][160/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2141e-01 (1.0206e-01)	Acc@1  95.31 ( 96.39)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:19 - Epoch: [13][170/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.6428e-02 (1.0182e-01)	Acc@1  97.66 ( 96.39)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:21 - Epoch: [13][180/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.7166e-01 (1.0224e-01)	Acc@1  92.19 ( 96.37)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:23 - Epoch: [13][190/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.5857e-01 (1.0131e-01)	Acc@1  93.75 ( 96.39)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:24 - Epoch: [13][200/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 2.4820e-01 (1.0245e-01)	Acc@1  94.53 ( 96.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:26 - Epoch: [13][210/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.5900e-01 (1.0287e-01)	Acc@1  94.53 ( 96.38)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:28 - Epoch: [13][220/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.8709e-01 (1.0436e-01)	Acc@1  96.09 ( 96.33)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:29 - Epoch: [13][230/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.7332e-02 (1.0425e-01)	Acc@1  97.66 ( 96.33)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:31 - Epoch: [13][240/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0391e-01 (1.0321e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:33 - Epoch: [13][250/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.4513e-02 (1.0236e-01)	Acc@1  96.88 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:34 - Epoch: [13][260/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1005e-01 (1.0255e-01)	Acc@1  96.09 ( 96.41)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:36 - Epoch: [13][270/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.7203e-02 (1.0159e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:38 - Epoch: [13][280/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4220e-01 (1.0165e-01)	Acc@1  94.53 ( 96.43)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:39 - Epoch: [13][290/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0332e-01 (1.0205e-01)	Acc@1  95.31 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:41 - Epoch: [13][300/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.2379e-02 (1.0200e-01)	Acc@1  97.66 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:43 - Epoch: [13][310/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0879e-01 (1.0223e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:44 - Epoch: [13][320/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.3685e-02 (1.0241e-01)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:46 - Epoch: [13][330/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.1558e-02 (1.0154e-01)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:48 - Epoch: [13][340/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.6483e-02 (1.0145e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:49 - Epoch: [13][350/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1505e-01 (1.0156e-01)	Acc@1  92.19 ( 96.43)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:50 - Test: [ 0/20]	Time  0.381 ( 0.381)	Loss 3.3911e-01 (3.3911e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.61 ( 99.61)
07-Mar-22 02:59:51 - Test: [10/20]	Time  0.099 ( 0.125)	Loss 3.9044e-01 (4.0154e-01)	Acc@1  87.50 ( 88.35)	Acc@5  99.61 ( 99.40)
07-Mar-22 02:59:52 -  * Acc@1 88.960 Acc@5 99.420
07-Mar-22 02:59:52 - Best acc at epoch 13: 89.0199966430664
07-Mar-22 02:59:53 - Epoch: [14][  0/352]	Time  0.398 ( 0.398)	Data  0.242 ( 0.242)	Loss 6.5588e-02 (6.5588e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 02:59:54 - Epoch: [14][ 10/352]	Time  0.169 ( 0.188)	Data  0.002 ( 0.024)	Loss 5.9356e-02 (8.4721e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 02:59:56 - Epoch: [14][ 20/352]	Time  0.168 ( 0.180)	Data  0.002 ( 0.014)	Loss 9.7797e-02 (9.7085e-02)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 (100.00)
07-Mar-22 02:59:58 - Epoch: [14][ 30/352]	Time  0.172 ( 0.176)	Data  0.002 ( 0.010)	Loss 4.4180e-02 (9.0479e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 02:59:59 - Epoch: [14][ 40/352]	Time  0.170 ( 0.175)	Data  0.003 ( 0.008)	Loss 1.0970e-01 (9.3686e-02)	Acc@1  96.09 ( 96.76)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:01 - Epoch: [14][ 50/352]	Time  0.168 ( 0.174)	Data  0.003 ( 0.007)	Loss 1.2543e-01 (9.6039e-02)	Acc@1  95.31 ( 96.61)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:03 - Epoch: [14][ 60/352]	Time  0.168 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.5243e-01 (9.6811e-02)	Acc@1  94.53 ( 96.55)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:05 - Epoch: [14][ 70/352]	Time  0.170 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.0563e-01 (9.5907e-02)	Acc@1  96.88 ( 96.57)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:06 - Epoch: [14][ 80/352]	Time  0.169 ( 0.172)	Data  0.003 ( 0.005)	Loss 1.4755e-01 (9.8075e-02)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:08 - Epoch: [14][ 90/352]	Time  0.165 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.2391e-01 (9.6709e-02)	Acc@1  95.31 ( 96.59)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:10 - Epoch: [14][100/352]	Time  0.171 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.5696e-01 (9.5450e-02)	Acc@1  96.09 ( 96.64)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:11 - Epoch: [14][110/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.1157e-01 (9.6588e-02)	Acc@1  94.53 ( 96.55)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:13 - Epoch: [14][120/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.0614e-01 (9.8664e-02)	Acc@1  94.53 ( 96.49)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:14 - Epoch: [14][130/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.0704e-02 (9.7699e-02)	Acc@1  98.44 ( 96.58)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:16 - Epoch: [14][140/352]	Time  0.131 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.1574e-02 (9.7029e-02)	Acc@1  98.44 ( 96.58)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:18 - Epoch: [14][150/352]	Time  0.151 ( 0.168)	Data  0.002 ( 0.004)	Loss 4.9180e-02 (9.7394e-02)	Acc@1  98.44 ( 96.57)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:19 - Epoch: [14][160/352]	Time  0.132 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.7000e-01 (9.7532e-02)	Acc@1  92.97 ( 96.59)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:21 - Epoch: [14][170/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.004)	Loss 1.5964e-01 (9.7030e-02)	Acc@1  93.75 ( 96.61)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:22 - Epoch: [14][180/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.004)	Loss 1.1833e-01 (9.6659e-02)	Acc@1  96.09 ( 96.61)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:24 - Epoch: [14][190/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.004)	Loss 4.8772e-02 (9.6383e-02)	Acc@1 100.00 ( 96.61)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:26 - Epoch: [14][200/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.004)	Loss 9.9074e-02 (9.6747e-02)	Acc@1  96.88 ( 96.59)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:27 - Epoch: [14][210/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.4434e-01 (9.7656e-02)	Acc@1  94.53 ( 96.57)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:29 - Epoch: [14][220/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.5089e-01 (9.7695e-02)	Acc@1  93.75 ( 96.57)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:31 - Epoch: [14][230/352]	Time  0.171 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.9481e-02 (9.8165e-02)	Acc@1  98.44 ( 96.56)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:32 - Epoch: [14][240/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0580e-01 (9.9071e-02)	Acc@1  95.31 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:34 - Epoch: [14][250/352]	Time  0.167 ( 0.166)	Data  0.003 ( 0.003)	Loss 6.9321e-02 (9.8006e-02)	Acc@1  97.66 ( 96.56)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:36 - Epoch: [14][260/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.8458e-02 (9.8352e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:37 - Epoch: [14][270/352]	Time  0.137 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0177e-01 (9.8388e-02)	Acc@1  97.66 ( 96.57)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:39 - Epoch: [14][280/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.8738e-02 (9.8890e-02)	Acc@1  98.44 ( 96.54)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:41 - Epoch: [14][290/352]	Time  0.170 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.1833e-02 (9.8017e-02)	Acc@1  97.66 ( 96.56)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:42 - Epoch: [14][300/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1464e-01 (9.8016e-02)	Acc@1  94.53 ( 96.56)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:44 - Epoch: [14][310/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1500e-01 (9.8029e-02)	Acc@1  95.31 ( 96.56)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:46 - Epoch: [14][320/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 3.8361e-02 (9.7784e-02)	Acc@1  99.22 ( 96.57)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:47 - Epoch: [14][330/352]	Time  0.169 ( 0.166)	Data  0.003 ( 0.003)	Loss 9.2520e-02 (9.7553e-02)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:49 - Epoch: [14][340/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1854e-01 (9.7867e-02)	Acc@1  96.09 ( 96.58)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:50 - Epoch: [14][350/352]	Time  0.141 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.2666e-01 (9.8338e-02)	Acc@1  94.53 ( 96.57)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:51 - Test: [ 0/20]	Time  0.384 ( 0.384)	Loss 3.2057e-01 (3.2057e-01)	Acc@1  88.67 ( 88.67)	Acc@5  98.83 ( 98.83)
07-Mar-22 03:00:52 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.5032e-01 (3.9542e-01)	Acc@1  89.06 ( 88.32)	Acc@5  99.22 ( 99.18)
07-Mar-22 03:00:53 -  * Acc@1 89.200 Acc@5 99.260
07-Mar-22 03:00:53 - Best acc at epoch 14: 89.19999694824219
07-Mar-22 03:00:53 - Epoch: [15][  0/352]	Time  0.404 ( 0.404)	Data  0.238 ( 0.238)	Loss 1.6242e-01 (1.6242e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:55 - Epoch: [15][ 10/352]	Time  0.167 ( 0.188)	Data  0.002 ( 0.024)	Loss 1.0489e-01 (9.1043e-02)	Acc@1  95.31 ( 97.02)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:57 - Epoch: [15][ 20/352]	Time  0.170 ( 0.176)	Data  0.002 ( 0.013)	Loss 1.3617e-01 (9.5276e-02)	Acc@1  95.31 ( 96.84)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:58 - Epoch: [15][ 30/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.010)	Loss 1.0884e-01 (9.5495e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 (100.00)
07-Mar-22 03:01:00 - Epoch: [15][ 40/352]	Time  0.165 ( 0.171)	Data  0.001 ( 0.008)	Loss 1.5274e-01 (9.8080e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 (100.00)
07-Mar-22 03:01:02 - Epoch: [15][ 50/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.007)	Loss 1.1980e-01 (9.8889e-02)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:03 - Epoch: [15][ 60/352]	Time  0.162 ( 0.169)	Data  0.002 ( 0.006)	Loss 6.5328e-02 (9.8663e-02)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:05 - Epoch: [15][ 70/352]	Time  0.162 ( 0.168)	Data  0.002 ( 0.005)	Loss 7.6503e-02 (9.9226e-02)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:07 - Epoch: [15][ 80/352]	Time  0.143 ( 0.167)	Data  0.002 ( 0.005)	Loss 1.5218e-01 (9.9430e-02)	Acc@1  94.53 ( 96.60)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:08 - Epoch: [15][ 90/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.005)	Loss 1.4407e-01 (1.0070e-01)	Acc@1  93.75 ( 96.53)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:10 - Epoch: [15][100/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.004)	Loss 7.7855e-02 (9.9140e-02)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:11 - Epoch: [15][110/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.6170e-01 (1.0061e-01)	Acc@1  95.31 ( 96.49)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:13 - Epoch: [15][120/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.1812e-01 (1.0072e-01)	Acc@1  93.75 ( 96.46)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:15 - Epoch: [15][130/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.004)	Loss 5.5572e-02 (9.9678e-02)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:16 - Epoch: [15][140/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.004)	Loss 6.3158e-02 (9.9406e-02)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:18 - Epoch: [15][150/352]	Time  0.165 ( 0.167)	Data  0.003 ( 0.004)	Loss 9.5624e-02 (9.9712e-02)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:20 - Epoch: [15][160/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.004)	Loss 9.8501e-02 (1.0054e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:21 - Epoch: [15][170/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.2821e-02 (1.0104e-01)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:01:23 - Epoch: [15][180/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2252e-01 (1.0199e-01)	Acc@1  93.75 ( 96.44)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:01:25 - Epoch: [15][190/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.6717e-02 (1.0200e-01)	Acc@1  98.44 ( 96.40)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:27 - Epoch: [15][200/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.5007e-02 (1.0138e-01)	Acc@1  96.88 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:28 - Epoch: [15][210/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.9840e-02 (1.0111e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:30 - Epoch: [15][220/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1278e-01 (1.0056e-01)	Acc@1  95.31 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:32 - Epoch: [15][230/352]	Time  0.170 ( 0.167)	Data  0.001 ( 0.003)	Loss 1.1173e-01 (1.0067e-01)	Acc@1  97.66 ( 96.38)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:33 - Epoch: [15][240/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.3742e-02 (1.0047e-01)	Acc@1  98.44 ( 96.40)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:35 - Epoch: [15][250/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.9351e-02 (1.0052e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:37 - Epoch: [15][260/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4748e-01 (1.0110e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:38 - Epoch: [15][270/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4039e-01 (1.0180e-01)	Acc@1  97.66 ( 96.36)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:40 - Epoch: [15][280/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.9288e-02 (1.0120e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:41 - Epoch: [15][290/352]	Time  0.168 ( 0.167)	Data  0.001 ( 0.003)	Loss 1.2315e-01 (1.0104e-01)	Acc@1  94.53 ( 96.40)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:43 - Epoch: [15][300/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1605e-01 (1.0079e-01)	Acc@1  96.88 ( 96.41)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:45 - Epoch: [15][310/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3722e-01 (1.0041e-01)	Acc@1  94.53 ( 96.43)	Acc@5 100.00 ( 99.98)
