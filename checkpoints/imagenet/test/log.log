03-Mar-22 09:01:51 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar100', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar100//checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:01:51 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:05:31 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar100', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar100/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:05:31 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:09:00 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar100', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar100/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:09:00 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:09:00 - match all modules defined in bit_config: False
03-Mar-22 09:09:00 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:09:05 - Epoch: [0][  0/352]	Time  0.435 ( 0.435)	Data  0.236 ( 0.236)	Loss 1.7540e+00 (1.7540e+00)	Acc@1  61.72 ( 61.72)	Acc@5  85.94 ( 85.94)
03-Mar-22 09:09:06 - Epoch: [0][ 10/352]	Time  0.155 ( 0.158)	Data  0.002 ( 0.023)	Loss 1.4186e+00 (1.4944e+00)	Acc@1  61.72 ( 65.70)	Acc@5  87.50 ( 89.91)
03-Mar-22 09:09:08 - Epoch: [0][ 20/352]	Time  0.130 ( 0.146)	Data  0.002 ( 0.013)	Loss 1.3235e+00 (1.4372e+00)	Acc@1  71.09 ( 66.37)	Acc@5  93.75 ( 90.81)
03-Mar-22 09:09:09 - Epoch: [0][ 30/352]	Time  0.159 ( 0.142)	Data  0.002 ( 0.010)	Loss 1.2741e+00 (1.4074e+00)	Acc@1  66.41 ( 66.76)	Acc@5  91.41 ( 90.57)
03-Mar-22 09:09:10 - Epoch: [0][ 40/352]	Time  0.134 ( 0.140)	Data  0.002 ( 0.008)	Loss 1.3852e+00 (1.3743e+00)	Acc@1  61.72 ( 67.04)	Acc@5  90.62 ( 90.66)
03-Mar-22 09:09:12 - Epoch: [0][ 50/352]	Time  0.141 ( 0.141)	Data  0.003 ( 0.007)	Loss 1.1733e+00 (1.3412e+00)	Acc@1  74.22 ( 67.86)	Acc@5  89.84 ( 90.98)
03-Mar-22 09:09:13 - Epoch: [0][ 60/352]	Time  0.136 ( 0.141)	Data  0.003 ( 0.006)	Loss 1.1195e+00 (1.3258e+00)	Acc@1  68.75 ( 68.03)	Acc@5  95.31 ( 91.16)
03-Mar-22 09:09:14 - Epoch: [0][ 70/352]	Time  0.162 ( 0.140)	Data  0.002 ( 0.006)	Loss 1.1554e+00 (1.3083e+00)	Acc@1  67.19 ( 68.32)	Acc@5  92.19 ( 91.26)
03-Mar-22 09:09:16 - Epoch: [0][ 80/352]	Time  0.127 ( 0.139)	Data  0.003 ( 0.005)	Loss 1.2360e+00 (1.2939e+00)	Acc@1  70.31 ( 68.65)	Acc@5  92.19 ( 91.39)
03-Mar-22 09:09:17 - Epoch: [0][ 90/352]	Time  0.132 ( 0.138)	Data  0.002 ( 0.005)	Loss 1.0652e+00 (1.2731e+00)	Acc@1  72.66 ( 69.08)	Acc@5  94.53 ( 91.60)
03-Mar-22 09:09:18 - Epoch: [0][100/352]	Time  0.129 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.0455e+00 (1.2594e+00)	Acc@1  75.78 ( 69.30)	Acc@5  95.31 ( 91.71)
03-Mar-22 09:09:20 - Epoch: [0][110/352]	Time  0.132 ( 0.137)	Data  0.002 ( 0.005)	Loss 9.3381e-01 (1.2398e+00)	Acc@1  78.91 ( 69.67)	Acc@5  96.88 ( 91.93)
03-Mar-22 09:09:21 - Epoch: [0][120/352]	Time  0.132 ( 0.137)	Data  0.003 ( 0.004)	Loss 1.1611e+00 (1.2289e+00)	Acc@1  72.66 ( 69.89)	Acc@5  90.62 ( 91.97)
03-Mar-22 09:09:22 - Epoch: [0][130/352]	Time  0.155 ( 0.136)	Data  0.002 ( 0.004)	Loss 1.0086e+00 (1.2181e+00)	Acc@1  74.22 ( 70.12)	Acc@5  96.09 ( 92.08)
03-Mar-22 09:09:24 - Epoch: [0][140/352]	Time  0.122 ( 0.136)	Data  0.002 ( 0.004)	Loss 1.0565e+00 (1.2113e+00)	Acc@1  76.56 ( 70.19)	Acc@5  92.97 ( 92.08)
03-Mar-22 09:09:25 - Epoch: [0][150/352]	Time  0.158 ( 0.136)	Data  0.002 ( 0.004)	Loss 1.1923e+00 (1.1999e+00)	Acc@1  67.97 ( 70.36)	Acc@5  94.53 ( 92.23)
03-Mar-22 09:09:26 - Epoch: [0][160/352]	Time  0.113 ( 0.136)	Data  0.002 ( 0.004)	Loss 1.0746e+00 (1.1932e+00)	Acc@1  71.09 ( 70.54)	Acc@5  92.97 ( 92.28)
03-Mar-22 09:09:28 - Epoch: [0][170/352]	Time  0.155 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.2766e+00 (1.1869e+00)	Acc@1  64.06 ( 70.60)	Acc@5  90.62 ( 92.32)
03-Mar-22 09:09:29 - Epoch: [0][180/352]	Time  0.131 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.2568e+00 (1.1800e+00)	Acc@1  68.75 ( 70.74)	Acc@5  90.62 ( 92.40)
03-Mar-22 09:09:30 - Epoch: [0][190/352]	Time  0.152 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.0162e+00 (1.1707e+00)	Acc@1  73.44 ( 70.88)	Acc@5  92.97 ( 92.50)
03-Mar-22 09:09:32 - Epoch: [0][200/352]	Time  0.130 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.0725e+00 (1.1673e+00)	Acc@1  73.44 ( 70.92)	Acc@5  91.41 ( 92.49)
03-Mar-22 09:09:33 - Epoch: [0][210/352]	Time  0.130 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.0683e+00 (1.1617e+00)	Acc@1  69.53 ( 70.94)	Acc@5  92.97 ( 92.57)
03-Mar-22 09:09:34 - Epoch: [0][220/352]	Time  0.128 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.1946e+00 (1.1590e+00)	Acc@1  64.06 ( 70.87)	Acc@5  92.19 ( 92.60)
03-Mar-22 09:09:36 - Epoch: [0][230/352]	Time  0.152 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.1550e+00 (1.1562e+00)	Acc@1  67.97 ( 70.87)	Acc@5  93.75 ( 92.60)
03-Mar-22 09:09:37 - Epoch: [0][240/352]	Time  0.128 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.0745e+00 (1.1531e+00)	Acc@1  71.88 ( 70.94)	Acc@5  95.31 ( 92.63)
03-Mar-22 09:09:38 - Epoch: [0][250/352]	Time  0.158 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.2385e+00 (1.1503e+00)	Acc@1  67.97 ( 70.97)	Acc@5  87.50 ( 92.65)
03-Mar-22 09:09:40 - Epoch: [0][260/352]	Time  0.112 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.0706e+00 (1.1458e+00)	Acc@1  67.97 ( 71.06)	Acc@5  95.31 ( 92.68)
03-Mar-22 09:09:41 - Epoch: [0][270/352]	Time  0.134 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.2071e+00 (1.1440e+00)	Acc@1  68.75 ( 71.07)	Acc@5  92.97 ( 92.69)
03-Mar-22 09:09:42 - Epoch: [0][280/352]	Time  0.124 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.1118e+00 (1.1416e+00)	Acc@1  69.53 ( 71.12)	Acc@5  93.75 ( 92.72)
03-Mar-22 09:09:43 - Epoch: [0][290/352]	Time  0.154 ( 0.134)	Data  0.002 ( 0.003)	Loss 9.0021e-01 (1.1371e+00)	Acc@1  71.88 ( 71.17)	Acc@5  96.09 ( 92.79)
03-Mar-22 09:09:45 - Epoch: [0][300/352]	Time  0.128 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.2259e+00 (1.1343e+00)	Acc@1  69.53 ( 71.18)	Acc@5  92.19 ( 92.82)
03-Mar-22 09:09:46 - Epoch: [0][310/352]	Time  0.157 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.0633e+00 (1.1325e+00)	Acc@1  70.31 ( 71.20)	Acc@5  92.97 ( 92.80)
03-Mar-22 09:09:47 - Epoch: [0][320/352]	Time  0.131 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.1271e+00 (1.1304e+00)	Acc@1  70.31 ( 71.20)	Acc@5  91.41 ( 92.83)
03-Mar-22 09:09:49 - Epoch: [0][330/352]	Time  0.152 ( 0.134)	Data  0.002 ( 0.003)	Loss 8.6877e-01 (1.1266e+00)	Acc@1  78.91 ( 71.29)	Acc@5  96.09 ( 92.86)
03-Mar-22 09:09:50 - Epoch: [0][340/352]	Time  0.123 ( 0.134)	Data  0.002 ( 0.003)	Loss 9.9527e-01 (1.1267e+00)	Acc@1  73.44 ( 71.27)	Acc@5  93.75 ( 92.82)
03-Mar-22 09:09:51 - Epoch: [0][350/352]	Time  0.121 ( 0.133)	Data  0.002 ( 0.003)	Loss 9.5544e-01 (1.1223e+00)	Acc@1  74.22 ( 71.37)	Acc@5  94.53 ( 92.86)
03-Mar-22 09:09:52 - Test: [ 0/20]	Time  0.364 ( 0.364)	Loss 1.0615e+00 (1.0615e+00)	Acc@1  69.53 ( 69.53)	Acc@5  94.14 ( 94.14)
03-Mar-22 09:09:53 - Test: [10/20]	Time  0.069 ( 0.100)	Loss 8.7278e-01 (1.0054e+00)	Acc@1  75.78 ( 73.37)	Acc@5  95.31 ( 94.00)
03-Mar-22 09:09:54 -  * Acc@1 73.220 Acc@5 93.860
03-Mar-22 09:09:54 - Best acc at epoch 0: 73.22000122070312
03-Mar-22 09:09:54 - Epoch: [1][  0/352]	Time  0.333 ( 0.333)	Data  0.232 ( 0.232)	Loss 9.8704e-01 (9.8704e-01)	Acc@1  75.78 ( 75.78)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:09:55 - Epoch: [1][ 10/352]	Time  0.106 ( 0.141)	Data  0.002 ( 0.023)	Loss 1.0265e+00 (1.0525e+00)	Acc@1  77.34 ( 73.51)	Acc@5  92.97 ( 92.83)
03-Mar-22 09:09:56 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar100', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar100/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:09:56 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:09:56 - match all modules defined in bit_config: False
03-Mar-22 09:09:56 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:09:56 - Epoch: [1][ 20/352]	Time  0.103 ( 0.132)	Data  0.001 ( 0.013)	Loss 1.0799e+00 (1.0789e+00)	Acc@1  71.09 ( 71.32)	Acc@5  95.31 ( 92.56)
03-Mar-22 09:09:58 - Epoch: [1][ 30/352]	Time  0.129 ( 0.132)	Data  0.002 ( 0.010)	Loss 7.6985e-01 (1.0340e+00)	Acc@1  82.81 ( 72.73)	Acc@5  97.66 ( 93.07)
03-Mar-22 09:09:59 - Epoch: [1][ 40/352]	Time  0.127 ( 0.132)	Data  0.002 ( 0.008)	Loss 9.8064e-01 (1.0289e+00)	Acc@1  70.31 ( 72.56)	Acc@5  94.53 ( 93.50)
03-Mar-22 09:10:00 - Epoch: [1][ 50/352]	Time  0.117 ( 0.133)	Data  0.002 ( 0.007)	Loss 9.9767e-01 (1.0340e+00)	Acc@1  72.66 ( 72.50)	Acc@5  94.53 ( 93.41)
03-Mar-22 09:10:01 - Epoch: [0][  0/352]	Time  0.488 ( 0.488)	Data  0.252 ( 0.252)	Loss 1.5176e+00 (1.5176e+00)	Acc@1  69.53 ( 69.53)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:10:02 - Epoch: [1][ 60/352]	Time  0.157 ( 0.134)	Data  0.002 ( 0.006)	Loss 8.9517e-01 (1.0337e+00)	Acc@1  76.56 ( 72.59)	Acc@5  94.53 ( 93.40)
03-Mar-22 09:10:02 - Epoch: [0][ 10/352]	Time  0.155 ( 0.183)	Data  0.002 ( 0.025)	Loss 1.5948e+00 (1.4596e+00)	Acc@1  64.06 ( 67.05)	Acc@5  87.50 ( 90.20)
03-Mar-22 09:10:03 - Epoch: [1][ 70/352]	Time  0.139 ( 0.134)	Data  0.003 ( 0.006)	Loss 1.2048e+00 (1.0416e+00)	Acc@1  71.88 ( 72.57)	Acc@5  91.41 ( 93.30)
03-Mar-22 09:10:04 - Epoch: [0][ 20/352]	Time  0.159 ( 0.167)	Data  0.002 ( 0.014)	Loss 1.2542e+00 (1.4121e+00)	Acc@1  71.09 ( 67.26)	Acc@5  94.53 ( 90.77)
03-Mar-22 09:10:04 - Epoch: [1][ 80/352]	Time  0.132 ( 0.134)	Data  0.002 ( 0.005)	Loss 9.3981e-01 (1.0415e+00)	Acc@1  73.44 ( 72.36)	Acc@5  92.97 ( 93.40)
03-Mar-22 09:10:05 - Epoch: [0][ 30/352]	Time  0.154 ( 0.164)	Data  0.002 ( 0.010)	Loss 1.1674e+00 (1.3601e+00)	Acc@1  72.66 ( 68.17)	Acc@5  94.53 ( 91.08)
03-Mar-22 09:10:06 - Epoch: [1][ 90/352]	Time  0.152 ( 0.135)	Data  0.002 ( 0.005)	Loss 1.1145e+00 (1.0421e+00)	Acc@1  70.31 ( 72.28)	Acc@5  90.62 ( 93.34)
03-Mar-22 09:10:07 - Epoch: [0][ 40/352]	Time  0.156 ( 0.160)	Data  0.002 ( 0.008)	Loss 1.1699e+00 (1.3326e+00)	Acc@1  70.31 ( 68.16)	Acc@5  94.53 ( 91.58)
03-Mar-22 09:10:07 - Epoch: [1][100/352]	Time  0.161 ( 0.136)	Data  0.003 ( 0.005)	Loss 9.3933e-01 (1.0387e+00)	Acc@1  74.22 ( 72.25)	Acc@5  93.75 ( 93.37)
03-Mar-22 09:10:08 - Epoch: [0][ 50/352]	Time  0.157 ( 0.158)	Data  0.002 ( 0.007)	Loss 9.7523e-01 (1.2992e+00)	Acc@1  75.78 ( 68.72)	Acc@5  97.66 ( 91.82)
03-Mar-22 09:10:09 - Epoch: [1][110/352]	Time  0.150 ( 0.138)	Data  0.002 ( 0.004)	Loss 1.2442e+00 (1.0394e+00)	Acc@1  65.62 ( 72.23)	Acc@5  92.97 ( 93.43)
03-Mar-22 09:10:10 - Epoch: [0][ 60/352]	Time  0.139 ( 0.156)	Data  0.002 ( 0.006)	Loss 1.0778e+00 (1.2902e+00)	Acc@1  73.44 ( 69.11)	Acc@5  94.53 ( 91.78)
03-Mar-22 09:10:10 - Epoch: [1][120/352]	Time  0.164 ( 0.139)	Data  0.002 ( 0.004)	Loss 1.1310e+00 (1.0405e+00)	Acc@1  65.62 ( 72.22)	Acc@5  91.41 ( 93.45)
03-Mar-22 09:10:11 - Epoch: [0][ 70/352]	Time  0.148 ( 0.155)	Data  0.003 ( 0.006)	Loss 1.1248e+00 (1.2714e+00)	Acc@1  72.66 ( 69.44)	Acc@5  91.41 ( 91.90)
03-Mar-22 09:10:12 - Epoch: [1][130/352]	Time  0.185 ( 0.140)	Data  0.003 ( 0.004)	Loss 1.0913e+00 (1.0385e+00)	Acc@1  66.41 ( 72.24)	Acc@5  93.75 ( 93.53)
03-Mar-22 09:10:13 - Epoch: [0][ 80/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.005)	Loss 1.0439e+00 (1.2512e+00)	Acc@1  75.78 ( 69.82)	Acc@5  94.53 ( 92.15)
03-Mar-22 09:10:14 - Epoch: [1][140/352]	Time  0.138 ( 0.141)	Data  0.002 ( 0.004)	Loss 9.4310e-01 (1.0354e+00)	Acc@1  75.78 ( 72.29)	Acc@5  94.53 ( 93.59)
03-Mar-22 09:10:14 - Epoch: [0][ 90/352]	Time  0.149 ( 0.154)	Data  0.003 ( 0.005)	Loss 1.0903e+00 (1.2383e+00)	Acc@1  75.00 ( 70.05)	Acc@5  91.41 ( 92.18)
03-Mar-22 09:10:15 - Epoch: [1][150/352]	Time  0.146 ( 0.141)	Data  0.002 ( 0.004)	Loss 1.1577e+00 (1.0389e+00)	Acc@1  66.41 ( 72.23)	Acc@5  92.19 ( 93.52)
03-Mar-22 09:10:16 - Epoch: [0][100/352]	Time  0.151 ( 0.153)	Data  0.002 ( 0.005)	Loss 1.1581e+00 (1.2264e+00)	Acc@1  71.09 ( 70.11)	Acc@5  92.19 ( 92.30)
03-Mar-22 09:10:16 - Epoch: [1][160/352]	Time  0.148 ( 0.141)	Data  0.003 ( 0.004)	Loss 8.7117e-01 (1.0388e+00)	Acc@1  79.69 ( 72.20)	Acc@5  97.66 ( 93.57)
03-Mar-22 09:10:17 - Epoch: [0][110/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.005)	Loss 1.1687e+00 (1.2170e+00)	Acc@1  70.31 ( 70.24)	Acc@5  90.62 ( 92.34)
03-Mar-22 09:10:18 - Epoch: [1][170/352]	Time  0.151 ( 0.142)	Data  0.002 ( 0.004)	Loss 8.6067e-01 (1.0330e+00)	Acc@1  79.69 ( 72.35)	Acc@5  96.09 ( 93.64)
03-Mar-22 09:10:19 - Epoch: [0][120/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1405e+00 (1.2073e+00)	Acc@1  68.75 ( 70.25)	Acc@5  93.75 ( 92.43)
03-Mar-22 09:10:19 - Epoch: [1][180/352]	Time  0.152 ( 0.142)	Data  0.002 ( 0.004)	Loss 8.7439e-01 (1.0306e+00)	Acc@1  77.34 ( 72.38)	Acc@5  93.75 ( 93.65)
03-Mar-22 09:10:20 - Epoch: [0][130/352]	Time  0.157 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1549e+00 (1.1987e+00)	Acc@1  72.66 ( 70.43)	Acc@5  90.62 ( 92.52)
03-Mar-22 09:10:21 - Epoch: [1][190/352]	Time  0.131 ( 0.143)	Data  0.002 ( 0.004)	Loss 1.0112e+00 (1.0297e+00)	Acc@1  72.66 ( 72.43)	Acc@5  94.53 ( 93.62)
03-Mar-22 09:10:22 - Epoch: [0][140/352]	Time  0.149 ( 0.152)	Data  0.003 ( 0.004)	Loss 8.6043e-01 (1.1898e+00)	Acc@1  80.47 ( 70.55)	Acc@5  94.53 ( 92.61)
03-Mar-22 09:10:22 - Epoch: [1][200/352]	Time  0.153 ( 0.143)	Data  0.002 ( 0.004)	Loss 1.2096e+00 (1.0312e+00)	Acc@1  64.06 ( 72.38)	Acc@5  90.62 ( 93.56)
03-Mar-22 09:10:23 - Epoch: [0][150/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1209e+00 (1.1796e+00)	Acc@1  72.66 ( 70.73)	Acc@5  92.97 ( 92.68)
03-Mar-22 09:10:24 - Epoch: [1][210/352]	Time  0.141 ( 0.143)	Data  0.002 ( 0.003)	Loss 9.9856e-01 (1.0326e+00)	Acc@1  72.66 ( 72.36)	Acc@5  97.66 ( 93.55)
03-Mar-22 09:10:24 - Epoch: [0][160/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1752e+00 (1.1757e+00)	Acc@1  67.97 ( 70.76)	Acc@5  92.97 ( 92.74)
03-Mar-22 09:10:25 - Epoch: [1][220/352]	Time  0.179 ( 0.144)	Data  0.002 ( 0.003)	Loss 8.7209e-01 (1.0310e+00)	Acc@1  77.34 ( 72.44)	Acc@5  95.31 ( 93.57)
03-Mar-22 09:10:26 - Epoch: [0][170/352]	Time  0.156 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.2048e+00 (1.1726e+00)	Acc@1  72.66 ( 70.82)	Acc@5  90.62 ( 92.71)
03-Mar-22 09:10:27 - Epoch: [1][230/352]	Time  0.146 ( 0.144)	Data  0.002 ( 0.003)	Loss 9.4343e-01 (1.0286e+00)	Acc@1  75.78 ( 72.50)	Acc@5  92.97 ( 93.61)
03-Mar-22 09:10:27 - Epoch: [0][180/352]	Time  0.150 ( 0.151)	Data  0.003 ( 0.004)	Loss 1.0019e+00 (1.1670e+00)	Acc@1  69.53 ( 70.90)	Acc@5  96.09 ( 92.77)
03-Mar-22 09:10:28 - Epoch: [1][240/352]	Time  0.125 ( 0.144)	Data  0.002 ( 0.003)	Loss 9.9233e-01 (1.0293e+00)	Acc@1  71.88 ( 72.41)	Acc@5  93.75 ( 93.60)
03-Mar-22 09:10:29 - Epoch: [0][190/352]	Time  0.146 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.4084e-01 (1.1611e+00)	Acc@1  75.00 ( 70.94)	Acc@5  94.53 ( 92.82)
03-Mar-22 09:10:30 - Epoch: [1][250/352]	Time  0.149 ( 0.144)	Data  0.002 ( 0.003)	Loss 1.1268e+00 (1.0277e+00)	Acc@1  70.31 ( 72.49)	Acc@5  91.41 ( 93.61)
03-Mar-22 09:10:30 - Epoch: [0][200/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.0456e+00 (1.1586e+00)	Acc@1  74.22 ( 70.97)	Acc@5  95.31 ( 92.80)
03-Mar-22 09:10:31 - Epoch: [1][260/352]	Time  0.150 ( 0.144)	Data  0.002 ( 0.003)	Loss 9.9108e-01 (1.0250e+00)	Acc@1  72.66 ( 72.58)	Acc@5  93.75 ( 93.64)
03-Mar-22 09:10:32 - Epoch: [0][210/352]	Time  0.156 ( 0.151)	Data  0.003 ( 0.004)	Loss 1.0136e+00 (1.1531e+00)	Acc@1  73.44 ( 71.02)	Acc@5  93.75 ( 92.86)
03-Mar-22 09:10:33 - Epoch: [1][270/352]	Time  0.148 ( 0.144)	Data  0.002 ( 0.003)	Loss 1.1788e+00 (1.0234e+00)	Acc@1  69.53 ( 72.62)	Acc@5  91.41 ( 93.66)
03-Mar-22 09:10:33 - Epoch: [0][220/352]	Time  0.155 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.1635e-01 (1.1470e+00)	Acc@1  80.47 ( 71.11)	Acc@5  96.88 ( 92.91)
03-Mar-22 09:10:34 - Epoch: [1][280/352]	Time  0.135 ( 0.144)	Data  0.002 ( 0.003)	Loss 1.0616e+00 (1.0228e+00)	Acc@1  65.62 ( 72.59)	Acc@5  96.88 ( 93.68)
03-Mar-22 09:10:35 - Epoch: [0][230/352]	Time  0.154 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.1147e+00 (1.1440e+00)	Acc@1  71.88 ( 71.15)	Acc@5  89.84 ( 92.94)
03-Mar-22 09:10:36 - Epoch: [1][290/352]	Time  0.152 ( 0.144)	Data  0.002 ( 0.003)	Loss 9.1484e-01 (1.0216e+00)	Acc@1  75.78 ( 72.58)	Acc@5  96.88 ( 93.69)
03-Mar-22 09:10:36 - Epoch: [0][240/352]	Time  0.157 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.6322e-01 (1.1401e+00)	Acc@1  74.22 ( 71.20)	Acc@5  96.09 ( 92.98)
03-Mar-22 09:10:37 - Epoch: [1][300/352]	Time  0.143 ( 0.144)	Data  0.002 ( 0.003)	Loss 1.0086e+00 (1.0208e+00)	Acc@1  74.22 ( 72.61)	Acc@5  92.97 ( 93.70)
03-Mar-22 09:10:38 - Epoch: [0][250/352]	Time  0.179 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.1529e+00 (1.1354e+00)	Acc@1  73.44 ( 71.30)	Acc@5  92.19 ( 93.02)
03-Mar-22 09:10:39 - Epoch: [1][310/352]	Time  0.147 ( 0.144)	Data  0.002 ( 0.003)	Loss 9.6277e-01 (1.0216e+00)	Acc@1  76.56 ( 72.60)	Acc@5  91.41 ( 93.65)
03-Mar-22 09:10:39 - Epoch: [0][260/352]	Time  0.155 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0709e+00 (1.1336e+00)	Acc@1  69.53 ( 71.28)	Acc@5  92.19 ( 93.04)
03-Mar-22 09:10:40 - Epoch: [1][320/352]	Time  0.165 ( 0.145)	Data  0.003 ( 0.003)	Loss 1.0568e+00 (1.0210e+00)	Acc@1  66.41 ( 72.59)	Acc@5  94.53 ( 93.67)
03-Mar-22 09:10:41 - Epoch: [0][270/352]	Time  0.146 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0964e+00 (1.1297e+00)	Acc@1  75.00 ( 71.33)	Acc@5  92.97 ( 93.11)
03-Mar-22 09:10:42 - Epoch: [1][330/352]	Time  0.155 ( 0.145)	Data  0.002 ( 0.003)	Loss 9.4207e-01 (1.0203e+00)	Acc@1  73.44 ( 72.63)	Acc@5  94.53 ( 93.68)
03-Mar-22 09:10:43 - Epoch: [0][280/352]	Time  0.164 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.2807e-01 (1.1267e+00)	Acc@1  73.44 ( 71.37)	Acc@5  96.88 ( 93.12)
03-Mar-22 09:10:43 - Epoch: [1][340/352]	Time  0.149 ( 0.145)	Data  0.003 ( 0.003)	Loss 1.1209e+00 (1.0199e+00)	Acc@1  69.53 ( 72.62)	Acc@5  91.41 ( 93.67)
03-Mar-22 09:10:44 - Epoch: [0][290/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.2027e+00 (1.1268e+00)	Acc@1  67.97 ( 71.33)	Acc@5  92.97 ( 93.12)
03-Mar-22 09:10:45 - Epoch: [1][350/352]	Time  0.149 ( 0.145)	Data  0.002 ( 0.003)	Loss 1.0919e+00 (1.0205e+00)	Acc@1  68.75 ( 72.63)	Acc@5  92.97 ( 93.67)
03-Mar-22 09:10:45 - Test: [ 0/20]	Time  0.366 ( 0.366)	Loss 1.0610e+00 (1.0610e+00)	Acc@1  69.14 ( 69.14)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:10:46 - Epoch: [0][300/352]	Time  0.159 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0888e+00 (1.1238e+00)	Acc@1  64.84 ( 71.33)	Acc@5  94.53 ( 93.14)
03-Mar-22 09:10:46 - Test: [10/20]	Time  0.097 ( 0.124)	Loss 8.5148e-01 (9.7810e-01)	Acc@1  77.73 ( 72.98)	Acc@5  93.36 ( 93.71)
03-Mar-22 09:10:47 -  * Acc@1 73.480 Acc@5 93.600
03-Mar-22 09:10:47 - Epoch: [0][310/352]	Time  0.109 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0904e+00 (1.1217e+00)	Acc@1  75.78 ( 71.34)	Acc@5  92.97 ( 93.13)
03-Mar-22 09:10:47 - Best acc at epoch 1: 73.47999572753906
03-Mar-22 09:10:48 - Epoch: [2][  0/352]	Time  0.382 ( 0.382)	Data  0.231 ( 0.231)	Loss 8.2319e-01 (8.2319e-01)	Acc@1  79.69 ( 79.69)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:10:48 - Epoch: [0][320/352]	Time  0.141 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.3252e-01 (1.1202e+00)	Acc@1  80.47 ( 71.33)	Acc@5  97.66 ( 93.14)
03-Mar-22 09:10:49 - Epoch: [2][ 10/352]	Time  0.133 ( 0.166)	Data  0.002 ( 0.024)	Loss 9.8632e-01 (9.9702e-01)	Acc@1  70.31 ( 72.30)	Acc@5  95.31 ( 94.25)
03-Mar-22 09:10:50 - Epoch: [0][330/352]	Time  0.136 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0716e+00 (1.1186e+00)	Acc@1  67.19 ( 71.34)	Acc@5  94.53 ( 93.15)
03-Mar-22 09:10:50 - Epoch: [2][ 20/352]	Time  0.144 ( 0.156)	Data  0.002 ( 0.013)	Loss 1.1357e+00 (9.9967e-01)	Acc@1  69.53 ( 72.84)	Acc@5  92.97 ( 94.05)
03-Mar-22 09:10:51 - Epoch: [0][340/352]	Time  0.152 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.4007e-01 (1.1142e+00)	Acc@1  77.34 ( 71.48)	Acc@5  92.97 ( 93.19)
03-Mar-22 09:10:52 - Epoch: [2][ 30/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.010)	Loss 8.5674e-01 (1.0027e+00)	Acc@1  76.56 ( 72.91)	Acc@5  94.53 ( 93.80)
03-Mar-22 09:10:53 - Epoch: [0][350/352]	Time  0.145 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0712e+00 (1.1120e+00)	Acc@1  69.53 ( 71.49)	Acc@5  92.97 ( 93.22)
03-Mar-22 09:10:53 - Epoch: [2][ 40/352]	Time  0.109 ( 0.150)	Data  0.002 ( 0.008)	Loss 1.1215e+00 (1.0105e+00)	Acc@1  64.84 ( 72.54)	Acc@5  92.19 ( 93.50)
03-Mar-22 09:10:53 - Test: [ 0/20]	Time  0.386 ( 0.386)	Loss 1.1059e+00 (1.1059e+00)	Acc@1  69.14 ( 69.14)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:10:55 - Test: [10/20]	Time  0.098 ( 0.129)	Loss 8.7308e-01 (1.0151e+00)	Acc@1  77.34 ( 72.55)	Acc@5  94.14 ( 93.61)
03-Mar-22 09:10:55 - Epoch: [2][ 50/352]	Time  0.165 ( 0.150)	Data  0.002 ( 0.007)	Loss 8.7777e-01 (9.9835e-01)	Acc@1  78.12 ( 72.82)	Acc@5  96.09 ( 93.73)
03-Mar-22 09:10:55 -  * Acc@1 73.100 Acc@5 93.800
03-Mar-22 09:10:56 - Best acc at epoch 0: 73.0999984741211
03-Mar-22 09:10:56 - Epoch: [1][  0/352]	Time  0.400 ( 0.400)	Data  0.248 ( 0.248)	Loss 8.5433e-01 (8.5433e-01)	Acc@1  78.91 ( 78.91)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:10:56 - Epoch: [2][ 60/352]	Time  0.150 ( 0.149)	Data  0.002 ( 0.006)	Loss 9.8278e-01 (9.8712e-01)	Acc@1  75.00 ( 73.45)	Acc@5  95.31 ( 93.89)
03-Mar-22 09:10:57 - Epoch: [1][ 10/352]	Time  0.132 ( 0.170)	Data  0.002 ( 0.024)	Loss 9.1422e-01 (1.0115e+00)	Acc@1  80.47 ( 74.01)	Acc@5  95.31 ( 93.96)
03-Mar-22 09:10:58 - Epoch: [2][ 70/352]	Time  0.159 ( 0.149)	Data  0.002 ( 0.006)	Loss 8.6314e-01 (9.8590e-01)	Acc@1  75.78 ( 73.35)	Acc@5  97.66 ( 93.90)
03-Mar-22 09:10:59 - Epoch: [1][ 20/352]	Time  0.143 ( 0.160)	Data  0.002 ( 0.014)	Loss 1.1363e+00 (1.0175e+00)	Acc@1  67.97 ( 73.66)	Acc@5  90.62 ( 93.64)
03-Mar-22 09:10:59 - Epoch: [2][ 80/352]	Time  0.136 ( 0.149)	Data  0.002 ( 0.005)	Loss 8.4967e-01 (9.8482e-01)	Acc@1  74.22 ( 73.40)	Acc@5  95.31 ( 93.79)
03-Mar-22 09:11:00 - Epoch: [1][ 30/352]	Time  0.141 ( 0.156)	Data  0.002 ( 0.010)	Loss 1.2426e+00 (1.0268e+00)	Acc@1  70.31 ( 73.14)	Acc@5  89.84 ( 93.80)
03-Mar-22 09:11:01 - Epoch: [2][ 90/352]	Time  0.156 ( 0.149)	Data  0.003 ( 0.005)	Loss 9.6408e-01 (9.8376e-01)	Acc@1  69.53 ( 73.36)	Acc@5  96.88 ( 93.84)
03-Mar-22 09:11:02 - Epoch: [1][ 40/352]	Time  0.131 ( 0.153)	Data  0.002 ( 0.008)	Loss 9.2495e-01 (1.0384e+00)	Acc@1  72.66 ( 72.62)	Acc@5  95.31 ( 93.60)
03-Mar-22 09:11:02 - Epoch: [2][100/352]	Time  0.154 ( 0.148)	Data  0.002 ( 0.005)	Loss 9.3084e-01 (9.7920e-01)	Acc@1  73.44 ( 73.46)	Acc@5  96.09 ( 93.93)
03-Mar-22 09:11:03 - Epoch: [1][ 50/352]	Time  0.169 ( 0.153)	Data  0.002 ( 0.007)	Loss 1.0605e+00 (1.0381e+00)	Acc@1  72.66 ( 72.58)	Acc@5  92.97 ( 93.64)
03-Mar-22 09:11:04 - Epoch: [2][110/352]	Time  0.146 ( 0.148)	Data  0.003 ( 0.004)	Loss 9.4590e-01 (9.8234e-01)	Acc@1  74.22 ( 73.25)	Acc@5  93.75 ( 93.92)
03-Mar-22 09:11:05 - Epoch: [1][ 60/352]	Time  0.164 ( 0.155)	Data  0.003 ( 0.006)	Loss 1.0721e+00 (1.0363e+00)	Acc@1  70.31 ( 72.78)	Acc@5  91.41 ( 93.51)
03-Mar-22 09:11:05 - Epoch: [2][120/352]	Time  0.152 ( 0.148)	Data  0.003 ( 0.004)	Loss 9.4644e-01 (9.8334e-01)	Acc@1  74.22 ( 73.26)	Acc@5  95.31 ( 93.89)
03-Mar-22 09:11:07 - Epoch: [1][ 70/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.006)	Loss 9.4744e-01 (1.0284e+00)	Acc@1  75.78 ( 72.99)	Acc@5  96.88 ( 93.73)
03-Mar-22 09:11:07 - Epoch: [2][130/352]	Time  0.146 ( 0.148)	Data  0.003 ( 0.004)	Loss 1.1243e+00 (9.8150e-01)	Acc@1  70.31 ( 73.25)	Acc@5  90.62 ( 93.92)
03-Mar-22 09:11:08 - Epoch: [2][140/352]	Time  0.144 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.6927e-01 (9.8334e-01)	Acc@1  75.78 ( 73.30)	Acc@5  94.53 ( 93.89)
03-Mar-22 09:11:08 - Epoch: [1][ 80/352]	Time  0.156 ( 0.154)	Data  0.002 ( 0.006)	Loss 8.8518e-01 (1.0297e+00)	Acc@1  78.12 ( 72.84)	Acc@5  95.31 ( 93.78)
03-Mar-22 09:11:09 - Epoch: [2][150/352]	Time  0.135 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.1614e-01 (9.8540e-01)	Acc@1  75.78 ( 73.20)	Acc@5  93.75 ( 93.85)
03-Mar-22 09:11:09 - Epoch: [1][ 90/352]	Time  0.138 ( 0.153)	Data  0.003 ( 0.005)	Loss 1.0440e+00 (1.0348e+00)	Acc@1  76.56 ( 72.69)	Acc@5  93.75 ( 93.64)
03-Mar-22 09:11:11 - Epoch: [1][100/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.6214e-01 (1.0283e+00)	Acc@1  71.09 ( 72.71)	Acc@5  91.41 ( 93.71)
03-Mar-22 09:11:11 - Epoch: [2][160/352]	Time  0.145 ( 0.148)	Data  0.002 ( 0.004)	Loss 1.1909e+00 (9.8712e-01)	Acc@1  66.41 ( 73.13)	Acc@5  89.84 ( 93.85)
03-Mar-22 09:11:12 - Epoch: [2][170/352]	Time  0.149 ( 0.148)	Data  0.003 ( 0.004)	Loss 9.5403e-01 (9.8677e-01)	Acc@1  75.78 ( 73.09)	Acc@5  92.19 ( 93.86)
03-Mar-22 09:11:12 - Epoch: [1][110/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.005)	Loss 9.7364e-01 (1.0318e+00)	Acc@1  75.00 ( 72.71)	Acc@5  94.53 ( 93.65)
03-Mar-22 09:11:14 - Epoch: [2][180/352]	Time  0.145 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.7942e-01 (9.8457e-01)	Acc@1  69.53 ( 73.12)	Acc@5  91.41 ( 93.88)
03-Mar-22 09:11:14 - Epoch: [1][120/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.005)	Loss 1.0069e+00 (1.0292e+00)	Acc@1  74.22 ( 72.84)	Acc@5  92.97 ( 93.71)
03-Mar-22 09:11:15 - Epoch: [2][190/352]	Time  0.136 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.8932e-01 (9.8536e-01)	Acc@1  74.22 ( 73.15)	Acc@5  92.97 ( 93.86)
03-Mar-22 09:11:15 - Epoch: [1][130/352]	Time  0.151 ( 0.152)	Data  0.003 ( 0.004)	Loss 8.6262e-01 (1.0252e+00)	Acc@1  77.34 ( 73.00)	Acc@5  96.09 ( 93.79)
03-Mar-22 09:11:17 - Epoch: [2][200/352]	Time  0.150 ( 0.148)	Data  0.002 ( 0.004)	Loss 1.1758e+00 (9.8824e-01)	Acc@1  71.88 ( 73.04)	Acc@5  90.62 ( 93.83)
03-Mar-22 09:11:17 - Epoch: [1][140/352]	Time  0.160 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1734e+00 (1.0248e+00)	Acc@1  67.97 ( 72.98)	Acc@5  91.41 ( 93.81)
03-Mar-22 09:11:18 - Epoch: [2][210/352]	Time  0.154 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.8031e-01 (9.8740e-01)	Acc@1  73.44 ( 73.06)	Acc@5  93.75 ( 93.82)
03-Mar-22 09:11:19 - Epoch: [1][150/352]	Time  0.155 ( 0.152)	Data  0.003 ( 0.004)	Loss 1.0782e+00 (1.0256e+00)	Acc@1  64.06 ( 72.88)	Acc@5  93.75 ( 93.79)
03-Mar-22 09:11:20 - Epoch: [2][220/352]	Time  0.149 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.1863e+00 (9.8904e-01)	Acc@1  68.75 ( 73.11)	Acc@5  93.75 ( 93.78)
03-Mar-22 09:11:20 - Epoch: [1][160/352]	Time  0.171 ( 0.153)	Data  0.004 ( 0.004)	Loss 1.1892e+00 (1.0270e+00)	Acc@1  64.06 ( 72.83)	Acc@5  94.53 ( 93.75)
03-Mar-22 09:11:21 - Epoch: [2][230/352]	Time  0.148 ( 0.148)	Data  0.003 ( 0.003)	Loss 1.1644e+00 (9.8804e-01)	Acc@1  66.41 ( 73.14)	Acc@5  91.41 ( 93.78)
03-Mar-22 09:11:22 - Epoch: [1][170/352]	Time  0.132 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.3626e-01 (1.0258e+00)	Acc@1  76.56 ( 72.93)	Acc@5  92.97 ( 93.67)
03-Mar-22 09:11:23 - Epoch: [2][240/352]	Time  0.150 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.0207e+00 (9.8690e-01)	Acc@1  69.53 ( 73.12)	Acc@5  95.31 ( 93.79)
03-Mar-22 09:11:23 - Epoch: [1][180/352]	Time  0.177 ( 0.154)	Data  0.003 ( 0.004)	Loss 1.0472e+00 (1.0260e+00)	Acc@1  64.06 ( 72.81)	Acc@5  96.09 ( 93.69)
03-Mar-22 09:11:24 - Epoch: [2][250/352]	Time  0.129 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.9330e-01 (9.8762e-01)	Acc@1  73.44 ( 73.07)	Acc@5  93.75 ( 93.78)
03-Mar-22 09:11:25 - Epoch: [1][190/352]	Time  0.148 ( 0.154)	Data  0.003 ( 0.004)	Loss 1.0190e+00 (1.0259e+00)	Acc@1  73.44 ( 72.75)	Acc@5  95.31 ( 93.69)
03-Mar-22 09:11:26 - Epoch: [2][260/352]	Time  0.156 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.0719e+00 (9.8678e-01)	Acc@1  67.19 ( 73.11)	Acc@5  90.62 ( 93.81)
03-Mar-22 09:11:27 - Epoch: [1][200/352]	Time  0.155 ( 0.154)	Data  0.003 ( 0.004)	Loss 8.1975e-01 (1.0238e+00)	Acc@1  76.56 ( 72.73)	Acc@5  95.31 ( 93.73)
03-Mar-22 09:11:27 - Epoch: [2][270/352]	Time  0.151 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.1675e-01 (9.8544e-01)	Acc@1  75.78 ( 73.15)	Acc@5  92.19 ( 93.80)
03-Mar-22 09:11:28 - Epoch: [1][210/352]	Time  0.158 ( 0.154)	Data  0.003 ( 0.004)	Loss 1.0087e+00 (1.0254e+00)	Acc@1  72.66 ( 72.65)	Acc@5  90.62 ( 93.70)
03-Mar-22 09:11:29 - Epoch: [2][280/352]	Time  0.152 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.0960e+00 (9.8446e-01)	Acc@1  70.31 ( 73.20)	Acc@5  93.75 ( 93.82)
03-Mar-22 09:11:30 - Epoch: [1][220/352]	Time  0.143 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.1567e-01 (1.0258e+00)	Acc@1  78.91 ( 72.64)	Acc@5  96.88 ( 93.68)
03-Mar-22 09:11:30 - Epoch: [2][290/352]	Time  0.160 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.5220e-01 (9.8321e-01)	Acc@1  75.00 ( 73.18)	Acc@5  96.09 ( 93.84)
03-Mar-22 09:11:31 - Epoch: [1][230/352]	Time  0.151 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0864e+00 (1.0257e+00)	Acc@1  75.00 ( 72.68)	Acc@5  92.97 ( 93.70)
03-Mar-22 09:11:32 - Epoch: [2][300/352]	Time  0.128 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.1841e-01 (9.8426e-01)	Acc@1  73.44 ( 73.12)	Acc@5  96.88 ( 93.85)
03-Mar-22 09:11:33 - Epoch: [1][240/352]	Time  0.142 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.1505e+00 (1.0271e+00)	Acc@1  69.53 ( 72.65)	Acc@5  90.62 ( 93.68)
03-Mar-22 09:11:33 - Epoch: [2][310/352]	Time  0.159 ( 0.148)	Data  0.003 ( 0.003)	Loss 7.1542e-01 (9.8206e-01)	Acc@1  80.47 ( 73.16)	Acc@5  96.88 ( 93.88)
03-Mar-22 09:11:34 - Epoch: [1][250/352]	Time  0.154 ( 0.154)	Data  0.003 ( 0.003)	Loss 1.0373e+00 (1.0262e+00)	Acc@1  67.97 ( 72.62)	Acc@5  92.19 ( 93.67)
03-Mar-22 09:11:35 - Epoch: [2][320/352]	Time  0.142 ( 0.148)	Data  0.003 ( 0.003)	Loss 1.0400e+00 (9.8373e-01)	Acc@1  67.97 ( 73.10)	Acc@5  94.53 ( 93.87)
03-Mar-22 09:11:36 - Epoch: [1][260/352]	Time  0.147 ( 0.154)	Data  0.003 ( 0.003)	Loss 1.0045e+00 (1.0247e+00)	Acc@1  71.88 ( 72.65)	Acc@5  95.31 ( 93.68)
03-Mar-22 09:11:36 - Epoch: [2][330/352]	Time  0.155 ( 0.148)	Data  0.003 ( 0.003)	Loss 9.3201e-01 (9.8306e-01)	Acc@1  80.47 ( 73.13)	Acc@5  92.19 ( 93.88)
03-Mar-22 09:11:37 - Epoch: [1][270/352]	Time  0.152 ( 0.154)	Data  0.003 ( 0.003)	Loss 9.6567e-01 (1.0257e+00)	Acc@1  72.66 ( 72.56)	Acc@5  92.97 ( 93.65)
03-Mar-22 09:11:38 - Epoch: [2][340/352]	Time  0.149 ( 0.148)	Data  0.003 ( 0.003)	Loss 9.1257e-01 (9.8214e-01)	Acc@1  75.78 ( 73.18)	Acc@5  93.75 ( 93.88)
03-Mar-22 09:11:39 - Epoch: [1][280/352]	Time  0.148 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.2215e-01 (1.0248e+00)	Acc@1  72.66 ( 72.60)	Acc@5  96.09 ( 93.61)
03-Mar-22 09:11:39 - Epoch: [2][350/352]	Time  0.151 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.3341e-01 (9.8245e-01)	Acc@1  73.44 ( 73.18)	Acc@5  97.66 ( 93.85)
03-Mar-22 09:11:40 - Test: [ 0/20]	Time  0.367 ( 0.367)	Loss 9.8246e-01 (9.8246e-01)	Acc@1  71.48 ( 71.48)	Acc@5  95.70 ( 95.70)
03-Mar-22 09:11:40 - Epoch: [1][290/352]	Time  0.178 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.2467e-01 (1.0239e+00)	Acc@1  71.88 ( 72.63)	Acc@5  96.88 ( 93.63)
03-Mar-22 09:11:41 - Test: [10/20]	Time  0.104 ( 0.129)	Loss 8.8707e-01 (9.7785e-01)	Acc@1  76.17 ( 73.33)	Acc@5  94.14 ( 94.00)
03-Mar-22 09:11:42 - Epoch: [1][300/352]	Time  0.180 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0666e+00 (1.0220e+00)	Acc@1  70.31 ( 72.62)	Acc@5  94.53 ( 93.65)
03-Mar-22 09:11:42 -  * Acc@1 73.280 Acc@5 93.920
03-Mar-22 09:11:42 - Best acc at epoch 2: 73.47999572753906
03-Mar-22 09:11:42 - Epoch: [3][  0/352]	Time  0.380 ( 0.380)	Data  0.237 ( 0.237)	Loss 1.0581e+00 (1.0581e+00)	Acc@1  71.09 ( 71.09)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:11:43 - Epoch: [1][310/352]	Time  0.162 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.1977e+00 (1.0237e+00)	Acc@1  69.53 ( 72.57)	Acc@5  91.41 ( 93.62)
03-Mar-22 09:11:44 - Epoch: [3][ 10/352]	Time  0.158 ( 0.169)	Data  0.002 ( 0.023)	Loss 1.0670e+00 (9.5376e-01)	Acc@1  68.75 ( 73.22)	Acc@5  94.53 ( 94.82)
03-Mar-22 09:11:45 - Epoch: [1][320/352]	Time  0.152 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.3438e-01 (1.0219e+00)	Acc@1  74.22 ( 72.63)	Acc@5  92.97 ( 93.61)
03-Mar-22 09:11:45 - Epoch: [3][ 20/352]	Time  0.149 ( 0.162)	Data  0.002 ( 0.014)	Loss 8.5733e-01 (9.3160e-01)	Acc@1  75.00 ( 73.96)	Acc@5 100.00 ( 94.90)
03-Mar-22 09:11:46 - Epoch: [1][330/352]	Time  0.135 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.1270e+00 (1.0202e+00)	Acc@1  70.31 ( 72.64)	Acc@5  92.97 ( 93.62)
03-Mar-22 09:11:47 - Epoch: [3][ 30/352]	Time  0.158 ( 0.160)	Data  0.002 ( 0.010)	Loss 9.5946e-01 (9.6085e-01)	Acc@1  74.22 ( 72.91)	Acc@5  92.19 ( 94.48)
03-Mar-22 09:11:48 - Epoch: [1][340/352]	Time  0.167 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0065e+00 (1.0191e+00)	Acc@1  75.78 ( 72.67)	Acc@5  93.75 ( 93.66)
03-Mar-22 09:11:49 - Epoch: [3][ 40/352]	Time  0.147 ( 0.158)	Data  0.002 ( 0.008)	Loss 8.8130e-01 (9.6299e-01)	Acc@1  75.00 ( 72.85)	Acc@5  96.88 ( 94.38)
03-Mar-22 09:11:49 - Epoch: [1][350/352]	Time  0.149 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.7916e-01 (1.0182e+00)	Acc@1  73.44 ( 72.68)	Acc@5  93.75 ( 93.67)
03-Mar-22 09:11:50 - Test: [ 0/20]	Time  0.351 ( 0.351)	Loss 1.0769e+00 (1.0769e+00)	Acc@1  71.48 ( 71.48)	Acc@5  91.02 ( 91.02)
03-Mar-22 09:11:50 - Epoch: [3][ 50/352]	Time  0.170 ( 0.155)	Data  0.002 ( 0.007)	Loss 9.7010e-01 (9.6136e-01)	Acc@1  75.78 ( 72.96)	Acc@5  94.53 ( 94.24)
03-Mar-22 09:11:51 - Test: [10/20]	Time  0.100 ( 0.120)	Loss 8.5466e-01 (9.8098e-01)	Acc@1  76.56 ( 73.83)	Acc@5  96.48 ( 93.89)
03-Mar-22 09:11:52 - Epoch: [3][ 60/352]	Time  0.142 ( 0.156)	Data  0.002 ( 0.006)	Loss 9.9789e-01 (9.6877e-01)	Acc@1  72.66 ( 72.67)	Acc@5  93.75 ( 94.13)
03-Mar-22 09:11:52 -  * Acc@1 73.540 Acc@5 93.920
03-Mar-22 09:11:52 - Best acc at epoch 1: 73.54000091552734
03-Mar-22 09:11:52 - Epoch: [2][  0/352]	Time  0.396 ( 0.396)	Data  0.227 ( 0.227)	Loss 8.4828e-01 (8.4828e-01)	Acc@1  77.34 ( 77.34)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:11:53 - Epoch: [3][ 70/352]	Time  0.142 ( 0.154)	Data  0.002 ( 0.006)	Loss 1.1829e+00 (9.6623e-01)	Acc@1  69.53 ( 72.72)	Acc@5  89.84 ( 94.12)
03-Mar-22 09:11:54 - Epoch: [2][ 10/352]	Time  0.142 ( 0.170)	Data  0.002 ( 0.023)	Loss 9.2640e-01 (9.6868e-01)	Acc@1  75.00 ( 73.86)	Acc@5  92.19 ( 93.89)
03-Mar-22 09:11:55 - Epoch: [3][ 80/352]	Time  0.168 ( 0.156)	Data  0.003 ( 0.005)	Loss 9.7158e-01 (9.6435e-01)	Acc@1  70.31 ( 72.96)	Acc@5  96.09 ( 94.16)
03-Mar-22 09:11:55 - Epoch: [2][ 20/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.013)	Loss 1.0835e+00 (9.8252e-01)	Acc@1  65.62 ( 73.07)	Acc@5  92.97 ( 93.71)
03-Mar-22 09:11:56 - Epoch: [3][ 90/352]	Time  0.157 ( 0.156)	Data  0.002 ( 0.005)	Loss 8.1304e-01 (9.5706e-01)	Acc@1  79.69 ( 73.20)	Acc@5  96.09 ( 94.26)
03-Mar-22 09:11:57 - Epoch: [2][ 30/352]	Time  0.150 ( 0.154)	Data  0.002 ( 0.009)	Loss 9.7038e-01 (9.8382e-01)	Acc@1  77.34 ( 73.41)	Acc@5  90.62 ( 93.55)
03-Mar-22 09:11:58 - Epoch: [3][100/352]	Time  0.182 ( 0.155)	Data  0.003 ( 0.005)	Loss 1.0897e+00 (9.5782e-01)	Acc@1  71.09 ( 73.19)	Acc@5  89.84 ( 94.19)
03-Mar-22 09:11:58 - Epoch: [2][ 40/352]	Time  0.159 ( 0.153)	Data  0.002 ( 0.008)	Loss 1.0371e+00 (1.0075e+00)	Acc@1  71.88 ( 72.52)	Acc@5  90.62 ( 93.35)
03-Mar-22 09:11:59 - Epoch: [3][110/352]	Time  0.146 ( 0.155)	Data  0.002 ( 0.005)	Loss 9.9441e-01 (9.6083e-01)	Acc@1  76.56 ( 73.18)	Acc@5  94.53 ( 94.12)
03-Mar-22 09:12:00 - Epoch: [2][ 50/352]	Time  0.166 ( 0.152)	Data  0.002 ( 0.007)	Loss 8.6704e-01 (1.0062e+00)	Acc@1  75.78 ( 72.52)	Acc@5  95.31 ( 93.34)
03-Mar-22 09:12:01 - Epoch: [3][120/352]	Time  0.151 ( 0.155)	Data  0.003 ( 0.005)	Loss 8.5311e-01 (9.5494e-01)	Acc@1  76.56 ( 73.34)	Acc@5  94.53 ( 94.21)
03-Mar-22 09:12:01 - Epoch: [2][ 60/352]	Time  0.164 ( 0.152)	Data  0.002 ( 0.006)	Loss 9.0798e-01 (1.0043e+00)	Acc@1  76.56 ( 72.80)	Acc@5  94.53 ( 93.40)
03-Mar-22 09:12:02 - Epoch: [3][130/352]	Time  0.149 ( 0.155)	Data  0.004 ( 0.004)	Loss 9.5596e-01 (9.5813e-01)	Acc@1  69.53 ( 73.26)	Acc@5  95.31 ( 94.17)
03-Mar-22 09:12:02 - Epoch: [2][ 70/352]	Time  0.155 ( 0.151)	Data  0.002 ( 0.006)	Loss 1.1666e+00 (1.0124e+00)	Acc@1  67.19 ( 72.46)	Acc@5  90.62 ( 93.44)
03-Mar-22 09:12:04 - Epoch: [3][140/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.7628e-01 (9.5565e-01)	Acc@1  75.78 ( 73.33)	Acc@5  88.28 ( 94.19)
03-Mar-22 09:12:04 - Epoch: [2][ 80/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.005)	Loss 8.9730e-01 (1.0133e+00)	Acc@1  73.44 ( 72.37)	Acc@5  93.75 ( 93.39)
03-Mar-22 09:12:05 - Epoch: [3][150/352]	Time  0.161 ( 0.154)	Data  0.003 ( 0.004)	Loss 1.0884e+00 (9.5921e-01)	Acc@1  67.97 ( 73.20)	Acc@5  94.53 ( 94.15)
03-Mar-22 09:12:05 - Epoch: [2][ 90/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.005)	Loss 1.0123e+00 (1.0115e+00)	Acc@1  72.66 ( 72.33)	Acc@5  93.75 ( 93.52)
03-Mar-22 09:12:07 - Epoch: [3][160/352]	Time  0.148 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.6627e-01 (9.5958e-01)	Acc@1  74.22 ( 73.23)	Acc@5  94.53 ( 94.14)
03-Mar-22 09:12:07 - Epoch: [2][100/352]	Time  0.150 ( 0.151)	Data  0.003 ( 0.005)	Loss 1.2190e+00 (1.0101e+00)	Acc@1  61.72 ( 72.39)	Acc@5  93.75 ( 93.64)
03-Mar-22 09:12:08 - Epoch: [3][170/352]	Time  0.152 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.5612e-01 (9.5856e-01)	Acc@1  72.66 ( 73.28)	Acc@5  96.09 ( 94.14)
03-Mar-22 09:12:08 - Epoch: [2][110/352]	Time  0.145 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.1233e+00 (1.0069e+00)	Acc@1  67.19 ( 72.52)	Acc@5  90.62 ( 93.69)
03-Mar-22 09:12:10 - Epoch: [3][180/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0380e+00 (9.5591e-01)	Acc@1  72.66 ( 73.41)	Acc@5  91.41 ( 94.11)
03-Mar-22 09:12:10 - Epoch: [2][120/352]	Time  0.148 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.8975e-01 (9.9932e-01)	Acc@1  78.12 ( 72.84)	Acc@5  92.97 ( 93.73)
03-Mar-22 09:12:11 - Epoch: [3][190/352]	Time  0.146 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.8277e-01 (9.5816e-01)	Acc@1  75.00 ( 73.40)	Acc@5  95.31 ( 94.09)
03-Mar-22 09:12:11 - Epoch: [2][130/352]	Time  0.150 ( 0.150)	Data  0.003 ( 0.004)	Loss 9.0538e-01 (9.9746e-01)	Acc@1  76.56 ( 72.95)	Acc@5  96.09 ( 93.77)
03-Mar-22 09:12:13 - Epoch: [3][200/352]	Time  0.160 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.2353e-01 (9.5815e-01)	Acc@1  77.34 ( 73.41)	Acc@5  94.53 ( 94.12)
03-Mar-22 09:12:13 - Epoch: [2][140/352]	Time  0.155 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0365e+00 (9.9830e-01)	Acc@1  70.31 ( 72.93)	Acc@5  92.19 ( 93.77)
03-Mar-22 09:12:14 - Epoch: [3][210/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0753e+00 (9.6032e-01)	Acc@1  70.31 ( 73.39)	Acc@5  92.97 ( 94.12)
03-Mar-22 09:12:14 - Epoch: [2][150/352]	Time  0.136 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0241e+00 (9.9716e-01)	Acc@1  75.00 ( 73.02)	Acc@5  94.53 ( 93.82)
03-Mar-22 09:12:16 - Epoch: [3][220/352]	Time  0.131 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.8683e-01 (9.6021e-01)	Acc@1  74.22 ( 73.36)	Acc@5  97.66 ( 94.14)
03-Mar-22 09:12:16 - Epoch: [2][160/352]	Time  0.156 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.4970e-01 (9.9992e-01)	Acc@1  72.66 ( 72.97)	Acc@5  95.31 ( 93.79)
03-Mar-22 09:12:17 - Epoch: [3][230/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0459e+00 (9.6256e-01)	Acc@1  69.53 ( 73.32)	Acc@5  95.31 ( 94.07)
03-Mar-22 09:12:17 - Epoch: [2][170/352]	Time  0.134 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.5907e-01 (9.9771e-01)	Acc@1  74.22 ( 73.01)	Acc@5  96.88 ( 93.85)
03-Mar-22 09:12:19 - Epoch: [3][240/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0360e+00 (9.6440e-01)	Acc@1  69.53 ( 73.23)	Acc@5  92.19 ( 94.04)
03-Mar-22 09:12:19 - Epoch: [2][180/352]	Time  0.161 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.1565e+00 (1.0002e+00)	Acc@1  64.84 ( 72.83)	Acc@5  92.19 ( 93.80)
03-Mar-22 09:12:20 - Epoch: [3][250/352]	Time  0.146 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.9937e-01 (9.6472e-01)	Acc@1  74.22 ( 73.23)	Acc@5  97.66 ( 94.05)
03-Mar-22 09:12:20 - Epoch: [2][190/352]	Time  0.163 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.6995e-01 (1.0021e+00)	Acc@1  79.69 ( 72.77)	Acc@5  93.75 ( 93.78)
03-Mar-22 09:12:22 - Epoch: [3][260/352]	Time  0.159 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.8531e-01 (9.6652e-01)	Acc@1  77.34 ( 73.20)	Acc@5  97.66 ( 94.01)
03-Mar-22 09:12:22 - Epoch: [2][200/352]	Time  0.163 ( 0.151)	Data  0.003 ( 0.004)	Loss 9.7392e-01 (1.0011e+00)	Acc@1  73.44 ( 72.75)	Acc@5  94.53 ( 93.82)
03-Mar-22 09:12:23 - Epoch: [3][270/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.2292e-01 (9.6682e-01)	Acc@1  77.34 ( 73.21)	Acc@5  96.09 ( 94.00)
03-Mar-22 09:12:24 - Epoch: [2][210/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.8533e-01 (9.9914e-01)	Acc@1  72.66 ( 72.80)	Acc@5  92.97 ( 93.83)
03-Mar-22 09:12:25 - Epoch: [3][280/352]	Time  0.143 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.8017e-01 (9.6853e-01)	Acc@1  76.56 ( 73.19)	Acc@5  94.53 ( 93.96)
03-Mar-22 09:12:25 - Epoch: [2][220/352]	Time  0.157 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.5441e-01 (9.9904e-01)	Acc@1  75.78 ( 72.78)	Acc@5  90.62 ( 93.81)
03-Mar-22 09:12:26 - Epoch: [3][290/352]	Time  0.150 ( 0.152)	Data  0.003 ( 0.003)	Loss 8.0160e-01 (9.6709e-01)	Acc@1  76.56 ( 73.25)	Acc@5  96.88 ( 93.99)
03-Mar-22 09:12:27 - Epoch: [2][230/352]	Time  0.156 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.9417e-01 (9.9650e-01)	Acc@1  76.56 ( 72.89)	Acc@5  93.75 ( 93.83)
03-Mar-22 09:12:28 - Epoch: [3][300/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.9926e-01 (9.6462e-01)	Acc@1  75.78 ( 73.30)	Acc@5  93.75 ( 94.03)
03-Mar-22 09:12:28 - Epoch: [2][240/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0640e+00 (9.9457e-01)	Acc@1  71.88 ( 72.96)	Acc@5  92.97 ( 93.86)
03-Mar-22 09:12:29 - Epoch: [3][310/352]	Time  0.135 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.3223e-01 (9.6573e-01)	Acc@1  67.97 ( 73.27)	Acc@5  94.53 ( 94.00)
03-Mar-22 09:12:30 - Epoch: [2][250/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0506e+00 (9.9491e-01)	Acc@1  67.97 ( 72.92)	Acc@5  94.53 ( 93.85)
03-Mar-22 09:12:31 - Epoch: [3][320/352]	Time  0.169 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.8546e-01 (9.6639e-01)	Acc@1  68.75 ( 73.27)	Acc@5  92.19 ( 94.00)
03-Mar-22 09:12:31 - Epoch: [2][260/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.2020e+00 (9.9539e-01)	Acc@1  67.97 ( 72.89)	Acc@5  91.41 ( 93.83)
03-Mar-22 09:12:32 - Epoch: [3][330/352]	Time  0.124 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.1038e+00 (9.6639e-01)	Acc@1  69.53 ( 73.25)	Acc@5  92.97 ( 94.02)
03-Mar-22 09:12:33 - Epoch: [2][270/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.9105e-01 (9.9344e-01)	Acc@1  72.66 ( 72.95)	Acc@5  94.53 ( 93.84)
03-Mar-22 09:12:34 - Epoch: [3][340/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.9288e-01 (9.6618e-01)	Acc@1  72.66 ( 73.27)	Acc@5  97.66 ( 94.00)
03-Mar-22 09:12:34 - Epoch: [2][280/352]	Time  0.145 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.1498e+00 (9.9265e-01)	Acc@1  64.84 ( 72.98)	Acc@5  90.62 ( 93.82)
03-Mar-22 09:12:35 - Epoch: [3][350/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.9907e-01 (9.6521e-01)	Acc@1  75.00 ( 73.31)	Acc@5  96.09 ( 94.02)
03-Mar-22 09:12:36 - Epoch: [2][290/352]	Time  0.137 ( 0.151)	Data  0.003 ( 0.003)	Loss 8.9197e-01 (9.9158e-01)	Acc@1  77.34 ( 73.01)	Acc@5  93.75 ( 93.82)
03-Mar-22 09:12:36 - Test: [ 0/20]	Time  0.387 ( 0.387)	Loss 9.8055e-01 (9.8055e-01)	Acc@1  71.88 ( 71.88)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:12:37 - Test: [10/20]	Time  0.095 ( 0.128)	Loss 8.5108e-01 (9.5467e-01)	Acc@1  77.34 ( 73.37)	Acc@5  94.14 ( 93.82)
03-Mar-22 09:12:37 - Epoch: [2][300/352]	Time  0.169 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.7314e-01 (9.9024e-01)	Acc@1  74.22 ( 73.04)	Acc@5  97.66 ( 93.84)
03-Mar-22 09:12:38 -  * Acc@1 73.040 Acc@5 93.920
03-Mar-22 09:12:38 - Best acc at epoch 3: 73.47999572753906
03-Mar-22 09:12:39 - Epoch: [4][  0/352]	Time  0.370 ( 0.370)	Data  0.238 ( 0.238)	Loss 8.9065e-01 (8.9065e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:12:39 - Epoch: [2][310/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0972e+00 (9.9058e-01)	Acc@1  71.88 ( 73.00)	Acc@5  93.75 ( 93.86)
03-Mar-22 09:12:40 - Epoch: [4][ 10/352]	Time  0.145 ( 0.168)	Data  0.002 ( 0.024)	Loss 9.2148e-01 (9.0829e-01)	Acc@1  75.78 ( 75.14)	Acc@5  94.53 ( 95.17)
03-Mar-22 09:12:40 - Epoch: [2][320/352]	Time  0.149 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0091e+00 (9.9010e-01)	Acc@1  71.88 ( 73.02)	Acc@5  92.19 ( 93.83)
03-Mar-22 09:12:42 - Epoch: [4][ 20/352]	Time  0.149 ( 0.159)	Data  0.002 ( 0.014)	Loss 1.0547e+00 (9.3479e-01)	Acc@1  70.31 ( 74.44)	Acc@5  91.41 ( 94.68)
03-Mar-22 09:12:42 - Epoch: [2][330/352]	Time  0.172 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.4555e-01 (9.9053e-01)	Acc@1  79.69 ( 73.05)	Acc@5  94.53 ( 93.81)
03-Mar-22 09:12:43 - Epoch: [4][ 30/352]	Time  0.138 ( 0.155)	Data  0.002 ( 0.010)	Loss 9.5017e-01 (9.3960e-01)	Acc@1  74.22 ( 74.14)	Acc@5  90.62 ( 94.46)
03-Mar-22 09:12:43 - Epoch: [2][340/352]	Time  0.151 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.3237e-01 (9.8998e-01)	Acc@1  78.12 ( 73.07)	Acc@5  92.19 ( 93.81)
03-Mar-22 09:12:44 - Epoch: [4][ 40/352]	Time  0.139 ( 0.152)	Data  0.002 ( 0.008)	Loss 8.9455e-01 (9.3765e-01)	Acc@1  79.69 ( 74.14)	Acc@5  92.19 ( 94.17)
03-Mar-22 09:12:45 - Epoch: [2][350/352]	Time  0.132 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0722e+00 (9.8957e-01)	Acc@1  67.19 ( 73.01)	Acc@5  95.31 ( 93.83)
03-Mar-22 09:12:45 - Test: [ 0/20]	Time  0.363 ( 0.363)	Loss 1.0392e+00 (1.0392e+00)	Acc@1  71.88 ( 71.88)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:12:46 - Epoch: [4][ 50/352]	Time  0.172 ( 0.150)	Data  0.002 ( 0.007)	Loss 9.6912e-01 (9.3839e-01)	Acc@1  73.44 ( 74.02)	Acc@5  92.97 ( 94.18)
03-Mar-22 09:12:46 - Test: [10/20]	Time  0.101 ( 0.125)	Loss 8.8246e-01 (9.9372e-01)	Acc@1  77.34 ( 72.73)	Acc@5  92.97 ( 93.25)
03-Mar-22 09:12:47 -  * Acc@1 72.920 Acc@5 93.440
03-Mar-22 09:12:47 - Best acc at epoch 2: 73.54000091552734
03-Mar-22 09:12:47 - Epoch: [4][ 60/352]	Time  0.105 ( 0.150)	Data  0.002 ( 0.006)	Loss 1.0240e+00 (9.3283e-01)	Acc@1  71.88 ( 74.37)	Acc@5  91.41 ( 94.17)
03-Mar-22 09:12:48 - Epoch: [3][  0/352]	Time  0.369 ( 0.369)	Data  0.229 ( 0.229)	Loss 9.0636e-01 (9.0636e-01)	Acc@1  75.78 ( 75.78)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:12:49 - Epoch: [4][ 70/352]	Time  0.145 ( 0.148)	Data  0.003 ( 0.006)	Loss 8.9461e-01 (9.3883e-01)	Acc@1  72.66 ( 74.20)	Acc@5  95.31 ( 94.11)
03-Mar-22 09:12:49 - Epoch: [3][ 10/352]	Time  0.134 ( 0.165)	Data  0.002 ( 0.023)	Loss 7.1194e-01 (9.4063e-01)	Acc@1  81.25 ( 72.94)	Acc@5  98.44 ( 94.74)
03-Mar-22 09:12:50 - Epoch: [4][ 80/352]	Time  0.151 ( 0.148)	Data  0.002 ( 0.005)	Loss 7.1759e-01 (9.4156e-01)	Acc@1  80.47 ( 74.03)	Acc@5  96.09 ( 94.07)
03-Mar-22 09:12:51 - Epoch: [3][ 20/352]	Time  0.133 ( 0.158)	Data  0.002 ( 0.013)	Loss 1.0971e+00 (9.4455e-01)	Acc@1  69.53 ( 74.22)	Acc@5  89.84 ( 94.05)
03-Mar-22 09:12:52 - Epoch: [4][ 90/352]	Time  0.156 ( 0.149)	Data  0.002 ( 0.005)	Loss 1.0950e+00 (9.4379e-01)	Acc@1  69.53 ( 73.92)	Acc@5  92.19 ( 94.03)
03-Mar-22 09:12:52 - Epoch: [3][ 30/352]	Time  0.150 ( 0.158)	Data  0.002 ( 0.010)	Loss 1.1593e+00 (9.6011e-01)	Acc@1  68.75 ( 74.14)	Acc@5  90.62 ( 93.65)
03-Mar-22 09:12:53 - Epoch: [4][100/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.005)	Loss 1.2731e+00 (9.4333e-01)	Acc@1  64.84 ( 74.00)	Acc@5  92.97 ( 94.05)
03-Mar-22 09:12:54 - Epoch: [3][ 40/352]	Time  0.153 ( 0.157)	Data  0.002 ( 0.008)	Loss 9.2052e-01 (9.6201e-01)	Acc@1  74.22 ( 73.74)	Acc@5  92.19 ( 93.65)
03-Mar-22 09:12:55 - Epoch: [4][110/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.005)	Loss 8.8654e-01 (9.4346e-01)	Acc@1  77.34 ( 73.78)	Acc@5  92.97 ( 94.07)
03-Mar-22 09:12:55 - Epoch: [3][ 50/352]	Time  0.152 ( 0.155)	Data  0.002 ( 0.007)	Loss 8.9899e-01 (9.6298e-01)	Acc@1  73.44 ( 73.70)	Acc@5  97.66 ( 93.70)
03-Mar-22 09:12:56 - Epoch: [4][120/352]	Time  0.149 ( 0.149)	Data  0.003 ( 0.004)	Loss 8.9693e-01 (9.4026e-01)	Acc@1  72.66 ( 73.93)	Acc@5  95.31 ( 94.11)
03-Mar-22 09:12:57 - Epoch: [3][ 60/352]	Time  0.144 ( 0.155)	Data  0.002 ( 0.006)	Loss 8.1843e-01 (9.6425e-01)	Acc@1  78.12 ( 73.74)	Acc@5  97.66 ( 93.71)
03-Mar-22 09:12:58 - Epoch: [4][130/352]	Time  0.149 ( 0.149)	Data  0.003 ( 0.004)	Loss 8.0814e-01 (9.3640e-01)	Acc@1  75.78 ( 74.06)	Acc@5  95.31 ( 94.16)
03-Mar-22 09:12:58 - Epoch: [3][ 70/352]	Time  0.162 ( 0.154)	Data  0.002 ( 0.006)	Loss 9.0184e-01 (9.6869e-01)	Acc@1  81.25 ( 73.71)	Acc@5  92.19 ( 93.72)
03-Mar-22 09:12:59 - Epoch: [4][140/352]	Time  0.136 ( 0.149)	Data  0.003 ( 0.004)	Loss 1.0134e+00 (9.4075e-01)	Acc@1  71.09 ( 73.88)	Acc@5  94.53 ( 94.15)
03-Mar-22 09:13:00 - Epoch: [3][ 80/352]	Time  0.130 ( 0.154)	Data  0.002 ( 0.005)	Loss 1.1108e+00 (9.7007e-01)	Acc@1  69.53 ( 73.74)	Acc@5  90.62 ( 93.70)
03-Mar-22 09:13:01 - Epoch: [4][150/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.9636e-01 (9.4262e-01)	Acc@1  71.09 ( 73.85)	Acc@5  92.97 ( 94.10)
03-Mar-22 09:13:01 - Epoch: [3][ 90/352]	Time  0.160 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.7781e-01 (9.7182e-01)	Acc@1  79.69 ( 73.71)	Acc@5  94.53 ( 93.64)
03-Mar-22 09:13:02 - Epoch: [4][160/352]	Time  0.150 ( 0.149)	Data  0.002 ( 0.004)	Loss 1.0534e+00 (9.4130e-01)	Acc@1  68.75 ( 73.87)	Acc@5  95.31 ( 94.11)
03-Mar-22 09:13:03 - Epoch: [3][100/352]	Time  0.163 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.2609e-01 (9.7283e-01)	Acc@1  80.47 ( 73.56)	Acc@5  96.09 ( 93.62)
03-Mar-22 09:13:04 - Epoch: [4][170/352]	Time  0.155 ( 0.149)	Data  0.002 ( 0.004)	Loss 7.9606e-01 (9.3750e-01)	Acc@1  79.69 ( 73.94)	Acc@5  96.09 ( 94.16)
03-Mar-22 09:13:04 - Epoch: [3][110/352]	Time  0.144 ( 0.153)	Data  0.004 ( 0.004)	Loss 9.1392e-01 (9.6656e-01)	Acc@1  75.00 ( 73.80)	Acc@5  95.31 ( 93.71)
03-Mar-22 09:13:05 - Epoch: [4][180/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.004)	Loss 1.0266e+00 (9.3902e-01)	Acc@1  71.88 ( 73.90)	Acc@5  89.84 ( 94.13)
03-Mar-22 09:13:06 - Epoch: [3][120/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.7681e-01 (9.6914e-01)	Acc@1  71.88 ( 73.66)	Acc@5  95.31 ( 93.67)
03-Mar-22 09:13:07 - Epoch: [4][190/352]	Time  0.169 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.1897e-01 (9.4001e-01)	Acc@1  81.25 ( 73.86)	Acc@5  96.09 ( 94.09)
03-Mar-22 09:13:07 - Epoch: [3][130/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.5037e-01 (9.7063e-01)	Acc@1  71.09 ( 73.52)	Acc@5  93.75 ( 93.67)
03-Mar-22 09:13:08 - Epoch: [4][200/352]	Time  0.138 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.9632e-01 (9.4310e-01)	Acc@1  75.00 ( 73.78)	Acc@5  91.41 ( 94.03)
03-Mar-22 09:13:09 - Epoch: [3][140/352]	Time  0.142 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.1502e-01 (9.6659e-01)	Acc@1  77.34 ( 73.59)	Acc@5  92.97 ( 93.71)
03-Mar-22 09:13:10 - Epoch: [4][210/352]	Time  0.186 ( 0.149)	Data  0.003 ( 0.004)	Loss 1.0144e+00 (9.4351e-01)	Acc@1  71.88 ( 73.79)	Acc@5  93.75 ( 94.05)
03-Mar-22 09:13:10 - Epoch: [3][150/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.0140e-01 (9.6566e-01)	Acc@1  78.91 ( 73.56)	Acc@5  93.75 ( 93.74)
03-Mar-22 09:13:11 - Epoch: [4][220/352]	Time  0.160 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0686e+00 (9.4365e-01)	Acc@1  71.09 ( 73.82)	Acc@5  91.41 ( 94.03)
03-Mar-22 09:13:12 - Epoch: [3][160/352]	Time  0.152 ( 0.153)	Data  0.003 ( 0.004)	Loss 1.0174e+00 (9.6169e-01)	Acc@1  71.88 ( 73.70)	Acc@5  90.62 ( 93.79)
03-Mar-22 09:13:13 - Epoch: [4][230/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0720e+00 (9.4445e-01)	Acc@1  72.66 ( 73.82)	Acc@5  93.75 ( 94.03)
03-Mar-22 09:13:13 - Epoch: [3][170/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.3211e-01 (9.6370e-01)	Acc@1  78.91 ( 73.66)	Acc@5  94.53 ( 93.76)
03-Mar-22 09:13:14 - Epoch: [4][240/352]	Time  0.149 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.1776e+00 (9.4686e-01)	Acc@1  69.53 ( 73.76)	Acc@5  89.06 ( 93.98)
03-Mar-22 09:13:15 - Epoch: [3][180/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.6844e-01 (9.6605e-01)	Acc@1  74.22 ( 73.68)	Acc@5  92.97 ( 93.75)
03-Mar-22 09:13:16 - Epoch: [4][250/352]	Time  0.155 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.0163e-01 (9.4555e-01)	Acc@1  75.78 ( 73.80)	Acc@5  94.53 ( 94.01)
03-Mar-22 09:13:16 - Epoch: [3][190/352]	Time  0.163 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.1825e-01 (9.6519e-01)	Acc@1  81.25 ( 73.69)	Acc@5  96.09 ( 93.78)
03-Mar-22 09:13:17 - Epoch: [4][260/352]	Time  0.150 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.9228e-01 (9.4845e-01)	Acc@1  75.00 ( 73.70)	Acc@5  92.19 ( 93.97)
03-Mar-22 09:13:18 - Epoch: [3][200/352]	Time  0.152 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.9695e-01 (9.6714e-01)	Acc@1  75.00 ( 73.59)	Acc@5  92.19 ( 93.74)
03-Mar-22 09:13:19 - Epoch: [4][270/352]	Time  0.181 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.9150e-01 (9.4882e-01)	Acc@1  74.22 ( 73.67)	Acc@5  96.88 ( 93.99)
03-Mar-22 09:13:19 - Epoch: [3][210/352]	Time  0.135 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1206e+00 (9.6812e-01)	Acc@1  68.75 ( 73.54)	Acc@5  89.84 ( 93.72)
03-Mar-22 09:13:21 - Epoch: [4][280/352]	Time  0.156 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0049e+00 (9.4666e-01)	Acc@1  68.75 ( 73.73)	Acc@5  93.75 ( 94.06)
03-Mar-22 09:13:21 - Epoch: [3][220/352]	Time  0.131 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.6242e-01 (9.6657e-01)	Acc@1  73.44 ( 73.56)	Acc@5  96.88 ( 93.71)
03-Mar-22 09:13:22 - Epoch: [4][290/352]	Time  0.168 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0052e+00 (9.4778e-01)	Acc@1  73.44 ( 73.69)	Acc@5  92.97 ( 94.05)
03-Mar-22 09:13:23 - Epoch: [3][230/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0571e+00 (9.6727e-01)	Acc@1  68.75 ( 73.55)	Acc@5  92.97 ( 93.70)
03-Mar-22 09:13:24 - Epoch: [4][300/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1002e-01 (9.4717e-01)	Acc@1  78.12 ( 73.70)	Acc@5  95.31 ( 94.07)
03-Mar-22 09:13:24 - Epoch: [3][240/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0679e+00 (9.6632e-01)	Acc@1  71.09 ( 73.54)	Acc@5  91.41 ( 93.73)
03-Mar-22 09:13:25 - Epoch: [4][310/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.8484e-01 (9.4558e-01)	Acc@1  73.44 ( 73.74)	Acc@5  91.41 ( 94.06)
03-Mar-22 09:13:26 - Epoch: [3][250/352]	Time  0.160 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.9316e-01 (9.6559e-01)	Acc@1  82.03 ( 73.55)	Acc@5  94.53 ( 93.77)
03-Mar-22 09:13:27 - Epoch: [4][320/352]	Time  0.156 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.7276e-01 (9.4740e-01)	Acc@1  71.88 ( 73.68)	Acc@5  92.97 ( 94.03)
03-Mar-22 09:13:27 - Epoch: [3][260/352]	Time  0.135 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0236e+00 (9.6495e-01)	Acc@1  73.44 ( 73.55)	Acc@5  94.53 ( 93.82)
03-Mar-22 09:13:28 - Epoch: [4][330/352]	Time  0.140 ( 0.151)	Data  0.003 ( 0.003)	Loss 1.0123e+00 (9.4708e-01)	Acc@1  71.88 ( 73.68)	Acc@5  96.09 ( 94.05)
03-Mar-22 09:13:29 - Epoch: [3][270/352]	Time  0.155 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.9737e-01 (9.6500e-01)	Acc@1  71.88 ( 73.52)	Acc@5  92.97 ( 93.81)
03-Mar-22 09:13:30 - Epoch: [4][340/352]	Time  0.151 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.4033e-01 (9.4678e-01)	Acc@1  69.53 ( 73.69)	Acc@5  95.31 ( 94.05)
03-Mar-22 09:13:30 - Epoch: [3][280/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.9813e-01 (9.6622e-01)	Acc@1  66.41 ( 73.42)	Acc@5  93.75 ( 93.79)
03-Mar-22 09:13:31 - Epoch: [4][350/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0806e+00 (9.4796e-01)	Acc@1  71.88 ( 73.67)	Acc@5  92.97 ( 94.03)
03-Mar-22 09:13:32 - Epoch: [3][290/352]	Time  0.141 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.8646e-01 (9.6365e-01)	Acc@1  77.34 ( 73.48)	Acc@5  99.22 ( 93.84)
03-Mar-22 09:13:32 - Test: [ 0/20]	Time  0.357 ( 0.357)	Loss 9.5988e-01 (9.5988e-01)	Acc@1  74.61 ( 74.61)	Acc@5  95.70 ( 95.70)
03-Mar-22 09:13:33 - Test: [10/20]	Time  0.105 ( 0.126)	Loss 8.2218e-01 (9.3248e-01)	Acc@1  75.39 ( 74.61)	Acc@5  95.31 ( 94.25)
03-Mar-22 09:13:33 - Epoch: [3][300/352]	Time  0.172 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.6466e-01 (9.6284e-01)	Acc@1  69.53 ( 73.48)	Acc@5  92.19 ( 93.86)
03-Mar-22 09:13:34 -  * Acc@1 73.980 Acc@5 93.840
03-Mar-22 09:13:34 - Best acc at epoch 4: 73.97999572753906
03-Mar-22 09:13:34 - Epoch: [5][  0/352]	Time  0.365 ( 0.365)	Data  0.227 ( 0.227)	Loss 8.6514e-01 (8.6514e-01)	Acc@1  77.34 ( 77.34)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:13:34 - Epoch: [3][310/352]	Time  0.155 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.2032e+00 (9.6483e-01)	Acc@1  62.50 ( 73.39)	Acc@5  95.31 ( 93.84)
03-Mar-22 09:13:36 - Epoch: [5][ 10/352]	Time  0.136 ( 0.158)	Data  0.002 ( 0.023)	Loss 9.4775e-01 (9.1139e-01)	Acc@1  75.00 ( 75.78)	Acc@5  94.53 ( 95.10)
03-Mar-22 09:13:36 - Epoch: [3][320/352]	Time  0.155 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.1116e-01 (9.6510e-01)	Acc@1  72.66 ( 73.39)	Acc@5  95.31 ( 93.83)
03-Mar-22 09:13:37 - Epoch: [5][ 20/352]	Time  0.163 ( 0.152)	Data  0.002 ( 0.013)	Loss 8.8826e-01 (9.2683e-01)	Acc@1  70.31 ( 74.96)	Acc@5  96.88 ( 94.49)
03-Mar-22 09:13:37 - Epoch: [3][330/352]	Time  0.173 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.8909e-01 (9.6516e-01)	Acc@1  71.88 ( 73.39)	Acc@5  92.97 ( 93.83)
03-Mar-22 09:13:39 - Epoch: [5][ 30/352]	Time  0.154 ( 0.154)	Data  0.002 ( 0.010)	Loss 1.0104e+00 (9.1550e-01)	Acc@1  72.66 ( 75.10)	Acc@5  93.75 ( 94.71)
03-Mar-22 09:13:39 - Epoch: [3][340/352]	Time  0.162 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.7129e-01 (9.6605e-01)	Acc@1  77.34 ( 73.36)	Acc@5  93.75 ( 93.81)
03-Mar-22 09:13:40 - Epoch: [3][350/352]	Time  0.144 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.4757e-01 (9.6488e-01)	Acc@1  78.12 ( 73.39)	Acc@5  95.31 ( 93.83)
03-Mar-22 09:13:40 - Epoch: [5][ 40/352]	Time  0.162 ( 0.157)	Data  0.003 ( 0.008)	Loss 8.0733e-01 (9.2383e-01)	Acc@1  79.69 ( 74.73)	Acc@5  94.53 ( 94.61)
03-Mar-22 09:13:41 - Test: [ 0/20]	Time  0.340 ( 0.340)	Loss 1.0497e+00 (1.0497e+00)	Acc@1  69.14 ( 69.14)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:13:42 - Epoch: [5][ 50/352]	Time  0.179 ( 0.155)	Data  0.002 ( 0.007)	Loss 1.0667e+00 (9.4955e-01)	Acc@1  67.97 ( 74.10)	Acc@5  92.97 ( 94.16)
03-Mar-22 09:13:42 - Test: [10/20]	Time  0.108 ( 0.125)	Loss 8.7967e-01 (9.9434e-01)	Acc@1  75.78 ( 72.94)	Acc@5  96.48 ( 93.47)
03-Mar-22 09:13:43 -  * Acc@1 72.360 Acc@5 93.420
03-Mar-22 09:13:43 - Best acc at epoch 3: 73.54000091552734
03-Mar-22 09:13:43 - Epoch: [5][ 60/352]	Time  0.131 ( 0.153)	Data  0.002 ( 0.006)	Loss 9.1982e-01 (9.4187e-01)	Acc@1  74.22 ( 74.23)	Acc@5  92.97 ( 94.16)
03-Mar-22 09:13:43 - Epoch: [4][  0/352]	Time  0.349 ( 0.349)	Data  0.219 ( 0.219)	Loss 9.0541e-01 (9.0541e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.88 ( 96.88)
03-Mar-22 09:13:45 - Epoch: [5][ 70/352]	Time  0.144 ( 0.151)	Data  0.003 ( 0.006)	Loss 8.6178e-01 (9.4450e-01)	Acc@1  75.00 ( 74.04)	Acc@5  95.31 ( 94.17)
03-Mar-22 09:13:45 - Epoch: [4][ 10/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.022)	Loss 9.6264e-01 (9.6963e-01)	Acc@1  71.88 ( 72.66)	Acc@5  96.09 ( 94.53)
03-Mar-22 09:13:46 - Epoch: [5][ 80/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.005)	Loss 9.0904e-01 (9.4892e-01)	Acc@1  73.44 ( 73.75)	Acc@5  92.19 ( 94.11)
03-Mar-22 09:13:46 - Epoch: [4][ 20/352]	Time  0.160 ( 0.165)	Data  0.002 ( 0.012)	Loss 7.8881e-01 (9.2807e-01)	Acc@1  78.12 ( 74.11)	Acc@5  96.09 ( 94.49)
03-Mar-22 09:13:48 - Epoch: [5][ 90/352]	Time  0.137 ( 0.151)	Data  0.002 ( 0.005)	Loss 8.8044e-01 (9.4898e-01)	Acc@1  80.47 ( 73.90)	Acc@5  92.19 ( 94.03)
03-Mar-22 09:13:48 - Epoch: [4][ 30/352]	Time  0.137 ( 0.160)	Data  0.002 ( 0.009)	Loss 9.4488e-01 (9.4250e-01)	Acc@1  75.00 ( 73.89)	Acc@5  95.31 ( 94.25)
03-Mar-22 09:13:49 - Epoch: [5][100/352]	Time  0.145 ( 0.150)	Data  0.002 ( 0.005)	Loss 1.0309e+00 (9.4711e-01)	Acc@1  70.31 ( 73.93)	Acc@5  96.09 ( 94.11)
03-Mar-22 09:13:49 - Epoch: [4][ 40/352]	Time  0.155 ( 0.158)	Data  0.002 ( 0.007)	Loss 1.0671e+00 (9.3598e-01)	Acc@1  70.31 ( 74.28)	Acc@5  93.75 ( 94.26)
03-Mar-22 09:13:51 - Epoch: [5][110/352]	Time  0.166 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.4978e-01 (9.4198e-01)	Acc@1  78.12 ( 74.08)	Acc@5  95.31 ( 94.17)
03-Mar-22 09:13:51 - Epoch: [4][ 50/352]	Time  0.140 ( 0.157)	Data  0.002 ( 0.007)	Loss 9.3809e-01 (9.2070e-01)	Acc@1  73.44 ( 74.59)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:13:52 - Epoch: [5][120/352]	Time  0.135 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.8204e-01 (9.4221e-01)	Acc@1  72.66 ( 74.03)	Acc@5  96.09 ( 94.12)
03-Mar-22 09:13:52 - Epoch: [4][ 60/352]	Time  0.152 ( 0.155)	Data  0.003 ( 0.006)	Loss 9.5727e-01 (9.3075e-01)	Acc@1  75.00 ( 74.46)	Acc@5  92.19 ( 94.34)
03-Mar-22 09:13:54 - Epoch: [5][130/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.3983e-01 (9.4268e-01)	Acc@1  75.00 ( 73.93)	Acc@5  95.31 ( 94.17)
03-Mar-22 09:13:54 - Epoch: [4][ 70/352]	Time  0.168 ( 0.155)	Data  0.002 ( 0.005)	Loss 1.0612e+00 (9.3435e-01)	Acc@1  65.62 ( 74.35)	Acc@5  90.62 ( 94.19)
03-Mar-22 09:13:55 - Epoch: [5][140/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0160e+00 (9.4118e-01)	Acc@1  71.88 ( 73.96)	Acc@5  92.19 ( 94.19)
03-Mar-22 09:13:55 - Epoch: [4][ 80/352]	Time  0.146 ( 0.155)	Data  0.002 ( 0.005)	Loss 8.8740e-01 (9.3728e-01)	Acc@1  71.09 ( 74.20)	Acc@5  94.53 ( 94.03)
03-Mar-22 09:13:57 - Epoch: [5][150/352]	Time  0.155 ( 0.150)	Data  0.003 ( 0.004)	Loss 1.0998e+00 (9.3812e-01)	Acc@1  67.97 ( 74.03)	Acc@5  91.41 ( 94.21)
03-Mar-22 09:13:57 - Epoch: [4][ 90/352]	Time  0.139 ( 0.155)	Data  0.003 ( 0.005)	Loss 1.1746e+00 (9.4376e-01)	Acc@1  67.97 ( 74.07)	Acc@5  89.84 ( 93.92)
03-Mar-22 09:13:58 - Epoch: [5][160/352]	Time  0.128 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.1129e+00 (9.3933e-01)	Acc@1  66.41 ( 73.97)	Acc@5  91.41 ( 94.16)
03-Mar-22 09:13:59 - Epoch: [4][100/352]	Time  0.144 ( 0.155)	Data  0.002 ( 0.005)	Loss 1.0001e+00 (9.4798e-01)	Acc@1  72.66 ( 73.90)	Acc@5  92.97 ( 93.87)
03-Mar-22 09:13:59 - Epoch: [5][170/352]	Time  0.155 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.1682e-01 (9.4267e-01)	Acc@1  78.12 ( 73.84)	Acc@5  93.75 ( 94.13)
03-Mar-22 09:14:00 - Epoch: [4][110/352]	Time  0.152 ( 0.155)	Data  0.002 ( 0.004)	Loss 1.0108e+00 (9.4840e-01)	Acc@1  68.75 ( 73.79)	Acc@5  96.09 ( 93.95)
03-Mar-22 09:14:01 - Epoch: [5][180/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0119e+00 (9.4383e-01)	Acc@1  71.88 ( 73.77)	Acc@5  92.97 ( 94.15)
03-Mar-22 09:14:02 - Epoch: [4][120/352]	Time  0.157 ( 0.155)	Data  0.002 ( 0.004)	Loss 1.0029e+00 (9.4702e-01)	Acc@1  75.00 ( 73.89)	Acc@5  90.62 ( 93.91)
03-Mar-22 09:14:02 - Epoch: [5][190/352]	Time  0.155 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.5223e-01 (9.4410e-01)	Acc@1  75.78 ( 73.72)	Acc@5  93.75 ( 94.16)
03-Mar-22 09:14:03 - Epoch: [4][130/352]	Time  0.152 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.1182e+00 (9.4770e-01)	Acc@1  72.66 ( 73.84)	Acc@5  91.41 ( 93.97)
03-Mar-22 09:14:04 - Epoch: [5][200/352]	Time  0.149 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.9303e-01 (9.4246e-01)	Acc@1  75.00 ( 73.78)	Acc@5  95.31 ( 94.17)
03-Mar-22 09:14:05 - Epoch: [4][140/352]	Time  0.169 ( 0.155)	Data  0.003 ( 0.004)	Loss 1.0028e+00 (9.4621e-01)	Acc@1  72.66 ( 73.81)	Acc@5  94.53 ( 94.00)
03-Mar-22 09:14:05 - Epoch: [5][210/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.4446e-01 (9.4076e-01)	Acc@1  69.53 ( 73.73)	Acc@5  97.66 ( 94.21)
03-Mar-22 09:14:06 - Epoch: [4][150/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.004)	Loss 8.1710e-01 (9.4621e-01)	Acc@1  79.69 ( 73.79)	Acc@5  96.09 ( 94.00)
03-Mar-22 09:14:07 - Epoch: [5][220/352]	Time  0.122 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.4311e-01 (9.4186e-01)	Acc@1  70.31 ( 73.66)	Acc@5  93.75 ( 94.20)
03-Mar-22 09:14:08 - Epoch: [4][160/352]	Time  0.153 ( 0.155)	Data  0.002 ( 0.004)	Loss 9.1854e-01 (9.4920e-01)	Acc@1  75.78 ( 73.69)	Acc@5  90.62 ( 94.01)
03-Mar-22 09:14:08 - Epoch: [5][230/352]	Time  0.137 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.1702e-01 (9.4146e-01)	Acc@1  71.09 ( 73.68)	Acc@5  93.75 ( 94.23)
03-Mar-22 09:14:10 - Epoch: [4][170/352]	Time  0.157 ( 0.156)	Data  0.003 ( 0.004)	Loss 9.6566e-01 (9.5047e-01)	Acc@1  70.31 ( 73.64)	Acc@5  92.97 ( 93.98)
03-Mar-22 09:14:10 - Epoch: [5][240/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.0620e-01 (9.4116e-01)	Acc@1  71.09 ( 73.70)	Acc@5  96.88 ( 94.25)
03-Mar-22 09:14:11 - Epoch: [4][180/352]	Time  0.169 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.1141e+00 (9.5144e-01)	Acc@1  72.66 ( 73.61)	Acc@5  93.75 ( 93.99)
03-Mar-22 09:14:11 - Epoch: [5][250/352]	Time  0.131 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.3310e-01 (9.4059e-01)	Acc@1  75.00 ( 73.74)	Acc@5  96.09 ( 94.26)
03-Mar-22 09:14:13 - Epoch: [5][260/352]	Time  0.145 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0963e+00 (9.4236e-01)	Acc@1  67.97 ( 73.73)	Acc@5  93.75 ( 94.25)
03-Mar-22 09:14:13 - Epoch: [4][190/352]	Time  0.180 ( 0.156)	Data  0.003 ( 0.004)	Loss 8.5839e-01 (9.5031e-01)	Acc@1  71.88 ( 73.67)	Acc@5  96.88 ( 93.99)
03-Mar-22 09:14:14 - Epoch: [5][270/352]	Time  0.156 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0549e+00 (9.4374e-01)	Acc@1  69.53 ( 73.65)	Acc@5  95.31 ( 94.24)
03-Mar-22 09:14:14 - Epoch: [4][200/352]	Time  0.154 ( 0.156)	Data  0.002 ( 0.004)	Loss 8.5366e-01 (9.5181e-01)	Acc@1  78.91 ( 73.62)	Acc@5  95.31 ( 94.00)
03-Mar-22 09:14:16 - Epoch: [5][280/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1367e+00 (9.4394e-01)	Acc@1  68.75 ( 73.64)	Acc@5  89.06 ( 94.21)
03-Mar-22 09:14:16 - Epoch: [4][210/352]	Time  0.153 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.0263e+00 (9.5479e-01)	Acc@1  72.66 ( 73.51)	Acc@5  92.19 ( 93.95)
03-Mar-22 09:14:17 - Epoch: [5][290/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.8536e-01 (9.4299e-01)	Acc@1  68.75 ( 73.66)	Acc@5  94.53 ( 94.20)
03-Mar-22 09:14:17 - Epoch: [4][220/352]	Time  0.157 ( 0.156)	Data  0.003 ( 0.003)	Loss 1.0355e+00 (9.5817e-01)	Acc@1  72.66 ( 73.39)	Acc@5  90.62 ( 93.89)
03-Mar-22 09:14:19 - Epoch: [5][300/352]	Time  0.129 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.8354e-01 (9.4401e-01)	Acc@1  71.88 ( 73.66)	Acc@5  94.53 ( 94.18)
03-Mar-22 09:14:19 - Epoch: [4][230/352]	Time  0.165 ( 0.156)	Data  0.003 ( 0.003)	Loss 9.8744e-01 (9.5958e-01)	Acc@1  67.19 ( 73.35)	Acc@5  96.09 ( 93.89)
03-Mar-22 09:14:20 - Epoch: [5][310/352]	Time  0.153 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.9412e-01 (9.4222e-01)	Acc@1  71.88 ( 73.67)	Acc@5  93.75 ( 94.19)
03-Mar-22 09:14:20 - Epoch: [4][240/352]	Time  0.144 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.0091e+00 (9.5811e-01)	Acc@1  69.53 ( 73.36)	Acc@5  95.31 ( 93.90)
03-Mar-22 09:14:22 - Epoch: [5][320/352]	Time  0.154 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.4952e-01 (9.4171e-01)	Acc@1  72.66 ( 73.69)	Acc@5  93.75 ( 94.19)
03-Mar-22 09:14:22 - Epoch: [4][250/352]	Time  0.175 ( 0.156)	Data  0.003 ( 0.003)	Loss 1.0214e+00 (9.5555e-01)	Acc@1  72.66 ( 73.45)	Acc@5  92.19 ( 93.90)
03-Mar-22 09:14:23 - Epoch: [5][330/352]	Time  0.144 ( 0.149)	Data  0.003 ( 0.003)	Loss 9.6615e-01 (9.4111e-01)	Acc@1  75.00 ( 73.70)	Acc@5  91.41 ( 94.21)
03-Mar-22 09:14:24 - Epoch: [4][260/352]	Time  0.147 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.1533e+00 (9.5520e-01)	Acc@1  69.53 ( 73.48)	Acc@5  90.62 ( 93.90)
03-Mar-22 09:14:25 - Epoch: [5][340/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.7124e-01 (9.4065e-01)	Acc@1  70.31 ( 73.70)	Acc@5  92.97 ( 94.22)
03-Mar-22 09:14:25 - Epoch: [4][270/352]	Time  0.156 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.6715e-01 (9.5342e-01)	Acc@1  78.91 ( 73.53)	Acc@5  95.31 ( 93.93)
03-Mar-22 09:14:26 - Epoch: [5][350/352]	Time  0.129 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1022e+00 (9.3889e-01)	Acc@1  72.66 ( 73.77)	Acc@5  89.06 ( 94.23)
03-Mar-22 09:14:27 - Epoch: [4][280/352]	Time  0.105 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.7212e-01 (9.5519e-01)	Acc@1  70.31 ( 73.50)	Acc@5  94.53 ( 93.91)
03-Mar-22 09:14:27 - Test: [ 0/20]	Time  0.367 ( 0.367)	Loss 9.6177e-01 (9.6177e-01)	Acc@1  73.44 ( 73.44)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:14:28 - Test: [10/20]	Time  0.116 ( 0.128)	Loss 7.9424e-01 (9.4898e-01)	Acc@1  76.17 ( 73.44)	Acc@5  95.70 ( 93.79)
03-Mar-22 09:14:28 - Epoch: [4][290/352]	Time  0.186 ( 0.156)	Data  0.003 ( 0.003)	Loss 1.1813e+00 (9.5670e-01)	Acc@1  67.97 ( 73.41)	Acc@5  90.62 ( 93.88)
03-Mar-22 09:14:29 -  * Acc@1 73.720 Acc@5 93.580
03-Mar-22 09:14:29 - Best acc at epoch 5: 73.97999572753906
03-Mar-22 09:14:29 - Epoch: [6][  0/352]	Time  0.361 ( 0.361)	Data  0.222 ( 0.222)	Loss 1.0536e+00 (1.0536e+00)	Acc@1  74.22 ( 74.22)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:14:30 - Epoch: [4][300/352]	Time  0.153 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.0857e+00 (9.5734e-01)	Acc@1  69.53 ( 73.41)	Acc@5  92.97 ( 93.86)
03-Mar-22 09:14:31 - Epoch: [6][ 10/352]	Time  0.160 ( 0.168)	Data  0.002 ( 0.022)	Loss 1.0978e+00 (9.5124e-01)	Acc@1  70.31 ( 73.08)	Acc@5  89.84 ( 94.74)
03-Mar-22 09:14:31 - Epoch: [4][310/352]	Time  0.144 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.0356e+00 (9.5593e-01)	Acc@1  71.09 ( 73.47)	Acc@5  89.84 ( 93.87)
03-Mar-22 09:14:32 - Epoch: [6][ 20/352]	Time  0.137 ( 0.158)	Data  0.002 ( 0.013)	Loss 1.0306e+00 (9.3624e-01)	Acc@1  72.66 ( 73.59)	Acc@5  92.97 ( 94.61)
03-Mar-22 09:14:33 - Epoch: [4][320/352]	Time  0.136 ( 0.155)	Data  0.003 ( 0.003)	Loss 9.0386e-01 (9.5536e-01)	Acc@1  72.66 ( 73.43)	Acc@5  95.31 ( 93.89)
03-Mar-22 09:14:34 - Epoch: [6][ 30/352]	Time  0.142 ( 0.156)	Data  0.002 ( 0.009)	Loss 8.6593e-01 (9.2335e-01)	Acc@1  77.34 ( 74.09)	Acc@5  95.31 ( 94.68)
03-Mar-22 09:14:34 - Epoch: [4][330/352]	Time  0.156 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.9541e-01 (9.5391e-01)	Acc@1  78.12 ( 73.47)	Acc@5  93.75 ( 93.92)
03-Mar-22 09:14:35 - Epoch: [6][ 40/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.008)	Loss 1.0515e+00 (9.2208e-01)	Acc@1  70.31 ( 74.28)	Acc@5  92.97 ( 94.59)
03-Mar-22 09:14:36 - Epoch: [4][340/352]	Time  0.131 ( 0.155)	Data  0.002 ( 0.003)	Loss 9.9845e-01 (9.5435e-01)	Acc@1  67.19 ( 73.44)	Acc@5  92.97 ( 93.91)
03-Mar-22 09:14:37 - Epoch: [6][ 50/352]	Time  0.156 ( 0.155)	Data  0.002 ( 0.007)	Loss 9.2302e-01 (9.1710e-01)	Acc@1  74.22 ( 74.36)	Acc@5  95.31 ( 94.59)
03-Mar-22 09:14:37 - Epoch: [4][350/352]	Time  0.155 ( 0.155)	Data  0.002 ( 0.003)	Loss 9.0359e-01 (9.5405e-01)	Acc@1  71.09 ( 73.45)	Acc@5  95.31 ( 93.90)
03-Mar-22 09:14:38 - Test: [ 0/20]	Time  0.359 ( 0.359)	Loss 9.7461e-01 (9.7461e-01)	Acc@1  71.48 ( 71.48)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:14:38 - Epoch: [6][ 60/352]	Time  0.142 ( 0.152)	Data  0.002 ( 0.006)	Loss 9.1412e-01 (9.2380e-01)	Acc@1  75.00 ( 74.27)	Acc@5  92.97 ( 94.54)
03-Mar-22 09:14:39 - Test: [10/20]	Time  0.091 ( 0.123)	Loss 8.3714e-01 (9.4499e-01)	Acc@1  74.22 ( 73.05)	Acc@5  95.31 ( 93.96)
03-Mar-22 09:14:40 -  * Acc@1 73.040 Acc@5 93.760
03-Mar-22 09:14:40 - Best acc at epoch 4: 73.54000091552734
03-Mar-22 09:14:40 - Epoch: [6][ 70/352]	Time  0.125 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.6373e-01 (9.2076e-01)	Acc@1  70.31 ( 74.30)	Acc@5  95.31 ( 94.58)
03-Mar-22 09:14:40 - Epoch: [5][  0/352]	Time  0.390 ( 0.390)	Data  0.230 ( 0.230)	Loss 1.0075e+00 (1.0075e+00)	Acc@1  72.66 ( 72.66)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:14:41 - Epoch: [6][ 80/352]	Time  0.128 ( 0.149)	Data  0.001 ( 0.005)	Loss 9.4328e-01 (9.1520e-01)	Acc@1  71.09 ( 74.35)	Acc@5  93.75 ( 94.65)
03-Mar-22 09:14:42 - Epoch: [5][ 10/352]	Time  0.153 ( 0.175)	Data  0.002 ( 0.023)	Loss 9.1507e-01 (9.3679e-01)	Acc@1  75.00 ( 73.08)	Acc@5  92.97 ( 94.89)
03-Mar-22 09:14:42 - Epoch: [6][ 90/352]	Time  0.151 ( 0.147)	Data  0.002 ( 0.005)	Loss 6.9844e-01 (9.1968e-01)	Acc@1  83.59 ( 74.22)	Acc@5  95.31 ( 94.60)
03-Mar-22 09:14:43 - Epoch: [5][ 20/352]	Time  0.157 ( 0.161)	Data  0.002 ( 0.013)	Loss 8.6007e-01 (9.4246e-01)	Acc@1  77.34 ( 73.51)	Acc@5  96.88 ( 94.72)
03-Mar-22 09:14:44 - Epoch: [6][100/352]	Time  0.135 ( 0.147)	Data  0.002 ( 0.004)	Loss 9.1066e-01 (9.2093e-01)	Acc@1  71.88 ( 74.20)	Acc@5  96.09 ( 94.57)
03-Mar-22 09:14:45 - Epoch: [5][ 30/352]	Time  0.145 ( 0.159)	Data  0.002 ( 0.010)	Loss 9.5417e-01 (9.3272e-01)	Acc@1  70.31 ( 73.54)	Acc@5  92.19 ( 94.53)
03-Mar-22 09:14:45 - Epoch: [6][110/352]	Time  0.151 ( 0.146)	Data  0.002 ( 0.004)	Loss 9.1088e-01 (9.2096e-01)	Acc@1  75.00 ( 74.15)	Acc@5  94.53 ( 94.63)
03-Mar-22 09:14:46 - Epoch: [5][ 40/352]	Time  0.151 ( 0.155)	Data  0.002 ( 0.008)	Loss 9.6572e-01 (9.3412e-01)	Acc@1  74.22 ( 73.30)	Acc@5  93.75 ( 94.42)
03-Mar-22 09:14:47 - Epoch: [6][120/352]	Time  0.149 ( 0.146)	Data  0.002 ( 0.004)	Loss 8.5427e-01 (9.2457e-01)	Acc@1  77.34 ( 74.08)	Acc@5  96.88 ( 94.56)
03-Mar-22 09:14:48 - Epoch: [5][ 50/352]	Time  0.152 ( 0.154)	Data  0.002 ( 0.007)	Loss 9.0912e-01 (9.2535e-01)	Acc@1  75.78 ( 73.65)	Acc@5  92.97 ( 94.53)
03-Mar-22 09:14:48 - Epoch: [6][130/352]	Time  0.147 ( 0.146)	Data  0.002 ( 0.004)	Loss 7.6709e-01 (9.2580e-01)	Acc@1  82.03 ( 74.11)	Acc@5  96.09 ( 94.53)
03-Mar-22 09:14:49 - Epoch: [5][ 60/352]	Time  0.155 ( 0.153)	Data  0.002 ( 0.006)	Loss 9.5367e-01 (9.3113e-01)	Acc@1  72.66 ( 73.40)	Acc@5  92.97 ( 94.44)
03-Mar-22 09:14:50 - Epoch: [6][140/352]	Time  0.145 ( 0.147)	Data  0.002 ( 0.004)	Loss 8.2970e-01 (9.2389e-01)	Acc@1  78.91 ( 74.16)	Acc@5  94.53 ( 94.51)
03-Mar-22 09:14:51 - Epoch: [5][ 70/352]	Time  0.157 ( 0.153)	Data  0.002 ( 0.006)	Loss 8.6418e-01 (9.2522e-01)	Acc@1  74.22 ( 73.61)	Acc@5  96.88 ( 94.56)
03-Mar-22 09:14:51 - Epoch: [6][150/352]	Time  0.145 ( 0.147)	Data  0.002 ( 0.004)	Loss 8.3639e-01 (9.2181e-01)	Acc@1  76.56 ( 74.16)	Acc@5  96.88 ( 94.53)
03-Mar-22 09:14:52 - Epoch: [5][ 80/352]	Time  0.146 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.8612e-01 (9.2562e-01)	Acc@1  76.56 ( 73.72)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:14:52 - Epoch: [6][160/352]	Time  0.133 ( 0.147)	Data  0.002 ( 0.004)	Loss 9.5735e-01 (9.2168e-01)	Acc@1  70.31 ( 74.17)	Acc@5  95.31 ( 94.54)
03-Mar-22 09:14:54 - Epoch: [5][ 90/352]	Time  0.132 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.5665e-01 (9.2951e-01)	Acc@1  76.56 ( 73.74)	Acc@5  93.75 ( 94.37)
03-Mar-22 09:14:54 - Epoch: [6][170/352]	Time  0.146 ( 0.147)	Data  0.002 ( 0.004)	Loss 1.0792e+00 (9.2318e-01)	Acc@1  69.53 ( 74.17)	Acc@5  91.41 ( 94.45)
03-Mar-22 09:14:55 - Epoch: [5][100/352]	Time  0.178 ( 0.153)	Data  0.002 ( 0.005)	Loss 1.1271e+00 (9.3063e-01)	Acc@1  68.75 ( 73.68)	Acc@5  90.62 ( 94.37)
03-Mar-22 09:14:55 - Epoch: [6][180/352]	Time  0.153 ( 0.147)	Data  0.002 ( 0.003)	Loss 8.3549e-01 (9.1968e-01)	Acc@1  75.78 ( 74.22)	Acc@5  98.44 ( 94.49)
03-Mar-22 09:14:57 - Epoch: [5][110/352]	Time  0.154 ( 0.153)	Data  0.003 ( 0.004)	Loss 6.5815e-01 (9.3637e-01)	Acc@1  82.03 ( 73.65)	Acc@5  96.88 ( 94.31)
03-Mar-22 09:14:57 - Epoch: [6][190/352]	Time  0.148 ( 0.147)	Data  0.002 ( 0.003)	Loss 8.4863e-01 (9.1990e-01)	Acc@1  75.78 ( 74.17)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:14:58 - Epoch: [5][120/352]	Time  0.159 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.7769e-01 (9.3438e-01)	Acc@1  79.69 ( 73.67)	Acc@5  94.53 ( 94.36)
03-Mar-22 09:14:59 - Epoch: [6][200/352]	Time  0.171 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.1546e+00 (9.2083e-01)	Acc@1  71.88 ( 74.14)	Acc@5  90.62 ( 94.40)
03-Mar-22 09:15:00 - Epoch: [5][130/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.4061e-01 (9.3255e-01)	Acc@1  73.44 ( 73.69)	Acc@5  93.75 ( 94.35)
03-Mar-22 09:15:00 - Epoch: [6][210/352]	Time  0.152 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.4792e-01 (9.2327e-01)	Acc@1  75.00 ( 74.04)	Acc@5  93.75 ( 94.40)
03-Mar-22 09:15:02 - Epoch: [6][220/352]	Time  0.142 ( 0.148)	Data  0.002 ( 0.003)	Loss 7.3405e-01 (9.2229e-01)	Acc@1  78.12 ( 74.10)	Acc@5  97.66 ( 94.43)
03-Mar-22 09:15:02 - Epoch: [5][140/352]	Time  0.177 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0517e+00 (9.3350e-01)	Acc@1  72.66 ( 73.74)	Acc@5  92.97 ( 94.32)
03-Mar-22 09:15:03 - Epoch: [6][230/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.6663e-01 (9.2454e-01)	Acc@1  71.09 ( 74.00)	Acc@5  89.84 ( 94.34)
03-Mar-22 09:15:03 - Epoch: [5][150/352]	Time  0.149 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.4920e-01 (9.3328e-01)	Acc@1  71.09 ( 73.72)	Acc@5  91.41 ( 94.27)
03-Mar-22 09:15:04 - Epoch: [6][240/352]	Time  0.178 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.2040e+00 (9.2791e-01)	Acc@1  65.62 ( 73.90)	Acc@5  89.84 ( 94.28)
03-Mar-22 09:15:05 - Epoch: [5][160/352]	Time  0.166 ( 0.154)	Data  0.003 ( 0.004)	Loss 9.4094e-01 (9.3829e-01)	Acc@1  75.00 ( 73.58)	Acc@5  92.97 ( 94.22)
03-Mar-22 09:15:06 - Epoch: [6][250/352]	Time  0.147 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.4142e-01 (9.2645e-01)	Acc@1  77.34 ( 73.96)	Acc@5  94.53 ( 94.27)
03-Mar-22 09:15:06 - Epoch: [5][170/352]	Time  0.137 ( 0.154)	Data  0.003 ( 0.004)	Loss 8.9468e-01 (9.3725e-01)	Acc@1  72.66 ( 73.66)	Acc@5  94.53 ( 94.23)
03-Mar-22 09:15:07 - Epoch: [6][260/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.7259e-01 (9.2671e-01)	Acc@1  71.88 ( 73.96)	Acc@5  94.53 ( 94.26)
03-Mar-22 09:15:08 - Epoch: [5][180/352]	Time  0.176 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.7539e-01 (9.3589e-01)	Acc@1  74.22 ( 73.78)	Acc@5  96.09 ( 94.25)
03-Mar-22 09:15:09 - Epoch: [6][270/352]	Time  0.138 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.1035e-01 (9.2848e-01)	Acc@1  77.34 ( 73.88)	Acc@5  95.31 ( 94.27)
03-Mar-22 09:15:09 - Epoch: [5][190/352]	Time  0.152 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.7465e-01 (9.3571e-01)	Acc@1  78.12 ( 73.82)	Acc@5  95.31 ( 94.26)
03-Mar-22 09:15:10 - Epoch: [6][280/352]	Time  0.151 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.3580e-01 (9.2805e-01)	Acc@1  75.00 ( 73.88)	Acc@5  92.97 ( 94.27)
03-Mar-22 09:15:11 - Epoch: [5][200/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.7852e-01 (9.3511e-01)	Acc@1  76.56 ( 73.88)	Acc@5  92.19 ( 94.20)
03-Mar-22 09:15:12 - Epoch: [6][290/352]	Time  0.147 ( 0.148)	Data  0.003 ( 0.003)	Loss 8.6346e-01 (9.2807e-01)	Acc@1  72.66 ( 73.87)	Acc@5  96.88 ( 94.28)
03-Mar-22 09:15:12 - Epoch: [5][210/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.6652e-01 (9.3399e-01)	Acc@1  77.34 ( 73.92)	Acc@5  94.53 ( 94.22)
03-Mar-22 09:15:13 - Epoch: [6][300/352]	Time  0.144 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.4541e-01 (9.2757e-01)	Acc@1  74.22 ( 73.91)	Acc@5  95.31 ( 94.27)
03-Mar-22 09:15:14 - Epoch: [5][220/352]	Time  0.177 ( 0.155)	Data  0.002 ( 0.003)	Loss 9.3780e-01 (9.3472e-01)	Acc@1  74.22 ( 73.94)	Acc@5  92.97 ( 94.20)
03-Mar-22 09:15:15 - Epoch: [6][310/352]	Time  0.150 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.5698e-01 (9.2717e-01)	Acc@1  72.66 ( 73.91)	Acc@5  93.75 ( 94.29)
03-Mar-22 09:15:15 - Epoch: [5][230/352]	Time  0.141 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0108e+00 (9.3554e-01)	Acc@1  72.66 ( 73.95)	Acc@5  91.41 ( 94.16)
03-Mar-22 09:15:16 - Epoch: [6][320/352]	Time  0.145 ( 0.147)	Data  0.003 ( 0.003)	Loss 9.2870e-01 (9.2711e-01)	Acc@1  75.00 ( 73.91)	Acc@5  93.75 ( 94.28)
03-Mar-22 09:15:17 - Epoch: [5][240/352]	Time  0.133 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.4844e-01 (9.3420e-01)	Acc@1  78.12 ( 74.04)	Acc@5  94.53 ( 94.14)
03-Mar-22 09:15:18 - Epoch: [6][330/352]	Time  0.148 ( 0.147)	Data  0.002 ( 0.003)	Loss 9.2471e-01 (9.2806e-01)	Acc@1  73.44 ( 73.89)	Acc@5  93.75 ( 94.27)
03-Mar-22 09:15:19 - Epoch: [5][250/352]	Time  0.169 ( 0.154)	Data  0.003 ( 0.003)	Loss 9.0862e-01 (9.3442e-01)	Acc@1  76.56 ( 74.04)	Acc@5  96.09 ( 94.14)
03-Mar-22 09:15:19 - Epoch: [6][340/352]	Time  0.158 ( 0.147)	Data  0.002 ( 0.003)	Loss 9.1154e-01 (9.3080e-01)	Acc@1  75.78 ( 73.79)	Acc@5  93.75 ( 94.23)
03-Mar-22 09:15:20 - Epoch: [5][260/352]	Time  0.173 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.4226e-01 (9.3360e-01)	Acc@1  78.12 ( 74.02)	Acc@5  95.31 ( 94.16)
03-Mar-22 09:15:21 - Epoch: [6][350/352]	Time  0.125 ( 0.147)	Data  0.001 ( 0.003)	Loss 1.0316e+00 (9.3124e-01)	Acc@1  75.78 ( 73.77)	Acc@5  92.97 ( 94.22)
03-Mar-22 09:15:21 - Test: [ 0/20]	Time  0.354 ( 0.354)	Loss 1.0155e+00 (1.0155e+00)	Acc@1  69.53 ( 69.53)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:15:21 - Epoch: [5][270/352]	Time  0.149 ( 0.154)	Data  0.001 ( 0.003)	Loss 1.0541e+00 (9.3311e-01)	Acc@1  67.19 ( 74.01)	Acc@5  91.41 ( 94.16)
03-Mar-22 09:15:22 - Test: [10/20]	Time  0.121 ( 0.130)	Loss 8.3065e-01 (9.5389e-01)	Acc@1  76.95 ( 72.59)	Acc@5  94.92 ( 94.03)
03-Mar-22 09:15:23 - Epoch: [5][280/352]	Time  0.157 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.7186e-01 (9.3002e-01)	Acc@1  75.00 ( 74.08)	Acc@5  94.53 ( 94.22)
03-Mar-22 09:15:23 -  * Acc@1 73.080 Acc@5 94.040
03-Mar-22 09:15:23 - Best acc at epoch 6: 73.97999572753906
03-Mar-22 09:15:24 - Epoch: [7][  0/352]	Time  0.362 ( 0.362)	Data  0.228 ( 0.228)	Loss 9.9459e-01 (9.9459e-01)	Acc@1  75.78 ( 75.78)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:15:24 - Epoch: [5][290/352]	Time  0.161 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0572e+00 (9.3237e-01)	Acc@1  71.09 ( 74.02)	Acc@5  92.19 ( 94.21)
03-Mar-22 09:15:25 - Epoch: [7][ 10/352]	Time  0.151 ( 0.173)	Data  0.003 ( 0.023)	Loss 9.3858e-01 (9.0491e-01)	Acc@1  74.22 ( 74.64)	Acc@5  96.09 ( 94.25)
03-Mar-22 09:15:26 - Epoch: [5][300/352]	Time  0.137 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.1205e-01 (9.3213e-01)	Acc@1  75.78 ( 73.97)	Acc@5  96.88 ( 94.21)
03-Mar-22 09:15:27 - Epoch: [7][ 20/352]	Time  0.147 ( 0.163)	Data  0.002 ( 0.013)	Loss 8.7821e-01 (9.2335e-01)	Acc@1  74.22 ( 73.88)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:15:27 - Epoch: [5][310/352]	Time  0.134 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.4686e-01 (9.3266e-01)	Acc@1  79.69 ( 73.96)	Acc@5  96.88 ( 94.19)
03-Mar-22 09:15:28 - Epoch: [7][ 30/352]	Time  0.146 ( 0.158)	Data  0.003 ( 0.010)	Loss 8.6116e-01 (9.2649e-01)	Acc@1  75.00 ( 74.12)	Acc@5  97.66 ( 94.61)
03-Mar-22 09:15:29 - Epoch: [5][320/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.6290e-01 (9.3462e-01)	Acc@1  75.00 ( 73.91)	Acc@5  92.97 ( 94.19)
03-Mar-22 09:15:30 - Epoch: [7][ 40/352]	Time  0.145 ( 0.156)	Data  0.002 ( 0.008)	Loss 1.1467e+00 (9.3250e-01)	Acc@1  67.19 ( 73.69)	Acc@5  89.06 ( 94.25)
03-Mar-22 09:15:30 - Epoch: [5][330/352]	Time  0.149 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.0937e-01 (9.3391e-01)	Acc@1  75.78 ( 73.92)	Acc@5  96.09 ( 94.20)
03-Mar-22 09:15:31 - Epoch: [7][ 50/352]	Time  0.150 ( 0.155)	Data  0.002 ( 0.007)	Loss 9.8636e-01 (9.3921e-01)	Acc@1  70.31 ( 73.45)	Acc@5  93.75 ( 94.15)
03-Mar-22 09:15:32 - Epoch: [5][340/352]	Time  0.150 ( 0.153)	Data  0.003 ( 0.003)	Loss 8.7669e-01 (9.3390e-01)	Acc@1  72.66 ( 73.89)	Acc@5  95.31 ( 94.20)
03-Mar-22 09:15:33 - Epoch: [7][ 60/352]	Time  0.144 ( 0.154)	Data  0.002 ( 0.006)	Loss 8.7777e-01 (9.3384e-01)	Acc@1  78.91 ( 73.60)	Acc@5  95.31 ( 94.25)
03-Mar-22 09:15:33 - Epoch: [5][350/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.1318e+00 (9.3456e-01)	Acc@1  68.75 ( 73.87)	Acc@5  92.97 ( 94.18)
03-Mar-22 09:15:34 - Epoch: [7][ 70/352]	Time  0.121 ( 0.151)	Data  0.002 ( 0.006)	Loss 1.0011e+00 (9.3742e-01)	Acc@1  72.66 ( 73.40)	Acc@5  89.84 ( 94.18)
03-Mar-22 09:15:34 - Test: [ 0/20]	Time  0.358 ( 0.358)	Loss 9.6162e-01 (9.6162e-01)	Acc@1  71.88 ( 71.88)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:15:35 - Test: [10/20]	Time  0.102 ( 0.128)	Loss 8.3993e-01 (9.3975e-01)	Acc@1  77.73 ( 73.72)	Acc@5  93.75 ( 93.57)
03-Mar-22 09:15:36 - Epoch: [7][ 80/352]	Time  0.160 ( 0.152)	Data  0.002 ( 0.005)	Loss 8.6425e-01 (9.3529e-01)	Acc@1  75.00 ( 73.52)	Acc@5  95.31 ( 94.21)
03-Mar-22 09:15:36 -  * Acc@1 73.360 Acc@5 93.640
03-Mar-22 09:15:36 - Best acc at epoch 5: 73.54000091552734
03-Mar-22 09:15:37 - Epoch: [6][  0/352]	Time  0.364 ( 0.364)	Data  0.227 ( 0.227)	Loss 9.0162e-01 (9.0162e-01)	Acc@1  71.88 ( 71.88)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:15:37 - Epoch: [7][ 90/352]	Time  0.140 ( 0.150)	Data  0.002 ( 0.005)	Loss 8.4213e-01 (9.3278e-01)	Acc@1  75.00 ( 73.76)	Acc@5  95.31 ( 94.19)
03-Mar-22 09:15:38 - Epoch: [6][ 10/352]	Time  0.187 ( 0.183)	Data  0.003 ( 0.023)	Loss 8.9382e-01 (9.0851e-01)	Acc@1  73.44 ( 73.86)	Acc@5  96.88 ( 94.89)
03-Mar-22 09:15:38 - Epoch: [7][100/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.005)	Loss 8.3958e-01 (9.3175e-01)	Acc@1  76.56 ( 73.89)	Acc@5  93.75 ( 94.18)
03-Mar-22 09:15:40 - Epoch: [6][ 20/352]	Time  0.152 ( 0.169)	Data  0.002 ( 0.013)	Loss 8.2687e-01 (8.9104e-01)	Acc@1  77.34 ( 74.14)	Acc@5  95.31 ( 95.13)
03-Mar-22 09:15:40 - Epoch: [7][110/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.9959e-01 (9.2832e-01)	Acc@1  71.09 ( 73.91)	Acc@5  96.09 ( 94.26)
03-Mar-22 09:15:41 - Epoch: [6][ 30/352]	Time  0.153 ( 0.164)	Data  0.002 ( 0.010)	Loss 7.8558e-01 (8.8274e-01)	Acc@1  80.47 ( 74.75)	Acc@5  95.31 ( 94.88)
03-Mar-22 09:15:42 - Epoch: [7][120/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.4573e-01 (9.2552e-01)	Acc@1  74.22 ( 74.04)	Acc@5  94.53 ( 94.29)
03-Mar-22 09:15:43 - Epoch: [6][ 40/352]	Time  0.154 ( 0.161)	Data  0.002 ( 0.008)	Loss 9.3694e-01 (8.8875e-01)	Acc@1  78.12 ( 74.66)	Acc@5  92.97 ( 94.82)
03-Mar-22 09:15:43 - Epoch: [7][130/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.1002e+00 (9.2343e-01)	Acc@1  67.97 ( 74.06)	Acc@5  92.19 ( 94.33)
03-Mar-22 09:15:44 - Epoch: [6][ 50/352]	Time  0.148 ( 0.158)	Data  0.002 ( 0.007)	Loss 1.0896e+00 (8.9226e-01)	Acc@1  75.00 ( 74.91)	Acc@5  87.50 ( 94.59)
03-Mar-22 09:15:45 - Epoch: [7][140/352]	Time  0.130 ( 0.151)	Data  0.003 ( 0.004)	Loss 7.7682e-01 (9.2433e-01)	Acc@1  76.56 ( 73.97)	Acc@5  96.09 ( 94.30)
03-Mar-22 09:15:46 - Epoch: [6][ 60/352]	Time  0.153 ( 0.157)	Data  0.002 ( 0.006)	Loss 8.4252e-01 (8.9981e-01)	Acc@1  78.12 ( 74.62)	Acc@5  95.31 ( 94.38)
03-Mar-22 09:15:46 - Epoch: [7][150/352]	Time  0.149 ( 0.151)	Data  0.003 ( 0.004)	Loss 1.0716e+00 (9.2342e-01)	Acc@1  68.75 ( 74.04)	Acc@5  90.62 ( 94.26)
03-Mar-22 09:15:47 - Epoch: [6][ 70/352]	Time  0.148 ( 0.156)	Data  0.002 ( 0.006)	Loss 9.5684e-01 (9.0186e-01)	Acc@1  71.09 ( 74.60)	Acc@5  92.19 ( 94.33)
03-Mar-22 09:15:48 - Epoch: [7][160/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.004)	Loss 7.8906e-01 (9.2227e-01)	Acc@1  78.12 ( 74.04)	Acc@5  95.31 ( 94.22)
03-Mar-22 09:15:49 - Epoch: [6][ 80/352]	Time  0.159 ( 0.155)	Data  0.002 ( 0.005)	Loss 9.8933e-01 (9.1001e-01)	Acc@1  71.09 ( 74.43)	Acc@5  92.97 ( 94.20)
03-Mar-22 09:15:49 - Epoch: [7][170/352]	Time  0.161 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.6516e-01 (9.2296e-01)	Acc@1  74.22 ( 74.04)	Acc@5  96.09 ( 94.24)
03-Mar-22 09:15:50 - Epoch: [6][ 90/352]	Time  0.150 ( 0.155)	Data  0.002 ( 0.005)	Loss 1.1895e+00 (9.1770e-01)	Acc@1  70.31 ( 74.15)	Acc@5  88.28 ( 94.12)
03-Mar-22 09:15:51 - Epoch: [7][180/352]	Time  0.139 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.1979e-01 (9.2289e-01)	Acc@1  75.78 ( 74.05)	Acc@5  97.66 ( 94.28)
03-Mar-22 09:15:52 - Epoch: [6][100/352]	Time  0.147 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.4669e-01 (9.1381e-01)	Acc@1  68.75 ( 74.27)	Acc@5  93.75 ( 94.15)
03-Mar-22 09:15:52 - Epoch: [7][190/352]	Time  0.150 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.1999e-01 (9.2262e-01)	Acc@1  73.44 ( 74.09)	Acc@5  92.97 ( 94.26)
03-Mar-22 09:15:53 - Epoch: [6][110/352]	Time  0.130 ( 0.154)	Data  0.001 ( 0.004)	Loss 1.1052e+00 (9.1639e-01)	Acc@1  68.75 ( 74.22)	Acc@5  88.28 ( 94.02)
03-Mar-22 09:15:54 - Epoch: [7][200/352]	Time  0.148 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.6218e-01 (9.2279e-01)	Acc@1  74.22 ( 74.09)	Acc@5  94.53 ( 94.27)
03-Mar-22 09:15:55 - Epoch: [6][120/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.9340e-01 (9.1666e-01)	Acc@1  68.75 ( 74.05)	Acc@5  92.97 ( 94.12)
03-Mar-22 09:15:55 - Epoch: [7][210/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.9742e-01 (9.2340e-01)	Acc@1  71.88 ( 74.00)	Acc@5  93.75 ( 94.29)
03-Mar-22 09:15:56 - Epoch: [6][130/352]	Time  0.161 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.3631e-01 (9.1303e-01)	Acc@1  75.00 ( 74.14)	Acc@5  95.31 ( 94.20)
03-Mar-22 09:15:57 - Epoch: [7][220/352]	Time  0.147 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.5913e-01 (9.2179e-01)	Acc@1  76.56 ( 74.08)	Acc@5  94.53 ( 94.33)
03-Mar-22 09:15:58 - Epoch: [6][140/352]	Time  0.160 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0117e+00 (9.1467e-01)	Acc@1  70.31 ( 74.11)	Acc@5  92.19 ( 94.13)
03-Mar-22 09:15:58 - Epoch: [7][230/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1800e-01 (9.2012e-01)	Acc@1  72.66 ( 74.10)	Acc@5  95.31 ( 94.32)
03-Mar-22 09:15:59 - Epoch: [6][150/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.5738e-01 (9.1770e-01)	Acc@1  77.34 ( 74.04)	Acc@5  96.09 ( 94.07)
03-Mar-22 09:16:00 - Epoch: [7][240/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.4896e-01 (9.2105e-01)	Acc@1  71.88 ( 74.04)	Acc@5  93.75 ( 94.30)
03-Mar-22 09:16:01 - Epoch: [6][160/352]	Time  0.157 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.2454e-01 (9.1719e-01)	Acc@1  73.44 ( 74.07)	Acc@5  96.09 ( 94.07)
03-Mar-22 09:16:01 - Epoch: [7][250/352]	Time  0.140 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0039e+00 (9.2367e-01)	Acc@1  71.09 ( 74.00)	Acc@5  95.31 ( 94.27)
03-Mar-22 09:16:02 - Epoch: [6][170/352]	Time  0.149 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.5687e-01 (9.1662e-01)	Acc@1  76.56 ( 74.05)	Acc@5  96.09 ( 94.09)
03-Mar-22 09:16:03 - Epoch: [7][260/352]	Time  0.145 ( 0.151)	Data  0.003 ( 0.003)	Loss 7.9790e-01 (9.2226e-01)	Acc@1  78.12 ( 74.02)	Acc@5  96.88 ( 94.32)
03-Mar-22 09:16:04 - Epoch: [6][180/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0851e+00 (9.1773e-01)	Acc@1  71.88 ( 74.05)	Acc@5  91.41 ( 94.08)
03-Mar-22 09:16:04 - Epoch: [7][270/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0948e+00 (9.2255e-01)	Acc@1  67.97 ( 73.99)	Acc@5  93.75 ( 94.32)
03-Mar-22 09:16:05 - Epoch: [6][190/352]	Time  0.177 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0325e+00 (9.1820e-01)	Acc@1  71.09 ( 74.08)	Acc@5  96.09 ( 94.09)
03-Mar-22 09:16:06 - Epoch: [7][280/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0680e+00 (9.2285e-01)	Acc@1  68.75 ( 73.98)	Acc@5  93.75 ( 94.31)
03-Mar-22 09:16:07 - Epoch: [6][200/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0774e+00 (9.2078e-01)	Acc@1  70.31 ( 74.03)	Acc@5  95.31 ( 94.06)
03-Mar-22 09:16:07 - Epoch: [7][290/352]	Time  0.154 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.9740e-01 (9.2493e-01)	Acc@1  68.75 ( 73.93)	Acc@5  95.31 ( 94.30)
03-Mar-22 09:16:08 - Epoch: [6][210/352]	Time  0.151 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.0767e-01 (9.2106e-01)	Acc@1  75.78 ( 73.99)	Acc@5  93.75 ( 94.08)
03-Mar-22 09:16:09 - Epoch: [7][300/352]	Time  0.145 ( 0.151)	Data  0.003 ( 0.003)	Loss 8.4308e-01 (9.2367e-01)	Acc@1  74.22 ( 73.92)	Acc@5  93.75 ( 94.33)
03-Mar-22 09:16:10 - Epoch: [6][220/352]	Time  0.152 ( 0.153)	Data  0.003 ( 0.004)	Loss 7.1022e-01 (9.2150e-01)	Acc@1  80.47 ( 73.96)	Acc@5  98.44 ( 94.12)
03-Mar-22 09:16:10 - Epoch: [7][310/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.4588e-01 (9.2235e-01)	Acc@1  74.22 ( 74.01)	Acc@5  96.88 ( 94.34)
03-Mar-22 09:16:11 - Epoch: [6][230/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.5846e-01 (9.2116e-01)	Acc@1  79.69 ( 73.97)	Acc@5  94.53 ( 94.13)
03-Mar-22 09:16:12 - Epoch: [7][320/352]	Time  0.151 ( 0.150)	Data  0.003 ( 0.003)	Loss 9.6994e-01 (9.2272e-01)	Acc@1  72.66 ( 73.99)	Acc@5  94.53 ( 94.32)
03-Mar-22 09:16:13 - Epoch: [6][240/352]	Time  0.159 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0304e+00 (9.2285e-01)	Acc@1  68.75 ( 73.95)	Acc@5  93.75 ( 94.14)
03-Mar-22 09:16:13 - Epoch: [7][330/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.8418e-01 (9.2292e-01)	Acc@1  72.66 ( 73.98)	Acc@5  94.53 ( 94.32)
03-Mar-22 09:16:15 - Epoch: [7][340/352]	Time  0.154 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.8314e-01 (9.2289e-01)	Acc@1  71.88 ( 73.99)	Acc@5  93.75 ( 94.30)
03-Mar-22 09:16:15 - Epoch: [6][250/352]	Time  0.154 ( 0.153)	Data  0.003 ( 0.003)	Loss 1.0723e+00 (9.2347e-01)	Acc@1  72.66 ( 73.97)	Acc@5  93.75 ( 94.15)
03-Mar-22 09:16:16 - Epoch: [7][350/352]	Time  0.128 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.0098e-01 (9.2132e-01)	Acc@1  73.44 ( 74.05)	Acc@5  93.75 ( 94.31)
03-Mar-22 09:16:16 - Epoch: [6][260/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.9729e-01 (9.2516e-01)	Acc@1  77.34 ( 73.98)	Acc@5  93.75 ( 94.16)
03-Mar-22 09:16:17 - Test: [ 0/20]	Time  0.359 ( 0.359)	Loss 1.0057e+00 (1.0057e+00)	Acc@1  69.53 ( 69.53)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:16:18 - Epoch: [6][270/352]	Time  0.159 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.4080e-01 (9.2427e-01)	Acc@1  79.69 ( 74.00)	Acc@5  96.88 ( 94.22)
03-Mar-22 09:16:18 - Test: [10/20]	Time  0.118 ( 0.131)	Loss 8.0035e-01 (9.3659e-01)	Acc@1  80.08 ( 74.64)	Acc@5  95.70 ( 93.93)
03-Mar-22 09:16:19 -  * Acc@1 73.840 Acc@5 93.660
03-Mar-22 09:16:19 - Best acc at epoch 7: 73.97999572753906
03-Mar-22 09:16:19 - Epoch: [6][280/352]	Time  0.133 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.7216e-01 (9.2560e-01)	Acc@1  73.44 ( 73.91)	Acc@5  91.41 ( 94.23)
03-Mar-22 09:16:19 - Epoch: [8][  0/352]	Time  0.363 ( 0.363)	Data  0.232 ( 0.232)	Loss 9.2094e-01 (9.2094e-01)	Acc@1  69.53 ( 69.53)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:16:21 - Epoch: [8][ 10/352]	Time  0.149 ( 0.166)	Data  0.002 ( 0.024)	Loss 8.2953e-01 (8.6726e-01)	Acc@1  76.56 ( 75.21)	Acc@5  95.31 ( 95.45)
03-Mar-22 09:16:21 - Epoch: [6][290/352]	Time  0.124 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.3022e-01 (9.2572e-01)	Acc@1  73.44 ( 73.94)	Acc@5  92.19 ( 94.22)
03-Mar-22 09:16:22 - Epoch: [8][ 20/352]	Time  0.136 ( 0.153)	Data  0.001 ( 0.013)	Loss 9.3763e-01 (8.9830e-01)	Acc@1  74.22 ( 75.15)	Acc@5  92.97 ( 94.42)
03-Mar-22 09:16:22 - Epoch: [6][300/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.3697e-01 (9.2618e-01)	Acc@1  67.19 ( 73.88)	Acc@5  98.44 ( 94.24)
03-Mar-22 09:16:23 - Epoch: [8][ 30/352]	Time  0.147 ( 0.151)	Data  0.002 ( 0.010)	Loss 8.2094e-01 (8.8954e-01)	Acc@1  77.34 ( 75.23)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:16:23 - Epoch: [6][310/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.3657e-01 (9.2612e-01)	Acc@1  73.44 ( 73.89)	Acc@5  94.53 ( 94.23)
03-Mar-22 09:16:25 - Epoch: [8][ 40/352]	Time  0.138 ( 0.151)	Data  0.002 ( 0.008)	Loss 9.6064e-01 (9.0191e-01)	Acc@1  75.00 ( 74.83)	Acc@5  92.97 ( 94.40)
03-Mar-22 09:16:25 - Epoch: [6][320/352]	Time  0.173 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.7516e-01 (9.2618e-01)	Acc@1  74.22 ( 73.90)	Acc@5  94.53 ( 94.22)
03-Mar-22 09:16:26 - Epoch: [8][ 50/352]	Time  0.174 ( 0.151)	Data  0.002 ( 0.007)	Loss 7.6632e-01 (9.0229e-01)	Acc@1  79.69 ( 74.57)	Acc@5  96.09 ( 94.47)
03-Mar-22 09:16:27 - Epoch: [6][330/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.1668e-01 (9.2729e-01)	Acc@1  71.09 ( 73.86)	Acc@5  93.75 ( 94.19)
03-Mar-22 09:16:28 - Epoch: [8][ 60/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.006)	Loss 8.5654e-01 (9.1458e-01)	Acc@1  76.56 ( 74.03)	Acc@5  95.31 ( 94.39)
03-Mar-22 09:16:28 - Epoch: [6][340/352]	Time  0.135 ( 0.152)	Data  0.001 ( 0.003)	Loss 1.1296e+00 (9.2607e-01)	Acc@1  69.53 ( 73.89)	Acc@5  92.97 ( 94.21)
03-Mar-22 09:16:29 - Epoch: [8][ 70/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.006)	Loss 9.7137e-01 (9.1212e-01)	Acc@1  69.53 ( 73.89)	Acc@5  93.75 ( 94.42)
03-Mar-22 09:16:30 - Epoch: [6][350/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.8696e-01 (9.2581e-01)	Acc@1  72.66 ( 73.87)	Acc@5  95.31 ( 94.24)
03-Mar-22 09:16:30 - Test: [ 0/20]	Time  0.361 ( 0.361)	Loss 9.9207e-01 (9.9207e-01)	Acc@1  71.09 ( 71.09)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:16:31 - Epoch: [8][ 80/352]	Time  0.160 ( 0.152)	Data  0.002 ( 0.005)	Loss 8.6914e-01 (9.0511e-01)	Acc@1  71.88 ( 74.20)	Acc@5  98.44 ( 94.50)
03-Mar-22 09:16:31 - Test: [10/20]	Time  0.104 ( 0.127)	Loss 8.3633e-01 (9.4249e-01)	Acc@1  76.17 ( 73.97)	Acc@5  95.70 ( 93.47)
03-Mar-22 09:16:32 -  * Acc@1 74.040 Acc@5 93.840
03-Mar-22 09:16:32 - Best acc at epoch 6: 74.04000091552734
03-Mar-22 09:16:32 - Epoch: [8][ 90/352]	Time  0.117 ( 0.152)	Data  0.002 ( 0.005)	Loss 9.1522e-01 (9.0105e-01)	Acc@1  74.22 ( 74.35)	Acc@5  94.53 ( 94.55)
03-Mar-22 09:16:33 - Epoch: [7][  0/352]	Time  0.357 ( 0.357)	Data  0.216 ( 0.216)	Loss 9.0551e-01 (9.0551e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.88 ( 96.88)
03-Mar-22 09:16:34 - Epoch: [8][100/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.005)	Loss 1.0787e+00 (9.0722e-01)	Acc@1  68.75 ( 74.26)	Acc@5  95.31 ( 94.51)
03-Mar-22 09:16:34 - Epoch: [7][ 10/352]	Time  0.131 ( 0.166)	Data  0.002 ( 0.022)	Loss 8.3136e-01 (9.3745e-01)	Acc@1  76.56 ( 73.30)	Acc@5  94.53 ( 94.60)
03-Mar-22 09:16:35 - Epoch: [8][110/352]	Time  0.154 ( 0.150)	Data  0.002 ( 0.005)	Loss 9.8195e-01 (9.0976e-01)	Acc@1  71.88 ( 74.14)	Acc@5  93.75 ( 94.43)
03-Mar-22 09:16:35 - Epoch: [7][ 20/352]	Time  0.150 ( 0.157)	Data  0.002 ( 0.012)	Loss 9.3964e-01 (9.2215e-01)	Acc@1  75.78 ( 74.22)	Acc@5  90.62 ( 94.46)
03-Mar-22 09:16:37 - Epoch: [8][120/352]	Time  0.157 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.9901e-01 (9.0998e-01)	Acc@1  67.19 ( 74.25)	Acc@5  92.19 ( 94.36)
03-Mar-22 09:16:37 - Epoch: [7][ 30/352]	Time  0.154 ( 0.154)	Data  0.002 ( 0.009)	Loss 1.1229e+00 (9.2360e-01)	Acc@1  69.53 ( 74.09)	Acc@5  89.84 ( 94.35)
03-Mar-22 09:16:38 - Epoch: [8][130/352]	Time  0.150 ( 0.150)	Data  0.003 ( 0.004)	Loss 9.0641e-01 (9.1491e-01)	Acc@1  73.44 ( 74.04)	Acc@5  93.75 ( 94.30)
03-Mar-22 09:16:39 - Epoch: [7][ 40/352]	Time  0.149 ( 0.154)	Data  0.003 ( 0.008)	Loss 9.1774e-01 (9.0718e-01)	Acc@1  75.00 ( 74.60)	Acc@5  96.09 ( 94.51)
03-Mar-22 09:16:40 - Epoch: [8][140/352]	Time  0.170 ( 0.150)	Data  0.003 ( 0.004)	Loss 7.7502e-01 (9.1218e-01)	Acc@1  79.69 ( 74.12)	Acc@5  97.66 ( 94.34)
03-Mar-22 09:16:40 - Epoch: [7][ 50/352]	Time  0.155 ( 0.154)	Data  0.003 ( 0.007)	Loss 9.2839e-01 (9.0472e-01)	Acc@1  75.00 ( 74.92)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:16:41 - Epoch: [8][150/352]	Time  0.147 ( 0.150)	Data  0.003 ( 0.004)	Loss 9.5294e-01 (9.1255e-01)	Acc@1  73.44 ( 74.12)	Acc@5  92.19 ( 94.29)
03-Mar-22 09:16:42 - Epoch: [7][ 60/352]	Time  0.151 ( 0.153)	Data  0.002 ( 0.006)	Loss 1.1887e+00 (9.0839e-01)	Acc@1  64.84 ( 74.67)	Acc@5  91.41 ( 94.49)
03-Mar-22 09:16:43 - Epoch: [8][160/352]	Time  0.148 ( 0.150)	Data  0.003 ( 0.004)	Loss 1.1133e+00 (9.1479e-01)	Acc@1  65.62 ( 74.04)	Acc@5  90.62 ( 94.28)
03-Mar-22 09:16:43 - Epoch: [7][ 70/352]	Time  0.151 ( 0.153)	Data  0.003 ( 0.006)	Loss 8.5015e-01 (9.0896e-01)	Acc@1  75.00 ( 74.55)	Acc@5  95.31 ( 94.55)
03-Mar-22 09:16:44 - Epoch: [8][170/352]	Time  0.145 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.4237e-01 (9.1499e-01)	Acc@1  74.22 ( 73.91)	Acc@5  94.53 ( 94.27)
03-Mar-22 09:16:45 - Epoch: [7][ 80/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.7374e-01 (9.0980e-01)	Acc@1  70.31 ( 74.44)	Acc@5  92.19 ( 94.56)
03-Mar-22 09:16:46 - Epoch: [8][180/352]	Time  0.151 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.7021e-01 (9.1583e-01)	Acc@1  75.00 ( 73.86)	Acc@5  97.66 ( 94.33)
03-Mar-22 09:16:46 - Epoch: [7][ 90/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.005)	Loss 8.0499e-01 (9.0738e-01)	Acc@1  78.91 ( 74.43)	Acc@5  96.88 ( 94.59)
03-Mar-22 09:16:48 - Epoch: [7][100/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.005)	Loss 9.5827e-01 (9.0556e-01)	Acc@1  69.53 ( 74.44)	Acc@5  93.75 ( 94.60)
03-Mar-22 09:16:48 - Epoch: [8][190/352]	Time  0.175 ( 0.152)	Data  0.003 ( 0.004)	Loss 9.7321e-01 (9.1490e-01)	Acc@1  75.00 ( 73.94)	Acc@5  92.19 ( 94.31)
03-Mar-22 09:16:49 - Epoch: [7][110/352]	Time  0.145 ( 0.152)	Data  0.003 ( 0.004)	Loss 9.7243e-01 (9.1040e-01)	Acc@1  67.19 ( 74.23)	Acc@5  93.75 ( 94.47)
03-Mar-22 09:16:49 - Epoch: [8][200/352]	Time  0.183 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.5068e-01 (9.1619e-01)	Acc@1  75.00 ( 73.95)	Acc@5  94.53 ( 94.33)
03-Mar-22 09:16:51 - Epoch: [7][120/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.1163e-01 (9.0878e-01)	Acc@1  75.78 ( 74.23)	Acc@5  96.09 ( 94.51)
03-Mar-22 09:16:51 - Epoch: [8][210/352]	Time  0.147 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.8656e-01 (9.1465e-01)	Acc@1  73.44 ( 74.03)	Acc@5  95.31 ( 94.36)
03-Mar-22 09:16:52 - Epoch: [7][130/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.8388e-01 (9.0841e-01)	Acc@1  78.91 ( 74.36)	Acc@5  96.09 ( 94.61)
03-Mar-22 09:16:53 - Epoch: [8][220/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0278e+00 (9.1599e-01)	Acc@1  71.88 ( 73.96)	Acc@5  92.97 ( 94.31)
03-Mar-22 09:16:54 - Epoch: [7][140/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.4362e-01 (9.0896e-01)	Acc@1  74.22 ( 74.32)	Acc@5  97.66 ( 94.63)
03-Mar-22 09:16:54 - Epoch: [8][230/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0876e+00 (9.1604e-01)	Acc@1  68.75 ( 73.96)	Acc@5  91.41 ( 94.32)
03-Mar-22 09:16:55 - Epoch: [7][150/352]	Time  0.152 ( 0.151)	Data  0.003 ( 0.004)	Loss 9.8753e-01 (9.1003e-01)	Acc@1  71.88 ( 74.27)	Acc@5  92.97 ( 94.63)
03-Mar-22 09:16:56 - Epoch: [8][240/352]	Time  0.176 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.7689e-01 (9.1558e-01)	Acc@1  77.34 ( 73.98)	Acc@5  96.09 ( 94.34)
03-Mar-22 09:16:57 - Epoch: [7][160/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.7035e-01 (9.0911e-01)	Acc@1  75.78 ( 74.27)	Acc@5  94.53 ( 94.65)
03-Mar-22 09:16:58 - Epoch: [8][250/352]	Time  0.168 ( 0.155)	Data  0.002 ( 0.003)	Loss 9.6440e-01 (9.1735e-01)	Acc@1  74.22 ( 73.90)	Acc@5  94.53 ( 94.32)
03-Mar-22 09:16:58 - Epoch: [7][170/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.1185e-01 (9.1398e-01)	Acc@1  71.09 ( 74.12)	Acc@5  96.88 ( 94.60)
03-Mar-22 09:16:59 - Epoch: [8][260/352]	Time  0.146 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.8074e-01 (9.1765e-01)	Acc@1  74.22 ( 73.87)	Acc@5  96.09 ( 94.31)
03-Mar-22 09:16:59 - Epoch: [7][180/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.0177e-01 (9.1016e-01)	Acc@1  72.66 ( 74.28)	Acc@5  95.31 ( 94.62)
03-Mar-22 09:17:01 - Epoch: [8][270/352]	Time  0.172 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.4573e-01 (9.1649e-01)	Acc@1  78.91 ( 73.88)	Acc@5  96.88 ( 94.35)
03-Mar-22 09:17:01 - Epoch: [7][190/352]	Time  0.136 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.0124e+00 (9.1031e-01)	Acc@1  73.44 ( 74.26)	Acc@5  94.53 ( 94.62)
03-Mar-22 09:17:02 - Epoch: [8][280/352]	Time  0.170 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.0292e+00 (9.1779e-01)	Acc@1  69.53 ( 73.88)	Acc@5  94.53 ( 94.34)
03-Mar-22 09:17:02 - Epoch: [7][200/352]	Time  0.148 ( 0.151)	Data  0.003 ( 0.003)	Loss 7.7231e-01 (9.0974e-01)	Acc@1  74.22 ( 74.26)	Acc@5  96.88 ( 94.61)
03-Mar-22 09:17:04 - Epoch: [8][290/352]	Time  0.157 ( 0.156)	Data  0.003 ( 0.003)	Loss 8.3222e-01 (9.1736e-01)	Acc@1  75.00 ( 73.83)	Acc@5  96.88 ( 94.36)
03-Mar-22 09:17:04 - Epoch: [7][210/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0043e+00 (9.1336e-01)	Acc@1  71.88 ( 74.15)	Acc@5  92.97 ( 94.54)
03-Mar-22 09:17:05 - Epoch: [7][220/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0290e+00 (9.1365e-01)	Acc@1  73.44 ( 74.12)	Acc@5  91.41 ( 94.52)
03-Mar-22 09:17:06 - Epoch: [8][300/352]	Time  0.178 ( 0.156)	Data  0.003 ( 0.003)	Loss 8.4967e-01 (9.1680e-01)	Acc@1  77.34 ( 73.88)	Acc@5  95.31 ( 94.34)
03-Mar-22 09:17:07 - Epoch: [7][230/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0901e+00 (9.1432e-01)	Acc@1  71.09 ( 74.06)	Acc@5  90.62 ( 94.53)
03-Mar-22 09:17:07 - Epoch: [8][310/352]	Time  0.176 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.0823e-01 (9.1618e-01)	Acc@1  67.19 ( 73.87)	Acc@5  97.66 ( 94.37)
03-Mar-22 09:17:08 - Epoch: [7][240/352]	Time  0.149 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.7801e-01 (9.1322e-01)	Acc@1  77.34 ( 74.15)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:17:09 - Epoch: [8][320/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.5109e-01 (9.1584e-01)	Acc@1  76.56 ( 73.92)	Acc@5  95.31 ( 94.35)
03-Mar-22 09:17:10 - Epoch: [7][250/352]	Time  0.135 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0543e+00 (9.1435e-01)	Acc@1  67.97 ( 74.08)	Acc@5  93.75 ( 94.51)
03-Mar-22 09:17:11 - Epoch: [8][330/352]	Time  0.152 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.0552e+00 (9.1675e-01)	Acc@1  71.88 ( 73.86)	Acc@5  90.62 ( 94.35)
03-Mar-22 09:17:11 - Epoch: [7][260/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.1392e-01 (9.1581e-01)	Acc@1  77.34 ( 74.05)	Acc@5  99.22 ( 94.50)
03-Mar-22 09:17:12 - Epoch: [8][340/352]	Time  0.154 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.6867e-01 (9.1764e-01)	Acc@1  74.22 ( 73.86)	Acc@5  96.09 ( 94.33)
03-Mar-22 09:17:13 - Epoch: [7][270/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.003)	Loss 6.7272e-01 (9.1578e-01)	Acc@1  83.59 ( 74.07)	Acc@5  96.09 ( 94.49)
03-Mar-22 09:17:14 - Epoch: [8][350/352]	Time  0.136 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.0010e+00 (9.1877e-01)	Acc@1  69.53 ( 73.82)	Acc@5  95.31 ( 94.29)
03-Mar-22 09:17:14 - Epoch: [7][280/352]	Time  0.096 ( 0.149)	Data  0.001 ( 0.003)	Loss 7.4493e-01 (9.1695e-01)	Acc@1  76.56 ( 74.04)	Acc@5  97.66 ( 94.49)
03-Mar-22 09:17:14 - Test: [ 0/20]	Time  0.368 ( 0.368)	Loss 1.0369e+00 (1.0369e+00)	Acc@1  70.31 ( 70.31)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:17:15 - Test: [10/20]	Time  0.106 ( 0.135)	Loss 8.6322e-01 (9.5752e-01)	Acc@1  75.00 ( 72.34)	Acc@5  94.53 ( 93.61)
03-Mar-22 09:17:16 - Epoch: [7][290/352]	Time  0.160 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.5598e-01 (9.1806e-01)	Acc@1  78.12 ( 74.05)	Acc@5  94.53 ( 94.45)
03-Mar-22 09:17:16 -  * Acc@1 72.680 Acc@5 93.580
03-Mar-22 09:17:17 - Best acc at epoch 8: 73.97999572753906
03-Mar-22 09:17:17 - Epoch: [9][  0/352]	Time  0.368 ( 0.368)	Data  0.218 ( 0.218)	Loss 1.0948e+00 (1.0948e+00)	Acc@1  67.97 ( 67.97)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:17:17 - Epoch: [7][300/352]	Time  0.126 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.8600e-01 (9.1970e-01)	Acc@1  78.91 ( 74.02)	Acc@5  92.19 ( 94.41)
03-Mar-22 09:17:18 - Epoch: [9][ 10/352]	Time  0.138 ( 0.156)	Data  0.002 ( 0.022)	Loss 7.9139e-01 (9.1585e-01)	Acc@1  78.91 ( 73.58)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:17:18 - Epoch: [7][310/352]	Time  0.129 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.1772e-01 (9.1896e-01)	Acc@1  74.22 ( 74.05)	Acc@5  92.97 ( 94.39)
03-Mar-22 09:17:20 - Epoch: [7][320/352]	Time  0.118 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.7745e-01 (9.1992e-01)	Acc@1  74.22 ( 73.99)	Acc@5  94.53 ( 94.40)
03-Mar-22 09:17:20 - Epoch: [9][ 20/352]	Time  0.134 ( 0.152)	Data  0.002 ( 0.012)	Loss 1.0410e+00 (9.2203e-01)	Acc@1  70.31 ( 74.00)	Acc@5  92.19 ( 94.27)
03-Mar-22 09:17:21 - Epoch: [7][330/352]	Time  0.133 ( 0.147)	Data  0.002 ( 0.003)	Loss 7.9056e-01 (9.1891e-01)	Acc@1  79.69 ( 74.02)	Acc@5  95.31 ( 94.39)
03-Mar-22 09:17:21 - Epoch: [9][ 30/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.009)	Loss 8.4749e-01 (9.3150e-01)	Acc@1  77.34 ( 73.92)	Acc@5  94.53 ( 94.13)
03-Mar-22 09:17:22 - Epoch: [7][340/352]	Time  0.141 ( 0.147)	Data  0.002 ( 0.003)	Loss 1.0592e+00 (9.2079e-01)	Acc@1  68.75 ( 73.94)	Acc@5  92.19 ( 94.37)
03-Mar-22 09:17:23 - Epoch: [9][ 40/352]	Time  0.149 ( 0.150)	Data  0.002 ( 0.008)	Loss 9.6588e-01 (9.2938e-01)	Acc@1  74.22 ( 74.09)	Acc@5  93.75 ( 93.88)
03-Mar-22 09:17:24 - Epoch: [7][350/352]	Time  0.127 ( 0.147)	Data  0.002 ( 0.003)	Loss 1.0301e+00 (9.2075e-01)	Acc@1  72.66 ( 73.98)	Acc@5  92.97 ( 94.36)
03-Mar-22 09:17:24 - Epoch: [9][ 50/352]	Time  0.126 ( 0.148)	Data  0.002 ( 0.006)	Loss 7.9641e-01 (9.2102e-01)	Acc@1  78.91 ( 74.42)	Acc@5  96.09 ( 94.03)
03-Mar-22 09:17:24 - Test: [ 0/20]	Time  0.380 ( 0.380)	Loss 9.9213e-01 (9.9213e-01)	Acc@1  69.92 ( 69.92)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:17:25 - Test: [10/20]	Time  0.107 ( 0.127)	Loss 8.4714e-01 (9.5769e-01)	Acc@1  77.73 ( 73.47)	Acc@5  92.58 ( 93.82)
03-Mar-22 09:17:26 - Epoch: [9][ 60/352]	Time  0.148 ( 0.147)	Data  0.002 ( 0.006)	Loss 8.6641e-01 (9.2358e-01)	Acc@1  76.56 ( 73.95)	Acc@5  96.09 ( 94.15)
03-Mar-22 09:17:26 -  * Acc@1 73.380 Acc@5 93.920
03-Mar-22 09:17:26 - Best acc at epoch 7: 74.04000091552734
03-Mar-22 09:17:27 - Epoch: [8][  0/352]	Time  0.385 ( 0.385)	Data  0.218 ( 0.218)	Loss 7.9514e-01 (7.9514e-01)	Acc@1  75.78 ( 75.78)	Acc@5  96.88 ( 96.88)
03-Mar-22 09:17:27 - Epoch: [9][ 70/352]	Time  0.132 ( 0.145)	Data  0.002 ( 0.005)	Loss 8.0040e-01 (9.1199e-01)	Acc@1  75.78 ( 74.24)	Acc@5  96.09 ( 94.29)
03-Mar-22 09:17:28 - Epoch: [8][ 10/352]	Time  0.155 ( 0.172)	Data  0.002 ( 0.022)	Loss 8.5178e-01 (8.4981e-01)	Acc@1  77.34 ( 77.70)	Acc@5  93.75 ( 95.24)
03-Mar-22 09:17:28 - Epoch: [9][ 80/352]	Time  0.162 ( 0.147)	Data  0.002 ( 0.005)	Loss 1.0043e+00 (9.0797e-01)	Acc@1  71.09 ( 74.26)	Acc@5  92.97 ( 94.40)
03-Mar-22 09:17:30 - Epoch: [8][ 20/352]	Time  0.150 ( 0.159)	Data  0.002 ( 0.012)	Loss 9.3686e-01 (8.8721e-01)	Acc@1  69.53 ( 75.07)	Acc@5  96.09 ( 94.87)
03-Mar-22 09:17:30 - Epoch: [9][ 90/352]	Time  0.152 ( 0.148)	Data  0.002 ( 0.005)	Loss 9.2124e-01 (9.0665e-01)	Acc@1  74.22 ( 74.48)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:17:31 - Epoch: [8][ 30/352]	Time  0.151 ( 0.157)	Data  0.003 ( 0.009)	Loss 9.2735e-01 (9.0020e-01)	Acc@1  75.00 ( 74.77)	Acc@5  92.97 ( 94.43)
03-Mar-22 09:17:32 - Epoch: [9][100/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.004)	Loss 7.9841e-01 (9.0235e-01)	Acc@1  78.91 ( 74.54)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:17:33 - Epoch: [8][ 40/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.008)	Loss 1.0580e+00 (8.9946e-01)	Acc@1  68.75 ( 74.85)	Acc@5  92.97 ( 94.26)
03-Mar-22 09:17:33 - Epoch: [9][110/352]	Time  0.154 ( 0.149)	Data  0.002 ( 0.004)	Loss 1.0197e+00 (9.0590e-01)	Acc@1  71.88 ( 74.51)	Acc@5  94.53 ( 94.47)
03-Mar-22 09:17:34 - Epoch: [8][ 50/352]	Time  0.155 ( 0.156)	Data  0.002 ( 0.007)	Loss 9.8837e-01 (9.0391e-01)	Acc@1  71.88 ( 74.66)	Acc@5  95.31 ( 94.39)
03-Mar-22 09:17:35 - Epoch: [9][120/352]	Time  0.134 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.5753e-01 (9.0600e-01)	Acc@1  71.88 ( 74.42)	Acc@5  97.66 ( 94.51)
03-Mar-22 09:17:36 - Epoch: [8][ 60/352]	Time  0.149 ( 0.155)	Data  0.002 ( 0.006)	Loss 7.3123e-01 (9.0202e-01)	Acc@1  78.91 ( 74.77)	Acc@5  97.66 ( 94.43)
03-Mar-22 09:17:36 - Epoch: [9][130/352]	Time  0.161 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0837e+00 (9.0350e-01)	Acc@1  67.97 ( 74.49)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:17:37 - Epoch: [8][ 70/352]	Time  0.149 ( 0.155)	Data  0.002 ( 0.005)	Loss 8.0217e-01 (9.0024e-01)	Acc@1  80.47 ( 74.71)	Acc@5  93.75 ( 94.43)
03-Mar-22 09:17:38 - Epoch: [9][140/352]	Time  0.156 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.7801e-01 (9.0297e-01)	Acc@1  75.00 ( 74.50)	Acc@5  94.53 ( 94.59)
03-Mar-22 09:17:39 - Epoch: [8][ 80/352]	Time  0.148 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.5595e-01 (8.9897e-01)	Acc@1  71.09 ( 74.67)	Acc@5  92.97 ( 94.41)
03-Mar-22 09:17:39 - Epoch: [9][150/352]	Time  0.165 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0125e+00 (9.0383e-01)	Acc@1  66.41 ( 74.42)	Acc@5  91.41 ( 94.59)
03-Mar-22 09:17:40 - Epoch: [8][ 90/352]	Time  0.147 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.1464e-01 (9.0242e-01)	Acc@1  77.34 ( 74.48)	Acc@5  95.31 ( 94.35)
03-Mar-22 09:17:41 - Epoch: [9][160/352]	Time  0.164 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.0876e-01 (9.0257e-01)	Acc@1  79.69 ( 74.45)	Acc@5  96.09 ( 94.58)
03-Mar-22 09:17:42 - Epoch: [8][100/352]	Time  0.150 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.1472e-01 (9.0308e-01)	Acc@1  74.22 ( 74.40)	Acc@5  95.31 ( 94.38)
03-Mar-22 09:17:43 - Epoch: [9][170/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.1155e+00 (9.0616e-01)	Acc@1  65.62 ( 74.35)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:17:44 - Epoch: [8][110/352]	Time  0.158 ( 0.153)	Data  0.003 ( 0.004)	Loss 1.0887e+00 (9.0337e-01)	Acc@1  71.09 ( 74.44)	Acc@5  90.62 ( 94.40)
03-Mar-22 09:17:44 - Epoch: [9][180/352]	Time  0.134 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.6375e-01 (9.1156e-01)	Acc@1  73.44 ( 74.08)	Acc@5  95.31 ( 94.50)
03-Mar-22 09:17:45 - Epoch: [8][120/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.9916e-01 (9.0058e-01)	Acc@1  77.34 ( 74.57)	Acc@5  94.53 ( 94.43)
03-Mar-22 09:17:46 - Epoch: [9][190/352]	Time  0.149 ( 0.152)	Data  0.003 ( 0.003)	Loss 8.8607e-01 (9.1132e-01)	Acc@1  74.22 ( 74.15)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:17:46 - Epoch: [8][130/352]	Time  0.154 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.3322e-01 (9.0240e-01)	Acc@1  77.34 ( 74.47)	Acc@5  95.31 ( 94.41)
03-Mar-22 09:17:47 - Epoch: [9][200/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.5439e-01 (9.1351e-01)	Acc@1  78.12 ( 74.15)	Acc@5  95.31 ( 94.39)
03-Mar-22 09:17:48 - Epoch: [8][140/352]	Time  0.143 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.8821e-01 (9.0265e-01)	Acc@1  76.56 ( 74.49)	Acc@5  97.66 ( 94.50)
03-Mar-22 09:17:49 - Epoch: [9][210/352]	Time  0.146 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0060e+00 (9.1516e-01)	Acc@1  74.22 ( 74.08)	Acc@5  92.97 ( 94.38)
03-Mar-22 09:17:49 - Epoch: [8][150/352]	Time  0.137 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.3144e-01 (9.0064e-01)	Acc@1  74.22 ( 74.47)	Acc@5  94.53 ( 94.60)
03-Mar-22 09:17:50 - Epoch: [9][220/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.7958e-01 (9.1460e-01)	Acc@1  77.34 ( 74.08)	Acc@5  93.75 ( 94.37)
03-Mar-22 09:17:51 - Epoch: [8][160/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.2688e-01 (9.0164e-01)	Acc@1  73.44 ( 74.45)	Acc@5  96.09 ( 94.62)
03-Mar-22 09:17:51 - Epoch: [9][230/352]	Time  0.141 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.5385e-01 (9.1615e-01)	Acc@1  71.88 ( 74.03)	Acc@5  92.97 ( 94.35)
03-Mar-22 09:17:52 - Epoch: [8][170/352]	Time  0.152 ( 0.151)	Data  0.003 ( 0.004)	Loss 1.0540e+00 (9.0215e-01)	Acc@1  71.09 ( 74.47)	Acc@5  92.19 ( 94.56)
03-Mar-22 09:17:53 - Epoch: [9][240/352]	Time  0.131 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3520e-01 (9.1625e-01)	Acc@1  75.00 ( 74.02)	Acc@5  94.53 ( 94.33)
03-Mar-22 09:17:54 - Epoch: [8][180/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.0239e+00 (9.0077e-01)	Acc@1  67.19 ( 74.47)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:17:54 - Epoch: [9][250/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.8426e-01 (9.1568e-01)	Acc@1  78.12 ( 74.05)	Acc@5  95.31 ( 94.32)
03-Mar-22 09:17:55 - Epoch: [8][190/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.4311e-01 (9.0290e-01)	Acc@1  71.88 ( 74.43)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:17:56 - Epoch: [9][260/352]	Time  0.144 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.6517e-01 (9.1521e-01)	Acc@1  77.34 ( 74.09)	Acc@5  93.75 ( 94.33)
03-Mar-22 09:17:57 - Epoch: [8][200/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.9863e-01 (9.0286e-01)	Acc@1  81.25 ( 74.47)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:17:57 - Epoch: [9][270/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.8675e-01 (9.1577e-01)	Acc@1  75.00 ( 74.07)	Acc@5  92.97 ( 94.31)
03-Mar-22 09:17:58 - Epoch: [8][210/352]	Time  0.154 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.9855e-01 (9.0303e-01)	Acc@1  71.88 ( 74.49)	Acc@5  92.97 ( 94.52)
03-Mar-22 09:17:59 - Epoch: [9][280/352]	Time  0.152 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.9383e-01 (9.1576e-01)	Acc@1  78.12 ( 74.11)	Acc@5  95.31 ( 94.31)
03-Mar-22 09:18:00 - Epoch: [8][220/352]	Time  0.156 ( 0.151)	Data  0.003 ( 0.003)	Loss 8.8316e-01 (9.0118e-01)	Acc@1  76.56 ( 74.55)	Acc@5  94.53 ( 94.57)
03-Mar-22 09:18:00 - Epoch: [9][290/352]	Time  0.142 ( 0.150)	Data  0.002 ( 0.003)	Loss 6.9669e-01 (9.1264e-01)	Acc@1  80.47 ( 74.19)	Acc@5  94.53 ( 94.34)
03-Mar-22 09:18:01 - Epoch: [8][230/352]	Time  0.146 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.0842e-01 (9.0333e-01)	Acc@1  73.44 ( 74.45)	Acc@5  93.75 ( 94.52)
03-Mar-22 09:18:02 - Epoch: [9][300/352]	Time  0.150 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.4125e-01 (9.1203e-01)	Acc@1  82.03 ( 74.23)	Acc@5  94.53 ( 94.32)
03-Mar-22 09:18:03 - Epoch: [8][240/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.3760e-01 (9.0688e-01)	Acc@1  74.22 ( 74.38)	Acc@5  91.41 ( 94.45)
03-Mar-22 09:18:03 - Epoch: [9][310/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.6293e-01 (9.1097e-01)	Acc@1  77.34 ( 74.22)	Acc@5  92.97 ( 94.32)
03-Mar-22 09:18:04 - Epoch: [8][250/352]	Time  0.147 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.4947e-01 (9.0649e-01)	Acc@1  71.88 ( 74.43)	Acc@5  94.53 ( 94.45)
03-Mar-22 09:18:05 - Epoch: [9][320/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.9308e-01 (9.1158e-01)	Acc@1  72.66 ( 74.20)	Acc@5  92.97 ( 94.30)
03-Mar-22 09:18:06 - Epoch: [8][260/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.3599e-01 (9.0543e-01)	Acc@1  71.88 ( 74.45)	Acc@5  96.09 ( 94.47)
03-Mar-22 09:18:06 - Epoch: [9][330/352]	Time  0.148 ( 0.150)	Data  0.003 ( 0.003)	Loss 1.0837e+00 (9.1160e-01)	Acc@1  66.41 ( 74.20)	Acc@5  91.41 ( 94.30)
03-Mar-22 09:18:07 - Epoch: [8][270/352]	Time  0.150 ( 0.151)	Data  0.003 ( 0.003)	Loss 8.7250e-01 (9.0682e-01)	Acc@1  78.12 ( 74.40)	Acc@5  95.31 ( 94.47)
03-Mar-22 09:18:08 - Epoch: [9][340/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.7953e-01 (9.1017e-01)	Acc@1  71.88 ( 74.28)	Acc@5  92.97 ( 94.33)
03-Mar-22 09:18:09 - Epoch: [8][280/352]	Time  0.142 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.4824e-01 (9.0740e-01)	Acc@1  73.44 ( 74.36)	Acc@5  92.97 ( 94.46)
03-Mar-22 09:18:09 - Epoch: [9][350/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.7550e-01 (9.1051e-01)	Acc@1  77.34 ( 74.29)	Acc@5  95.31 ( 94.33)
03-Mar-22 09:18:10 - Test: [ 0/20]	Time  0.361 ( 0.361)	Loss 1.0141e+00 (1.0141e+00)	Acc@1  70.70 ( 70.70)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:18:10 - Epoch: [8][290/352]	Time  0.160 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.6805e-01 (9.0790e-01)	Acc@1  78.12 ( 74.38)	Acc@5  94.53 ( 94.43)
03-Mar-22 09:18:11 - Test: [10/20]	Time  0.094 ( 0.122)	Loss 8.0585e-01 (9.2974e-01)	Acc@1  77.34 ( 73.93)	Acc@5  97.27 ( 93.79)
03-Mar-22 09:18:12 -  * Acc@1 73.440 Acc@5 93.840
03-Mar-22 09:18:12 - Best acc at epoch 9: 73.97999572753906
03-Mar-22 09:18:12 - Epoch: [8][300/352]	Time  0.112 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.4593e-01 (9.0921e-01)	Acc@1  75.78 ( 74.33)	Acc@5  96.88 ( 94.43)
03-Mar-22 09:18:12 - Epoch: [10][  0/352]	Time  0.375 ( 0.375)	Data  0.228 ( 0.228)	Loss 1.0807e+00 (1.0807e+00)	Acc@1  67.97 ( 67.97)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:18:13 - Epoch: [8][310/352]	Time  0.120 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0098e+00 (9.0995e-01)	Acc@1  75.78 ( 74.33)	Acc@5  93.75 ( 94.43)
03-Mar-22 09:18:14 - Epoch: [10][ 10/352]	Time  0.155 ( 0.171)	Data  0.002 ( 0.023)	Loss 8.4546e-01 (8.9209e-01)	Acc@1  76.56 ( 74.15)	Acc@5  96.09 ( 95.03)
03-Mar-22 09:18:14 - Epoch: [8][320/352]	Time  0.128 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0568e+00 (9.1082e-01)	Acc@1  69.53 ( 74.30)	Acc@5  95.31 ( 94.41)
03-Mar-22 09:18:15 - Epoch: [10][ 20/352]	Time  0.155 ( 0.162)	Data  0.002 ( 0.013)	Loss 9.2348e-01 (8.6505e-01)	Acc@1  72.66 ( 74.93)	Acc@5  94.53 ( 95.24)
03-Mar-22 09:18:16 - Epoch: [8][330/352]	Time  0.150 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.3329e-01 (9.1109e-01)	Acc@1  73.44 ( 74.29)	Acc@5  91.41 ( 94.38)
03-Mar-22 09:18:17 - Epoch: [10][ 30/352]	Time  0.133 ( 0.157)	Data  0.002 ( 0.010)	Loss 1.1159e+00 (8.9407e-01)	Acc@1  68.75 ( 74.12)	Acc@5  91.41 ( 94.61)
03-Mar-22 09:18:17 - Epoch: [8][340/352]	Time  0.145 ( 0.148)	Data  0.002 ( 0.003)	Loss 7.9963e-01 (9.0962e-01)	Acc@1  79.69 ( 74.35)	Acc@5  94.53 ( 94.37)
03-Mar-22 09:18:18 - Epoch: [10][ 40/352]	Time  0.155 ( 0.155)	Data  0.002 ( 0.008)	Loss 9.8772e-01 (9.0057e-01)	Acc@1  74.22 ( 73.80)	Acc@5  92.97 ( 94.40)
03-Mar-22 09:18:19 - Epoch: [8][350/352]	Time  0.155 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.0788e+00 (9.0878e-01)	Acc@1  74.22 ( 74.40)	Acc@5  89.06 ( 94.38)
03-Mar-22 09:18:19 - Test: [ 0/20]	Time  0.356 ( 0.356)	Loss 9.6605e-01 (9.6605e-01)	Acc@1  72.27 ( 72.27)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:18:20 - Epoch: [10][ 50/352]	Time  0.186 ( 0.154)	Data  0.003 ( 0.007)	Loss 8.4382e-01 (8.9662e-01)	Acc@1  76.56 ( 74.17)	Acc@5  93.75 ( 94.41)
03-Mar-22 09:18:20 - Test: [10/20]	Time  0.089 ( 0.120)	Loss 8.2554e-01 (9.3767e-01)	Acc@1  76.17 ( 73.40)	Acc@5  96.09 ( 93.79)
03-Mar-22 09:18:21 -  * Acc@1 73.420 Acc@5 93.940
03-Mar-22 09:18:21 - Best acc at epoch 8: 74.04000091552734
03-Mar-22 09:18:21 - Epoch: [10][ 60/352]	Time  0.131 ( 0.156)	Data  0.002 ( 0.006)	Loss 1.0083e+00 (8.9396e-01)	Acc@1  70.31 ( 74.17)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:18:21 - Epoch: [9][  0/352]	Time  0.389 ( 0.389)	Data  0.221 ( 0.221)	Loss 9.3042e-01 (9.3042e-01)	Acc@1  71.09 ( 71.09)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:18:22 - Epoch: [10][ 70/352]	Time  0.125 ( 0.151)	Data  0.002 ( 0.005)	Loss 8.1142e-01 (8.9591e-01)	Acc@1  75.00 ( 74.22)	Acc@5  95.31 ( 94.54)
03-Mar-22 09:18:23 - Epoch: [9][ 10/352]	Time  0.156 ( 0.175)	Data  0.002 ( 0.023)	Loss 9.9385e-01 (9.3616e-01)	Acc@1  73.44 ( 72.59)	Acc@5  95.31 ( 93.96)
03-Mar-22 09:18:24 - Epoch: [10][ 80/352]	Time  0.129 ( 0.148)	Data  0.002 ( 0.005)	Loss 9.3920e-01 (8.9477e-01)	Acc@1  79.69 ( 74.44)	Acc@5  92.19 ( 94.48)
03-Mar-22 09:18:25 - Epoch: [9][ 20/352]	Time  0.155 ( 0.164)	Data  0.002 ( 0.013)	Loss 8.3270e-01 (9.2507e-01)	Acc@1  75.78 ( 72.99)	Acc@5  96.09 ( 94.12)
03-Mar-22 09:18:25 - Epoch: [10][ 90/352]	Time  0.124 ( 0.145)	Data  0.002 ( 0.005)	Loss 8.5309e-01 (8.9737e-01)	Acc@1  76.56 ( 74.30)	Acc@5  96.09 ( 94.45)
03-Mar-22 09:18:26 - Epoch: [9][ 30/352]	Time  0.156 ( 0.157)	Data  0.002 ( 0.009)	Loss 9.7847e-01 (9.2300e-01)	Acc@1  72.66 ( 73.26)	Acc@5  93.75 ( 94.10)
03-Mar-22 09:18:26 - Epoch: [10][100/352]	Time  0.128 ( 0.144)	Data  0.002 ( 0.004)	Loss 7.9782e-01 (8.9762e-01)	Acc@1  75.00 ( 74.29)	Acc@5  96.09 ( 94.41)
03-Mar-22 09:18:27 - Epoch: [10][110/352]	Time  0.122 ( 0.142)	Data  0.002 ( 0.004)	Loss 6.3581e-01 (8.8966e-01)	Acc@1  84.38 ( 74.64)	Acc@5  99.22 ( 94.52)
03-Mar-22 09:18:27 - Epoch: [9][ 40/352]	Time  0.159 ( 0.156)	Data  0.002 ( 0.008)	Loss 9.6613e-01 (9.1557e-01)	Acc@1  73.44 ( 73.55)	Acc@5  93.75 ( 94.26)
03-Mar-22 09:18:29 - Epoch: [10][120/352]	Time  0.126 ( 0.140)	Data  0.002 ( 0.004)	Loss 1.1580e+00 (8.9342e-01)	Acc@1  65.62 ( 74.54)	Acc@5  90.62 ( 94.47)
03-Mar-22 09:18:29 - Epoch: [9][ 50/352]	Time  0.156 ( 0.156)	Data  0.003 ( 0.007)	Loss 1.0235e+00 (9.0660e-01)	Acc@1  71.88 ( 74.07)	Acc@5  96.09 ( 94.50)
03-Mar-22 09:18:30 - Epoch: [10][130/352]	Time  0.126 ( 0.139)	Data  0.002 ( 0.004)	Loss 9.1386e-01 (8.9974e-01)	Acc@1  72.66 ( 74.41)	Acc@5  94.53 ( 94.44)
03-Mar-22 09:18:31 - Epoch: [9][ 60/352]	Time  0.158 ( 0.156)	Data  0.002 ( 0.006)	Loss 8.8992e-01 (9.1346e-01)	Acc@1  78.91 ( 74.10)	Acc@5  93.75 ( 94.42)
03-Mar-22 09:18:31 - Epoch: [10][140/352]	Time  0.128 ( 0.138)	Data  0.002 ( 0.004)	Loss 1.1512e+00 (9.0055e-01)	Acc@1  67.19 ( 74.40)	Acc@5  89.84 ( 94.44)
03-Mar-22 09:18:32 - Epoch: [9][ 70/352]	Time  0.157 ( 0.155)	Data  0.002 ( 0.005)	Loss 1.1725e+00 (9.2333e-01)	Acc@1  69.53 ( 74.05)	Acc@5  94.53 ( 94.29)
03-Mar-22 09:18:32 - Epoch: [10][150/352]	Time  0.118 ( 0.137)	Data  0.002 ( 0.003)	Loss 9.0738e-01 (8.9618e-01)	Acc@1  75.78 ( 74.58)	Acc@5  92.19 ( 94.46)
03-Mar-22 09:18:34 - Epoch: [10][160/352]	Time  0.118 ( 0.136)	Data  0.002 ( 0.003)	Loss 8.8973e-01 (8.9718e-01)	Acc@1  71.88 ( 74.54)	Acc@5  96.88 ( 94.42)
03-Mar-22 09:18:34 - Epoch: [9][ 80/352]	Time  0.151 ( 0.155)	Data  0.002 ( 0.005)	Loss 8.5306e-01 (9.2456e-01)	Acc@1  77.34 ( 74.07)	Acc@5  94.53 ( 94.24)
03-Mar-22 09:18:35 - Epoch: [10][170/352]	Time  0.121 ( 0.135)	Data  0.002 ( 0.003)	Loss 8.6629e-01 (8.9955e-01)	Acc@1  73.44 ( 74.42)	Acc@5  92.97 ( 94.38)
03-Mar-22 09:18:35 - Epoch: [9][ 90/352]	Time  0.155 ( 0.155)	Data  0.002 ( 0.005)	Loss 8.9941e-01 (9.2443e-01)	Acc@1  76.56 ( 74.06)	Acc@5  96.09 ( 94.31)
03-Mar-22 09:18:36 - Epoch: [10][180/352]	Time  0.121 ( 0.135)	Data  0.002 ( 0.003)	Loss 1.0031e+00 (8.9708e-01)	Acc@1  75.78 ( 74.46)	Acc@5  95.31 ( 94.41)
03-Mar-22 09:18:37 - Epoch: [9][100/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.6930e-01 (9.2458e-01)	Acc@1  67.97 ( 74.02)	Acc@5  92.19 ( 94.31)
03-Mar-22 09:18:38 - Epoch: [10][190/352]	Time  0.150 ( 0.135)	Data  0.002 ( 0.003)	Loss 7.9938e-01 (8.9747e-01)	Acc@1  77.34 ( 74.46)	Acc@5  94.53 ( 94.41)
03-Mar-22 09:18:38 - Epoch: [9][110/352]	Time  0.155 ( 0.154)	Data  0.003 ( 0.004)	Loss 9.5380e-01 (9.2554e-01)	Acc@1  67.97 ( 73.99)	Acc@5  95.31 ( 94.26)
03-Mar-22 09:18:39 - Epoch: [10][200/352]	Time  0.153 ( 0.136)	Data  0.002 ( 0.003)	Loss 8.7034e-01 (8.9595e-01)	Acc@1  73.44 ( 74.53)	Acc@5  96.88 ( 94.47)
03-Mar-22 09:18:40 - Epoch: [9][120/352]	Time  0.147 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.1575e-01 (9.2816e-01)	Acc@1  76.56 ( 73.81)	Acc@5  95.31 ( 94.25)
03-Mar-22 09:18:41 - Epoch: [10][210/352]	Time  0.136 ( 0.136)	Data  0.002 ( 0.003)	Loss 8.0351e-01 (8.9685e-01)	Acc@1  72.66 ( 74.43)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:18:41 - Epoch: [9][130/352]	Time  0.132 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0128e+00 (9.2653e-01)	Acc@1  71.88 ( 73.84)	Acc@5  94.53 ( 94.22)
03-Mar-22 09:18:42 - Epoch: [10][220/352]	Time  0.150 ( 0.137)	Data  0.002 ( 0.003)	Loss 1.0823e+00 (8.9877e-01)	Acc@1  73.44 ( 74.43)	Acc@5  90.62 ( 94.40)
03-Mar-22 09:18:43 - Epoch: [9][140/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.2894e-01 (9.2530e-01)	Acc@1  78.12 ( 73.91)	Acc@5  96.09 ( 94.28)
03-Mar-22 09:18:43 - Epoch: [10][230/352]	Time  0.143 ( 0.137)	Data  0.002 ( 0.003)	Loss 7.2496e-01 (8.9765e-01)	Acc@1  80.47 ( 74.53)	Acc@5  96.09 ( 94.40)
03-Mar-22 09:18:44 - Epoch: [9][150/352]	Time  0.150 ( 0.152)	Data  0.003 ( 0.004)	Loss 1.0084e+00 (9.2466e-01)	Acc@1  76.56 ( 73.93)	Acc@5  91.41 ( 94.28)
03-Mar-22 09:18:45 - Epoch: [10][240/352]	Time  0.154 ( 0.138)	Data  0.002 ( 0.003)	Loss 8.5213e-01 (9.0209e-01)	Acc@1  75.78 ( 74.39)	Acc@5  96.09 ( 94.39)
03-Mar-22 09:18:46 - Epoch: [9][160/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.6604e-01 (9.2292e-01)	Acc@1  78.12 ( 74.06)	Acc@5  99.22 ( 94.28)
03-Mar-22 09:18:46 - Epoch: [10][250/352]	Time  0.152 ( 0.138)	Data  0.002 ( 0.003)	Loss 9.7267e-01 (9.0159e-01)	Acc@1  72.66 ( 74.39)	Acc@5  93.75 ( 94.42)
03-Mar-22 09:18:47 - Epoch: [9][170/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.8526e-01 (9.2664e-01)	Acc@1  75.00 ( 73.94)	Acc@5  97.66 ( 94.25)
03-Mar-22 09:18:48 - Epoch: [10][260/352]	Time  0.154 ( 0.139)	Data  0.002 ( 0.003)	Loss 9.1750e-01 (9.0188e-01)	Acc@1  77.34 ( 74.40)	Acc@5  91.41 ( 94.40)
03-Mar-22 09:18:49 - Epoch: [9][180/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0404e+00 (9.2619e-01)	Acc@1  66.41 ( 73.88)	Acc@5  93.75 ( 94.25)
03-Mar-22 09:18:49 - Epoch: [10][270/352]	Time  0.155 ( 0.139)	Data  0.002 ( 0.003)	Loss 9.4760e-01 (9.0143e-01)	Acc@1  71.09 ( 74.37)	Acc@5  92.97 ( 94.42)
03-Mar-22 09:18:50 - Epoch: [9][190/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0018e+00 (9.2738e-01)	Acc@1  68.75 ( 73.83)	Acc@5  90.62 ( 94.20)
03-Mar-22 09:18:51 - Epoch: [10][280/352]	Time  0.147 ( 0.140)	Data  0.002 ( 0.003)	Loss 9.6830e-01 (9.0302e-01)	Acc@1  70.31 ( 74.31)	Acc@5  95.31 ( 94.42)
03-Mar-22 09:18:52 - Epoch: [9][200/352]	Time  0.136 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0382e+00 (9.2804e-01)	Acc@1  73.44 ( 73.79)	Acc@5  92.19 ( 94.17)
03-Mar-22 09:18:52 - Epoch: [10][290/352]	Time  0.152 ( 0.140)	Data  0.002 ( 0.003)	Loss 7.1983e-01 (9.0188e-01)	Acc@1  78.12 ( 74.33)	Acc@5  98.44 ( 94.45)
03-Mar-22 09:18:53 - Epoch: [9][210/352]	Time  0.130 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.1704e+00 (9.2906e-01)	Acc@1  67.19 ( 73.78)	Acc@5  89.84 ( 94.11)
03-Mar-22 09:18:54 - Epoch: [10][300/352]	Time  0.134 ( 0.140)	Data  0.002 ( 0.003)	Loss 9.8303e-01 (9.0354e-01)	Acc@1  68.75 ( 74.29)	Acc@5  93.75 ( 94.43)
03-Mar-22 09:18:54 - Epoch: [9][220/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.5936e-01 (9.2544e-01)	Acc@1  71.09 ( 73.87)	Acc@5  96.09 ( 94.15)
03-Mar-22 09:18:55 - Epoch: [10][310/352]	Time  0.155 ( 0.140)	Data  0.002 ( 0.003)	Loss 8.5672e-01 (9.0219e-01)	Acc@1  75.00 ( 74.31)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:18:56 - Epoch: [9][230/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.0795e-01 (9.2435e-01)	Acc@1  78.12 ( 73.94)	Acc@5  96.09 ( 94.17)
03-Mar-22 09:18:57 - Epoch: [10][320/352]	Time  0.145 ( 0.140)	Data  0.002 ( 0.003)	Loss 7.2375e-01 (9.0251e-01)	Acc@1  77.34 ( 74.33)	Acc@5  96.88 ( 94.45)
03-Mar-22 09:18:57 - Epoch: [9][240/352]	Time  0.152 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.4505e-01 (9.2263e-01)	Acc@1  71.88 ( 73.98)	Acc@5  93.75 ( 94.20)
03-Mar-22 09:18:58 - Epoch: [10][330/352]	Time  0.149 ( 0.140)	Data  0.002 ( 0.003)	Loss 8.5258e-01 (9.0176e-01)	Acc@1  77.34 ( 74.35)	Acc@5  92.97 ( 94.43)
03-Mar-22 09:18:59 - Epoch: [9][250/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.0855e-01 (9.1972e-01)	Acc@1  81.25 ( 74.07)	Acc@5  93.75 ( 94.20)
03-Mar-22 09:19:00 - Epoch: [10][340/352]	Time  0.152 ( 0.141)	Data  0.002 ( 0.003)	Loss 9.7033e-01 (9.0192e-01)	Acc@1  72.66 ( 74.36)	Acc@5  94.53 ( 94.44)
03-Mar-22 09:19:00 - Epoch: [9][260/352]	Time  0.156 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.7751e-01 (9.1814e-01)	Acc@1  74.22 ( 74.09)	Acc@5  93.75 ( 94.23)
03-Mar-22 09:19:01 - Epoch: [10][350/352]	Time  0.153 ( 0.141)	Data  0.002 ( 0.003)	Loss 9.7728e-01 (9.0137e-01)	Acc@1  72.66 ( 74.39)	Acc@5  96.88 ( 94.48)
03-Mar-22 09:19:02 - Epoch: [9][270/352]	Time  0.108 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.7549e-01 (9.1684e-01)	Acc@1  75.78 ( 74.12)	Acc@5  97.66 ( 94.26)
03-Mar-22 09:19:02 - Test: [ 0/20]	Time  0.365 ( 0.365)	Loss 1.0782e+00 (1.0782e+00)	Acc@1  67.58 ( 67.58)	Acc@5  91.41 ( 91.41)
03-Mar-22 09:19:03 - Test: [10/20]	Time  0.107 ( 0.123)	Loss 8.4485e-01 (9.3410e-01)	Acc@1  73.44 ( 73.05)	Acc@5  94.53 ( 94.03)
03-Mar-22 09:19:03 - Epoch: [9][280/352]	Time  0.155 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.6464e-01 (9.1647e-01)	Acc@1  78.12 ( 74.15)	Acc@5  96.88 ( 94.26)
03-Mar-22 09:19:04 -  * Acc@1 73.640 Acc@5 94.220
03-Mar-22 09:19:04 - Best acc at epoch 10: 73.97999572753906
03-Mar-22 09:19:04 - Epoch: [11][  0/352]	Time  0.389 ( 0.389)	Data  0.257 ( 0.257)	Loss 9.9409e-01 (9.9409e-01)	Acc@1  70.31 ( 70.31)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:19:05 - Epoch: [9][290/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.0416e-01 (9.1429e-01)	Acc@1  82.03 ( 74.21)	Acc@5  96.88 ( 94.27)
03-Mar-22 09:19:06 - Epoch: [11][ 10/352]	Time  0.127 ( 0.164)	Data  0.002 ( 0.025)	Loss 8.7583e-01 (9.0003e-01)	Acc@1  78.12 ( 74.43)	Acc@5  93.75 ( 93.68)
03-Mar-22 09:19:06 - Epoch: [9][300/352]	Time  0.134 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1357e+00 (9.1402e-01)	Acc@1  66.41 ( 74.23)	Acc@5  92.97 ( 94.25)
03-Mar-22 09:19:07 - Epoch: [11][ 20/352]	Time  0.175 ( 0.168)	Data  0.002 ( 0.014)	Loss 8.4828e-01 (8.8765e-01)	Acc@1  78.12 ( 74.96)	Acc@5  96.09 ( 94.08)
03-Mar-22 09:19:07 - Epoch: [9][310/352]	Time  0.156 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0603e+00 (9.1439e-01)	Acc@1  71.88 ( 74.21)	Acc@5  92.97 ( 94.24)
03-Mar-22 09:19:09 - Epoch: [9][320/352]	Time  0.137 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.6211e-01 (9.1171e-01)	Acc@1  79.69 ( 74.25)	Acc@5  96.09 ( 94.28)
03-Mar-22 09:19:09 - Epoch: [11][ 30/352]	Time  0.174 ( 0.169)	Data  0.002 ( 0.010)	Loss 7.5157e-01 (8.9484e-01)	Acc@1  76.56 ( 74.55)	Acc@5  95.31 ( 94.13)
03-Mar-22 09:19:10 - Epoch: [9][330/352]	Time  0.150 ( 0.149)	Data  0.003 ( 0.003)	Loss 9.0582e-01 (9.1005e-01)	Acc@1  73.44 ( 74.27)	Acc@5  94.53 ( 94.32)
03-Mar-22 09:19:11 - Epoch: [11][ 40/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.008)	Loss 8.1962e-01 (8.9967e-01)	Acc@1  75.78 ( 74.24)	Acc@5  95.31 ( 94.32)
03-Mar-22 09:19:12 - Epoch: [9][340/352]	Time  0.156 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0312e+00 (9.0969e-01)	Acc@1  71.88 ( 74.24)	Acc@5  91.41 ( 94.33)
03-Mar-22 09:19:12 - Epoch: [11][ 50/352]	Time  0.175 ( 0.169)	Data  0.002 ( 0.007)	Loss 8.9016e-01 (8.9423e-01)	Acc@1  77.34 ( 74.46)	Acc@5  92.97 ( 94.27)
03-Mar-22 09:19:13 - Epoch: [9][350/352]	Time  0.145 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.8558e-01 (9.0965e-01)	Acc@1  71.88 ( 74.26)	Acc@5  93.75 ( 94.34)
03-Mar-22 09:19:14 - Epoch: [11][ 60/352]	Time  0.101 ( 0.167)	Data  0.002 ( 0.006)	Loss 8.8277e-01 (8.8753e-01)	Acc@1  71.88 ( 74.54)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:19:14 - Test: [ 0/20]	Time  0.366 ( 0.366)	Loss 1.0390e+00 (1.0390e+00)	Acc@1  68.75 ( 68.75)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:19:15 - Test: [10/20]	Time  0.094 ( 0.124)	Loss 8.2269e-01 (9.6485e-01)	Acc@1  75.78 ( 73.01)	Acc@5  94.14 ( 93.39)
03-Mar-22 09:19:16 - Epoch: [11][ 70/352]	Time  0.142 ( 0.165)	Data  0.002 ( 0.006)	Loss 1.0492e+00 (8.9041e-01)	Acc@1  73.44 ( 74.33)	Acc@5  92.19 ( 94.54)
03-Mar-22 09:19:16 -  * Acc@1 72.460 Acc@5 93.700
03-Mar-22 09:19:16 - Best acc at epoch 9: 74.04000091552734
03-Mar-22 09:19:17 - Epoch: [10][  0/352]	Time  0.347 ( 0.347)	Data  0.218 ( 0.218)	Loss 7.5992e-01 (7.5992e-01)	Acc@1  78.91 ( 78.91)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:19:17 - Epoch: [11][ 80/352]	Time  0.141 ( 0.161)	Data  0.002 ( 0.005)	Loss 9.2002e-01 (8.8842e-01)	Acc@1  73.44 ( 74.36)	Acc@5  93.75 ( 94.51)
03-Mar-22 09:19:18 - Epoch: [10][ 10/352]	Time  0.151 ( 0.163)	Data  0.002 ( 0.023)	Loss 8.9890e-01 (8.4083e-01)	Acc@1  69.53 ( 75.99)	Acc@5  94.53 ( 94.60)
03-Mar-22 09:19:18 - Epoch: [11][ 90/352]	Time  0.154 ( 0.159)	Data  0.002 ( 0.005)	Loss 9.1407e-01 (8.8657e-01)	Acc@1  78.91 ( 74.54)	Acc@5  90.62 ( 94.46)
03-Mar-22 09:19:20 - Epoch: [10][ 20/352]	Time  0.146 ( 0.159)	Data  0.002 ( 0.013)	Loss 9.0409e-01 (8.6172e-01)	Acc@1  75.00 ( 75.74)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:19:20 - Epoch: [11][100/352]	Time  0.148 ( 0.158)	Data  0.002 ( 0.005)	Loss 8.3074e-01 (8.8453e-01)	Acc@1  76.56 ( 74.63)	Acc@5  92.97 ( 94.52)
03-Mar-22 09:19:21 - Epoch: [10][ 30/352]	Time  0.137 ( 0.156)	Data  0.001 ( 0.009)	Loss 1.0043e+00 (8.8398e-01)	Acc@1  67.97 ( 75.13)	Acc@5  96.88 ( 94.81)
03-Mar-22 09:19:21 - Epoch: [11][110/352]	Time  0.145 ( 0.157)	Data  0.002 ( 0.004)	Loss 7.6199e-01 (8.8523e-01)	Acc@1  78.91 ( 74.66)	Acc@5  96.88 ( 94.51)
03-Mar-22 09:19:23 - Epoch: [10][ 40/352]	Time  0.153 ( 0.156)	Data  0.003 ( 0.008)	Loss 7.4039e-01 (8.9510e-01)	Acc@1  78.12 ( 74.89)	Acc@5  96.09 ( 94.59)
03-Mar-22 09:19:23 - Epoch: [11][120/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.0166e+00 (8.8751e-01)	Acc@1  68.75 ( 74.61)	Acc@5  92.97 ( 94.49)
03-Mar-22 09:19:24 - Epoch: [10][ 50/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.007)	Loss 8.9051e-01 (8.9237e-01)	Acc@1  75.00 ( 74.91)	Acc@5  95.31 ( 94.61)
03-Mar-22 09:19:24 - Epoch: [11][130/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.004)	Loss 9.0574e-01 (8.8791e-01)	Acc@1  75.00 ( 74.62)	Acc@5  92.97 ( 94.50)
03-Mar-22 09:19:26 - Epoch: [10][ 60/352]	Time  0.155 ( 0.154)	Data  0.003 ( 0.006)	Loss 7.6705e-01 (8.8333e-01)	Acc@1  81.25 ( 75.12)	Acc@5  95.31 ( 94.80)
03-Mar-22 09:19:26 - Epoch: [11][140/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.004)	Loss 8.4786e-01 (8.8621e-01)	Acc@1  76.56 ( 74.75)	Acc@5  94.53 ( 94.50)
03-Mar-22 09:19:27 - Epoch: [10][ 70/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.006)	Loss 7.7875e-01 (8.8564e-01)	Acc@1  76.56 ( 74.91)	Acc@5  98.44 ( 94.78)
03-Mar-22 09:19:27 - Epoch: [11][150/352]	Time  0.137 ( 0.155)	Data  0.002 ( 0.004)	Loss 8.4088e-01 (8.8317e-01)	Acc@1  75.78 ( 74.78)	Acc@5  95.31 ( 94.54)
03-Mar-22 09:19:29 - Epoch: [10][ 80/352]	Time  0.139 ( 0.153)	Data  0.002 ( 0.005)	Loss 7.7385e-01 (8.8671e-01)	Acc@1  82.03 ( 74.99)	Acc@5  94.53 ( 94.79)
03-Mar-22 09:19:29 - Epoch: [11][160/352]	Time  0.150 ( 0.155)	Data  0.002 ( 0.004)	Loss 8.7183e-01 (8.8560e-01)	Acc@1  71.88 ( 74.72)	Acc@5  97.66 ( 94.54)
03-Mar-22 09:19:30 - Epoch: [11][170/352]	Time  0.152 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.2016e-01 (8.8641e-01)	Acc@1  77.34 ( 74.73)	Acc@5  96.88 ( 94.56)
03-Mar-22 09:19:30 - Epoch: [10][ 90/352]	Time  0.136 ( 0.153)	Data  0.002 ( 0.005)	Loss 1.1963e+00 (8.8848e-01)	Acc@1  68.75 ( 74.98)	Acc@5  89.84 ( 94.76)
03-Mar-22 09:19:32 - Epoch: [11][180/352]	Time  0.151 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.9187e-01 (8.8718e-01)	Acc@1  72.66 ( 74.74)	Acc@5  96.09 ( 94.60)
03-Mar-22 09:19:32 - Epoch: [10][100/352]	Time  0.151 ( 0.153)	Data  0.003 ( 0.005)	Loss 9.5711e-01 (8.9028e-01)	Acc@1  72.66 ( 74.92)	Acc@5  94.53 ( 94.72)
03-Mar-22 09:19:33 - Epoch: [11][190/352]	Time  0.141 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0813e+00 (8.8821e-01)	Acc@1  68.75 ( 74.70)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:19:33 - Epoch: [10][110/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.1434e-01 (8.8726e-01)	Acc@1  82.03 ( 74.99)	Acc@5  95.31 ( 94.77)
03-Mar-22 09:19:35 - Epoch: [11][200/352]	Time  0.149 ( 0.153)	Data  0.003 ( 0.003)	Loss 1.2133e+00 (8.9154e-01)	Acc@1  65.62 ( 74.61)	Acc@5  92.97 ( 94.52)
03-Mar-22 09:19:35 - Epoch: [10][120/352]	Time  0.158 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.4208e-01 (8.8976e-01)	Acc@1  72.66 ( 74.93)	Acc@5  93.75 ( 94.75)
03-Mar-22 09:19:36 - Epoch: [11][210/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0244e+00 (8.9199e-01)	Acc@1  71.88 ( 74.60)	Acc@5  93.75 ( 94.53)
03-Mar-22 09:19:36 - Epoch: [10][130/352]	Time  0.147 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.4258e-01 (8.8946e-01)	Acc@1  70.31 ( 74.97)	Acc@5  94.53 ( 94.76)
03-Mar-22 09:19:38 - Epoch: [11][220/352]	Time  0.149 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.5576e-01 (8.9366e-01)	Acc@1  71.09 ( 74.52)	Acc@5  94.53 ( 94.49)
03-Mar-22 09:19:38 - Epoch: [10][140/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.5425e-01 (8.9404e-01)	Acc@1  68.75 ( 74.82)	Acc@5  95.31 ( 94.67)
03-Mar-22 09:19:39 - Epoch: [11][230/352]	Time  0.155 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.7986e-01 (8.9382e-01)	Acc@1  73.44 ( 74.47)	Acc@5  92.97 ( 94.46)
03-Mar-22 09:19:39 - Epoch: [10][150/352]	Time  0.156 ( 0.153)	Data  0.003 ( 0.004)	Loss 1.0385e+00 (8.9802e-01)	Acc@1  70.31 ( 74.74)	Acc@5  95.31 ( 94.66)
03-Mar-22 09:19:41 - Epoch: [11][240/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.6135e-01 (8.9506e-01)	Acc@1  75.00 ( 74.42)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:19:41 - Epoch: [10][160/352]	Time  0.153 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.9758e-01 (9.0253e-01)	Acc@1  70.31 ( 74.55)	Acc@5  91.41 ( 94.58)
03-Mar-22 09:19:42 - Epoch: [11][250/352]	Time  0.154 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0527e+00 (8.9796e-01)	Acc@1  69.53 ( 74.27)	Acc@5  93.75 ( 94.44)
03-Mar-22 09:19:42 - Epoch: [10][170/352]	Time  0.151 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0308e+00 (9.0424e-01)	Acc@1  68.75 ( 74.52)	Acc@5  92.97 ( 94.58)
03-Mar-22 09:19:44 - Epoch: [11][260/352]	Time  0.152 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.2103e-01 (8.9915e-01)	Acc@1  76.56 ( 74.26)	Acc@5  92.19 ( 94.44)
03-Mar-22 09:19:44 - Epoch: [10][180/352]	Time  0.146 ( 0.153)	Data  0.003 ( 0.004)	Loss 1.0714e+00 (9.0609e-01)	Acc@1  71.09 ( 74.34)	Acc@5  92.19 ( 94.56)
03-Mar-22 09:19:45 - Epoch: [11][270/352]	Time  0.135 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.8694e-01 (8.9996e-01)	Acc@1  75.00 ( 74.26)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:19:45 - Epoch: [10][190/352]	Time  0.155 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.7345e-01 (9.0335e-01)	Acc@1  67.97 ( 74.37)	Acc@5  94.53 ( 94.61)
03-Mar-22 09:19:47 - Epoch: [11][280/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.4668e-01 (9.0020e-01)	Acc@1  71.09 ( 74.21)	Acc@5  89.06 ( 94.44)
03-Mar-22 09:19:47 - Epoch: [10][200/352]	Time  0.132 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0464e+00 (9.0532e-01)	Acc@1  67.19 ( 74.30)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:19:48 - Epoch: [11][290/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.6674e-01 (8.9944e-01)	Acc@1  75.00 ( 74.23)	Acc@5  93.75 ( 94.46)
03-Mar-22 09:19:48 - Epoch: [10][210/352]	Time  0.156 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.1762e-01 (9.0552e-01)	Acc@1  78.91 ( 74.34)	Acc@5  96.88 ( 94.55)
03-Mar-22 09:19:50 - Epoch: [11][300/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.003)	Loss 1.0135e+00 (8.9895e-01)	Acc@1  76.56 ( 74.31)	Acc@5  89.06 ( 94.46)
03-Mar-22 09:19:50 - Epoch: [10][220/352]	Time  0.157 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.7270e-01 (9.0515e-01)	Acc@1  68.75 ( 74.34)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:19:51 - Epoch: [11][310/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.4600e-01 (8.9841e-01)	Acc@1  79.69 ( 74.35)	Acc@5  95.31 ( 94.47)
03-Mar-22 09:19:51 - Epoch: [10][230/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.3381e-01 (9.0151e-01)	Acc@1  75.78 ( 74.43)	Acc@5  96.88 ( 94.62)
03-Mar-22 09:19:53 - Epoch: [11][320/352]	Time  0.169 ( 0.153)	Data  0.003 ( 0.003)	Loss 1.0894e+00 (9.0015e-01)	Acc@1  68.75 ( 74.29)	Acc@5  91.41 ( 94.44)
03-Mar-22 09:19:53 - Epoch: [10][240/352]	Time  0.151 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.6282e-01 (9.0309e-01)	Acc@1  71.88 ( 74.39)	Acc@5  92.19 ( 94.56)
03-Mar-22 09:19:54 - Epoch: [10][250/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.1786e-01 (9.0512e-01)	Acc@1  74.22 ( 74.27)	Acc@5  96.88 ( 94.57)
03-Mar-22 09:19:55 - Epoch: [11][330/352]	Time  0.175 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.2412e-01 (9.0099e-01)	Acc@1  74.22 ( 74.29)	Acc@5  94.53 ( 94.39)
03-Mar-22 09:19:56 - Epoch: [10][260/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.9492e-01 (9.0423e-01)	Acc@1  73.44 ( 74.30)	Acc@5  96.09 ( 94.57)
03-Mar-22 09:19:56 - Epoch: [11][340/352]	Time  0.169 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.6397e-01 (9.0087e-01)	Acc@1  73.44 ( 74.33)	Acc@5  97.66 ( 94.38)
03-Mar-22 09:19:57 - Epoch: [10][270/352]	Time  0.131 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.8377e-01 (9.0382e-01)	Acc@1  76.56 ( 74.32)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:19:58 - Epoch: [11][350/352]	Time  0.174 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0265e+00 (9.0292e-01)	Acc@1  72.66 ( 74.27)	Acc@5  93.75 ( 94.35)
03-Mar-22 09:19:59 - Test: [ 0/20]	Time  0.354 ( 0.354)	Loss 9.7291e-01 (9.7291e-01)	Acc@1  71.48 ( 71.48)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:19:59 - Epoch: [10][280/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.9288e-01 (9.0481e-01)	Acc@1  64.84 ( 74.25)	Acc@5  96.09 ( 94.56)
03-Mar-22 09:20:00 - Test: [10/20]	Time  0.097 ( 0.121)	Loss 8.5112e-01 (9.3193e-01)	Acc@1  76.17 ( 73.08)	Acc@5  95.31 ( 94.11)
03-Mar-22 09:20:00 - Epoch: [10][290/352]	Time  0.145 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0570e+00 (9.0596e-01)	Acc@1  68.75 ( 74.25)	Acc@5  92.19 ( 94.53)
03-Mar-22 09:20:00 -  * Acc@1 73.040 Acc@5 94.020
03-Mar-22 09:20:01 - Best acc at epoch 11: 73.97999572753906
03-Mar-22 09:20:01 - Epoch: [12][  0/352]	Time  0.385 ( 0.385)	Data  0.226 ( 0.226)	Loss 8.1578e-01 (8.1578e-01)	Acc@1  79.69 ( 79.69)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:20:02 - Epoch: [10][300/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1482e-01 (9.0721e-01)	Acc@1  81.25 ( 74.22)	Acc@5  93.75 ( 94.51)
03-Mar-22 09:20:02 - Epoch: [12][ 10/352]	Time  0.147 ( 0.165)	Data  0.002 ( 0.022)	Loss 1.0606e+00 (8.8892e-01)	Acc@1  74.22 ( 74.72)	Acc@5  89.84 ( 94.39)
03-Mar-22 09:20:03 - Epoch: [10][310/352]	Time  0.132 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0958e+00 (9.0592e-01)	Acc@1  64.84 ( 74.26)	Acc@5  92.97 ( 94.52)
03-Mar-22 09:20:04 - Epoch: [12][ 20/352]	Time  0.174 ( 0.160)	Data  0.003 ( 0.013)	Loss 8.8961e-01 (8.8113e-01)	Acc@1  78.12 ( 74.93)	Acc@5  92.97 ( 94.72)
03-Mar-22 09:20:05 - Epoch: [10][320/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.5756e-01 (9.0528e-01)	Acc@1  78.91 ( 74.29)	Acc@5  96.88 ( 94.52)
03-Mar-22 09:20:05 - Epoch: [12][ 30/352]	Time  0.169 ( 0.159)	Data  0.002 ( 0.010)	Loss 8.3255e-01 (8.5911e-01)	Acc@1  73.44 ( 75.53)	Acc@5  95.31 ( 94.86)
03-Mar-22 09:20:06 - Epoch: [10][330/352]	Time  0.150 ( 0.151)	Data  0.003 ( 0.003)	Loss 8.0659e-01 (9.0281e-01)	Acc@1  76.56 ( 74.36)	Acc@5  97.66 ( 94.56)
03-Mar-22 09:20:07 - Epoch: [12][ 40/352]	Time  0.149 ( 0.159)	Data  0.002 ( 0.008)	Loss 8.5419e-01 (8.6951e-01)	Acc@1  71.88 ( 75.32)	Acc@5  96.09 ( 94.72)
03-Mar-22 09:20:08 - Epoch: [10][340/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.3682e-01 (9.0384e-01)	Acc@1  73.44 ( 74.31)	Acc@5  93.75 ( 94.54)
03-Mar-22 09:20:09 - Epoch: [12][ 50/352]	Time  0.150 ( 0.157)	Data  0.003 ( 0.007)	Loss 9.0951e-01 (8.7385e-01)	Acc@1  71.88 ( 75.20)	Acc@5  92.97 ( 94.81)
03-Mar-22 09:20:09 - Epoch: [10][350/352]	Time  0.147 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.2127e-01 (9.0429e-01)	Acc@1  71.09 ( 74.28)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:20:10 - Test: [ 0/20]	Time  0.357 ( 0.357)	Loss 1.0434e+00 (1.0434e+00)	Acc@1  67.97 ( 67.97)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:20:10 - Epoch: [12][ 60/352]	Time  0.160 ( 0.154)	Data  0.002 ( 0.006)	Loss 9.4926e-01 (8.7749e-01)	Acc@1  72.66 ( 74.90)	Acc@5  93.75 ( 94.83)
03-Mar-22 09:20:11 - Test: [10/20]	Time  0.095 ( 0.127)	Loss 7.9182e-01 (9.4513e-01)	Acc@1  79.30 ( 72.66)	Acc@5  96.09 ( 94.07)
03-Mar-22 09:20:12 - Epoch: [12][ 70/352]	Time  0.182 ( 0.155)	Data  0.003 ( 0.006)	Loss 9.0789e-01 (8.7982e-01)	Acc@1  72.66 ( 74.91)	Acc@5  94.53 ( 94.75)
03-Mar-22 09:20:12 -  * Acc@1 72.940 Acc@5 94.040
03-Mar-22 09:20:12 - Best acc at epoch 10: 74.04000091552734
03-Mar-22 09:20:12 - Epoch: [11][  0/352]	Time  0.359 ( 0.359)	Data  0.223 ( 0.223)	Loss 7.4421e-01 (7.4421e-01)	Acc@1  78.91 ( 78.91)	Acc@5  98.44 ( 98.44)
03-Mar-22 09:20:13 - Epoch: [12][ 80/352]	Time  0.151 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.8759e-01 (8.7875e-01)	Acc@1  78.12 ( 75.00)	Acc@5  92.19 ( 94.61)
03-Mar-22 09:20:14 - Epoch: [11][ 10/352]	Time  0.158 ( 0.172)	Data  0.002 ( 0.022)	Loss 7.3580e-01 (8.1209e-01)	Acc@1  82.03 ( 77.06)	Acc@5  98.44 ( 95.53)
03-Mar-22 09:20:15 - Epoch: [12][ 90/352]	Time  0.149 ( 0.155)	Data  0.003 ( 0.005)	Loss 6.8409e-01 (8.7861e-01)	Acc@1  82.81 ( 75.00)	Acc@5  96.88 ( 94.53)
03-Mar-22 09:20:15 - Epoch: [11][ 20/352]	Time  0.145 ( 0.163)	Data  0.002 ( 0.013)	Loss 8.6233e-01 (8.4378e-01)	Acc@1  76.56 ( 75.86)	Acc@5  96.09 ( 95.46)
03-Mar-22 09:20:16 - Epoch: [12][100/352]	Time  0.139 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.9133e-01 (8.7896e-01)	Acc@1  70.31 ( 74.99)	Acc@5  90.62 ( 94.53)
03-Mar-22 09:20:17 - Epoch: [11][ 30/352]	Time  0.148 ( 0.159)	Data  0.002 ( 0.010)	Loss 8.9358e-01 (8.6235e-01)	Acc@1  74.22 ( 75.18)	Acc@5  94.53 ( 94.88)
03-Mar-22 09:20:18 - Epoch: [12][110/352]	Time  0.147 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.1705e-01 (8.7454e-01)	Acc@1  72.66 ( 75.18)	Acc@5  95.31 ( 94.64)
03-Mar-22 09:20:18 - Epoch: [11][ 40/352]	Time  0.154 ( 0.157)	Data  0.002 ( 0.008)	Loss 7.9466e-01 (8.7089e-01)	Acc@1  73.44 ( 74.92)	Acc@5  97.66 ( 94.87)
03-Mar-22 09:20:19 - Epoch: [12][120/352]	Time  0.140 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0586e+00 (8.7604e-01)	Acc@1  69.53 ( 75.09)	Acc@5  92.97 ( 94.65)
03-Mar-22 09:20:20 - Epoch: [11][ 50/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.007)	Loss 8.7640e-01 (8.7269e-01)	Acc@1  77.34 ( 75.15)	Acc@5  93.75 ( 94.75)
03-Mar-22 09:20:21 - Epoch: [12][130/352]	Time  0.138 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.6749e-01 (8.7634e-01)	Acc@1  75.00 ( 75.02)	Acc@5  90.62 ( 94.64)
03-Mar-22 09:20:21 - Epoch: [11][ 60/352]	Time  0.143 ( 0.156)	Data  0.003 ( 0.006)	Loss 8.3885e-01 (8.8177e-01)	Acc@1  76.56 ( 74.86)	Acc@5  97.66 ( 94.57)
03-Mar-22 09:20:22 - Epoch: [12][140/352]	Time  0.147 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.0260e-01 (8.7975e-01)	Acc@1  72.66 ( 74.90)	Acc@5  92.97 ( 94.56)
03-Mar-22 09:20:23 - Epoch: [11][ 70/352]	Time  0.147 ( 0.155)	Data  0.002 ( 0.006)	Loss 8.8405e-01 (8.7608e-01)	Acc@1  75.00 ( 74.99)	Acc@5  95.31 ( 94.71)
03-Mar-22 09:20:24 - Epoch: [12][150/352]	Time  0.156 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.7687e-01 (8.8157e-01)	Acc@1  74.22 ( 74.81)	Acc@5  92.97 ( 94.56)
03-Mar-22 09:20:24 - Epoch: [11][ 80/352]	Time  0.152 ( 0.154)	Data  0.003 ( 0.005)	Loss 9.4191e-01 (8.7583e-01)	Acc@1  71.88 ( 74.98)	Acc@5  95.31 ( 94.65)
03-Mar-22 09:20:25 - Epoch: [12][160/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.9136e-01 (8.7996e-01)	Acc@1  74.22 ( 74.87)	Acc@5  92.97 ( 94.58)
03-Mar-22 09:20:26 - Epoch: [11][ 90/352]	Time  0.151 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.9062e-01 (8.7999e-01)	Acc@1  75.78 ( 75.08)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:20:27 - Epoch: [12][170/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.004)	Loss 6.6925e-01 (8.7946e-01)	Acc@1  83.59 ( 74.92)	Acc@5  95.31 ( 94.60)
03-Mar-22 09:20:27 - Epoch: [11][100/352]	Time  0.154 ( 0.154)	Data  0.003 ( 0.005)	Loss 1.0354e+00 (8.8376e-01)	Acc@1  71.09 ( 74.95)	Acc@5  94.53 ( 94.54)
03-Mar-22 09:20:28 - Epoch: [12][180/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0420e+00 (8.8127e-01)	Acc@1  71.09 ( 74.87)	Acc@5  92.97 ( 94.58)
03-Mar-22 09:20:29 - Epoch: [11][110/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.8836e-01 (8.8336e-01)	Acc@1  64.84 ( 74.82)	Acc@5  94.53 ( 94.59)
03-Mar-22 09:20:30 - Epoch: [12][190/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.9896e-01 (8.8468e-01)	Acc@1  76.56 ( 74.81)	Acc@5  93.75 ( 94.54)
03-Mar-22 09:20:30 - Epoch: [11][120/352]	Time  0.155 ( 0.153)	Data  0.003 ( 0.004)	Loss 8.2520e-01 (8.8579e-01)	Acc@1  73.44 ( 74.74)	Acc@5  93.75 ( 94.50)
03-Mar-22 09:20:31 - Epoch: [12][200/352]	Time  0.174 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0137e+00 (8.8672e-01)	Acc@1  75.78 ( 74.72)	Acc@5  92.19 ( 94.53)
03-Mar-22 09:20:32 - Epoch: [11][130/352]	Time  0.131 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.3626e-01 (8.8397e-01)	Acc@1  75.00 ( 74.81)	Acc@5  92.19 ( 94.49)
03-Mar-22 09:20:33 - Epoch: [12][210/352]	Time  0.172 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.2998e-01 (8.8527e-01)	Acc@1  71.88 ( 74.76)	Acc@5  93.75 ( 94.53)
03-Mar-22 09:20:33 - Epoch: [11][140/352]	Time  0.158 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0376e+00 (8.8640e-01)	Acc@1  71.09 ( 74.75)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:20:35 - Epoch: [12][220/352]	Time  0.172 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.1023e+00 (8.8746e-01)	Acc@1  66.41 ( 74.68)	Acc@5  93.75 ( 94.50)
03-Mar-22 09:20:35 - Epoch: [11][150/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.9717e-01 (8.8946e-01)	Acc@1  78.12 ( 74.74)	Acc@5  94.53 ( 94.40)
03-Mar-22 09:20:36 - Epoch: [12][230/352]	Time  0.146 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0641e+00 (8.8784e-01)	Acc@1  71.09 ( 74.73)	Acc@5  92.97 ( 94.49)
03-Mar-22 09:20:36 - Epoch: [11][160/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0187e+00 (8.9440e-01)	Acc@1  70.31 ( 74.57)	Acc@5  92.97 ( 94.42)
03-Mar-22 09:20:38 - Epoch: [12][240/352]	Time  0.138 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.0707e-01 (8.8734e-01)	Acc@1  75.00 ( 74.74)	Acc@5  96.88 ( 94.52)
03-Mar-22 09:20:38 - Epoch: [11][170/352]	Time  0.146 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.5085e-01 (8.9447e-01)	Acc@1  73.44 ( 74.59)	Acc@5  96.88 ( 94.47)
03-Mar-22 09:20:39 - Epoch: [12][250/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.003)	Loss 7.9902e-01 (8.8755e-01)	Acc@1  78.91 ( 74.78)	Acc@5  95.31 ( 94.51)
03-Mar-22 09:20:39 - Epoch: [11][180/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.3420e-01 (8.9557e-01)	Acc@1  75.78 ( 74.53)	Acc@5  95.31 ( 94.49)
03-Mar-22 09:20:41 - Epoch: [12][260/352]	Time  0.150 ( 0.154)	Data  0.002 ( 0.003)	Loss 7.2785e-01 (8.8853e-01)	Acc@1  80.47 ( 74.78)	Acc@5  97.66 ( 94.52)
03-Mar-22 09:20:41 - Epoch: [11][190/352]	Time  0.144 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.0796e-01 (8.9654e-01)	Acc@1  73.44 ( 74.43)	Acc@5  94.53 ( 94.48)
03-Mar-22 09:20:42 - Epoch: [11][200/352]	Time  0.129 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.4355e-01 (8.9828e-01)	Acc@1  78.12 ( 74.36)	Acc@5  96.88 ( 94.48)
03-Mar-22 09:20:42 - Epoch: [12][270/352]	Time  0.175 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.8216e-01 (8.8734e-01)	Acc@1  75.00 ( 74.84)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:20:44 - Epoch: [11][210/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.2861e-01 (8.9895e-01)	Acc@1  73.44 ( 74.37)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:20:44 - Epoch: [12][280/352]	Time  0.170 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.2181e-01 (8.8838e-01)	Acc@1  73.44 ( 74.78)	Acc@5  94.53 ( 94.52)
03-Mar-22 09:20:45 - Epoch: [11][220/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0770e+00 (9.0018e-01)	Acc@1  61.72 ( 74.34)	Acc@5  96.09 ( 94.45)
03-Mar-22 09:20:45 - Epoch: [12][290/352]	Time  0.146 ( 0.154)	Data  0.002 ( 0.003)	Loss 7.9403e-01 (8.8892e-01)	Acc@1  75.00 ( 74.75)	Acc@5  96.88 ( 94.52)
03-Mar-22 09:20:47 - Epoch: [11][230/352]	Time  0.156 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.3386e-01 (8.9949e-01)	Acc@1  70.31 ( 74.41)	Acc@5  97.66 ( 94.45)
03-Mar-22 09:20:47 - Epoch: [12][300/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.9606e-01 (8.9089e-01)	Acc@1  76.56 ( 74.68)	Acc@5  95.31 ( 94.49)
03-Mar-22 09:20:48 - Epoch: [11][240/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.7283e-01 (8.9873e-01)	Acc@1  78.12 ( 74.42)	Acc@5  93.75 ( 94.43)
03-Mar-22 09:20:48 - Epoch: [12][310/352]	Time  0.148 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.6453e-01 (8.9196e-01)	Acc@1  73.44 ( 74.65)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:20:50 - Epoch: [11][250/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.0054e-01 (8.9761e-01)	Acc@1  71.09 ( 74.34)	Acc@5  96.88 ( 94.47)
03-Mar-22 09:20:50 - Epoch: [12][320/352]	Time  0.145 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.7409e-01 (8.9095e-01)	Acc@1  74.22 ( 74.65)	Acc@5  95.31 ( 94.48)
03-Mar-22 09:20:51 - Epoch: [11][260/352]	Time  0.146 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.9588e-01 (8.9672e-01)	Acc@1  71.09 ( 74.36)	Acc@5  92.19 ( 94.51)
03-Mar-22 09:20:51 - Epoch: [12][330/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.7950e-01 (8.9171e-01)	Acc@1  75.78 ( 74.61)	Acc@5  95.31 ( 94.46)
03-Mar-22 09:20:53 - Epoch: [11][270/352]	Time  0.146 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.7364e-01 (8.9764e-01)	Acc@1  75.78 ( 74.31)	Acc@5  92.19 ( 94.49)
03-Mar-22 09:20:53 - Epoch: [12][340/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.2834e-01 (8.9120e-01)	Acc@1  78.91 ( 74.62)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:20:54 - Epoch: [11][280/352]	Time  0.130 ( 0.150)	Data  0.001 ( 0.003)	Loss 8.2206e-01 (8.9971e-01)	Acc@1  77.34 ( 74.30)	Acc@5  94.53 ( 94.45)
03-Mar-22 09:20:54 - Epoch: [12][350/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.2911e-01 (8.9014e-01)	Acc@1  74.22 ( 74.65)	Acc@5  96.09 ( 94.47)
03-Mar-22 09:20:55 - Test: [ 0/20]	Time  0.393 ( 0.393)	Loss 1.0150e+00 (1.0150e+00)	Acc@1  71.48 ( 71.48)	Acc@5  91.41 ( 91.41)
03-Mar-22 09:20:55 - Epoch: [11][290/352]	Time  0.154 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.5191e-01 (8.9926e-01)	Acc@1  75.00 ( 74.33)	Acc@5  94.53 ( 94.45)
03-Mar-22 09:20:56 - Test: [10/20]	Time  0.116 ( 0.135)	Loss 8.3822e-01 (9.5305e-01)	Acc@1  75.39 ( 73.40)	Acc@5  94.92 ( 93.61)
03-Mar-22 09:20:57 - Epoch: [11][300/352]	Time  0.143 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.9455e-01 (9.0079e-01)	Acc@1  78.91 ( 74.28)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:20:57 -  * Acc@1 72.900 Acc@5 93.640
03-Mar-22 09:20:57 - Best acc at epoch 12: 73.97999572753906
03-Mar-22 09:20:57 - Epoch: [13][  0/352]	Time  0.405 ( 0.405)	Data  0.255 ( 0.255)	Loss 9.4827e-01 (9.4827e-01)	Acc@1  73.44 ( 73.44)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:20:58 - Epoch: [11][310/352]	Time  0.167 ( 0.150)	Data  0.003 ( 0.003)	Loss 9.3349e-01 (9.0148e-01)	Acc@1  74.22 ( 74.28)	Acc@5  91.41 ( 94.42)
03-Mar-22 09:20:59 - Epoch: [13][ 10/352]	Time  0.171 ( 0.180)	Data  0.002 ( 0.025)	Loss 9.2685e-01 (8.7962e-01)	Acc@1  74.22 ( 74.22)	Acc@5  92.19 ( 94.53)
03-Mar-22 09:21:00 - Epoch: [11][320/352]	Time  0.153 ( 0.149)	Data  0.003 ( 0.003)	Loss 8.2313e-01 (9.0135e-01)	Acc@1  77.34 ( 74.24)	Acc@5  95.31 ( 94.43)
03-Mar-22 09:21:01 - Epoch: [13][ 20/352]	Time  0.140 ( 0.171)	Data  0.003 ( 0.014)	Loss 8.0421e-01 (8.8453e-01)	Acc@1  80.47 ( 74.26)	Acc@5  95.31 ( 94.42)
03-Mar-22 09:21:01 - Epoch: [11][330/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0179e+00 (9.0211e-01)	Acc@1  69.53 ( 74.21)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:21:02 - Epoch: [13][ 30/352]	Time  0.150 ( 0.163)	Data  0.002 ( 0.010)	Loss 1.1159e+00 (8.9358e-01)	Acc@1  66.41 ( 74.19)	Acc@5  90.62 ( 94.33)
03-Mar-22 09:21:03 - Epoch: [11][340/352]	Time  0.135 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.1092e-01 (9.0288e-01)	Acc@1  72.66 ( 74.15)	Acc@5  93.75 ( 94.45)
03-Mar-22 09:21:04 - Epoch: [13][ 40/352]	Time  0.172 ( 0.166)	Data  0.003 ( 0.009)	Loss 8.7053e-01 (9.0358e-01)	Acc@1  74.22 ( 73.91)	Acc@5  94.53 ( 94.30)
03-Mar-22 09:21:04 - Epoch: [11][350/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.0808e-01 (9.0325e-01)	Acc@1  75.78 ( 74.15)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:21:05 - Test: [ 0/20]	Time  0.356 ( 0.356)	Loss 9.8638e-01 (9.8638e-01)	Acc@1  73.83 ( 73.83)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:21:05 - Epoch: [13][ 50/352]	Time  0.146 ( 0.163)	Data  0.002 ( 0.007)	Loss 9.0374e-01 (9.1103e-01)	Acc@1  71.88 ( 73.58)	Acc@5  96.09 ( 94.41)
03-Mar-22 09:21:06 - Test: [10/20]	Time  0.096 ( 0.122)	Loss 8.3975e-01 (9.5003e-01)	Acc@1  74.61 ( 73.54)	Acc@5  94.92 ( 93.68)
03-Mar-22 09:21:07 -  * Acc@1 73.500 Acc@5 93.760
03-Mar-22 09:21:07 - Best acc at epoch 11: 74.04000091552734
03-Mar-22 09:21:07 - Epoch: [13][ 60/352]	Time  0.133 ( 0.163)	Data  0.002 ( 0.007)	Loss 1.0531e+00 (9.0727e-01)	Acc@1  72.66 ( 73.78)	Acc@5  89.84 ( 94.42)
03-Mar-22 09:21:07 - Epoch: [12][  0/352]	Time  0.372 ( 0.372)	Data  0.242 ( 0.242)	Loss 9.9779e-01 (9.9779e-01)	Acc@1  68.75 ( 68.75)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:21:09 - Epoch: [13][ 70/352]	Time  0.146 ( 0.162)	Data  0.002 ( 0.006)	Loss 8.3850e-01 (9.0107e-01)	Acc@1  77.34 ( 74.03)	Acc@5  94.53 ( 94.49)
03-Mar-22 09:21:09 - Epoch: [12][ 10/352]	Time  0.148 ( 0.168)	Data  0.002 ( 0.024)	Loss 1.0389e+00 (9.1082e-01)	Acc@1  72.66 ( 73.58)	Acc@5  93.75 ( 94.60)
03-Mar-22 09:21:10 - Epoch: [13][ 80/352]	Time  0.153 ( 0.161)	Data  0.002 ( 0.006)	Loss 9.4656e-01 (8.9669e-01)	Acc@1  73.44 ( 74.27)	Acc@5  92.19 ( 94.49)
03-Mar-22 09:21:10 - Epoch: [12][ 20/352]	Time  0.140 ( 0.159)	Data  0.002 ( 0.014)	Loss 9.8916e-01 (8.9378e-01)	Acc@1  69.53 ( 74.07)	Acc@5  92.97 ( 94.87)
03-Mar-22 09:21:12 - Epoch: [13][ 90/352]	Time  0.137 ( 0.159)	Data  0.002 ( 0.005)	Loss 9.9198e-01 (8.9665e-01)	Acc@1  71.88 ( 74.42)	Acc@5  94.53 ( 94.51)
03-Mar-22 09:21:12 - Epoch: [12][ 30/352]	Time  0.135 ( 0.154)	Data  0.002 ( 0.010)	Loss 1.0084e+00 (8.8308e-01)	Acc@1  70.31 ( 74.34)	Acc@5  93.75 ( 95.06)
03-Mar-22 09:21:13 - Epoch: [13][100/352]	Time  0.135 ( 0.158)	Data  0.002 ( 0.005)	Loss 9.0208e-01 (8.9281e-01)	Acc@1  75.00 ( 74.57)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:21:13 - Epoch: [12][ 40/352]	Time  0.152 ( 0.153)	Data  0.003 ( 0.008)	Loss 8.4344e-01 (8.8916e-01)	Acc@1  75.00 ( 74.45)	Acc@5  94.53 ( 94.87)
03-Mar-22 09:21:15 - Epoch: [13][110/352]	Time  0.152 ( 0.157)	Data  0.002 ( 0.005)	Loss 1.1064e+00 (8.9323e-01)	Acc@1  67.19 ( 74.58)	Acc@5  92.97 ( 94.46)
03-Mar-22 09:21:15 - Epoch: [12][ 50/352]	Time  0.134 ( 0.153)	Data  0.002 ( 0.007)	Loss 7.9779e-01 (8.8657e-01)	Acc@1  78.12 ( 74.71)	Acc@5  94.53 ( 94.87)
03-Mar-22 09:21:16 - Epoch: [13][120/352]	Time  0.152 ( 0.157)	Data  0.002 ( 0.005)	Loss 7.1679e-01 (8.9427e-01)	Acc@1  77.34 ( 74.43)	Acc@5  96.09 ( 94.45)
03-Mar-22 09:21:16 - Epoch: [12][ 60/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.006)	Loss 8.2193e-01 (8.8296e-01)	Acc@1  73.44 ( 74.85)	Acc@5  96.88 ( 94.94)
03-Mar-22 09:21:17 - Epoch: [13][130/352]	Time  0.141 ( 0.156)	Data  0.002 ( 0.004)	Loss 7.7095e-01 (8.9137e-01)	Acc@1  78.12 ( 74.53)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:21:18 - Epoch: [12][ 70/352]	Time  0.131 ( 0.153)	Data  0.002 ( 0.006)	Loss 8.7095e-01 (8.8954e-01)	Acc@1  73.44 ( 74.48)	Acc@5  94.53 ( 94.94)
03-Mar-22 09:21:19 - Epoch: [13][140/352]	Time  0.168 ( 0.156)	Data  0.002 ( 0.004)	Loss 9.2561e-01 (8.8722e-01)	Acc@1  74.22 ( 74.70)	Acc@5  92.97 ( 94.49)
03-Mar-22 09:21:19 - Epoch: [12][ 80/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.3899e-01 (8.8818e-01)	Acc@1  76.56 ( 74.54)	Acc@5  92.97 ( 94.88)
03-Mar-22 09:21:21 - Epoch: [12][ 90/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.6437e-01 (8.8750e-01)	Acc@1  76.56 ( 74.67)	Acc@5  92.97 ( 94.82)
03-Mar-22 09:21:21 - Epoch: [13][150/352]	Time  0.176 ( 0.157)	Data  0.002 ( 0.004)	Loss 7.5933e-01 (8.8582e-01)	Acc@1  74.22 ( 74.71)	Acc@5  94.53 ( 94.52)
03-Mar-22 09:21:22 - Epoch: [12][100/352]	Time  0.159 ( 0.153)	Data  0.002 ( 0.005)	Loss 7.6420e-01 (8.8619e-01)	Acc@1  77.34 ( 74.63)	Acc@5  94.53 ( 94.80)
03-Mar-22 09:21:22 - Epoch: [13][160/352]	Time  0.169 ( 0.158)	Data  0.002 ( 0.004)	Loss 1.0106e+00 (8.8588e-01)	Acc@1  71.88 ( 74.71)	Acc@5  94.53 ( 94.54)
03-Mar-22 09:21:24 - Epoch: [12][110/352]	Time  0.149 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.1194e-01 (8.8790e-01)	Acc@1  73.44 ( 74.47)	Acc@5  96.88 ( 94.76)
03-Mar-22 09:21:24 - Epoch: [13][170/352]	Time  0.159 ( 0.158)	Data  0.002 ( 0.004)	Loss 9.1726e-01 (8.8582e-01)	Acc@1  76.56 ( 74.78)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:21:25 - Epoch: [12][120/352]	Time  0.155 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0846e+00 (8.9274e-01)	Acc@1  67.19 ( 74.30)	Acc@5  91.41 ( 94.67)
03-Mar-22 09:21:26 - Epoch: [13][180/352]	Time  0.153 ( 0.158)	Data  0.002 ( 0.004)	Loss 8.4127e-01 (8.8466e-01)	Acc@1  77.34 ( 74.72)	Acc@5  92.97 ( 94.55)
03-Mar-22 09:21:27 - Epoch: [12][130/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.3903e-01 (8.9465e-01)	Acc@1  75.00 ( 74.21)	Acc@5  96.88 ( 94.64)
03-Mar-22 09:21:27 - Epoch: [13][190/352]	Time  0.153 ( 0.158)	Data  0.002 ( 0.004)	Loss 9.8384e-01 (8.8640e-01)	Acc@1  71.09 ( 74.69)	Acc@5  94.53 ( 94.52)
03-Mar-22 09:21:28 - Epoch: [12][140/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.2674e-01 (8.9933e-01)	Acc@1  71.88 ( 74.07)	Acc@5  94.53 ( 94.57)
03-Mar-22 09:21:29 - Epoch: [13][200/352]	Time  0.136 ( 0.158)	Data  0.002 ( 0.004)	Loss 8.9220e-01 (8.8636e-01)	Acc@1  75.00 ( 74.68)	Acc@5  92.97 ( 94.49)
03-Mar-22 09:21:30 - Epoch: [12][150/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.5312e-01 (8.9911e-01)	Acc@1  75.00 ( 74.12)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:21:30 - Epoch: [13][210/352]	Time  0.133 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.2740e-01 (8.8820e-01)	Acc@1  75.00 ( 74.57)	Acc@5  95.31 ( 94.46)
03-Mar-22 09:21:31 - Epoch: [12][160/352]	Time  0.152 ( 0.152)	Data  0.003 ( 0.004)	Loss 9.2173e-01 (8.9634e-01)	Acc@1  75.00 ( 74.23)	Acc@5  90.62 ( 94.57)
03-Mar-22 09:21:32 - Epoch: [13][220/352]	Time  0.150 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.3227e-01 (8.8487e-01)	Acc@1  75.00 ( 74.71)	Acc@5  94.53 ( 94.48)
03-Mar-22 09:21:33 - Epoch: [12][170/352]	Time  0.128 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.5239e-01 (8.9490e-01)	Acc@1  77.34 ( 74.21)	Acc@5  99.22 ( 94.60)
03-Mar-22 09:21:33 - Epoch: [13][230/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.1387e+00 (8.8475e-01)	Acc@1  69.53 ( 74.75)	Acc@5  88.28 ( 94.49)
03-Mar-22 09:21:34 - Epoch: [12][180/352]	Time  0.140 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.9324e-01 (8.9517e-01)	Acc@1  71.09 ( 74.18)	Acc@5  92.97 ( 94.57)
03-Mar-22 09:21:35 - Epoch: [13][240/352]	Time  0.150 ( 0.156)	Data  0.002 ( 0.003)	Loss 7.4896e-01 (8.8175e-01)	Acc@1  79.69 ( 74.86)	Acc@5  96.09 ( 94.55)
03-Mar-22 09:21:36 - Epoch: [12][190/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.004)	Loss 9.1656e-01 (8.9451e-01)	Acc@1  75.00 ( 74.26)	Acc@5  96.09 ( 94.55)
03-Mar-22 09:21:36 - Epoch: [13][250/352]	Time  0.127 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.6723e-01 (8.8167e-01)	Acc@1  70.31 ( 74.83)	Acc@5  92.97 ( 94.57)
03-Mar-22 09:21:37 - Epoch: [12][200/352]	Time  0.155 ( 0.152)	Data  0.003 ( 0.004)	Loss 7.7349e-01 (8.9473e-01)	Acc@1  79.69 ( 74.23)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:21:38 - Epoch: [13][260/352]	Time  0.149 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.4602e-01 (8.8219e-01)	Acc@1  75.00 ( 74.81)	Acc@5  95.31 ( 94.55)
03-Mar-22 09:21:39 - Epoch: [12][210/352]	Time  0.145 ( 0.152)	Data  0.001 ( 0.003)	Loss 9.4956e-01 (8.9443e-01)	Acc@1  71.88 ( 74.22)	Acc@5  94.53 ( 94.54)
03-Mar-22 09:21:39 - Epoch: [13][270/352]	Time  0.132 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.9561e-01 (8.8507e-01)	Acc@1  75.78 ( 74.78)	Acc@5  92.19 ( 94.52)
03-Mar-22 09:21:40 - Epoch: [12][220/352]	Time  0.157 ( 0.152)	Data  0.003 ( 0.003)	Loss 9.2961e-01 (8.9725e-01)	Acc@1  74.22 ( 74.07)	Acc@5  95.31 ( 94.55)
03-Mar-22 09:21:41 - Epoch: [13][280/352]	Time  0.150 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.2585e-01 (8.8431e-01)	Acc@1  77.34 ( 74.81)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:21:42 - Epoch: [12][230/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.0800e-01 (9.0080e-01)	Acc@1  75.78 ( 74.01)	Acc@5  94.53 ( 94.51)
03-Mar-22 09:21:42 - Epoch: [13][290/352]	Time  0.170 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.5371e-01 (8.8529e-01)	Acc@1  73.44 ( 74.83)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:21:43 - Epoch: [12][240/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0577e+00 (9.0112e-01)	Acc@1  69.53 ( 73.98)	Acc@5  93.75 ( 94.51)
03-Mar-22 09:21:44 - Epoch: [13][300/352]	Time  0.154 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.1774e-01 (8.8629e-01)	Acc@1  75.00 ( 74.83)	Acc@5  93.75 ( 94.49)
03-Mar-22 09:21:45 - Epoch: [12][250/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.0672e-01 (9.0205e-01)	Acc@1  71.88 ( 73.96)	Acc@5  92.97 ( 94.49)
03-Mar-22 09:21:46 - Epoch: [13][310/352]	Time  0.176 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.5817e-01 (8.8582e-01)	Acc@1  72.66 ( 74.82)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:21:46 - Epoch: [12][260/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.5658e-01 (9.0225e-01)	Acc@1  72.66 ( 73.97)	Acc@5  92.97 ( 94.50)
03-Mar-22 09:21:47 - Epoch: [13][320/352]	Time  0.177 ( 0.157)	Data  0.002 ( 0.003)	Loss 7.5285e-01 (8.8571e-01)	Acc@1  82.03 ( 74.82)	Acc@5  98.44 ( 94.52)
03-Mar-22 09:21:48 - Epoch: [12][270/352]	Time  0.138 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.6555e-01 (9.0140e-01)	Acc@1  74.22 ( 74.00)	Acc@5  96.88 ( 94.52)
03-Mar-22 09:21:49 - Epoch: [13][330/352]	Time  0.156 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.8413e-01 (8.8529e-01)	Acc@1  75.00 ( 74.84)	Acc@5  93.75 ( 94.52)
03-Mar-22 09:21:49 - Epoch: [12][280/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.7026e-01 (9.0148e-01)	Acc@1  74.22 ( 74.03)	Acc@5  95.31 ( 94.51)
03-Mar-22 09:21:51 - Epoch: [12][290/352]	Time  0.146 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.8568e-01 (8.9952e-01)	Acc@1  77.34 ( 74.09)	Acc@5  98.44 ( 94.54)
03-Mar-22 09:21:51 - Epoch: [13][340/352]	Time  0.177 ( 0.158)	Data  0.002 ( 0.003)	Loss 7.9734e-01 (8.8479e-01)	Acc@1  78.12 ( 74.88)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:21:52 - Epoch: [12][300/352]	Time  0.157 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.4345e-01 (8.9902e-01)	Acc@1  78.12 ( 74.17)	Acc@5  93.75 ( 94.52)
03-Mar-22 09:21:53 - Epoch: [13][350/352]	Time  0.172 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.0684e-01 (8.8507e-01)	Acc@1  71.09 ( 74.86)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:21:53 - Test: [ 0/20]	Time  0.397 ( 0.397)	Loss 1.0813e+00 (1.0813e+00)	Acc@1  69.14 ( 69.14)	Acc@5  91.02 ( 91.02)
03-Mar-22 09:21:54 - Epoch: [12][310/352]	Time  0.143 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3415e-01 (8.9956e-01)	Acc@1  77.34 ( 74.17)	Acc@5  96.88 ( 94.52)
03-Mar-22 09:21:54 - Test: [10/20]	Time  0.106 ( 0.141)	Loss 7.5184e-01 (9.4661e-01)	Acc@1  78.52 ( 73.19)	Acc@5  95.31 ( 93.57)
03-Mar-22 09:21:55 - Epoch: [12][320/352]	Time  0.140 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.8453e-01 (8.9993e-01)	Acc@1  75.00 ( 74.18)	Acc@5  92.19 ( 94.52)
03-Mar-22 09:21:55 -  * Acc@1 72.980 Acc@5 93.420
03-Mar-22 09:21:55 - Best acc at epoch 13: 73.97999572753906
03-Mar-22 09:21:56 - Epoch: [14][  0/352]	Time  0.376 ( 0.376)	Data  0.220 ( 0.220)	Loss 6.9882e-01 (6.9882e-01)	Acc@1  78.91 ( 78.91)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:21:57 - Epoch: [12][330/352]	Time  0.130 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.5758e-01 (8.9975e-01)	Acc@1  75.78 ( 74.23)	Acc@5  94.53 ( 94.49)
03-Mar-22 09:21:58 - Epoch: [14][ 10/352]	Time  0.174 ( 0.190)	Data  0.003 ( 0.023)	Loss 7.8175e-01 (8.2127e-01)	Acc@1  76.56 ( 75.50)	Acc@5  96.88 ( 95.38)
03-Mar-22 09:21:58 - Epoch: [12][340/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.3639e-01 (9.0024e-01)	Acc@1  79.69 ( 74.21)	Acc@5  95.31 ( 94.48)
03-Mar-22 09:21:59 - Epoch: [14][ 20/352]	Time  0.158 ( 0.177)	Data  0.002 ( 0.013)	Loss 9.8266e-01 (8.8328e-01)	Acc@1  72.66 ( 74.63)	Acc@5  92.97 ( 94.27)
03-Mar-22 09:21:59 - Epoch: [12][350/352]	Time  0.145 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.8969e-01 (8.9945e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.88 ( 94.49)
03-Mar-22 09:22:00 - Test: [ 0/20]	Time  0.352 ( 0.352)	Loss 9.9855e-01 (9.9855e-01)	Acc@1  68.36 ( 68.36)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:22:01 - Epoch: [14][ 30/352]	Time  0.159 ( 0.164)	Data  0.002 ( 0.009)	Loss 9.2460e-01 (8.8363e-01)	Acc@1  71.88 ( 75.13)	Acc@5  96.88 ( 94.46)
03-Mar-22 09:22:01 - Test: [10/20]	Time  0.096 ( 0.124)	Loss 8.7156e-01 (9.3880e-01)	Acc@1  73.05 ( 73.08)	Acc@5  95.31 ( 93.82)
03-Mar-22 09:22:02 -  * Acc@1 73.340 Acc@5 93.620
03-Mar-22 09:22:02 - Best acc at epoch 12: 74.04000091552734
03-Mar-22 09:22:02 - Epoch: [14][ 40/352]	Time  0.114 ( 0.165)	Data  0.002 ( 0.008)	Loss 8.2627e-01 (8.8409e-01)	Acc@1  77.34 ( 75.25)	Acc@5  95.31 ( 94.42)
03-Mar-22 09:22:02 - Epoch: [13][  0/352]	Time  0.359 ( 0.359)	Data  0.218 ( 0.218)	Loss 7.9315e-01 (7.9315e-01)	Acc@1  75.78 ( 75.78)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:22:04 - Epoch: [14][ 50/352]	Time  0.136 ( 0.161)	Data  0.002 ( 0.007)	Loss 8.5970e-01 (8.8600e-01)	Acc@1  76.56 ( 75.26)	Acc@5  96.09 ( 94.44)
03-Mar-22 09:22:04 - Epoch: [13][ 10/352]	Time  0.145 ( 0.168)	Data  0.003 ( 0.022)	Loss 9.8455e-01 (9.3915e-01)	Acc@1  70.31 ( 73.30)	Acc@5  96.88 ( 94.18)
03-Mar-22 09:22:05 - Epoch: [14][ 60/352]	Time  0.149 ( 0.160)	Data  0.003 ( 0.006)	Loss 7.7619e-01 (8.7897e-01)	Acc@1  77.34 ( 75.46)	Acc@5  96.09 ( 94.48)
03-Mar-22 09:22:05 - Epoch: [13][ 20/352]	Time  0.171 ( 0.160)	Data  0.003 ( 0.013)	Loss 1.0422e+00 (9.2541e-01)	Acc@1  71.09 ( 73.77)	Acc@5  92.97 ( 94.53)
03-Mar-22 09:22:07 - Epoch: [14][ 70/352]	Time  0.148 ( 0.159)	Data  0.002 ( 0.005)	Loss 1.0384e+00 (8.7122e-01)	Acc@1  73.44 ( 75.66)	Acc@5  91.41 ( 94.48)
03-Mar-22 09:22:07 - Epoch: [13][ 30/352]	Time  0.170 ( 0.159)	Data  0.003 ( 0.009)	Loss 1.0283e+00 (9.2117e-01)	Acc@1  75.78 ( 74.04)	Acc@5  89.84 ( 94.51)
03-Mar-22 09:22:08 - Epoch: [14][ 80/352]	Time  0.149 ( 0.157)	Data  0.002 ( 0.005)	Loss 8.0824e-01 (8.6874e-01)	Acc@1  75.00 ( 75.66)	Acc@5  99.22 ( 94.52)
03-Mar-22 09:22:09 - Epoch: [13][ 40/352]	Time  0.159 ( 0.158)	Data  0.003 ( 0.008)	Loss 8.5819e-01 (9.0464e-01)	Acc@1  76.56 ( 74.75)	Acc@5  94.53 ( 94.57)
03-Mar-22 09:22:10 - Epoch: [14][ 90/352]	Time  0.172 ( 0.157)	Data  0.002 ( 0.005)	Loss 8.6144e-01 (8.6876e-01)	Acc@1  76.56 ( 75.59)	Acc@5  92.19 ( 94.60)
03-Mar-22 09:22:10 - Epoch: [13][ 50/352]	Time  0.149 ( 0.157)	Data  0.002 ( 0.007)	Loss 7.6894e-01 (9.0090e-01)	Acc@1  78.91 ( 74.77)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:22:11 - Epoch: [14][100/352]	Time  0.177 ( 0.158)	Data  0.003 ( 0.004)	Loss 9.6100e-01 (8.6581e-01)	Acc@1  73.44 ( 75.63)	Acc@5  96.09 ( 94.66)
03-Mar-22 09:22:12 - Epoch: [13][ 60/352]	Time  0.151 ( 0.158)	Data  0.003 ( 0.006)	Loss 7.4166e-01 (8.9090e-01)	Acc@1  76.56 ( 74.85)	Acc@5  96.09 ( 94.67)
03-Mar-22 09:22:13 - Epoch: [14][110/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.004)	Loss 8.6580e-01 (8.6828e-01)	Acc@1  74.22 ( 75.57)	Acc@5  92.19 ( 94.67)
03-Mar-22 09:22:13 - Epoch: [13][ 70/352]	Time  0.136 ( 0.158)	Data  0.002 ( 0.005)	Loss 8.8211e-01 (8.9389e-01)	Acc@1  74.22 ( 74.68)	Acc@5  94.53 ( 94.72)
03-Mar-22 09:22:15 - Epoch: [14][120/352]	Time  0.151 ( 0.158)	Data  0.002 ( 0.004)	Loss 9.8918e-01 (8.7188e-01)	Acc@1  71.88 ( 75.38)	Acc@5  91.41 ( 94.64)
03-Mar-22 09:22:15 - Epoch: [13][ 80/352]	Time  0.151 ( 0.158)	Data  0.002 ( 0.005)	Loss 7.3672e-01 (8.8752e-01)	Acc@1  78.12 ( 74.74)	Acc@5  97.66 ( 94.75)
03-Mar-22 09:22:16 - Epoch: [14][130/352]	Time  0.151 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.2115e-01 (8.6659e-01)	Acc@1  75.78 ( 75.51)	Acc@5  92.97 ( 94.71)
03-Mar-22 09:22:16 - Epoch: [13][ 90/352]	Time  0.156 ( 0.158)	Data  0.002 ( 0.005)	Loss 9.0052e-01 (8.8382e-01)	Acc@1  71.88 ( 74.88)	Acc@5  93.75 ( 94.75)
03-Mar-22 09:22:17 - Epoch: [14][140/352]	Time  0.130 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.0340e+00 (8.7116e-01)	Acc@1  70.31 ( 75.39)	Acc@5  88.28 ( 94.61)
03-Mar-22 09:22:18 - Epoch: [13][100/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.005)	Loss 9.8472e-01 (8.8400e-01)	Acc@1  76.56 ( 74.80)	Acc@5  92.19 ( 94.76)
03-Mar-22 09:22:19 - Epoch: [14][150/352]	Time  0.149 ( 0.156)	Data  0.002 ( 0.004)	Loss 8.8211e-01 (8.7132e-01)	Acc@1  74.22 ( 75.35)	Acc@5  92.97 ( 94.61)
03-Mar-22 09:22:20 - Epoch: [13][110/352]	Time  0.144 ( 0.158)	Data  0.002 ( 0.004)	Loss 6.5002e-01 (8.8041e-01)	Acc@1  80.47 ( 74.78)	Acc@5  98.44 ( 94.86)
03-Mar-22 09:22:21 - Epoch: [14][160/352]	Time  0.156 ( 0.156)	Data  0.002 ( 0.004)	Loss 8.3958e-01 (8.6806e-01)	Acc@1  79.69 ( 75.42)	Acc@5  96.88 ( 94.68)
03-Mar-22 09:22:21 - Epoch: [13][120/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.004)	Loss 9.3972e-01 (8.8561e-01)	Acc@1  71.09 ( 74.65)	Acc@5  96.88 ( 94.79)
03-Mar-22 09:22:22 - Epoch: [14][170/352]	Time  0.176 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.6633e-01 (8.7206e-01)	Acc@1  77.34 ( 75.34)	Acc@5  94.53 ( 94.60)
03-Mar-22 09:22:23 - Epoch: [13][130/352]	Time  0.131 ( 0.157)	Data  0.002 ( 0.004)	Loss 9.5521e-01 (8.8322e-01)	Acc@1  75.00 ( 74.73)	Acc@5  96.09 ( 94.79)
03-Mar-22 09:22:24 - Epoch: [14][180/352]	Time  0.144 ( 0.157)	Data  0.002 ( 0.004)	Loss 1.0784e+00 (8.7403e-01)	Acc@1  66.41 ( 75.11)	Acc@5  94.53 ( 94.64)
03-Mar-22 09:22:24 - Epoch: [13][140/352]	Time  0.158 ( 0.156)	Data  0.003 ( 0.004)	Loss 8.9359e-01 (8.8476e-01)	Acc@1  77.34 ( 74.66)	Acc@5  92.97 ( 94.71)
03-Mar-22 09:22:26 - Epoch: [14][190/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.0500e+00 (8.7511e-01)	Acc@1  70.31 ( 75.09)	Acc@5  93.75 ( 94.63)
03-Mar-22 09:22:26 - Epoch: [13][150/352]	Time  0.147 ( 0.156)	Data  0.003 ( 0.004)	Loss 8.7307e-01 (8.8728e-01)	Acc@1  74.22 ( 74.63)	Acc@5  92.19 ( 94.69)
03-Mar-22 09:22:27 - Epoch: [13][160/352]	Time  0.147 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.0498e+00 (8.8705e-01)	Acc@1  68.75 ( 74.71)	Acc@5  92.19 ( 94.69)
03-Mar-22 09:22:27 - Epoch: [14][200/352]	Time  0.171 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.9339e-01 (8.7825e-01)	Acc@1  72.66 ( 75.04)	Acc@5  90.62 ( 94.54)
03-Mar-22 09:22:29 - Epoch: [13][170/352]	Time  0.153 ( 0.156)	Data  0.003 ( 0.004)	Loss 8.4923e-01 (8.8807e-01)	Acc@1  74.22 ( 74.67)	Acc@5  97.66 ( 94.68)
03-Mar-22 09:22:29 - Epoch: [14][210/352]	Time  0.156 ( 0.158)	Data  0.003 ( 0.003)	Loss 9.9491e-01 (8.7881e-01)	Acc@1  69.53 ( 74.97)	Acc@5  92.97 ( 94.57)
03-Mar-22 09:22:30 - Epoch: [13][180/352]	Time  0.171 ( 0.156)	Data  0.003 ( 0.004)	Loss 9.7907e-01 (8.9264e-01)	Acc@1  70.31 ( 74.56)	Acc@5  96.09 ( 94.64)
03-Mar-22 09:22:30 - Epoch: [14][220/352]	Time  0.136 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.3014e-01 (8.7970e-01)	Acc@1  76.56 ( 74.93)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:22:32 - Epoch: [13][190/352]	Time  0.152 ( 0.156)	Data  0.002 ( 0.004)	Loss 7.0321e-01 (8.9167e-01)	Acc@1  80.47 ( 74.54)	Acc@5  97.66 ( 94.68)
03-Mar-22 09:22:32 - Epoch: [14][230/352]	Time  0.181 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.7938e-01 (8.7999e-01)	Acc@1  75.78 ( 74.87)	Acc@5  90.62 ( 94.55)
03-Mar-22 09:22:33 - Epoch: [13][200/352]	Time  0.158 ( 0.155)	Data  0.002 ( 0.003)	Loss 9.0730e-01 (8.9288e-01)	Acc@1  72.66 ( 74.49)	Acc@5  92.19 ( 94.63)
03-Mar-22 09:22:34 - Epoch: [14][240/352]	Time  0.152 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.7041e-01 (8.7928e-01)	Acc@1  76.56 ( 74.88)	Acc@5  96.09 ( 94.55)
03-Mar-22 09:22:35 - Epoch: [13][210/352]	Time  0.154 ( 0.156)	Data  0.003 ( 0.003)	Loss 9.2973e-01 (8.9407e-01)	Acc@1  71.88 ( 74.50)	Acc@5  92.97 ( 94.58)
03-Mar-22 09:22:35 - Epoch: [14][250/352]	Time  0.154 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.6627e-01 (8.7789e-01)	Acc@1  75.78 ( 74.94)	Acc@5  94.53 ( 94.57)
03-Mar-22 09:22:36 - Epoch: [13][220/352]	Time  0.170 ( 0.155)	Data  0.003 ( 0.003)	Loss 9.7946e-01 (8.9483e-01)	Acc@1  71.09 ( 74.47)	Acc@5  92.97 ( 94.55)
03-Mar-22 09:22:37 - Epoch: [14][260/352]	Time  0.174 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.8867e-01 (8.7782e-01)	Acc@1  75.78 ( 74.89)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:22:38 - Epoch: [13][230/352]	Time  0.152 ( 0.155)	Data  0.003 ( 0.003)	Loss 9.2287e-01 (8.9529e-01)	Acc@1  71.09 ( 74.37)	Acc@5  93.75 ( 94.51)
03-Mar-22 09:22:38 - Epoch: [14][270/352]	Time  0.156 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.0270e+00 (8.8202e-01)	Acc@1  71.09 ( 74.75)	Acc@5  94.53 ( 94.50)
03-Mar-22 09:22:39 - Epoch: [13][240/352]	Time  0.153 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.9204e-01 (8.9385e-01)	Acc@1  73.44 ( 74.38)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:22:40 - Epoch: [14][280/352]	Time  0.153 ( 0.159)	Data  0.002 ( 0.003)	Loss 7.9068e-01 (8.8201e-01)	Acc@1  80.47 ( 74.74)	Acc@5  95.31 ( 94.50)
03-Mar-22 09:22:41 - Epoch: [13][250/352]	Time  0.158 ( 0.155)	Data  0.002 ( 0.003)	Loss 9.8394e-01 (8.9138e-01)	Acc@1  71.09 ( 74.46)	Acc@5  90.62 ( 94.60)
03-Mar-22 09:22:42 - Epoch: [14][290/352]	Time  0.167 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.0352e+00 (8.8158e-01)	Acc@1  68.75 ( 74.74)	Acc@5  92.97 ( 94.48)
03-Mar-22 09:22:42 - Epoch: [13][260/352]	Time  0.152 ( 0.155)	Data  0.003 ( 0.003)	Loss 9.5089e-01 (8.9086e-01)	Acc@1  67.19 ( 74.47)	Acc@5  93.75 ( 94.58)
03-Mar-22 09:22:43 - Epoch: [14][300/352]	Time  0.151 ( 0.159)	Data  0.002 ( 0.003)	Loss 7.5163e-01 (8.8198e-01)	Acc@1  82.03 ( 74.74)	Acc@5  97.66 ( 94.50)
03-Mar-22 09:22:44 - Epoch: [13][270/352]	Time  0.150 ( 0.154)	Data  0.002 ( 0.003)	Loss 7.8684e-01 (8.9122e-01)	Acc@1  82.03 ( 74.46)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:22:45 - Epoch: [14][310/352]	Time  0.147 ( 0.159)	Data  0.002 ( 0.003)	Loss 9.6638e-01 (8.8343e-01)	Acc@1  71.88 ( 74.69)	Acc@5  94.53 ( 94.49)
03-Mar-22 09:22:45 - Epoch: [13][280/352]	Time  0.149 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.5371e-01 (8.9119e-01)	Acc@1  75.78 ( 74.52)	Acc@5  89.84 ( 94.54)
03-Mar-22 09:22:46 - Epoch: [14][320/352]	Time  0.148 ( 0.158)	Data  0.002 ( 0.003)	Loss 7.6975e-01 (8.8262e-01)	Acc@1  79.69 ( 74.72)	Acc@5  96.09 ( 94.52)
03-Mar-22 09:22:47 - Epoch: [13][290/352]	Time  0.151 ( 0.154)	Data  0.003 ( 0.003)	Loss 8.7343e-01 (8.9014e-01)	Acc@1  74.22 ( 74.52)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:22:48 - Epoch: [14][330/352]	Time  0.147 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.3084e-01 (8.8368e-01)	Acc@1  75.00 ( 74.67)	Acc@5  92.19 ( 94.51)
03-Mar-22 09:22:48 - Epoch: [13][300/352]	Time  0.156 ( 0.154)	Data  0.003 ( 0.003)	Loss 8.7021e-01 (8.9073e-01)	Acc@1  73.44 ( 74.46)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:22:49 - Epoch: [14][340/352]	Time  0.154 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.0820e-01 (8.8326e-01)	Acc@1  76.56 ( 74.70)	Acc@5  96.09 ( 94.51)
03-Mar-22 09:22:50 - Epoch: [13][310/352]	Time  0.138 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.5174e-01 (8.9080e-01)	Acc@1  74.22 ( 74.49)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:22:51 - Epoch: [14][350/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.3704e-01 (8.8342e-01)	Acc@1  71.09 ( 74.68)	Acc@5  91.41 ( 94.50)
03-Mar-22 09:22:51 - Epoch: [13][320/352]	Time  0.101 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.3711e-01 (8.9282e-01)	Acc@1  74.22 ( 74.48)	Acc@5  96.09 ( 94.53)
03-Mar-22 09:22:51 - Test: [ 0/20]	Time  0.391 ( 0.391)	Loss 1.0349e+00 (1.0349e+00)	Acc@1  70.31 ( 70.31)	Acc@5  91.80 ( 91.80)
03-Mar-22 09:22:52 - Test: [10/20]	Time  0.098 ( 0.129)	Loss 7.8009e-01 (9.3339e-01)	Acc@1  76.95 ( 73.47)	Acc@5  96.48 ( 93.61)
03-Mar-22 09:22:53 - Epoch: [13][330/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.7227e-01 (8.9202e-01)	Acc@1  73.44 ( 74.47)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:22:53 -  * Acc@1 73.500 Acc@5 93.660
03-Mar-22 09:22:53 - Best acc at epoch 14: 73.97999572753906
03-Mar-22 09:22:54 - Epoch: [15][  0/352]	Time  0.370 ( 0.370)	Data  0.248 ( 0.248)	Loss 9.5236e-01 (9.5236e-01)	Acc@1  69.53 ( 69.53)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:22:54 - Epoch: [13][340/352]	Time  0.146 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.5438e-01 (8.9275e-01)	Acc@1  76.56 ( 74.47)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:22:55 - Epoch: [15][ 10/352]	Time  0.179 ( 0.177)	Data  0.002 ( 0.025)	Loss 8.9228e-01 (8.6453e-01)	Acc@1  71.88 ( 74.01)	Acc@5  90.62 ( 94.60)
03-Mar-22 09:22:56 - Epoch: [13][350/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.1444e+00 (8.9393e-01)	Acc@1  64.06 ( 74.42)	Acc@5  92.19 ( 94.50)
03-Mar-22 09:22:56 - Test: [ 0/20]	Time  0.342 ( 0.342)	Loss 1.0447e+00 (1.0447e+00)	Acc@1  71.09 ( 71.09)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:22:57 - Epoch: [15][ 20/352]	Time  0.156 ( 0.164)	Data  0.002 ( 0.014)	Loss 9.2573e-01 (8.7684e-01)	Acc@1  71.88 ( 74.48)	Acc@5  92.97 ( 94.75)
03-Mar-22 09:22:57 - Test: [10/20]	Time  0.091 ( 0.122)	Loss 8.2450e-01 (9.5757e-01)	Acc@1  76.17 ( 72.69)	Acc@5  95.31 ( 93.79)
03-Mar-22 09:22:58 -  * Acc@1 72.320 Acc@5 93.600
03-Mar-22 09:22:58 - Best acc at epoch 13: 74.04000091552734
03-Mar-22 09:22:58 - Epoch: [15][ 30/352]	Time  0.104 ( 0.158)	Data  0.002 ( 0.010)	Loss 8.7249e-01 (8.6605e-01)	Acc@1  72.66 ( 75.05)	Acc@5  94.53 ( 94.73)
03-Mar-22 09:22:58 - Epoch: [14][  0/352]	Time  0.362 ( 0.362)	Data  0.217 ( 0.217)	Loss 9.7796e-01 (9.7796e-01)	Acc@1  71.09 ( 71.09)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:23:00 - Epoch: [15][ 40/352]	Time  0.147 ( 0.154)	Data  0.003 ( 0.008)	Loss 8.4790e-01 (8.6487e-01)	Acc@1  71.88 ( 74.68)	Acc@5  93.75 ( 94.93)
03-Mar-22 09:23:00 - Epoch: [14][ 10/352]	Time  0.147 ( 0.168)	Data  0.002 ( 0.022)	Loss 9.0449e-01 (8.4306e-01)	Acc@1  74.22 ( 76.28)	Acc@5  93.75 ( 94.89)
03-Mar-22 09:23:01 - Epoch: [15][ 50/352]	Time  0.154 ( 0.154)	Data  0.003 ( 0.007)	Loss 9.1831e-01 (8.6682e-01)	Acc@1  76.56 ( 74.97)	Acc@5  94.53 ( 94.85)
03-Mar-22 09:23:01 - Epoch: [14][ 20/352]	Time  0.151 ( 0.158)	Data  0.002 ( 0.012)	Loss 9.7377e-01 (8.6214e-01)	Acc@1  65.62 ( 75.00)	Acc@5  95.31 ( 94.64)
03-Mar-22 09:23:03 - Epoch: [14][ 30/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.009)	Loss 9.7395e-01 (8.6759e-01)	Acc@1  72.66 ( 74.77)	Acc@5  92.97 ( 94.76)
03-Mar-22 09:23:03 - Epoch: [15][ 60/352]	Time  0.174 ( 0.155)	Data  0.002 ( 0.006)	Loss 9.7077e-01 (8.7830e-01)	Acc@1  71.09 ( 74.68)	Acc@5  94.53 ( 94.71)
03-Mar-22 09:23:04 - Epoch: [14][ 40/352]	Time  0.152 ( 0.154)	Data  0.002 ( 0.007)	Loss 9.7424e-01 (8.7886e-01)	Acc@1  71.09 ( 74.62)	Acc@5  95.31 ( 94.80)
03-Mar-22 09:23:05 - Epoch: [15][ 70/352]	Time  0.176 ( 0.157)	Data  0.003 ( 0.006)	Loss 8.6683e-01 (8.7890e-01)	Acc@1  75.00 ( 74.65)	Acc@5  94.53 ( 94.70)
03-Mar-22 09:23:06 - Epoch: [14][ 50/352]	Time  0.155 ( 0.153)	Data  0.002 ( 0.006)	Loss 8.9575e-01 (8.7611e-01)	Acc@1  71.88 ( 74.77)	Acc@5  95.31 ( 94.78)
03-Mar-22 09:23:06 - Epoch: [15][ 80/352]	Time  0.158 ( 0.158)	Data  0.002 ( 0.005)	Loss 8.0868e-01 (8.7170e-01)	Acc@1  80.47 ( 74.88)	Acc@5  92.97 ( 94.78)
03-Mar-22 09:23:07 - Epoch: [14][ 60/352]	Time  0.152 ( 0.153)	Data  0.002 ( 0.006)	Loss 1.0939e+00 (8.8148e-01)	Acc@1  68.75 ( 74.77)	Acc@5  91.41 ( 94.67)
03-Mar-22 09:23:08 - Epoch: [15][ 90/352]	Time  0.184 ( 0.158)	Data  0.003 ( 0.005)	Loss 9.7391e-01 (8.7496e-01)	Acc@1  73.44 ( 74.79)	Acc@5  92.19 ( 94.71)
03-Mar-22 09:23:09 - Epoch: [14][ 70/352]	Time  0.147 ( 0.152)	Data  0.002 ( 0.005)	Loss 8.6365e-01 (8.8539e-01)	Acc@1  73.44 ( 74.68)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:23:10 - Epoch: [15][100/352]	Time  0.171 ( 0.160)	Data  0.002 ( 0.005)	Loss 7.0274e-01 (8.7191e-01)	Acc@1  80.47 ( 74.91)	Acc@5  95.31 ( 94.72)
03-Mar-22 09:23:10 - Epoch: [14][ 80/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.005)	Loss 8.1793e-01 (8.8442e-01)	Acc@1  74.22 ( 74.68)	Acc@5  93.75 ( 94.58)
03-Mar-22 09:23:11 - Epoch: [15][110/352]	Time  0.174 ( 0.160)	Data  0.002 ( 0.005)	Loss 8.5418e-01 (8.7887e-01)	Acc@1  71.88 ( 74.72)	Acc@5  94.53 ( 94.71)
03-Mar-22 09:23:12 - Epoch: [14][ 90/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.005)	Loss 8.1648e-01 (8.8125e-01)	Acc@1  75.78 ( 74.78)	Acc@5  93.75 ( 94.63)
03-Mar-22 09:23:13 - Epoch: [15][120/352]	Time  0.174 ( 0.161)	Data  0.003 ( 0.004)	Loss 9.0751e-01 (8.8329e-01)	Acc@1  73.44 ( 74.49)	Acc@5  94.53 ( 94.67)
03-Mar-22 09:23:13 - Epoch: [14][100/352]	Time  0.152 ( 0.150)	Data  0.002 ( 0.004)	Loss 7.7359e-01 (8.7997e-01)	Acc@1  76.56 ( 74.74)	Acc@5  96.88 ( 94.66)
03-Mar-22 09:23:15 - Epoch: [15][130/352]	Time  0.151 ( 0.161)	Data  0.002 ( 0.004)	Loss 9.3772e-01 (8.8076e-01)	Acc@1  71.88 ( 74.57)	Acc@5  92.97 ( 94.67)
03-Mar-22 09:23:15 - Epoch: [14][110/352]	Time  0.129 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.6822e-01 (8.8038e-01)	Acc@1  75.00 ( 74.73)	Acc@5  94.53 ( 94.66)
03-Mar-22 09:23:16 - Epoch: [14][120/352]	Time  0.152 ( 0.150)	Data  0.002 ( 0.004)	Loss 7.7510e-01 (8.8107e-01)	Acc@1  75.78 ( 74.61)	Acc@5  96.88 ( 94.71)
03-Mar-22 09:23:16 - Epoch: [15][140/352]	Time  0.172 ( 0.162)	Data  0.002 ( 0.004)	Loss 8.8046e-01 (8.7980e-01)	Acc@1  78.91 ( 74.61)	Acc@5  92.19 ( 94.68)
03-Mar-22 09:23:18 - Epoch: [14][130/352]	Time  0.146 ( 0.150)	Data  0.002 ( 0.004)	Loss 6.9607e-01 (8.8431e-01)	Acc@1  79.69 ( 74.51)	Acc@5  96.88 ( 94.66)
03-Mar-22 09:23:18 - Epoch: [15][150/352]	Time  0.175 ( 0.163)	Data  0.002 ( 0.004)	Loss 9.6942e-01 (8.7977e-01)	Acc@1  73.44 ( 74.67)	Acc@5  96.09 ( 94.64)
03-Mar-22 09:23:19 - Epoch: [14][140/352]	Time  0.158 ( 0.150)	Data  0.002 ( 0.004)	Loss 9.5772e-01 (8.8811e-01)	Acc@1  70.31 ( 74.42)	Acc@5  94.53 ( 94.65)
03-Mar-22 09:23:20 - Epoch: [15][160/352]	Time  0.177 ( 0.163)	Data  0.002 ( 0.004)	Loss 1.0684e+00 (8.8044e-01)	Acc@1  74.22 ( 74.73)	Acc@5  92.19 ( 94.59)
03-Mar-22 09:23:21 - Epoch: [14][150/352]	Time  0.154 ( 0.150)	Data  0.002 ( 0.004)	Loss 7.8958e-01 (8.8797e-01)	Acc@1  80.47 ( 74.41)	Acc@5  95.31 ( 94.65)
03-Mar-22 09:23:21 - Epoch: [15][170/352]	Time  0.152 ( 0.163)	Data  0.003 ( 0.004)	Loss 8.6777e-01 (8.7945e-01)	Acc@1  75.00 ( 74.78)	Acc@5  93.75 ( 94.60)
03-Mar-22 09:23:22 - Epoch: [14][160/352]	Time  0.149 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.2450e-01 (8.8834e-01)	Acc@1  77.34 ( 74.45)	Acc@5  93.75 ( 94.63)
03-Mar-22 09:23:23 - Epoch: [15][180/352]	Time  0.178 ( 0.163)	Data  0.003 ( 0.004)	Loss 8.6530e-01 (8.7735e-01)	Acc@1  73.44 ( 74.88)	Acc@5  96.09 ( 94.61)
03-Mar-22 09:23:24 - Epoch: [14][170/352]	Time  0.136 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.9865e-01 (8.8693e-01)	Acc@1  72.66 ( 74.43)	Acc@5  94.53 ( 94.64)
03-Mar-22 09:23:25 - Epoch: [15][190/352]	Time  0.171 ( 0.163)	Data  0.002 ( 0.004)	Loss 8.7461e-01 (8.7844e-01)	Acc@1  79.69 ( 74.83)	Acc@5  94.53 ( 94.63)
03-Mar-22 09:23:25 - Epoch: [14][180/352]	Time  0.152 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0972e+00 (8.8598e-01)	Acc@1  73.44 ( 74.55)	Acc@5  92.19 ( 94.58)
03-Mar-22 09:23:26 - Epoch: [15][200/352]	Time  0.150 ( 0.163)	Data  0.002 ( 0.004)	Loss 8.1016e-01 (8.7546e-01)	Acc@1  78.91 ( 74.95)	Acc@5  93.75 ( 94.62)
03-Mar-22 09:23:27 - Epoch: [14][190/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.2212e-01 (8.8475e-01)	Acc@1  79.69 ( 74.66)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:23:28 - Epoch: [15][210/352]	Time  0.180 ( 0.163)	Data  0.003 ( 0.004)	Loss 9.1318e-01 (8.7666e-01)	Acc@1  71.09 ( 74.89)	Acc@5  95.31 ( 94.63)
03-Mar-22 09:23:28 - Epoch: [14][200/352]	Time  0.131 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.6906e-01 (8.8428e-01)	Acc@1  78.91 ( 74.70)	Acc@5  94.53 ( 94.55)
03-Mar-22 09:23:29 - Epoch: [14][210/352]	Time  0.125 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.8048e-01 (8.8514e-01)	Acc@1  76.56 ( 74.67)	Acc@5  96.09 ( 94.54)
03-Mar-22 09:23:30 - Epoch: [15][220/352]	Time  0.169 ( 0.164)	Data  0.002 ( 0.004)	Loss 8.7034e-01 (8.7569e-01)	Acc@1  75.00 ( 74.95)	Acc@5  96.88 ( 94.64)
03-Mar-22 09:23:31 - Epoch: [14][220/352]	Time  0.153 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.8355e-01 (8.8799e-01)	Acc@1  75.78 ( 74.59)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:23:31 - Epoch: [15][230/352]	Time  0.134 ( 0.164)	Data  0.002 ( 0.003)	Loss 9.2487e-01 (8.7630e-01)	Acc@1  71.88 ( 74.88)	Acc@5  93.75 ( 94.61)
03-Mar-22 09:23:32 - Epoch: [14][230/352]	Time  0.150 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.5373e-01 (8.8824e-01)	Acc@1  82.03 ( 74.57)	Acc@5  96.09 ( 94.51)
03-Mar-22 09:23:33 - Epoch: [15][240/352]	Time  0.140 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.8404e-01 (8.7769e-01)	Acc@1  75.00 ( 74.86)	Acc@5  96.09 ( 94.56)
03-Mar-22 09:23:34 - Epoch: [14][240/352]	Time  0.133 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.3128e-01 (8.9027e-01)	Acc@1  75.78 ( 74.53)	Acc@5  93.75 ( 94.43)
03-Mar-22 09:23:34 - Epoch: [15][250/352]	Time  0.160 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.6352e-01 (8.7805e-01)	Acc@1  78.12 ( 74.88)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:23:35 - Epoch: [14][250/352]	Time  0.133 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.6110e-01 (8.9180e-01)	Acc@1  66.41 ( 74.45)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:23:36 - Epoch: [15][260/352]	Time  0.146 ( 0.162)	Data  0.003 ( 0.003)	Loss 7.3959e-01 (8.7808e-01)	Acc@1  77.34 ( 74.84)	Acc@5  96.09 ( 94.55)
03-Mar-22 09:23:37 - Epoch: [14][260/352]	Time  0.144 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.1939e-01 (8.8999e-01)	Acc@1  78.91 ( 74.50)	Acc@5  93.75 ( 94.45)
03-Mar-22 09:23:37 - Epoch: [15][270/352]	Time  0.146 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.0294e-01 (8.7567e-01)	Acc@1  79.69 ( 74.87)	Acc@5  97.66 ( 94.57)
03-Mar-22 09:23:38 - Epoch: [14][270/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.0448e+00 (8.9143e-01)	Acc@1  72.66 ( 74.46)	Acc@5  94.53 ( 94.48)
03-Mar-22 09:23:39 - Epoch: [15][280/352]	Time  0.147 ( 0.161)	Data  0.003 ( 0.003)	Loss 8.3702e-01 (8.7459e-01)	Acc@1  73.44 ( 74.86)	Acc@5  96.88 ( 94.58)
03-Mar-22 09:23:40 - Epoch: [14][280/352]	Time  0.154 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.1956e-01 (8.8866e-01)	Acc@1  74.22 ( 74.57)	Acc@5  93.75 ( 94.51)
03-Mar-22 09:23:40 - Epoch: [15][290/352]	Time  0.155 ( 0.161)	Data  0.002 ( 0.003)	Loss 8.2256e-01 (8.7593e-01)	Acc@1  77.34 ( 74.81)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:23:41 - Epoch: [14][290/352]	Time  0.136 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.7162e-01 (8.8903e-01)	Acc@1  74.22 ( 74.59)	Acc@5  94.53 ( 94.48)
03-Mar-22 09:23:42 - Epoch: [15][300/352]	Time  0.151 ( 0.160)	Data  0.002 ( 0.003)	Loss 7.0062e-01 (8.7620e-01)	Acc@1  80.47 ( 74.84)	Acc@5  98.44 ( 94.54)
03-Mar-22 09:23:43 - Epoch: [14][300/352]	Time  0.148 ( 0.148)	Data  0.002 ( 0.003)	Loss 7.6372e-01 (8.8791e-01)	Acc@1  81.25 ( 74.63)	Acc@5  96.88 ( 94.51)
03-Mar-22 09:23:43 - Epoch: [15][310/352]	Time  0.145 ( 0.160)	Data  0.002 ( 0.003)	Loss 9.1995e-01 (8.7876e-01)	Acc@1  71.88 ( 74.76)	Acc@5  92.97 ( 94.53)
03-Mar-22 09:23:44 - Epoch: [14][310/352]	Time  0.155 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.3380e-01 (8.8747e-01)	Acc@1  75.78 ( 74.61)	Acc@5  96.09 ( 94.54)
03-Mar-22 09:23:45 - Epoch: [15][320/352]	Time  0.146 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.0295e+00 (8.7952e-01)	Acc@1  72.66 ( 74.73)	Acc@5  92.97 ( 94.53)
03-Mar-22 09:23:46 - Epoch: [14][320/352]	Time  0.147 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.9947e-01 (8.8816e-01)	Acc@1  71.09 ( 74.57)	Acc@5  93.75 ( 94.52)
03-Mar-22 09:23:46 - Epoch: [15][330/352]	Time  0.137 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1663e+00 (8.7869e-01)	Acc@1  63.28 ( 74.76)	Acc@5  93.75 ( 94.56)
03-Mar-22 09:23:47 - Epoch: [14][330/352]	Time  0.148 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.3340e-01 (8.8910e-01)	Acc@1  71.88 ( 74.53)	Acc@5  93.75 ( 94.50)
03-Mar-22 09:23:48 - Epoch: [15][340/352]	Time  0.150 ( 0.159)	Data  0.002 ( 0.003)	Loss 8.3361e-01 (8.8147e-01)	Acc@1  75.00 ( 74.70)	Acc@5  94.53 ( 94.51)
03-Mar-22 09:23:49 - Epoch: [14][340/352]	Time  0.148 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.2984e-01 (8.9034e-01)	Acc@1  75.78 ( 74.48)	Acc@5  92.97 ( 94.50)
03-Mar-22 09:23:49 - Epoch: [15][350/352]	Time  0.146 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.5758e-01 (8.8250e-01)	Acc@1  75.00 ( 74.68)	Acc@5  93.75 ( 94.49)
03-Mar-22 09:23:50 - Test: [ 0/20]	Time  0.350 ( 0.350)	Loss 1.0294e+00 (1.0294e+00)	Acc@1  71.88 ( 71.88)	Acc@5  91.41 ( 91.41)
03-Mar-22 09:23:50 - Epoch: [14][350/352]	Time  0.152 ( 0.148)	Data  0.001 ( 0.003)	Loss 8.5507e-01 (8.9087e-01)	Acc@1  75.78 ( 74.47)	Acc@5  94.53 ( 94.50)
03-Mar-22 09:23:51 - Test: [10/20]	Time  0.092 ( 0.110)	Loss 8.1087e-01 (9.3881e-01)	Acc@1  78.12 ( 73.51)	Acc@5  94.53 ( 93.75)
03-Mar-22 09:23:51 - Test: [ 0/20]	Time  0.342 ( 0.342)	Loss 1.0098e+00 (1.0098e+00)	Acc@1  71.09 ( 71.09)	Acc@5  92.19 ( 92.19)
03-Mar-22 09:23:52 -  * Acc@1 73.160 Acc@5 93.860
03-Mar-22 09:23:52 - Test: [10/20]	Time  0.070 ( 0.124)	Loss 8.2507e-01 (9.6389e-01)	Acc@1  75.78 ( 72.37)	Acc@5  96.09 ( 93.08)
03-Mar-22 09:23:52 - Best acc at epoch 15: 73.97999572753906
03-Mar-22 09:23:52 - Epoch: [16][  0/352]	Time  0.410 ( 0.410)	Data  0.253 ( 0.253)	Loss 8.8196e-01 (8.8196e-01)	Acc@1  73.44 ( 73.44)	Acc@5  97.66 ( 97.66)
03-Mar-22 09:23:52 -  * Acc@1 72.440 Acc@5 93.500
03-Mar-22 09:23:52 - Best acc at epoch 14: 74.04000091552734
03-Mar-22 09:23:53 - Epoch: [15][  0/352]	Time  0.359 ( 0.359)	Data  0.233 ( 0.233)	Loss 9.7796e-01 (9.7796e-01)	Acc@1  74.22 ( 74.22)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:23:53 - Epoch: [16][ 10/352]	Time  0.125 ( 0.151)	Data  0.002 ( 0.025)	Loss 1.0805e+00 (8.6919e-01)	Acc@1  70.31 ( 73.93)	Acc@5  92.19 ( 95.95)
03-Mar-22 09:23:54 - Epoch: [15][ 10/352]	Time  0.152 ( 0.173)	Data  0.002 ( 0.024)	Loss 8.1198e-01 (9.4921e-01)	Acc@1  73.44 ( 73.44)	Acc@5  96.09 ( 94.74)
03-Mar-22 09:23:55 - Epoch: [16][ 20/352]	Time  0.151 ( 0.149)	Data  0.002 ( 0.014)	Loss 7.8318e-01 (8.7292e-01)	Acc@1  79.69 ( 74.48)	Acc@5  95.31 ( 95.28)
03-Mar-22 09:23:56 - Epoch: [15][ 20/352]	Time  0.134 ( 0.163)	Data  0.002 ( 0.014)	Loss 9.1968e-01 (9.0608e-01)	Acc@1  75.00 ( 74.59)	Acc@5  94.53 ( 94.68)
03-Mar-22 09:23:57 - Epoch: [16][ 30/352]	Time  0.178 ( 0.157)	Data  0.002 ( 0.010)	Loss 9.4570e-01 (8.5686e-01)	Acc@1  71.88 ( 74.97)	Acc@5  92.19 ( 95.26)
03-Mar-22 09:23:57 - Epoch: [15][ 30/352]	Time  0.121 ( 0.158)	Data  0.002 ( 0.010)	Loss 9.6843e-01 (9.1072e-01)	Acc@1  74.22 ( 74.29)	Acc@5  92.19 ( 94.41)
03-Mar-22 09:23:58 - Epoch: [16][ 40/352]	Time  0.173 ( 0.160)	Data  0.002 ( 0.008)	Loss 8.9035e-01 (8.6147e-01)	Acc@1  76.56 ( 75.08)	Acc@5  92.97 ( 95.16)
03-Mar-22 09:23:59 - Epoch: [15][ 40/352]	Time  0.146 ( 0.156)	Data  0.002 ( 0.008)	Loss 7.6510e-01 (8.8979e-01)	Acc@1  74.22 ( 74.87)	Acc@5  97.66 ( 94.47)
03-Mar-22 09:24:00 - Epoch: [16][ 50/352]	Time  0.172 ( 0.162)	Data  0.002 ( 0.007)	Loss 9.6459e-01 (8.6729e-01)	Acc@1  71.09 ( 74.69)	Acc@5  91.41 ( 95.02)
03-Mar-22 09:24:00 - Epoch: [15][ 50/352]	Time  0.166 ( 0.155)	Data  0.002 ( 0.007)	Loss 7.3345e-01 (8.8841e-01)	Acc@1  78.12 ( 74.75)	Acc@5  96.09 ( 94.42)
03-Mar-22 09:24:02 - Epoch: [16][ 60/352]	Time  0.173 ( 0.162)	Data  0.002 ( 0.006)	Loss 8.5571e-01 (8.6712e-01)	Acc@1  74.22 ( 74.77)	Acc@5  96.88 ( 95.12)
03-Mar-22 09:24:02 - Epoch: [15][ 60/352]	Time  0.163 ( 0.155)	Data  0.002 ( 0.006)	Loss 8.7388e-01 (8.9518e-01)	Acc@1  75.00 ( 74.37)	Acc@5  93.75 ( 94.22)
03-Mar-22 09:24:03 - Epoch: [16][ 70/352]	Time  0.151 ( 0.160)	Data  0.002 ( 0.006)	Loss 9.4395e-01 (8.6707e-01)	Acc@1  77.34 ( 74.90)	Acc@5  92.97 ( 94.99)
03-Mar-22 09:24:03 - Epoch: [15][ 70/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.006)	Loss 9.9887e-01 (9.0227e-01)	Acc@1  71.09 ( 74.17)	Acc@5  93.75 ( 94.19)
03-Mar-22 09:24:04 - Epoch: [16][ 80/352]	Time  0.137 ( 0.158)	Data  0.002 ( 0.005)	Loss 8.5255e-01 (8.6222e-01)	Acc@1  74.22 ( 75.09)	Acc@5  94.53 ( 94.99)
03-Mar-22 09:24:05 - Epoch: [15][ 80/352]	Time  0.152 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.0767e-01 (8.9951e-01)	Acc@1  75.00 ( 74.32)	Acc@5  94.53 ( 94.12)
03-Mar-22 09:24:06 - Epoch: [16][ 90/352]	Time  0.145 ( 0.157)	Data  0.002 ( 0.005)	Loss 8.2007e-01 (8.6203e-01)	Acc@1  75.00 ( 74.97)	Acc@5  96.09 ( 95.05)
03-Mar-22 09:24:06 - Epoch: [15][ 90/352]	Time  0.134 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.2555e-01 (9.0134e-01)	Acc@1  76.56 ( 74.30)	Acc@5  94.53 ( 94.14)
03-Mar-22 09:24:07 - Epoch: [16][100/352]	Time  0.145 ( 0.156)	Data  0.002 ( 0.005)	Loss 7.6419e-01 (8.6245e-01)	Acc@1  80.47 ( 74.94)	Acc@5  95.31 ( 95.06)
03-Mar-22 09:24:08 - Epoch: [15][100/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.005)	Loss 7.0346e-01 (8.9195e-01)	Acc@1  83.59 ( 74.59)	Acc@5  96.88 ( 94.31)
03-Mar-22 09:24:09 - Epoch: [16][110/352]	Time  0.140 ( 0.155)	Data  0.002 ( 0.005)	Loss 8.2444e-01 (8.6079e-01)	Acc@1  80.47 ( 75.03)	Acc@5  92.97 ( 95.07)
03-Mar-22 09:24:09 - Epoch: [15][110/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.005)	Loss 9.6062e-01 (8.9350e-01)	Acc@1  73.44 ( 74.54)	Acc@5  94.53 ( 94.31)
03-Mar-22 09:24:10 - Epoch: [16][120/352]	Time  0.133 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.4324e-01 (8.6633e-01)	Acc@1  78.12 ( 74.95)	Acc@5  95.31 ( 94.96)
03-Mar-22 09:24:11 - Epoch: [15][120/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.2835e-01 (8.9150e-01)	Acc@1  79.69 ( 74.59)	Acc@5  95.31 ( 94.32)
03-Mar-22 09:24:12 - Epoch: [16][130/352]	Time  0.148 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.9281e-01 (8.6551e-01)	Acc@1  74.22 ( 75.02)	Acc@5  92.97 ( 94.91)
03-Mar-22 09:24:12 - Epoch: [15][130/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.004)	Loss 6.2772e-01 (8.9087e-01)	Acc@1  82.81 ( 74.62)	Acc@5  96.09 ( 94.32)
03-Mar-22 09:24:13 - Epoch: [16][140/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.7306e-01 (8.6763e-01)	Acc@1  67.97 ( 74.88)	Acc@5  97.66 ( 94.89)
03-Mar-22 09:24:14 - Epoch: [15][140/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0233e+00 (8.8924e-01)	Acc@1  70.31 ( 74.69)	Acc@5  92.97 ( 94.43)
03-Mar-22 09:24:15 - Epoch: [16][150/352]	Time  0.152 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.7068e-01 (8.7138e-01)	Acc@1  75.00 ( 74.74)	Acc@5  95.31 ( 94.85)
03-Mar-22 09:24:15 - Epoch: [15][150/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.5411e-01 (8.9394e-01)	Acc@1  77.34 ( 74.59)	Acc@5  92.97 ( 94.30)
03-Mar-22 09:24:16 - Epoch: [16][160/352]	Time  0.156 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.1162e-01 (8.7012e-01)	Acc@1  71.09 ( 74.67)	Acc@5  96.88 ( 94.91)
03-Mar-22 09:24:17 - Epoch: [15][160/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.004)	Loss 7.9736e-01 (8.9266e-01)	Acc@1  78.12 ( 74.59)	Acc@5  96.88 ( 94.32)
03-Mar-22 09:24:18 - Epoch: [16][170/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.5619e-01 (8.7097e-01)	Acc@1  74.22 ( 74.72)	Acc@5  93.75 ( 94.86)
03-Mar-22 09:24:18 - Epoch: [15][170/352]	Time  0.155 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.2092e-01 (8.9193e-01)	Acc@1  78.12 ( 74.63)	Acc@5  92.97 ( 94.36)
03-Mar-22 09:24:19 - Epoch: [16][180/352]	Time  0.163 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.4703e-01 (8.7254e-01)	Acc@1  77.34 ( 74.74)	Acc@5  95.31 ( 94.85)
03-Mar-22 09:24:20 - Epoch: [15][180/352]	Time  0.143 ( 0.151)	Data  0.002 ( 0.004)	Loss 7.7122e-01 (8.9260e-01)	Acc@1  75.00 ( 74.60)	Acc@5  96.09 ( 94.33)
03-Mar-22 09:24:21 - Epoch: [16][190/352]	Time  0.155 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.0010e-01 (8.7524e-01)	Acc@1  78.91 ( 74.71)	Acc@5  93.75 ( 94.82)
03-Mar-22 09:24:21 - Epoch: [15][190/352]	Time  0.154 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.2043e-01 (8.9544e-01)	Acc@1  75.78 ( 74.52)	Acc@5  95.31 ( 94.32)
03-Mar-22 09:24:23 - Epoch: [16][200/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.2576e-01 (8.7424e-01)	Acc@1  79.69 ( 74.74)	Acc@5  97.66 ( 94.84)
03-Mar-22 09:24:23 - Epoch: [15][200/352]	Time  0.128 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.4368e-01 (8.9356e-01)	Acc@1  71.09 ( 74.52)	Acc@5  94.53 ( 94.38)
03-Mar-22 09:24:24 - Epoch: [16][210/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.3161e-01 (8.7483e-01)	Acc@1  79.69 ( 74.69)	Acc@5  93.75 ( 94.85)
03-Mar-22 09:24:24 - Epoch: [15][210/352]	Time  0.146 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.7270e-01 (8.9263e-01)	Acc@1  71.09 ( 74.49)	Acc@5  96.88 ( 94.40)
03-Mar-22 09:24:26 - Epoch: [16][220/352]	Time  0.162 ( 0.154)	Data  0.003 ( 0.003)	Loss 8.6713e-01 (8.7662e-01)	Acc@1  80.47 ( 74.68)	Acc@5  95.31 ( 94.80)
03-Mar-22 09:24:26 - Epoch: [15][220/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.1268e-01 (8.9338e-01)	Acc@1  75.00 ( 74.49)	Acc@5  91.41 ( 94.39)
03-Mar-22 09:24:27 - Epoch: [15][230/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.1817e-01 (8.9239e-01)	Acc@1  71.88 ( 74.51)	Acc@5  92.97 ( 94.42)
03-Mar-22 09:24:27 - Epoch: [16][230/352]	Time  0.159 ( 0.154)	Data  0.002 ( 0.003)	Loss 7.6810e-01 (8.7963e-01)	Acc@1  76.56 ( 74.58)	Acc@5  97.66 ( 94.75)
03-Mar-22 09:24:29 - Epoch: [15][240/352]	Time  0.150 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.6746e-01 (8.9206e-01)	Acc@1  76.56 ( 74.50)	Acc@5  95.31 ( 94.43)
03-Mar-22 09:24:29 - Epoch: [16][240/352]	Time  0.156 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.5027e-01 (8.8119e-01)	Acc@1  78.91 ( 74.49)	Acc@5  97.66 ( 94.73)
03-Mar-22 09:24:30 - Epoch: [15][250/352]	Time  0.150 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.7966e-01 (8.9308e-01)	Acc@1  70.31 ( 74.44)	Acc@5  92.19 ( 94.44)
03-Mar-22 09:24:30 - Epoch: [16][250/352]	Time  0.151 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.3286e-01 (8.8006e-01)	Acc@1  75.78 ( 74.51)	Acc@5  93.75 ( 94.70)
03-Mar-22 09:24:31 - Epoch: [15][260/352]	Time  0.136 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1308e+00 (8.9480e-01)	Acc@1  71.09 ( 74.41)	Acc@5  89.06 ( 94.42)
03-Mar-22 09:24:32 - Epoch: [16][260/352]	Time  0.158 ( 0.154)	Data  0.002 ( 0.003)	Loss 6.7397e-01 (8.7933e-01)	Acc@1  78.91 ( 74.52)	Acc@5  97.66 ( 94.73)
03-Mar-22 09:24:33 - Epoch: [15][270/352]	Time  0.136 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.7933e-01 (8.9367e-01)	Acc@1  78.12 ( 74.42)	Acc@5  93.75 ( 94.42)
03-Mar-22 09:24:33 - Epoch: [16][270/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.6610e-01 (8.7986e-01)	Acc@1  69.53 ( 74.48)	Acc@5  94.53 ( 94.74)
03-Mar-22 09:24:34 - Epoch: [15][280/352]	Time  0.138 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.6203e-01 (8.9359e-01)	Acc@1  73.44 ( 74.45)	Acc@5  94.53 ( 94.41)
03-Mar-22 09:24:35 - Epoch: [16][280/352]	Time  0.160 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0686e+00 (8.8257e-01)	Acc@1  73.44 ( 74.42)	Acc@5  89.84 ( 94.67)
03-Mar-22 09:24:36 - Epoch: [15][290/352]	Time  0.166 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.2319e-01 (8.9165e-01)	Acc@1  78.91 ( 74.53)	Acc@5  94.53 ( 94.42)
03-Mar-22 09:24:36 - Epoch: [16][290/352]	Time  0.161 ( 0.154)	Data  0.003 ( 0.003)	Loss 9.7762e-01 (8.8415e-01)	Acc@1  67.19 ( 74.36)	Acc@5  92.97 ( 94.65)
03-Mar-22 09:24:37 - Epoch: [15][300/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.3494e-01 (8.8995e-01)	Acc@1  80.47 ( 74.57)	Acc@5  96.09 ( 94.44)
03-Mar-22 09:24:38 - Epoch: [16][300/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.2638e-01 (8.8477e-01)	Acc@1  71.88 ( 74.34)	Acc@5  95.31 ( 94.65)
03-Mar-22 09:24:39 - Epoch: [15][310/352]	Time  0.120 ( 0.149)	Data  0.001 ( 0.003)	Loss 8.6068e-01 (8.8908e-01)	Acc@1  70.31 ( 74.58)	Acc@5  95.31 ( 94.43)
03-Mar-22 09:24:40 - Epoch: [16][310/352]	Time  0.156 ( 0.154)	Data  0.003 ( 0.003)	Loss 1.0012e+00 (8.8306e-01)	Acc@1  72.66 ( 74.39)	Acc@5  92.19 ( 94.63)
03-Mar-22 09:24:40 - Epoch: [15][320/352]	Time  0.138 ( 0.149)	Data  0.001 ( 0.003)	Loss 8.1665e-01 (8.8725e-01)	Acc@1  76.56 ( 74.63)	Acc@5  94.53 ( 94.45)
03-Mar-22 09:24:41 - Epoch: [16][320/352]	Time  0.159 ( 0.154)	Data  0.002 ( 0.003)	Loss 7.4952e-01 (8.8240e-01)	Acc@1  75.00 ( 74.39)	Acc@5  96.09 ( 94.64)
03-Mar-22 09:24:42 - Epoch: [15][330/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0520e+00 (8.8760e-01)	Acc@1  71.88 ( 74.61)	Acc@5  92.19 ( 94.44)
03-Mar-22 09:24:43 - Epoch: [16][330/352]	Time  0.158 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0425e+00 (8.8440e-01)	Acc@1  70.31 ( 74.34)	Acc@5  92.19 ( 94.61)
03-Mar-22 09:24:43 - Epoch: [15][340/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.5903e-01 (8.8598e-01)	Acc@1  78.12 ( 74.66)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:24:44 - Epoch: [16][340/352]	Time  0.150 ( 0.154)	Data  0.002 ( 0.003)	Loss 9.2861e-01 (8.8525e-01)	Acc@1  75.78 ( 74.34)	Acc@5  92.97 ( 94.60)
03-Mar-22 09:24:45 - Epoch: [15][350/352]	Time  0.137 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0202e+00 (8.8741e-01)	Acc@1  68.75 ( 74.62)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:24:45 - Test: [ 0/20]	Time  0.348 ( 0.348)	Loss 1.0633e+00 (1.0633e+00)	Acc@1  71.09 ( 71.09)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:24:46 - Epoch: [16][350/352]	Time  0.159 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.0468e+00 (8.8434e-01)	Acc@1  67.19 ( 74.37)	Acc@5  90.62 ( 94.60)
03-Mar-22 09:24:46 - Test: [ 0/20]	Time  0.351 ( 0.351)	Loss 1.0259e+00 (1.0259e+00)	Acc@1  69.53 ( 69.53)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:24:46 - Test: [10/20]	Time  0.111 ( 0.119)	Loss 8.5694e-01 (9.8052e-01)	Acc@1  75.78 ( 72.90)	Acc@5  95.31 ( 93.15)
03-Mar-22 09:24:47 - Test: [10/20]	Time  0.114 ( 0.130)	Loss 8.1481e-01 (9.5344e-01)	Acc@1  76.56 ( 72.62)	Acc@5  94.92 ( 93.57)
03-Mar-22 09:24:47 -  * Acc@1 73.000 Acc@5 93.440
03-Mar-22 09:24:47 - Best acc at epoch 15: 74.04000091552734
03-Mar-22 09:24:48 - Epoch: [16][  0/352]	Time  0.372 ( 0.372)	Data  0.226 ( 0.226)	Loss 7.3826e-01 (7.3826e-01)	Acc@1  81.25 ( 81.25)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:24:48 -  * Acc@1 72.360 Acc@5 93.700
03-Mar-22 09:24:48 - Best acc at epoch 16: 73.97999572753906
03-Mar-22 09:24:49 - Epoch: [17][  0/352]	Time  0.483 ( 0.483)	Data  0.324 ( 0.324)	Loss 8.3945e-01 (8.3945e-01)	Acc@1  75.78 ( 75.78)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:24:49 - Epoch: [16][ 10/352]	Time  0.142 ( 0.160)	Data  0.002 ( 0.023)	Loss 9.1263e-01 (9.2469e-01)	Acc@1  70.31 ( 73.15)	Acc@5  95.31 ( 93.89)
03-Mar-22 09:24:50 - Epoch: [17][ 10/352]	Time  0.150 ( 0.188)	Data  0.002 ( 0.031)	Loss 9.9491e-01 (8.9155e-01)	Acc@1  70.31 ( 72.87)	Acc@5  90.62 ( 94.11)
03-Mar-22 09:24:51 - Epoch: [16][ 20/352]	Time  0.164 ( 0.158)	Data  0.002 ( 0.013)	Loss 7.5621e-01 (8.4795e-01)	Acc@1  73.44 ( 75.15)	Acc@5  98.44 ( 95.20)
03-Mar-22 09:24:52 - Epoch: [17][ 20/352]	Time  0.138 ( 0.169)	Data  0.002 ( 0.018)	Loss 9.8581e-01 (8.7967e-01)	Acc@1  71.88 ( 74.14)	Acc@5  92.19 ( 94.23)
03-Mar-22 09:24:52 - Epoch: [16][ 30/352]	Time  0.176 ( 0.155)	Data  0.002 ( 0.009)	Loss 7.6091e-01 (8.3765e-01)	Acc@1  78.12 ( 75.81)	Acc@5  96.09 ( 95.19)
03-Mar-22 09:24:53 - Epoch: [17][ 30/352]	Time  0.151 ( 0.162)	Data  0.002 ( 0.013)	Loss 1.0799e+00 (8.7114e-01)	Acc@1  69.53 ( 74.57)	Acc@5  89.84 ( 94.30)
03-Mar-22 09:24:54 - Epoch: [16][ 40/352]	Time  0.137 ( 0.153)	Data  0.002 ( 0.008)	Loss 8.8388e-01 (8.4505e-01)	Acc@1  75.00 ( 75.50)	Acc@5  96.09 ( 95.12)
03-Mar-22 09:24:55 - Epoch: [17][ 40/352]	Time  0.133 ( 0.158)	Data  0.002 ( 0.010)	Loss 8.4523e-01 (8.6467e-01)	Acc@1  78.91 ( 74.92)	Acc@5  93.75 ( 94.46)
03-Mar-22 09:24:55 - Epoch: [16][ 50/352]	Time  0.166 ( 0.152)	Data  0.002 ( 0.007)	Loss 8.4595e-01 (8.6170e-01)	Acc@1  75.00 ( 74.92)	Acc@5  92.97 ( 94.87)
03-Mar-22 09:24:56 - Epoch: [17][ 50/352]	Time  0.148 ( 0.155)	Data  0.002 ( 0.008)	Loss 1.0092e+00 (8.6489e-01)	Acc@1  69.53 ( 75.26)	Acc@5  95.31 ( 94.47)
03-Mar-22 09:24:57 - Epoch: [16][ 60/352]	Time  0.147 ( 0.152)	Data  0.002 ( 0.006)	Loss 7.7932e-01 (8.6148e-01)	Acc@1  75.00 ( 74.81)	Acc@5  98.44 ( 94.97)
03-Mar-22 09:24:58 - Epoch: [17][ 60/352]	Time  0.140 ( 0.154)	Data  0.002 ( 0.007)	Loss 9.5979e-01 (8.7185e-01)	Acc@1  70.31 ( 74.78)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:24:58 - Epoch: [16][ 70/352]	Time  0.183 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.1790e-01 (8.7106e-01)	Acc@1  75.00 ( 74.44)	Acc@5  96.88 ( 94.87)
03-Mar-22 09:24:59 - Epoch: [17][ 70/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.007)	Loss 8.9597e-01 (8.7139e-01)	Acc@1  71.09 ( 74.68)	Acc@5  94.53 ( 94.55)
03-Mar-22 09:25:00 - Epoch: [16][ 80/352]	Time  0.153 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.5261e-01 (8.7311e-01)	Acc@1  73.44 ( 74.34)	Acc@5  92.97 ( 94.82)
03-Mar-22 09:25:00 - Epoch: [17][ 80/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.006)	Loss 8.9616e-01 (8.6708e-01)	Acc@1  71.88 ( 74.78)	Acc@5  91.41 ( 94.57)
03-Mar-22 09:25:02 - Epoch: [16][ 90/352]	Time  0.178 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.2434e-01 (8.7033e-01)	Acc@1  80.47 ( 74.59)	Acc@5  92.97 ( 94.84)
03-Mar-22 09:25:02 - Epoch: [17][ 90/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.006)	Loss 9.6531e-01 (8.6944e-01)	Acc@1  74.22 ( 74.69)	Acc@5  91.41 ( 94.56)
03-Mar-22 09:25:03 - Epoch: [16][100/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.9269e-01 (8.7173e-01)	Acc@1  74.22 ( 74.64)	Acc@5  94.53 ( 94.83)
03-Mar-22 09:25:04 - Epoch: [17][100/352]	Time  0.145 ( 0.154)	Data  0.002 ( 0.005)	Loss 7.7750e-01 (8.7109e-01)	Acc@1  78.12 ( 74.74)	Acc@5  97.66 ( 94.55)
03-Mar-22 09:25:05 - Epoch: [16][110/352]	Time  0.166 ( 0.154)	Data  0.003 ( 0.004)	Loss 9.3058e-01 (8.7702e-01)	Acc@1  72.66 ( 74.58)	Acc@5  92.19 ( 94.74)
03-Mar-22 09:25:05 - Epoch: [17][110/352]	Time  0.152 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.5007e-01 (8.7506e-01)	Acc@1  76.56 ( 74.61)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:25:06 - Epoch: [16][120/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.3186e-01 (8.7549e-01)	Acc@1  78.12 ( 74.66)	Acc@5  95.31 ( 94.75)
03-Mar-22 09:25:07 - Epoch: [17][120/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.3138e-01 (8.7883e-01)	Acc@1  74.22 ( 74.55)	Acc@5  94.53 ( 94.52)
03-Mar-22 09:25:08 - Epoch: [16][130/352]	Time  0.154 ( 0.153)	Data  0.002 ( 0.004)	Loss 8.8037e-01 (8.7311e-01)	Acc@1  74.22 ( 74.74)	Acc@5  95.31 ( 94.76)
03-Mar-22 09:25:08 - Epoch: [17][130/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.9410e-01 (8.8117e-01)	Acc@1  71.88 ( 74.48)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:25:09 - Epoch: [16][140/352]	Time  0.157 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.2446e-01 (8.7346e-01)	Acc@1  77.34 ( 74.68)	Acc@5  98.44 ( 94.71)
03-Mar-22 09:25:10 - Epoch: [17][140/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.005)	Loss 8.1086e-01 (8.8328e-01)	Acc@1  78.91 ( 74.46)	Acc@5  94.53 ( 94.40)
03-Mar-22 09:25:11 - Epoch: [16][150/352]	Time  0.183 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0648e+00 (8.7768e-01)	Acc@1  66.41 ( 74.58)	Acc@5  91.41 ( 94.63)
03-Mar-22 09:25:11 - Epoch: [17][150/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.5483e-01 (8.8199e-01)	Acc@1  81.25 ( 74.54)	Acc@5  95.31 ( 94.39)
03-Mar-22 09:25:12 - Epoch: [16][160/352]	Time  0.173 ( 0.154)	Data  0.002 ( 0.004)	Loss 9.0675e-01 (8.8239e-01)	Acc@1  76.56 ( 74.42)	Acc@5  89.84 ( 94.58)
03-Mar-22 09:25:13 - Epoch: [17][160/352]	Time  0.155 ( 0.152)	Data  0.003 ( 0.004)	Loss 9.3909e-01 (8.7797e-01)	Acc@1  76.56 ( 74.72)	Acc@5  93.75 ( 94.45)
03-Mar-22 09:25:14 - Epoch: [16][170/352]	Time  0.155 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.6475e-01 (8.8509e-01)	Acc@1  78.91 ( 74.30)	Acc@5  93.75 ( 94.59)
03-Mar-22 09:25:14 - Epoch: [17][170/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.2335e-01 (8.7605e-01)	Acc@1  75.00 ( 74.87)	Acc@5  95.31 ( 94.48)
03-Mar-22 09:25:15 - Epoch: [16][180/352]	Time  0.168 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.4123e-01 (8.8731e-01)	Acc@1  78.91 ( 74.32)	Acc@5  92.97 ( 94.54)
03-Mar-22 09:25:15 - Epoch: [17][180/352]	Time  0.138 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.6502e-01 (8.7756e-01)	Acc@1  70.31 ( 74.75)	Acc@5  94.53 ( 94.50)
03-Mar-22 09:25:17 - Epoch: [16][190/352]	Time  0.135 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.0419e-01 (8.8651e-01)	Acc@1  78.91 ( 74.40)	Acc@5  96.88 ( 94.58)
03-Mar-22 09:25:17 - Epoch: [17][190/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.1085e-01 (8.7856e-01)	Acc@1  75.00 ( 74.68)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:25:18 - Epoch: [16][200/352]	Time  0.146 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.9609e-01 (8.8547e-01)	Acc@1  82.03 ( 74.43)	Acc@5  95.31 ( 94.58)
03-Mar-22 09:25:18 - Epoch: [17][200/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.004)	Loss 7.3284e-01 (8.7876e-01)	Acc@1  82.03 ( 74.67)	Acc@5  96.09 ( 94.51)
03-Mar-22 09:25:20 - Epoch: [16][210/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.9425e-01 (8.8531e-01)	Acc@1  77.34 ( 74.44)	Acc@5  93.75 ( 94.61)
03-Mar-22 09:25:20 - Epoch: [17][210/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.9682e-01 (8.7954e-01)	Acc@1  73.44 ( 74.68)	Acc@5  95.31 ( 94.51)
03-Mar-22 09:25:21 - Epoch: [16][220/352]	Time  0.164 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.0394e-01 (8.8520e-01)	Acc@1  74.22 ( 74.44)	Acc@5  94.53 ( 94.61)
03-Mar-22 09:25:22 - Epoch: [17][220/352]	Time  0.156 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.8114e-01 (8.7762e-01)	Acc@1  78.12 ( 74.75)	Acc@5  92.97 ( 94.55)
03-Mar-22 09:25:23 - Epoch: [16][230/352]	Time  0.144 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0487e+00 (8.8536e-01)	Acc@1  67.97 ( 74.44)	Acc@5  95.31 ( 94.62)
03-Mar-22 09:25:23 - Epoch: [17][230/352]	Time  0.159 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.0648e+00 (8.7978e-01)	Acc@1  70.31 ( 74.72)	Acc@5  92.97 ( 94.55)
03-Mar-22 09:25:24 - Epoch: [16][240/352]	Time  0.147 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.9914e-01 (8.8726e-01)	Acc@1  68.75 ( 74.40)	Acc@5  93.75 ( 94.59)
03-Mar-22 09:25:25 - Epoch: [17][240/352]	Time  0.144 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.2093e-01 (8.7944e-01)	Acc@1  79.69 ( 74.66)	Acc@5  94.53 ( 94.54)
03-Mar-22 09:25:26 - Epoch: [16][250/352]	Time  0.136 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.0688e-01 (8.8706e-01)	Acc@1  72.66 ( 74.43)	Acc@5  96.88 ( 94.61)
03-Mar-22 09:25:27 - Epoch: [17][250/352]	Time  0.140 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.2882e-01 (8.7946e-01)	Acc@1  75.78 ( 74.63)	Acc@5  97.66 ( 94.57)
03-Mar-22 09:25:27 - Epoch: [16][260/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.1439e-01 (8.8588e-01)	Acc@1  76.56 ( 74.48)	Acc@5  96.88 ( 94.61)
03-Mar-22 09:25:28 - Epoch: [17][260/352]	Time  0.150 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.7236e-01 (8.7857e-01)	Acc@1  76.56 ( 74.68)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:25:29 - Epoch: [16][270/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.003)	Loss 6.8510e-01 (8.8575e-01)	Acc@1  81.25 ( 74.51)	Acc@5  95.31 ( 94.63)
03-Mar-22 09:25:29 - Epoch: [17][270/352]	Time  0.147 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.1957e-01 (8.7841e-01)	Acc@1  75.78 ( 74.69)	Acc@5  93.75 ( 94.56)
03-Mar-22 09:25:30 - Epoch: [16][280/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.6808e-01 (8.8618e-01)	Acc@1  71.88 ( 74.49)	Acc@5  94.53 ( 94.62)
03-Mar-22 09:25:31 - Epoch: [17][280/352]	Time  0.161 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0206e+00 (8.7757e-01)	Acc@1  65.62 ( 74.70)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:25:32 - Epoch: [16][290/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.5867e-01 (8.8635e-01)	Acc@1  71.88 ( 74.46)	Acc@5  96.09 ( 94.64)
03-Mar-22 09:25:32 - Epoch: [17][290/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.0787e-01 (8.7790e-01)	Acc@1  75.78 ( 74.67)	Acc@5  96.09 ( 94.52)
03-Mar-22 09:25:33 - Epoch: [16][300/352]	Time  0.147 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0205e+00 (8.8662e-01)	Acc@1  64.06 ( 74.44)	Acc@5  94.53 ( 94.64)
03-Mar-22 09:25:34 - Epoch: [17][300/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.9916e-01 (8.7848e-01)	Acc@1  77.34 ( 74.66)	Acc@5  95.31 ( 94.52)
03-Mar-22 09:25:35 - Epoch: [16][310/352]	Time  0.136 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0078e+00 (8.8756e-01)	Acc@1  71.88 ( 74.43)	Acc@5  92.97 ( 94.64)
03-Mar-22 09:25:35 - Epoch: [17][310/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.8128e-01 (8.7830e-01)	Acc@1  72.66 ( 74.65)	Acc@5  96.09 ( 94.53)
03-Mar-22 09:25:36 - Epoch: [16][320/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.8482e-01 (8.8798e-01)	Acc@1  70.31 ( 74.39)	Acc@5  95.31 ( 94.62)
03-Mar-22 09:25:37 - Epoch: [17][320/352]	Time  0.147 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.2900e-01 (8.7638e-01)	Acc@1  75.78 ( 74.69)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:25:38 - Epoch: [16][330/352]	Time  0.133 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.2942e-01 (8.8734e-01)	Acc@1  76.56 ( 74.41)	Acc@5  95.31 ( 94.65)
03-Mar-22 09:25:38 - Epoch: [17][330/352]	Time  0.158 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.0293e-01 (8.7620e-01)	Acc@1  78.12 ( 74.66)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:25:39 - Epoch: [16][340/352]	Time  0.154 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.8980e-01 (8.8571e-01)	Acc@1  80.47 ( 74.47)	Acc@5  93.75 ( 94.67)
03-Mar-22 09:25:40 - Epoch: [17][340/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0015e+00 (8.7759e-01)	Acc@1  68.75 ( 74.66)	Acc@5  92.97 ( 94.54)
03-Mar-22 09:25:40 - Epoch: [16][350/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3428e-01 (8.8414e-01)	Acc@1  71.88 ( 74.46)	Acc@5  95.31 ( 94.72)
03-Mar-22 09:25:41 - Test: [ 0/20]	Time  0.344 ( 0.344)	Loss 9.9083e-01 (9.9083e-01)	Acc@1  70.31 ( 70.31)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:25:41 - Epoch: [17][350/352]	Time  0.144 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3540e-01 (8.7667e-01)	Acc@1  77.34 ( 74.68)	Acc@5  93.75 ( 94.54)
03-Mar-22 09:25:42 - Test: [ 0/20]	Time  0.415 ( 0.415)	Loss 1.0904e+00 (1.0904e+00)	Acc@1  68.36 ( 68.36)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:25:42 - Test: [10/20]	Time  0.081 ( 0.109)	Loss 8.1155e-01 (9.3823e-01)	Acc@1  76.17 ( 73.15)	Acc@5  94.92 ( 93.39)
03-Mar-22 09:25:43 - Test: [10/20]	Time  0.089 ( 0.136)	Loss 8.1682e-01 (9.4460e-01)	Acc@1  77.73 ( 73.33)	Acc@5  96.09 ( 93.75)
03-Mar-22 09:25:43 -  * Acc@1 73.180 Acc@5 93.540
03-Mar-22 09:25:43 - Best acc at epoch 16: 74.04000091552734
03-Mar-22 09:25:44 - Epoch: [17][  0/352]	Time  0.491 ( 0.491)	Data  0.343 ( 0.343)	Loss 6.9856e-01 (6.9856e-01)	Acc@1  79.69 ( 79.69)	Acc@5  97.66 ( 97.66)
03-Mar-22 09:25:44 -  * Acc@1 73.340 Acc@5 93.920
03-Mar-22 09:25:44 - Best acc at epoch 17: 73.97999572753906
03-Mar-22 09:25:44 - Epoch: [18][  0/352]	Time  0.383 ( 0.383)	Data  0.236 ( 0.236)	Loss 8.7629e-01 (8.7629e-01)	Acc@1  75.00 ( 75.00)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:25:45 - Epoch: [17][ 10/352]	Time  0.158 ( 0.170)	Data  0.002 ( 0.033)	Loss 8.3906e-01 (8.6730e-01)	Acc@1  71.09 ( 74.50)	Acc@5  96.09 ( 94.53)
03-Mar-22 09:25:46 - Epoch: [18][ 10/352]	Time  0.201 ( 0.207)	Data  0.003 ( 0.024)	Loss 8.9050e-01 (8.8507e-01)	Acc@1  69.53 ( 74.36)	Acc@5  97.66 ( 94.25)
03-Mar-22 09:25:46 - Epoch: [17][ 20/352]	Time  0.149 ( 0.160)	Data  0.002 ( 0.018)	Loss 7.7275e-01 (8.8637e-01)	Acc@1  75.00 ( 74.03)	Acc@5  99.22 ( 94.53)
03-Mar-22 09:25:48 - Epoch: [18][ 20/352]	Time  0.169 ( 0.190)	Data  0.002 ( 0.013)	Loss 9.8464e-01 (8.8275e-01)	Acc@1  68.75 ( 74.22)	Acc@5  93.75 ( 94.53)
03-Mar-22 09:25:48 - Epoch: [17][ 30/352]	Time  0.151 ( 0.158)	Data  0.002 ( 0.013)	Loss 7.0990e-01 (8.8466e-01)	Acc@1  77.34 ( 74.07)	Acc@5  96.09 ( 94.28)
03-Mar-22 09:25:49 - Epoch: [17][ 40/352]	Time  0.133 ( 0.156)	Data  0.002 ( 0.010)	Loss 7.7173e-01 (8.7606e-01)	Acc@1  79.69 ( 74.49)	Acc@5  96.88 ( 94.53)
03-Mar-22 09:25:50 - Epoch: [18][ 30/352]	Time  0.188 ( 0.184)	Data  0.002 ( 0.010)	Loss 7.5895e-01 (8.7238e-01)	Acc@1  78.12 ( 74.57)	Acc@5  96.88 ( 94.93)
03-Mar-22 09:25:51 - Epoch: [17][ 50/352]	Time  0.157 ( 0.155)	Data  0.002 ( 0.009)	Loss 8.7668e-01 (8.7469e-01)	Acc@1  80.47 ( 74.53)	Acc@5  92.19 ( 94.70)
03-Mar-22 09:25:51 - Epoch: [18][ 40/352]	Time  0.145 ( 0.179)	Data  0.002 ( 0.008)	Loss 9.0035e-01 (8.7557e-01)	Acc@1  69.53 ( 74.52)	Acc@5  94.53 ( 94.95)
03-Mar-22 09:25:53 - Epoch: [17][ 60/352]	Time  0.155 ( 0.155)	Data  0.003 ( 0.008)	Loss 8.2796e-01 (8.7937e-01)	Acc@1  74.22 ( 74.41)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:25:53 - Epoch: [18][ 50/352]	Time  0.159 ( 0.173)	Data  0.002 ( 0.007)	Loss 7.9641e-01 (8.7293e-01)	Acc@1  77.34 ( 74.92)	Acc@5  96.88 ( 94.96)
03-Mar-22 09:25:54 - Epoch: [17][ 70/352]	Time  0.168 ( 0.156)	Data  0.002 ( 0.007)	Loss 9.7952e-01 (8.8011e-01)	Acc@1  72.66 ( 74.44)	Acc@5  92.97 ( 94.54)
03-Mar-22 09:25:54 - Epoch: [18][ 60/352]	Time  0.152 ( 0.170)	Data  0.002 ( 0.006)	Loss 6.8256e-01 (8.7232e-01)	Acc@1  79.69 ( 74.99)	Acc@5  97.66 ( 94.94)
03-Mar-22 09:25:56 - Epoch: [17][ 80/352]	Time  0.155 ( 0.155)	Data  0.002 ( 0.006)	Loss 7.0781e-01 (8.8146e-01)	Acc@1  78.91 ( 74.59)	Acc@5  96.88 ( 94.48)
03-Mar-22 09:25:56 - Epoch: [18][ 70/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.006)	Loss 1.0875e+00 (8.6853e-01)	Acc@1  66.41 ( 75.10)	Acc@5  93.75 ( 95.05)
03-Mar-22 09:25:57 - Epoch: [18][ 80/352]	Time  0.146 ( 0.165)	Data  0.002 ( 0.005)	Loss 7.9726e-01 (8.6814e-01)	Acc@1  80.47 ( 75.14)	Acc@5  95.31 ( 95.01)
03-Mar-22 09:25:57 - Epoch: [17][ 90/352]	Time  0.182 ( 0.156)	Data  0.002 ( 0.006)	Loss 7.7810e-01 (8.8046e-01)	Acc@1  78.91 ( 74.56)	Acc@5  95.31 ( 94.45)
03-Mar-22 09:25:59 - Epoch: [18][ 90/352]	Time  0.152 ( 0.163)	Data  0.002 ( 0.005)	Loss 9.2226e-01 (8.6216e-01)	Acc@1  70.31 ( 75.31)	Acc@5  98.44 ( 95.12)
03-Mar-22 09:25:59 - Epoch: [17][100/352]	Time  0.153 ( 0.156)	Data  0.002 ( 0.006)	Loss 8.4537e-01 (8.7810e-01)	Acc@1  75.78 ( 74.58)	Acc@5  96.09 ( 94.52)
03-Mar-22 09:26:00 - Epoch: [18][100/352]	Time  0.152 ( 0.162)	Data  0.002 ( 0.005)	Loss 9.9383e-01 (8.6241e-01)	Acc@1  70.31 ( 75.29)	Acc@5  93.75 ( 95.12)
03-Mar-22 09:26:00 - Epoch: [17][110/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.005)	Loss 7.8183e-01 (8.7686e-01)	Acc@1  71.09 ( 74.51)	Acc@5  97.66 ( 94.57)
03-Mar-22 09:26:02 - Epoch: [18][110/352]	Time  0.143 ( 0.161)	Data  0.003 ( 0.004)	Loss 9.7943e-01 (8.6612e-01)	Acc@1  68.75 ( 75.06)	Acc@5  93.75 ( 95.06)
03-Mar-22 09:26:02 - Epoch: [17][120/352]	Time  0.154 ( 0.156)	Data  0.002 ( 0.005)	Loss 8.2652e-01 (8.7613e-01)	Acc@1  73.44 ( 74.44)	Acc@5  94.53 ( 94.63)
03-Mar-22 09:26:03 - Epoch: [18][120/352]	Time  0.136 ( 0.158)	Data  0.002 ( 0.004)	Loss 9.0624e-01 (8.6560e-01)	Acc@1  73.44 ( 75.01)	Acc@5  96.09 ( 95.06)
03-Mar-22 09:26:03 - Epoch: [17][130/352]	Time  0.162 ( 0.156)	Data  0.003 ( 0.005)	Loss 7.9939e-01 (8.8037e-01)	Acc@1  77.34 ( 74.31)	Acc@5  95.31 ( 94.60)
03-Mar-22 09:26:04 - Epoch: [18][130/352]	Time  0.150 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.2790e-01 (8.6673e-01)	Acc@1  70.31 ( 74.77)	Acc@5  96.88 ( 95.06)
03-Mar-22 09:26:05 - Epoch: [17][140/352]	Time  0.146 ( 0.156)	Data  0.002 ( 0.005)	Loss 9.0565e-01 (8.7984e-01)	Acc@1  75.78 ( 74.39)	Acc@5  95.31 ( 94.60)
03-Mar-22 09:26:06 - Epoch: [18][140/352]	Time  0.133 ( 0.157)	Data  0.002 ( 0.004)	Loss 7.6569e-01 (8.6764e-01)	Acc@1  75.78 ( 74.76)	Acc@5  95.31 ( 95.02)
03-Mar-22 09:26:07 - Epoch: [17][150/352]	Time  0.178 ( 0.157)	Data  0.002 ( 0.005)	Loss 9.5004e-01 (8.8071e-01)	Acc@1  75.78 ( 74.41)	Acc@5  93.75 ( 94.60)
03-Mar-22 09:26:07 - Epoch: [18][150/352]	Time  0.146 ( 0.156)	Data  0.002 ( 0.004)	Loss 8.5882e-01 (8.6634e-01)	Acc@1  76.56 ( 74.82)	Acc@5  95.31 ( 94.99)
03-Mar-22 09:26:08 - Epoch: [17][160/352]	Time  0.157 ( 0.157)	Data  0.002 ( 0.004)	Loss 1.0042e+00 (8.8507e-01)	Acc@1  67.97 ( 74.33)	Acc@5  96.09 ( 94.57)
03-Mar-22 09:26:09 - Epoch: [18][160/352]	Time  0.129 ( 0.155)	Data  0.002 ( 0.004)	Loss 8.7354e-01 (8.6712e-01)	Acc@1  77.34 ( 74.87)	Acc@5  93.75 ( 94.98)
03-Mar-22 09:26:10 - Epoch: [17][170/352]	Time  0.153 ( 0.157)	Data  0.003 ( 0.004)	Loss 9.5267e-01 (8.8697e-01)	Acc@1  75.00 ( 74.26)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:26:10 - Epoch: [18][170/352]	Time  0.136 ( 0.154)	Data  0.002 ( 0.004)	Loss 7.3205e-01 (8.6541e-01)	Acc@1  83.59 ( 75.00)	Acc@5  95.31 ( 94.96)
03-Mar-22 09:26:11 - Epoch: [17][180/352]	Time  0.159 ( 0.157)	Data  0.002 ( 0.004)	Loss 7.7695e-01 (8.8597e-01)	Acc@1  78.91 ( 74.27)	Acc@5  96.88 ( 94.60)
03-Mar-22 09:26:12 - Epoch: [18][180/352]	Time  0.146 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.0330e+00 (8.6744e-01)	Acc@1  71.88 ( 74.92)	Acc@5  92.19 ( 94.93)
03-Mar-22 09:26:13 - Epoch: [17][190/352]	Time  0.177 ( 0.157)	Data  0.002 ( 0.004)	Loss 6.6256e-01 (8.8468e-01)	Acc@1  82.81 ( 74.35)	Acc@5  97.66 ( 94.62)
03-Mar-22 09:26:13 - Epoch: [18][190/352]	Time  0.143 ( 0.154)	Data  0.002 ( 0.003)	Loss 8.4720e-01 (8.7036e-01)	Acc@1  78.12 ( 74.87)	Acc@5  93.75 ( 94.87)
03-Mar-22 09:26:15 - Epoch: [17][200/352]	Time  0.154 ( 0.157)	Data  0.002 ( 0.004)	Loss 1.0367e+00 (8.8307e-01)	Acc@1  68.75 ( 74.41)	Acc@5  92.19 ( 94.65)
03-Mar-22 09:26:15 - Epoch: [18][200/352]	Time  0.151 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.5403e-01 (8.6897e-01)	Acc@1  73.44 ( 74.93)	Acc@5  95.31 ( 94.87)
03-Mar-22 09:26:16 - Epoch: [18][210/352]	Time  0.148 ( 0.153)	Data  0.002 ( 0.003)	Loss 9.1070e-01 (8.6859e-01)	Acc@1  70.31 ( 74.91)	Acc@5  93.75 ( 94.86)
03-Mar-22 09:26:16 - Epoch: [17][210/352]	Time  0.180 ( 0.157)	Data  0.003 ( 0.004)	Loss 7.7017e-01 (8.8262e-01)	Acc@1  77.34 ( 74.37)	Acc@5  95.31 ( 94.66)
03-Mar-22 09:26:18 - Epoch: [18][220/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.003)	Loss 8.9487e-01 (8.6925e-01)	Acc@1  73.44 ( 74.88)	Acc@5  92.19 ( 94.84)
03-Mar-22 09:26:18 - Epoch: [17][220/352]	Time  0.146 ( 0.157)	Data  0.002 ( 0.004)	Loss 9.7436e-01 (8.8061e-01)	Acc@1  71.88 ( 74.47)	Acc@5  92.97 ( 94.69)
03-Mar-22 09:26:19 - Epoch: [18][230/352]	Time  0.146 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.4937e-01 (8.6898e-01)	Acc@1  70.31 ( 74.89)	Acc@5  92.19 ( 94.82)
03-Mar-22 09:26:19 - Epoch: [17][230/352]	Time  0.185 ( 0.157)	Data  0.009 ( 0.004)	Loss 1.0141e+00 (8.8261e-01)	Acc@1  67.97 ( 74.39)	Acc@5  90.62 ( 94.66)
03-Mar-22 09:26:20 - Epoch: [18][240/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.3720e-01 (8.6659e-01)	Acc@1  73.44 ( 74.98)	Acc@5  93.75 ( 94.86)
03-Mar-22 09:26:21 - Epoch: [17][240/352]	Time  0.151 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.6990e-01 (8.8016e-01)	Acc@1  78.91 ( 74.48)	Acc@5  92.97 ( 94.69)
03-Mar-22 09:26:22 - Epoch: [18][250/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.9906e-01 (8.6845e-01)	Acc@1  73.44 ( 74.94)	Acc@5  94.53 ( 94.82)
03-Mar-22 09:26:22 - Epoch: [17][250/352]	Time  0.165 ( 0.157)	Data  0.002 ( 0.004)	Loss 7.6272e-01 (8.7893e-01)	Acc@1  77.34 ( 74.54)	Acc@5  96.88 ( 94.71)
03-Mar-22 09:26:23 - Epoch: [18][260/352]	Time  0.146 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.3754e-01 (8.7106e-01)	Acc@1  71.88 ( 74.87)	Acc@5  94.53 ( 94.79)
03-Mar-22 09:26:24 - Epoch: [17][260/352]	Time  0.151 ( 0.157)	Data  0.002 ( 0.004)	Loss 7.5044e-01 (8.8034e-01)	Acc@1  80.47 ( 74.52)	Acc@5  95.31 ( 94.67)
03-Mar-22 09:26:25 - Epoch: [18][270/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0648e+00 (8.7024e-01)	Acc@1  69.53 ( 74.88)	Acc@5  91.41 ( 94.77)
03-Mar-22 09:26:26 - Epoch: [17][270/352]	Time  0.181 ( 0.157)	Data  0.002 ( 0.004)	Loss 8.5717e-01 (8.8085e-01)	Acc@1  74.22 ( 74.47)	Acc@5  95.31 ( 94.65)
03-Mar-22 09:26:26 - Epoch: [18][280/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.9108e-01 (8.7150e-01)	Acc@1  73.44 ( 74.83)	Acc@5  94.53 ( 94.77)
03-Mar-22 09:26:27 - Epoch: [17][280/352]	Time  0.156 ( 0.157)	Data  0.002 ( 0.004)	Loss 9.4374e-01 (8.8119e-01)	Acc@1  74.22 ( 74.44)	Acc@5  95.31 ( 94.64)
03-Mar-22 09:26:28 - Epoch: [18][290/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.0361e-01 (8.7196e-01)	Acc@1  72.66 ( 74.81)	Acc@5  92.97 ( 94.74)
03-Mar-22 09:26:29 - Epoch: [17][290/352]	Time  0.182 ( 0.157)	Data  0.003 ( 0.004)	Loss 9.9500e-01 (8.8205e-01)	Acc@1  70.31 ( 74.43)	Acc@5  92.19 ( 94.59)
03-Mar-22 09:26:29 - Epoch: [18][300/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.3520e-01 (8.7243e-01)	Acc@1  71.09 ( 74.78)	Acc@5  97.66 ( 94.76)
03-Mar-22 09:26:30 - Epoch: [17][300/352]	Time  0.151 ( 0.157)	Data  0.002 ( 0.004)	Loss 9.0406e-01 (8.8270e-01)	Acc@1  73.44 ( 74.42)	Acc@5  90.62 ( 94.57)
03-Mar-22 09:26:31 - Epoch: [18][310/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 6.9840e-01 (8.7240e-01)	Acc@1  78.12 ( 74.75)	Acc@5  98.44 ( 94.76)
03-Mar-22 09:26:32 - Epoch: [17][310/352]	Time  0.178 ( 0.157)	Data  0.003 ( 0.004)	Loss 7.8743e-01 (8.8143e-01)	Acc@1  78.12 ( 74.46)	Acc@5  96.09 ( 94.58)
03-Mar-22 09:26:32 - Epoch: [18][320/352]	Time  0.135 ( 0.151)	Data  0.001 ( 0.003)	Loss 7.5952e-01 (8.7354e-01)	Acc@1  75.78 ( 74.72)	Acc@5  96.09 ( 94.74)
03-Mar-22 09:26:33 - Epoch: [17][320/352]	Time  0.153 ( 0.157)	Data  0.003 ( 0.004)	Loss 8.3305e-01 (8.8139e-01)	Acc@1  75.78 ( 74.48)	Acc@5  96.09 ( 94.59)
03-Mar-22 09:26:34 - Epoch: [18][330/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.5351e-01 (8.7348e-01)	Acc@1  79.69 ( 74.73)	Acc@5  98.44 ( 94.73)
03-Mar-22 09:26:35 - Epoch: [17][330/352]	Time  0.181 ( 0.156)	Data  0.003 ( 0.003)	Loss 9.2880e-01 (8.8038e-01)	Acc@1  78.12 ( 74.52)	Acc@5  90.62 ( 94.59)
03-Mar-22 09:26:35 - Epoch: [18][340/352]	Time  0.144 ( 0.151)	Data  0.002 ( 0.003)	Loss 9.3345e-01 (8.7221e-01)	Acc@1  75.78 ( 74.73)	Acc@5  92.97 ( 94.75)
03-Mar-22 09:26:36 - Epoch: [17][340/352]	Time  0.151 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.2951e-01 (8.8090e-01)	Acc@1  75.00 ( 74.52)	Acc@5  96.09 ( 94.59)
03-Mar-22 09:26:37 - Epoch: [18][350/352]	Time  0.147 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0200e+00 (8.7307e-01)	Acc@1  72.66 ( 74.70)	Acc@5  92.19 ( 94.75)
03-Mar-22 09:26:37 - Test: [ 0/20]	Time  0.371 ( 0.371)	Loss 9.8515e-01 (9.8515e-01)	Acc@1  73.05 ( 73.05)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:26:38 - Epoch: [17][350/352]	Time  0.173 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.5634e-01 (8.8031e-01)	Acc@1  75.00 ( 74.57)	Acc@5  93.75 ( 94.58)
03-Mar-22 09:26:38 - Test: [10/20]	Time  0.070 ( 0.111)	Loss 8.2861e-01 (9.3327e-01)	Acc@1  74.22 ( 72.90)	Acc@5  96.09 ( 93.61)
03-Mar-22 09:26:38 - Test: [ 0/20]	Time  0.429 ( 0.429)	Loss 1.0368e+00 (1.0368e+00)	Acc@1  70.31 ( 70.31)	Acc@5  91.80 ( 91.80)
03-Mar-22 09:26:39 -  * Acc@1 73.180 Acc@5 93.880
03-Mar-22 09:26:39 - Best acc at epoch 18: 73.97999572753906
03-Mar-22 09:26:39 - Test: [10/20]	Time  0.070 ( 0.128)	Loss 8.5495e-01 (9.4888e-01)	Acc@1  74.61 ( 72.59)	Acc@5  94.14 ( 93.15)
03-Mar-22 09:26:40 - Epoch: [19][  0/352]	Time  0.398 ( 0.398)	Data  0.234 ( 0.234)	Loss 6.8738e-01 (6.8738e-01)	Acc@1  82.03 ( 82.03)	Acc@5  96.09 ( 96.09)
03-Mar-22 09:26:40 -  * Acc@1 72.720 Acc@5 93.320
03-Mar-22 09:26:40 - Best acc at epoch 17: 74.04000091552734
03-Mar-22 09:26:41 - Epoch: [18][  0/352]	Time  0.359 ( 0.359)	Data  0.215 ( 0.215)	Loss 8.7634e-01 (8.7634e-01)	Acc@1  74.22 ( 74.22)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:26:41 - Epoch: [19][ 10/352]	Time  0.150 ( 0.169)	Data  0.002 ( 0.024)	Loss 8.8526e-01 (9.2155e-01)	Acc@1  71.09 ( 73.30)	Acc@5  94.53 ( 93.39)
03-Mar-22 09:26:42 - Epoch: [18][ 10/352]	Time  0.151 ( 0.172)	Data  0.002 ( 0.022)	Loss 8.4519e-01 (8.6045e-01)	Acc@1  74.22 ( 74.86)	Acc@5  96.88 ( 95.10)
03-Mar-22 09:26:43 - Epoch: [19][ 20/352]	Time  0.156 ( 0.161)	Data  0.003 ( 0.014)	Loss 9.5139e-01 (9.2211e-01)	Acc@1  74.22 ( 73.85)	Acc@5  91.41 ( 93.23)
03-Mar-22 09:26:44 - Epoch: [18][ 20/352]	Time  0.149 ( 0.162)	Data  0.002 ( 0.013)	Loss 8.7481e-01 (8.8030e-01)	Acc@1  72.66 ( 75.07)	Acc@5  92.97 ( 94.20)
03-Mar-22 09:26:44 - Epoch: [19][ 30/352]	Time  0.173 ( 0.161)	Data  0.003 ( 0.010)	Loss 7.5414e-01 (9.0301e-01)	Acc@1  79.69 ( 74.50)	Acc@5  96.09 ( 93.65)
03-Mar-22 09:26:45 - Epoch: [18][ 30/352]	Time  0.149 ( 0.159)	Data  0.003 ( 0.009)	Loss 7.9891e-01 (8.5920e-01)	Acc@1  79.69 ( 75.45)	Acc@5  94.53 ( 94.61)
03-Mar-22 09:26:46 - Epoch: [19][ 40/352]	Time  0.173 ( 0.161)	Data  0.003 ( 0.008)	Loss 8.1138e-01 (8.8163e-01)	Acc@1  76.56 ( 75.25)	Acc@5  96.09 ( 93.90)
03-Mar-22 09:26:47 - Epoch: [18][ 40/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.008)	Loss 9.0730e-01 (8.6122e-01)	Acc@1  77.34 ( 75.40)	Acc@5  94.53 ( 94.57)
03-Mar-22 09:26:48 - Epoch: [19][ 50/352]	Time  0.173 ( 0.162)	Data  0.002 ( 0.007)	Loss 7.1900e-01 (8.8482e-01)	Acc@1  75.78 ( 75.11)	Acc@5  97.66 ( 94.01)
03-Mar-22 09:26:48 - Epoch: [18][ 50/352]	Time  0.143 ( 0.155)	Data  0.002 ( 0.007)	Loss 1.0014e+00 (8.7428e-01)	Acc@1  70.31 ( 74.77)	Acc@5  92.19 ( 94.62)
03-Mar-22 09:26:49 - Epoch: [19][ 60/352]	Time  0.131 ( 0.162)	Data  0.002 ( 0.006)	Loss 9.1656e-01 (8.8778e-01)	Acc@1  70.31 ( 74.94)	Acc@5  96.09 ( 94.19)
03-Mar-22 09:26:50 - Epoch: [18][ 60/352]	Time  0.153 ( 0.155)	Data  0.003 ( 0.006)	Loss 8.5310e-01 (8.7065e-01)	Acc@1  76.56 ( 75.04)	Acc@5  96.88 ( 94.63)
03-Mar-22 09:26:51 - Epoch: [19][ 70/352]	Time  0.155 ( 0.161)	Data  0.002 ( 0.006)	Loss 9.7482e-01 (8.8574e-01)	Acc@1  73.44 ( 74.92)	Acc@5  92.19 ( 94.34)
03-Mar-22 09:26:51 - Epoch: [18][ 70/352]	Time  0.147 ( 0.155)	Data  0.002 ( 0.005)	Loss 1.0285e+00 (8.7478e-01)	Acc@1  70.31 ( 74.97)	Acc@5  93.75 ( 94.61)
03-Mar-22 09:26:52 - Epoch: [19][ 80/352]	Time  0.141 ( 0.161)	Data  0.003 ( 0.005)	Loss 9.8899e-01 (8.8323e-01)	Acc@1  71.88 ( 74.96)	Acc@5  92.19 ( 94.34)
03-Mar-22 09:26:53 - Epoch: [18][ 80/352]	Time  0.149 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.1022e-01 (8.7860e-01)	Acc@1  75.00 ( 74.89)	Acc@5  92.97 ( 94.49)
03-Mar-22 09:26:54 - Epoch: [19][ 90/352]	Time  0.147 ( 0.160)	Data  0.002 ( 0.005)	Loss 8.3497e-01 (8.8414e-01)	Acc@1  81.25 ( 74.84)	Acc@5  95.31 ( 94.35)
03-Mar-22 09:26:54 - Epoch: [18][ 90/352]	Time  0.152 ( 0.154)	Data  0.003 ( 0.005)	Loss 7.7329e-01 (8.7421e-01)	Acc@1  78.12 ( 75.03)	Acc@5  96.88 ( 94.55)
03-Mar-22 09:26:56 - Epoch: [19][100/352]	Time  0.143 ( 0.159)	Data  0.002 ( 0.005)	Loss 1.0156e+00 (8.7564e-01)	Acc@1  66.41 ( 74.99)	Acc@5  92.97 ( 94.48)
03-Mar-22 09:26:56 - Epoch: [18][100/352]	Time  0.154 ( 0.154)	Data  0.002 ( 0.005)	Loss 7.9675e-01 (8.7406e-01)	Acc@1  81.25 ( 75.05)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:26:57 - Epoch: [19][110/352]	Time  0.171 ( 0.160)	Data  0.003 ( 0.005)	Loss 8.0971e-01 (8.7890e-01)	Acc@1  76.56 ( 74.75)	Acc@5  96.88 ( 94.45)
03-Mar-22 09:26:58 - Epoch: [18][110/352]	Time  0.155 ( 0.154)	Data  0.003 ( 0.004)	Loss 9.1250e-01 (8.7585e-01)	Acc@1  75.00 ( 74.89)	Acc@5  92.97 ( 94.55)
03-Mar-22 09:26:59 - Epoch: [19][120/352]	Time  0.174 ( 0.161)	Data  0.003 ( 0.004)	Loss 8.8056e-01 (8.8003e-01)	Acc@1  73.44 ( 74.68)	Acc@5  96.09 ( 94.45)
03-Mar-22 09:26:59 - Epoch: [18][120/352]	Time  0.151 ( 0.154)	Data  0.003 ( 0.004)	Loss 1.0010e+00 (8.8164e-01)	Acc@1  73.44 ( 74.72)	Acc@5  92.19 ( 94.52)
03-Mar-22 09:27:00 - Epoch: [19][130/352]	Time  0.158 ( 0.160)	Data  0.002 ( 0.004)	Loss 8.8862e-01 (8.7756e-01)	Acc@1  76.56 ( 74.79)	Acc@5  95.31 ( 94.51)
03-Mar-22 09:27:01 - Epoch: [18][130/352]	Time  0.140 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.9376e-01 (8.8287e-01)	Acc@1  71.88 ( 74.75)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:27:02 - Epoch: [18][140/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.0486e-01 (8.8114e-01)	Acc@1  75.00 ( 74.88)	Acc@5  93.75 ( 94.58)
03-Mar-22 09:27:02 - Epoch: [19][140/352]	Time  0.175 ( 0.162)	Data  0.003 ( 0.004)	Loss 6.7712e-01 (8.7673e-01)	Acc@1  80.47 ( 74.72)	Acc@5  97.66 ( 94.56)
03-Mar-22 09:27:03 - Epoch: [18][150/352]	Time  0.147 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.5827e-01 (8.8111e-01)	Acc@1  69.53 ( 74.87)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:27:04 - Epoch: [19][150/352]	Time  0.171 ( 0.161)	Data  0.002 ( 0.004)	Loss 8.2417e-01 (8.7829e-01)	Acc@1  74.22 ( 74.68)	Acc@5  95.31 ( 94.54)
03-Mar-22 09:27:05 - Epoch: [18][160/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.004)	Loss 7.9389e-01 (8.8097e-01)	Acc@1  75.78 ( 74.75)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:27:06 - Epoch: [19][160/352]	Time  0.176 ( 0.162)	Data  0.003 ( 0.004)	Loss 7.0273e-01 (8.7691e-01)	Acc@1  76.56 ( 74.70)	Acc@5  96.88 ( 94.55)
03-Mar-22 09:27:06 - Epoch: [18][170/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.0626e-01 (8.8375e-01)	Acc@1  75.00 ( 74.68)	Acc@5  95.31 ( 94.54)
03-Mar-22 09:27:07 - Epoch: [19][170/352]	Time  0.149 ( 0.161)	Data  0.002 ( 0.004)	Loss 9.0478e-01 (8.7448e-01)	Acc@1  73.44 ( 74.78)	Acc@5  92.97 ( 94.58)
03-Mar-22 09:27:08 - Epoch: [18][180/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.9724e-01 (8.8290e-01)	Acc@1  75.00 ( 74.76)	Acc@5  93.75 ( 94.54)
03-Mar-22 09:27:09 - Epoch: [19][180/352]	Time  0.181 ( 0.161)	Data  0.003 ( 0.004)	Loss 7.8044e-01 (8.7318e-01)	Acc@1  78.91 ( 74.89)	Acc@5  95.31 ( 94.55)
03-Mar-22 09:27:09 - Epoch: [18][190/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.1106e-01 (8.8528e-01)	Acc@1  75.00 ( 74.67)	Acc@5  92.97 ( 94.51)
03-Mar-22 09:27:10 - Epoch: [19][190/352]	Time  0.171 ( 0.161)	Data  0.002 ( 0.004)	Loss 9.0965e-01 (8.7371e-01)	Acc@1  75.00 ( 74.90)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:27:11 - Epoch: [18][200/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.5389e-01 (8.8417e-01)	Acc@1  75.78 ( 74.69)	Acc@5  94.53 ( 94.52)
03-Mar-22 09:27:12 - Epoch: [19][200/352]	Time  0.172 ( 0.161)	Data  0.002 ( 0.004)	Loss 8.5674e-01 (8.7295e-01)	Acc@1  75.00 ( 74.93)	Acc@5  96.09 ( 94.56)
03-Mar-22 09:27:12 - Epoch: [18][210/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.9420e-01 (8.8291e-01)	Acc@1  73.44 ( 74.73)	Acc@5  92.97 ( 94.52)
03-Mar-22 09:27:14 - Epoch: [19][210/352]	Time  0.147 ( 0.161)	Data  0.002 ( 0.004)	Loss 8.6102e-01 (8.7314e-01)	Acc@1  75.00 ( 74.89)	Acc@5  94.53 ( 94.59)
03-Mar-22 09:27:14 - Epoch: [18][220/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.9145e-01 (8.8484e-01)	Acc@1  75.78 ( 74.65)	Acc@5  97.66 ( 94.52)
03-Mar-22 09:27:15 - Epoch: [19][220/352]	Time  0.170 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.8889e-01 (8.7313e-01)	Acc@1  71.09 ( 74.88)	Acc@5  94.53 ( 94.60)
03-Mar-22 09:27:15 - Epoch: [18][230/352]	Time  0.140 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.6880e-01 (8.8675e-01)	Acc@1  70.31 ( 74.67)	Acc@5  92.97 ( 94.45)
03-Mar-22 09:27:17 - Epoch: [19][230/352]	Time  0.130 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.1298e-01 (8.7313e-01)	Acc@1  78.12 ( 74.86)	Acc@5  95.31 ( 94.60)
03-Mar-22 09:27:17 - Epoch: [18][240/352]	Time  0.151 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.8725e-01 (8.8672e-01)	Acc@1  77.34 ( 74.62)	Acc@5  96.09 ( 94.46)
03-Mar-22 09:27:18 - Epoch: [19][240/352]	Time  0.149 ( 0.161)	Data  0.002 ( 0.003)	Loss 8.2392e-01 (8.7323e-01)	Acc@1  78.91 ( 74.89)	Acc@5  92.19 ( 94.57)
03-Mar-22 09:27:18 - Epoch: [18][250/352]	Time  0.154 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.0308e-01 (8.8672e-01)	Acc@1  73.44 ( 74.66)	Acc@5  92.97 ( 94.45)
03-Mar-22 09:27:20 - Epoch: [19][250/352]	Time  0.150 ( 0.160)	Data  0.002 ( 0.003)	Loss 7.7821e-01 (8.7407e-01)	Acc@1  82.81 ( 74.86)	Acc@5  92.97 ( 94.57)
03-Mar-22 09:27:20 - Epoch: [18][260/352]	Time  0.150 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.8467e-01 (8.8713e-01)	Acc@1  71.09 ( 74.65)	Acc@5  96.09 ( 94.43)
03-Mar-22 09:27:21 - Epoch: [19][260/352]	Time  0.135 ( 0.160)	Data  0.002 ( 0.003)	Loss 8.4807e-01 (8.7372e-01)	Acc@1  73.44 ( 74.82)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:27:21 - Epoch: [18][270/352]	Time  0.161 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1170e-01 (8.8661e-01)	Acc@1  78.12 ( 74.63)	Acc@5  92.19 ( 94.46)
03-Mar-22 09:27:23 - Epoch: [19][270/352]	Time  0.151 ( 0.160)	Data  0.002 ( 0.003)	Loss 9.6729e-01 (8.7304e-01)	Acc@1  71.09 ( 74.89)	Acc@5  93.75 ( 94.57)
03-Mar-22 09:27:23 - Epoch: [18][280/352]	Time  0.156 ( 0.152)	Data  0.003 ( 0.003)	Loss 7.3332e-01 (8.8571e-01)	Acc@1  78.12 ( 74.67)	Acc@5  95.31 ( 94.44)
03-Mar-22 09:27:24 - Epoch: [19][280/352]	Time  0.146 ( 0.159)	Data  0.002 ( 0.003)	Loss 8.1393e-01 (8.7468e-01)	Acc@1  76.56 ( 74.84)	Acc@5  96.09 ( 94.56)
03-Mar-22 09:27:25 - Epoch: [18][290/352]	Time  0.147 ( 0.152)	Data  0.003 ( 0.003)	Loss 1.0119e+00 (8.8312e-01)	Acc@1  69.53 ( 74.75)	Acc@5  90.62 ( 94.46)
03-Mar-22 09:27:26 - Epoch: [19][290/352]	Time  0.156 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.0326e+00 (8.7563e-01)	Acc@1  68.75 ( 74.79)	Acc@5  92.19 ( 94.57)
03-Mar-22 09:27:26 - Epoch: [18][300/352]	Time  0.155 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.6978e-01 (8.8357e-01)	Acc@1  73.44 ( 74.74)	Acc@5  94.53 ( 94.46)
03-Mar-22 09:27:27 - Epoch: [19][300/352]	Time  0.151 ( 0.159)	Data  0.002 ( 0.003)	Loss 8.6360e-01 (8.7641e-01)	Acc@1  75.78 ( 74.75)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:27:28 - Epoch: [18][310/352]	Time  0.158 ( 0.152)	Data  0.003 ( 0.003)	Loss 8.7010e-01 (8.8446e-01)	Acc@1  76.56 ( 74.70)	Acc@5  94.53 ( 94.45)
03-Mar-22 09:27:29 - Epoch: [19][310/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.6906e-01 (8.7711e-01)	Acc@1  71.09 ( 74.74)	Acc@5  93.75 ( 94.55)
03-Mar-22 09:27:29 - Epoch: [18][320/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.1324e-01 (8.8218e-01)	Acc@1  71.09 ( 74.74)	Acc@5  95.31 ( 94.48)
03-Mar-22 09:27:30 - Epoch: [19][320/352]	Time  0.149 ( 0.158)	Data  0.002 ( 0.003)	Loss 6.3576e-01 (8.7732e-01)	Acc@1  82.03 ( 74.70)	Acc@5  96.88 ( 94.57)
03-Mar-22 09:27:31 - Epoch: [18][330/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.9764e-01 (8.8278e-01)	Acc@1  77.34 ( 74.71)	Acc@5  92.97 ( 94.47)
03-Mar-22 09:27:32 - Epoch: [19][330/352]	Time  0.152 ( 0.157)	Data  0.002 ( 0.003)	Loss 7.2942e-01 (8.7825e-01)	Acc@1  81.25 ( 74.72)	Acc@5  96.88 ( 94.57)
03-Mar-22 09:27:32 - Epoch: [18][340/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.8389e-01 (8.8213e-01)	Acc@1  81.25 ( 74.73)	Acc@5  96.09 ( 94.49)
03-Mar-22 09:27:33 - Epoch: [19][340/352]	Time  0.153 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.9418e-01 (8.7790e-01)	Acc@1  67.97 ( 74.69)	Acc@5  94.53 ( 94.60)
03-Mar-22 09:27:34 - Epoch: [18][350/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.8595e-01 (8.8161e-01)	Acc@1  80.47 ( 74.75)	Acc@5  95.31 ( 94.49)
03-Mar-22 09:27:34 - Test: [ 0/20]	Time  0.345 ( 0.345)	Loss 1.0131e+00 (1.0131e+00)	Acc@1  68.36 ( 68.36)	Acc@5  93.36 ( 93.36)
03-Mar-22 09:27:34 - Epoch: [19][350/352]	Time  0.143 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.1545e+00 (8.7822e-01)	Acc@1  65.62 ( 74.71)	Acc@5  89.06 ( 94.57)
03-Mar-22 09:27:35 - Test: [ 0/20]	Time  0.365 ( 0.365)	Loss 9.9966e-01 (9.9966e-01)	Acc@1  68.75 ( 68.75)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:27:35 - Test: [10/20]	Time  0.117 ( 0.117)	Loss 8.0115e-01 (9.4575e-01)	Acc@1  77.34 ( 72.48)	Acc@5  95.31 ( 94.00)
03-Mar-22 09:27:36 -  * Acc@1 71.940 Acc@5 93.580
03-Mar-22 09:27:36 - Best acc at epoch 18: 74.04000091552734
03-Mar-22 09:27:36 - Test: [10/20]	Time  0.086 ( 0.139)	Loss 8.3104e-01 (9.5184e-01)	Acc@1  75.00 ( 72.48)	Acc@5  94.53 ( 93.47)
03-Mar-22 09:27:37 - Epoch: [19][  0/352]	Time  0.386 ( 0.386)	Data  0.229 ( 0.229)	Loss 7.3511e-01 (7.3511e-01)	Acc@1  75.78 ( 75.78)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:27:37 -  * Acc@1 72.480 Acc@5 93.640
03-Mar-22 09:27:37 - Best acc at epoch 19: 73.97999572753906
03-Mar-22 09:27:38 - Epoch: [20][  0/352]	Time  0.401 ( 0.401)	Data  0.230 ( 0.230)	Loss 7.9055e-01 (7.9055e-01)	Acc@1  78.91 ( 78.91)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:27:38 - Epoch: [19][ 10/352]	Time  0.121 ( 0.155)	Data  0.002 ( 0.023)	Loss 8.7749e-01 (9.1504e-01)	Acc@1  74.22 ( 73.79)	Acc@5  95.31 ( 94.25)
03-Mar-22 09:27:39 - Epoch: [19][ 20/352]	Time  0.117 ( 0.145)	Data  0.001 ( 0.013)	Loss 9.1569e-01 (8.8140e-01)	Acc@1  74.22 ( 74.37)	Acc@5  91.41 ( 94.46)
03-Mar-22 09:27:39 - Epoch: [20][ 10/352]	Time  0.179 ( 0.193)	Data  0.003 ( 0.023)	Loss 9.4961e-01 (8.8409e-01)	Acc@1  73.44 ( 74.29)	Acc@5  96.09 ( 94.39)
03-Mar-22 09:27:41 - Epoch: [19][ 30/352]	Time  0.149 ( 0.142)	Data  0.002 ( 0.009)	Loss 9.3937e-01 (8.8331e-01)	Acc@1  72.66 ( 74.62)	Acc@5  92.19 ( 94.15)
03-Mar-22 09:27:41 - Epoch: [20][ 20/352]	Time  0.177 ( 0.182)	Data  0.002 ( 0.013)	Loss 9.5205e-01 (8.5495e-01)	Acc@1  73.44 ( 75.11)	Acc@5  91.41 ( 94.42)
03-Mar-22 09:27:42 - Epoch: [19][ 40/352]	Time  0.127 ( 0.142)	Data  0.002 ( 0.008)	Loss 8.4310e-01 (8.8047e-01)	Acc@1  77.34 ( 74.60)	Acc@5  94.53 ( 94.26)
03-Mar-22 09:27:43 - Epoch: [20][ 30/352]	Time  0.176 ( 0.179)	Data  0.003 ( 0.010)	Loss 7.8170e-01 (8.6164e-01)	Acc@1  79.69 ( 75.28)	Acc@5  96.88 ( 94.56)
03-Mar-22 09:27:43 - Epoch: [19][ 50/352]	Time  0.130 ( 0.139)	Data  0.002 ( 0.007)	Loss 8.4634e-01 (8.7811e-01)	Acc@1  71.09 ( 74.54)	Acc@5  96.88 ( 94.35)
03-Mar-22 09:27:45 - Epoch: [20][ 40/352]	Time  0.175 ( 0.178)	Data  0.002 ( 0.008)	Loss 9.0920e-01 (8.5624e-01)	Acc@1  75.00 ( 75.34)	Acc@5  96.09 ( 94.70)
03-Mar-22 09:27:45 - Epoch: [19][ 60/352]	Time  0.149 ( 0.140)	Data  0.002 ( 0.006)	Loss 8.5649e-01 (8.7677e-01)	Acc@1  75.00 ( 74.62)	Acc@5  96.09 ( 94.40)
03-Mar-22 09:27:46 - Epoch: [19][ 70/352]	Time  0.139 ( 0.140)	Data  0.002 ( 0.005)	Loss 9.1017e-01 (8.8395e-01)	Acc@1  73.44 ( 74.21)	Acc@5  92.97 ( 94.37)
03-Mar-22 09:27:46 - Epoch: [20][ 50/352]	Time  0.173 ( 0.176)	Data  0.002 ( 0.007)	Loss 9.1989e-01 (8.6139e-01)	Acc@1  74.22 ( 75.09)	Acc@5  92.19 ( 94.64)
03-Mar-22 09:27:48 - Epoch: [19][ 80/352]	Time  0.122 ( 0.140)	Data  0.002 ( 0.005)	Loss 7.9452e-01 (8.8574e-01)	Acc@1  75.00 ( 74.24)	Acc@5  93.75 ( 94.21)
03-Mar-22 09:27:48 - Epoch: [20][ 60/352]	Time  0.182 ( 0.176)	Data  0.005 ( 0.006)	Loss 9.0048e-01 (8.6513e-01)	Acc@1  78.12 ( 74.94)	Acc@5  96.09 ( 94.60)
03-Mar-22 09:27:49 - Epoch: [19][ 90/352]	Time  0.146 ( 0.140)	Data  0.002 ( 0.005)	Loss 1.0163e+00 (8.8365e-01)	Acc@1  68.75 ( 74.29)	Acc@5  92.19 ( 94.23)
03-Mar-22 09:27:50 - Epoch: [20][ 70/352]	Time  0.150 ( 0.175)	Data  0.002 ( 0.006)	Loss 1.0464e+00 (8.6785e-01)	Acc@1  69.53 ( 74.70)	Acc@5  92.97 ( 94.66)
03-Mar-22 09:27:50 - Epoch: [19][100/352]	Time  0.157 ( 0.141)	Data  0.002 ( 0.004)	Loss 7.9288e-01 (8.8241e-01)	Acc@1  78.91 ( 74.37)	Acc@5  94.53 ( 94.28)
03-Mar-22 09:27:51 - Epoch: [20][ 80/352]	Time  0.158 ( 0.174)	Data  0.002 ( 0.005)	Loss 9.0589e-01 (8.6574e-01)	Acc@1  74.22 ( 74.89)	Acc@5  93.75 ( 94.76)
03-Mar-22 09:27:52 - Epoch: [19][110/352]	Time  0.156 ( 0.142)	Data  0.003 ( 0.004)	Loss 8.9540e-01 (8.7270e-01)	Acc@1  75.00 ( 74.85)	Acc@5  92.19 ( 94.38)
03-Mar-22 09:27:53 - Epoch: [20][ 90/352]	Time  0.154 ( 0.173)	Data  0.002 ( 0.005)	Loss 9.4807e-01 (8.6261e-01)	Acc@1  75.78 ( 75.12)	Acc@5  92.19 ( 94.72)
03-Mar-22 09:27:53 - Epoch: [19][120/352]	Time  0.119 ( 0.142)	Data  0.002 ( 0.004)	Loss 7.5094e-01 (8.6668e-01)	Acc@1  75.78 ( 75.01)	Acc@5  96.88 ( 94.47)
03-Mar-22 09:27:55 - Epoch: [20][100/352]	Time  0.157 ( 0.172)	Data  0.002 ( 0.005)	Loss 7.8209e-01 (8.6405e-01)	Acc@1  78.91 ( 75.02)	Acc@5  96.09 ( 94.63)
03-Mar-22 09:27:55 - Epoch: [19][130/352]	Time  0.128 ( 0.142)	Data  0.002 ( 0.004)	Loss 8.4854e-01 (8.6965e-01)	Acc@1  74.22 ( 74.84)	Acc@5  95.31 ( 94.50)
03-Mar-22 09:27:56 - Epoch: [19][140/352]	Time  0.118 ( 0.142)	Data  0.002 ( 0.004)	Loss 9.5081e-01 (8.6784e-01)	Acc@1  72.66 ( 74.86)	Acc@5  96.09 ( 94.54)
03-Mar-22 09:27:56 - Epoch: [20][110/352]	Time  0.171 ( 0.172)	Data  0.002 ( 0.004)	Loss 7.1794e-01 (8.6228e-01)	Acc@1  75.00 ( 75.12)	Acc@5  96.88 ( 94.61)
03-Mar-22 09:27:58 - Epoch: [19][150/352]	Time  0.143 ( 0.143)	Data  0.003 ( 0.004)	Loss 7.6023e-01 (8.6771e-01)	Acc@1  75.78 ( 74.74)	Acc@5  97.66 ( 94.62)
03-Mar-22 09:27:58 - Epoch: [20][120/352]	Time  0.142 ( 0.170)	Data  0.002 ( 0.004)	Loss 9.0537e-01 (8.6723e-01)	Acc@1  73.44 ( 74.94)	Acc@5  93.75 ( 94.56)
03-Mar-22 09:27:59 - Epoch: [19][160/352]	Time  0.130 ( 0.143)	Data  0.002 ( 0.004)	Loss 8.7294e-01 (8.7225e-01)	Acc@1  73.44 ( 74.57)	Acc@5  96.09 ( 94.58)
03-Mar-22 09:27:59 - Epoch: [20][130/352]	Time  0.148 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.9790e-01 (8.6962e-01)	Acc@1  69.53 ( 74.90)	Acc@5  92.19 ( 94.57)
03-Mar-22 09:28:01 - Epoch: [19][170/352]	Time  0.163 ( 0.143)	Data  0.002 ( 0.003)	Loss 7.1083e-01 (8.6942e-01)	Acc@1  78.12 ( 74.64)	Acc@5  96.88 ( 94.66)
03-Mar-22 09:28:01 - Epoch: [20][140/352]	Time  0.151 ( 0.167)	Data  0.003 ( 0.004)	Loss 9.7328e-01 (8.7075e-01)	Acc@1  76.56 ( 74.90)	Acc@5  92.19 ( 94.54)
03-Mar-22 09:28:02 - Epoch: [19][180/352]	Time  0.154 ( 0.143)	Data  0.002 ( 0.003)	Loss 8.3233e-01 (8.6683e-01)	Acc@1  78.12 ( 74.74)	Acc@5  96.09 ( 94.71)
03-Mar-22 09:28:02 - Epoch: [20][150/352]	Time  0.149 ( 0.165)	Data  0.002 ( 0.004)	Loss 7.5290e-01 (8.7190e-01)	Acc@1  78.91 ( 74.93)	Acc@5  96.88 ( 94.54)
03-Mar-22 09:28:04 - Epoch: [19][190/352]	Time  0.132 ( 0.144)	Data  0.002 ( 0.003)	Loss 7.4950e-01 (8.6297e-01)	Acc@1  79.69 ( 74.87)	Acc@5  96.09 ( 94.75)
03-Mar-22 09:28:04 - Epoch: [20][160/352]	Time  0.152 ( 0.164)	Data  0.002 ( 0.004)	Loss 9.4302e-01 (8.7840e-01)	Acc@1  75.00 ( 74.77)	Acc@5  93.75 ( 94.45)
03-Mar-22 09:28:05 - Epoch: [20][170/352]	Time  0.141 ( 0.163)	Data  0.002 ( 0.004)	Loss 6.5822e-01 (8.7751e-01)	Acc@1  80.47 ( 74.80)	Acc@5  97.66 ( 94.45)
03-Mar-22 09:28:05 - Epoch: [19][200/352]	Time  0.149 ( 0.144)	Data  0.002 ( 0.003)	Loss 8.6774e-01 (8.6172e-01)	Acc@1  78.91 ( 74.95)	Acc@5  93.75 ( 94.75)
03-Mar-22 09:28:07 - Epoch: [20][180/352]	Time  0.150 ( 0.162)	Data  0.002 ( 0.004)	Loss 8.2314e-01 (8.7454e-01)	Acc@1  78.12 ( 74.87)	Acc@5  96.88 ( 94.52)
03-Mar-22 09:28:07 - Epoch: [19][210/352]	Time  0.152 ( 0.145)	Data  0.002 ( 0.003)	Loss 8.4376e-01 (8.6276e-01)	Acc@1  74.22 ( 74.94)	Acc@5  93.75 ( 94.70)
03-Mar-22 09:28:08 - Epoch: [20][190/352]	Time  0.149 ( 0.161)	Data  0.002 ( 0.004)	Loss 9.1503e-01 (8.7370e-01)	Acc@1  72.66 ( 74.92)	Acc@5  95.31 ( 94.55)
03-Mar-22 09:28:08 - Epoch: [19][220/352]	Time  0.161 ( 0.145)	Data  0.002 ( 0.003)	Loss 8.6333e-01 (8.6519e-01)	Acc@1  74.22 ( 74.87)	Acc@5  94.53 ( 94.66)
03-Mar-22 09:28:10 - Epoch: [20][200/352]	Time  0.151 ( 0.160)	Data  0.002 ( 0.003)	Loss 9.0071e-01 (8.7363e-01)	Acc@1  74.22 ( 74.90)	Acc@5  96.88 ( 94.62)
03-Mar-22 09:28:10 - Epoch: [19][230/352]	Time  0.151 ( 0.146)	Data  0.003 ( 0.003)	Loss 8.0481e-01 (8.6487e-01)	Acc@1  76.56 ( 74.89)	Acc@5  96.09 ( 94.66)
03-Mar-22 09:28:11 - Epoch: [20][210/352]	Time  0.173 ( 0.160)	Data  0.002 ( 0.003)	Loss 9.0595e-01 (8.7364e-01)	Acc@1  75.78 ( 74.89)	Acc@5  95.31 ( 94.63)
03-Mar-22 09:28:11 - Epoch: [19][240/352]	Time  0.155 ( 0.146)	Data  0.002 ( 0.003)	Loss 1.0458e+00 (8.6814e-01)	Acc@1  67.97 ( 74.77)	Acc@5  91.41 ( 94.58)
03-Mar-22 09:28:13 - Epoch: [20][220/352]	Time  0.154 ( 0.161)	Data  0.002 ( 0.003)	Loss 8.2996e-01 (8.7275e-01)	Acc@1  77.34 ( 74.95)	Acc@5  96.09 ( 94.67)
03-Mar-22 09:28:13 - Epoch: [19][250/352]	Time  0.151 ( 0.146)	Data  0.002 ( 0.003)	Loss 7.6135e-01 (8.7019e-01)	Acc@1  78.91 ( 74.71)	Acc@5  98.44 ( 94.61)
03-Mar-22 09:28:14 - Epoch: [19][260/352]	Time  0.152 ( 0.146)	Data  0.002 ( 0.003)	Loss 9.6814e-01 (8.7168e-01)	Acc@1  73.44 ( 74.63)	Acc@5  92.97 ( 94.58)
03-Mar-22 09:28:15 - Epoch: [20][230/352]	Time  0.198 ( 0.162)	Data  0.003 ( 0.003)	Loss 6.0829e-01 (8.6995e-01)	Acc@1  82.03 ( 75.08)	Acc@5  97.66 ( 94.66)
03-Mar-22 09:28:16 - Epoch: [19][270/352]	Time  0.154 ( 0.146)	Data  0.003 ( 0.003)	Loss 7.1556e-01 (8.7186e-01)	Acc@1  79.69 ( 74.60)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:28:16 - Epoch: [20][240/352]	Time  0.161 ( 0.162)	Data  0.002 ( 0.003)	Loss 9.9713e-01 (8.7155e-01)	Acc@1  69.53 ( 75.02)	Acc@5  91.41 ( 94.64)
03-Mar-22 09:28:17 - Epoch: [19][280/352]	Time  0.152 ( 0.146)	Data  0.002 ( 0.003)	Loss 9.1619e-01 (8.7306e-01)	Acc@1  75.00 ( 74.59)	Acc@5  92.97 ( 94.51)
03-Mar-22 09:28:18 - Epoch: [20][250/352]	Time  0.173 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.9987e-01 (8.7051e-01)	Acc@1  75.00 ( 74.98)	Acc@5  97.66 ( 94.68)
03-Mar-22 09:28:19 - Epoch: [19][290/352]	Time  0.146 ( 0.146)	Data  0.002 ( 0.003)	Loss 8.1976e-01 (8.7224e-01)	Acc@1  79.69 ( 74.65)	Acc@5  95.31 ( 94.55)
03-Mar-22 09:28:20 - Epoch: [20][260/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.4413e-01 (8.6963e-01)	Acc@1  78.91 ( 75.04)	Acc@5  96.88 ( 94.67)
03-Mar-22 09:28:20 - Epoch: [19][300/352]	Time  0.140 ( 0.146)	Data  0.002 ( 0.003)	Loss 1.0189e+00 (8.7308e-01)	Acc@1  72.66 ( 74.65)	Acc@5  93.75 ( 94.54)
03-Mar-22 09:28:21 - Epoch: [20][270/352]	Time  0.172 ( 0.163)	Data  0.003 ( 0.003)	Loss 9.6910e-01 (8.7008e-01)	Acc@1  73.44 ( 75.06)	Acc@5  92.97 ( 94.64)
03-Mar-22 09:28:22 - Epoch: [19][310/352]	Time  0.138 ( 0.146)	Data  0.002 ( 0.003)	Loss 8.2403e-01 (8.7205e-01)	Acc@1  81.25 ( 74.69)	Acc@5  95.31 ( 94.57)
03-Mar-22 09:28:23 - Epoch: [19][320/352]	Time  0.156 ( 0.146)	Data  0.002 ( 0.003)	Loss 1.0456e+00 (8.7228e-01)	Acc@1  69.53 ( 74.69)	Acc@5  89.06 ( 94.56)
03-Mar-22 09:28:23 - Epoch: [20][280/352]	Time  0.173 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.5519e-01 (8.7088e-01)	Acc@1  72.66 ( 74.97)	Acc@5  94.53 ( 94.66)
03-Mar-22 09:28:24 - Epoch: [19][330/352]	Time  0.152 ( 0.146)	Data  0.002 ( 0.003)	Loss 9.9747e-01 (8.7460e-01)	Acc@1  70.31 ( 74.65)	Acc@5  92.97 ( 94.53)
03-Mar-22 09:28:25 - Epoch: [20][290/352]	Time  0.153 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.0820e-01 (8.7092e-01)	Acc@1  74.22 ( 74.95)	Acc@5  97.66 ( 94.66)
03-Mar-22 09:28:26 - Epoch: [19][340/352]	Time  0.144 ( 0.146)	Data  0.002 ( 0.003)	Loss 7.5136e-01 (8.7409e-01)	Acc@1  80.47 ( 74.66)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:28:27 - Epoch: [20][300/352]	Time  0.174 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.9760e-01 (8.7184e-01)	Acc@1  76.56 ( 74.92)	Acc@5  94.53 ( 94.65)
03-Mar-22 09:28:27 - Epoch: [19][350/352]	Time  0.156 ( 0.146)	Data  0.002 ( 0.003)	Loss 9.1398e-01 (8.7512e-01)	Acc@1  72.66 ( 74.69)	Acc@5  92.97 ( 94.56)
03-Mar-22 09:28:28 - Test: [ 0/20]	Time  0.325 ( 0.325)	Loss 1.0313e+00 (1.0313e+00)	Acc@1  73.44 ( 73.44)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:28:28 - Epoch: [20][310/352]	Time  0.160 ( 0.164)	Data  0.002 ( 0.003)	Loss 6.9697e-01 (8.7241e-01)	Acc@1  82.03 ( 74.92)	Acc@5  97.66 ( 94.66)
03-Mar-22 09:28:29 - Test: [10/20]	Time  0.106 ( 0.129)	Loss 8.4908e-01 (9.5587e-01)	Acc@1  73.05 ( 73.58)	Acc@5  94.53 ( 93.22)
03-Mar-22 09:28:30 - Epoch: [20][320/352]	Time  0.142 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.5329e-01 (8.7249e-01)	Acc@1  78.12 ( 74.92)	Acc@5  94.53 ( 94.66)
03-Mar-22 09:28:30 -  * Acc@1 73.260 Acc@5 93.460
03-Mar-22 09:28:30 - Best acc at epoch 19: 74.04000091552734
03-Mar-22 09:28:30 - Epoch: [20][  0/352]	Time  0.365 ( 0.365)	Data  0.228 ( 0.228)	Loss 9.9669e-01 (9.9669e-01)	Acc@1  71.88 ( 71.88)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:28:31 - Epoch: [20][330/352]	Time  0.144 ( 0.162)	Data  0.002 ( 0.003)	Loss 9.7235e-01 (8.7470e-01)	Acc@1  67.97 ( 74.88)	Acc@5  93.75 ( 94.62)
03-Mar-22 09:28:32 - Epoch: [20][ 10/352]	Time  0.132 ( 0.160)	Data  0.002 ( 0.022)	Loss 9.4972e-01 (8.9178e-01)	Acc@1  71.88 ( 74.93)	Acc@5  95.31 ( 94.60)
03-Mar-22 09:28:32 - Epoch: [20][340/352]	Time  0.146 ( 0.161)	Data  0.002 ( 0.003)	Loss 7.9241e-01 (8.7463e-01)	Acc@1  77.34 ( 74.86)	Acc@5  95.31 ( 94.65)
03-Mar-22 09:28:33 - Epoch: [20][ 20/352]	Time  0.137 ( 0.152)	Data  0.002 ( 0.013)	Loss 9.0585e-01 (8.7974e-01)	Acc@1  75.00 ( 74.81)	Acc@5  92.19 ( 94.53)
03-Mar-22 09:28:34 - Epoch: [20][350/352]	Time  0.145 ( 0.161)	Data  0.002 ( 0.003)	Loss 7.5283e-01 (8.7250e-01)	Acc@1  75.78 ( 74.92)	Acc@5  96.09 ( 94.67)
03-Mar-22 09:28:34 - Test: [ 0/20]	Time  0.332 ( 0.332)	Loss 9.5214e-01 (9.5214e-01)	Acc@1  71.48 ( 71.48)	Acc@5  93.75 ( 93.75)
03-Mar-22 09:28:35 - Epoch: [20][ 30/352]	Time  0.179 ( 0.146)	Data  0.002 ( 0.010)	Loss 9.2104e-01 (8.8322e-01)	Acc@1  71.88 ( 74.57)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:28:35 - Test: [10/20]	Time  0.092 ( 0.125)	Loss 9.0232e-01 (9.4781e-01)	Acc@1  71.88 ( 72.34)	Acc@5  94.53 ( 93.75)
03-Mar-22 09:28:36 - Epoch: [20][ 40/352]	Time  0.158 ( 0.150)	Data  0.002 ( 0.008)	Loss 7.9950e-01 (8.6014e-01)	Acc@1  76.56 ( 75.04)	Acc@5  95.31 ( 94.76)
03-Mar-22 09:28:36 -  * Acc@1 72.580 Acc@5 93.680
03-Mar-22 09:28:36 - Best acc at epoch 20: 73.97999572753906
03-Mar-22 09:28:37 - Epoch: [21][  0/352]	Time  0.369 ( 0.369)	Data  0.218 ( 0.218)	Loss 9.0287e-01 (9.0287e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.88 ( 96.88)
03-Mar-22 09:28:38 - Epoch: [20][ 50/352]	Time  0.166 ( 0.148)	Data  0.002 ( 0.007)	Loss 1.2895e+00 (8.6954e-01)	Acc@1  58.59 ( 74.72)	Acc@5  89.06 ( 94.70)
03-Mar-22 09:28:38 - Epoch: [21][ 10/352]	Time  0.148 ( 0.173)	Data  0.003 ( 0.022)	Loss 9.1496e-01 (8.2753e-01)	Acc@1  75.00 ( 76.85)	Acc@5  92.97 ( 94.96)
03-Mar-22 09:28:39 - Epoch: [20][ 60/352]	Time  0.144 ( 0.147)	Data  0.002 ( 0.006)	Loss 7.4490e-01 (8.6816e-01)	Acc@1  80.47 ( 74.83)	Acc@5  97.66 ( 94.75)
03-Mar-22 09:28:40 - Epoch: [21][ 20/352]	Time  0.152 ( 0.161)	Data  0.002 ( 0.013)	Loss 9.7082e-01 (8.3886e-01)	Acc@1  74.22 ( 76.41)	Acc@5  94.53 ( 95.20)
03-Mar-22 09:28:41 - Epoch: [20][ 70/352]	Time  0.150 ( 0.148)	Data  0.002 ( 0.006)	Loss 7.2092e-01 (8.6760e-01)	Acc@1  82.81 ( 74.90)	Acc@5  93.75 ( 94.66)
03-Mar-22 09:28:41 - Epoch: [21][ 30/352]	Time  0.153 ( 0.158)	Data  0.002 ( 0.009)	Loss 9.6612e-01 (8.4839e-01)	Acc@1  71.09 ( 75.66)	Acc@5  93.75 ( 95.09)
03-Mar-22 09:28:42 - Epoch: [20][ 80/352]	Time  0.155 ( 0.149)	Data  0.002 ( 0.005)	Loss 8.0445e-01 (8.5972e-01)	Acc@1  72.66 ( 75.18)	Acc@5  95.31 ( 94.79)
03-Mar-22 09:28:43 - Epoch: [21][ 40/352]	Time  0.153 ( 0.157)	Data  0.002 ( 0.008)	Loss 8.8858e-01 (8.5525e-01)	Acc@1  74.22 ( 75.50)	Acc@5  94.53 ( 95.14)
03-Mar-22 09:28:44 - Epoch: [20][ 90/352]	Time  0.149 ( 0.148)	Data  0.002 ( 0.005)	Loss 8.7348e-01 (8.6227e-01)	Acc@1  75.00 ( 75.20)	Acc@5  93.75 ( 94.74)
03-Mar-22 09:28:44 - Epoch: [21][ 50/352]	Time  0.146 ( 0.155)	Data  0.002 ( 0.007)	Loss 8.8284e-01 (8.5807e-01)	Acc@1  78.12 ( 75.64)	Acc@5  93.75 ( 95.07)
03-Mar-22 09:28:45 - Epoch: [20][100/352]	Time  0.163 ( 0.149)	Data  0.002 ( 0.005)	Loss 8.4243e-01 (8.6021e-01)	Acc@1  75.78 ( 75.28)	Acc@5  95.31 ( 94.80)
03-Mar-22 09:28:46 - Epoch: [21][ 60/352]	Time  0.147 ( 0.154)	Data  0.002 ( 0.006)	Loss 6.0172e-01 (8.5205e-01)	Acc@1  78.12 ( 75.61)	Acc@5  99.22 ( 95.08)
03-Mar-22 09:28:47 - Epoch: [20][110/352]	Time  0.153 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.8279e-01 (8.6608e-01)	Acc@1  72.66 ( 75.10)	Acc@5  95.31 ( 94.73)
03-Mar-22 09:28:47 - Epoch: [21][ 70/352]	Time  0.149 ( 0.153)	Data  0.002 ( 0.005)	Loss 1.0027e+00 (8.5325e-01)	Acc@1  75.78 ( 75.66)	Acc@5  92.19 ( 95.06)
03-Mar-22 09:28:48 - Epoch: [20][120/352]	Time  0.162 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.1110e-01 (8.6473e-01)	Acc@1  71.88 ( 75.15)	Acc@5  93.75 ( 94.75)
03-Mar-22 09:28:49 - Epoch: [21][ 80/352]	Time  0.151 ( 0.153)	Data  0.002 ( 0.005)	Loss 1.0655e+00 (8.6320e-01)	Acc@1  67.97 ( 75.38)	Acc@5  90.62 ( 94.90)
03-Mar-22 09:28:50 - Epoch: [20][130/352]	Time  0.152 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.4161e-01 (8.6547e-01)	Acc@1  75.78 ( 75.10)	Acc@5  95.31 ( 94.71)
03-Mar-22 09:28:50 - Epoch: [21][ 90/352]	Time  0.135 ( 0.152)	Data  0.002 ( 0.005)	Loss 8.3994e-01 (8.5728e-01)	Acc@1  75.00 ( 75.49)	Acc@5  93.75 ( 94.88)
03-Mar-22 09:28:51 - Epoch: [20][140/352]	Time  0.161 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.1269e-01 (8.6504e-01)	Acc@1  76.56 ( 75.07)	Acc@5  92.97 ( 94.66)
03-Mar-22 09:28:52 - Epoch: [21][100/352]	Time  0.148 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.6818e-01 (8.5276e-01)	Acc@1  75.00 ( 75.62)	Acc@5  92.97 ( 94.88)
03-Mar-22 09:28:53 - Epoch: [20][150/352]	Time  0.152 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.1505e-01 (8.6629e-01)	Acc@1  70.31 ( 74.96)	Acc@5  97.66 ( 94.67)
03-Mar-22 09:28:53 - Epoch: [21][110/352]	Time  0.140 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.1165e-01 (8.5541e-01)	Acc@1  74.22 ( 75.53)	Acc@5  92.19 ( 94.74)
03-Mar-22 09:28:54 - Epoch: [20][160/352]	Time  0.166 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.1748e-01 (8.6244e-01)	Acc@1  75.00 ( 75.08)	Acc@5  96.09 ( 94.73)
03-Mar-22 09:28:55 - Epoch: [21][120/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0567e+00 (8.5654e-01)	Acc@1  71.88 ( 75.44)	Acc@5  92.97 ( 94.72)
03-Mar-22 09:28:56 - Epoch: [20][170/352]	Time  0.153 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.3078e-01 (8.6438e-01)	Acc@1  74.22 ( 75.01)	Acc@5  96.09 ( 94.72)
03-Mar-22 09:28:56 - Epoch: [21][130/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.004)	Loss 8.1647e-01 (8.5992e-01)	Acc@1  76.56 ( 75.33)	Acc@5  96.88 ( 94.74)
03-Mar-22 09:28:57 - Epoch: [20][180/352]	Time  0.167 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.8979e-01 (8.6403e-01)	Acc@1  76.56 ( 75.00)	Acc@5  92.19 ( 94.72)
03-Mar-22 09:28:58 - Epoch: [21][140/352]	Time  0.147 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.8749e-01 (8.6048e-01)	Acc@1  70.31 ( 75.21)	Acc@5  95.31 ( 94.82)
03-Mar-22 09:28:59 - Epoch: [20][190/352]	Time  0.152 ( 0.149)	Data  0.002 ( 0.004)	Loss 6.5822e-01 (8.6244e-01)	Acc@1  85.94 ( 75.15)	Acc@5  96.09 ( 94.71)
03-Mar-22 09:28:59 - Epoch: [21][150/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.9905e-01 (8.6037e-01)	Acc@1  76.56 ( 75.16)	Acc@5  95.31 ( 94.80)
03-Mar-22 09:29:00 - Epoch: [20][200/352]	Time  0.163 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.2020e-01 (8.6347e-01)	Acc@1  74.22 ( 75.10)	Acc@5  95.31 ( 94.71)
03-Mar-22 09:29:01 - Epoch: [21][160/352]	Time  0.148 ( 0.151)	Data  0.002 ( 0.004)	Loss 7.9454e-01 (8.6092e-01)	Acc@1  78.91 ( 75.16)	Acc@5  96.88 ( 94.79)
03-Mar-22 09:29:02 - Epoch: [20][210/352]	Time  0.149 ( 0.150)	Data  0.003 ( 0.003)	Loss 9.1411e-01 (8.6244e-01)	Acc@1  73.44 ( 75.09)	Acc@5  92.97 ( 94.71)
03-Mar-22 09:29:02 - Epoch: [21][170/352]	Time  0.138 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.0588e-01 (8.6360e-01)	Acc@1  79.69 ( 75.06)	Acc@5  96.09 ( 94.74)
03-Mar-22 09:29:03 - Epoch: [20][220/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.6035e-01 (8.6205e-01)	Acc@1  73.44 ( 75.10)	Acc@5  95.31 ( 94.72)
03-Mar-22 09:29:04 - Epoch: [21][180/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.0247e+00 (8.6488e-01)	Acc@1  69.53 ( 75.04)	Acc@5  93.75 ( 94.69)
03-Mar-22 09:29:05 - Epoch: [20][230/352]	Time  0.149 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.2713e-01 (8.6543e-01)	Acc@1  75.00 ( 75.02)	Acc@5  94.53 ( 94.67)
03-Mar-22 09:29:05 - Epoch: [21][190/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.7719e-01 (8.6753e-01)	Acc@1  75.78 ( 75.00)	Acc@5  93.75 ( 94.61)
03-Mar-22 09:29:06 - Epoch: [20][240/352]	Time  0.149 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.7056e-01 (8.6504e-01)	Acc@1  76.56 ( 75.00)	Acc@5  94.53 ( 94.67)
03-Mar-22 09:29:07 - Epoch: [21][200/352]	Time  0.146 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3513e-01 (8.6666e-01)	Acc@1  78.12 ( 75.05)	Acc@5  95.31 ( 94.66)
03-Mar-22 09:29:08 - Epoch: [20][250/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.8946e-01 (8.6560e-01)	Acc@1  72.66 ( 74.98)	Acc@5  92.19 ( 94.65)
03-Mar-22 09:29:08 - Epoch: [21][210/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.6595e-01 (8.6471e-01)	Acc@1  76.56 ( 75.11)	Acc@5  93.75 ( 94.68)
03-Mar-22 09:29:09 - Epoch: [20][260/352]	Time  0.132 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.1790e-01 (8.6874e-01)	Acc@1  78.91 ( 74.91)	Acc@5  96.88 ( 94.63)
03-Mar-22 09:29:10 - Epoch: [21][220/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.9623e-01 (8.6530e-01)	Acc@1  71.09 ( 75.14)	Acc@5  93.75 ( 94.64)
03-Mar-22 09:29:11 - Epoch: [20][270/352]	Time  0.149 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.7222e-01 (8.6987e-01)	Acc@1  74.22 ( 74.85)	Acc@5  96.88 ( 94.61)
03-Mar-22 09:29:11 - Epoch: [21][230/352]	Time  0.145 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.1349e-01 (8.6602e-01)	Acc@1  72.66 ( 75.08)	Acc@5  93.75 ( 94.64)
03-Mar-22 09:29:12 - Epoch: [20][280/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1442e+00 (8.6928e-01)	Acc@1  69.53 ( 74.91)	Acc@5  94.53 ( 94.62)
03-Mar-22 09:29:13 - Epoch: [21][240/352]	Time  0.136 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0100e+00 (8.6465e-01)	Acc@1  73.44 ( 75.08)	Acc@5  92.97 ( 94.68)
03-Mar-22 09:29:13 - Epoch: [20][290/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.6357e-01 (8.7021e-01)	Acc@1  69.53 ( 74.90)	Acc@5  92.19 ( 94.60)
03-Mar-22 09:29:14 - Epoch: [21][250/352]	Time  0.146 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.9165e-01 (8.6302e-01)	Acc@1  76.56 ( 75.12)	Acc@5  95.31 ( 94.68)
03-Mar-22 09:29:15 - Epoch: [20][300/352]	Time  0.134 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.0323e-01 (8.7090e-01)	Acc@1  78.91 ( 74.87)	Acc@5  96.09 ( 94.59)
03-Mar-22 09:29:16 - Epoch: [21][260/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.5439e-01 (8.6247e-01)	Acc@1  77.34 ( 75.16)	Acc@5  93.75 ( 94.68)
03-Mar-22 09:29:16 - Epoch: [20][310/352]	Time  0.149 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.2519e-01 (8.7148e-01)	Acc@1  76.56 ( 74.85)	Acc@5  96.09 ( 94.59)
03-Mar-22 09:29:17 - Epoch: [21][270/352]	Time  0.144 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.2736e-01 (8.6419e-01)	Acc@1  81.25 ( 75.08)	Acc@5  96.09 ( 94.66)
03-Mar-22 09:29:18 - Epoch: [20][320/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.8832e-01 (8.7287e-01)	Acc@1  70.31 ( 74.83)	Acc@5  92.97 ( 94.56)
03-Mar-22 09:29:18 - Epoch: [21][280/352]	Time  0.146 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.5197e-01 (8.6433e-01)	Acc@1  78.12 ( 75.12)	Acc@5  95.31 ( 94.64)
03-Mar-22 09:29:19 - Epoch: [20][330/352]	Time  0.154 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.5149e-01 (8.7295e-01)	Acc@1  77.34 ( 74.81)	Acc@5  94.53 ( 94.56)
03-Mar-22 09:29:20 - Epoch: [21][290/352]	Time  0.151 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.9480e-01 (8.6266e-01)	Acc@1  73.44 ( 75.19)	Acc@5  95.31 ( 94.64)
03-Mar-22 09:29:21 - Epoch: [20][340/352]	Time  0.152 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.5370e-01 (8.7248e-01)	Acc@1  76.56 ( 74.85)	Acc@5  96.09 ( 94.56)
03-Mar-22 09:29:21 - Epoch: [21][300/352]	Time  0.154 ( 0.149)	Data  0.002 ( 0.003)	Loss 9.0867e-01 (8.6410e-01)	Acc@1  75.00 ( 75.13)	Acc@5  95.31 ( 94.61)
03-Mar-22 09:29:22 - Epoch: [20][350/352]	Time  0.148 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.1778e-01 (8.7342e-01)	Acc@1  78.91 ( 74.84)	Acc@5  95.31 ( 94.56)
03-Mar-22 09:29:23 - Epoch: [21][310/352]	Time  0.095 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.0338e+00 (8.6427e-01)	Acc@1  65.62 ( 75.10)	Acc@5  94.53 ( 94.64)
03-Mar-22 09:29:23 - Test: [ 0/20]	Time  0.375 ( 0.375)	Loss 1.0047e+00 (1.0047e+00)	Acc@1  69.14 ( 69.14)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:29:24 - Test: [10/20]	Time  0.096 ( 0.127)	Loss 7.6748e-01 (9.5346e-01)	Acc@1  79.69 ( 72.02)	Acc@5  96.09 ( 93.57)
03-Mar-22 09:29:24 - Epoch: [21][320/352]	Time  0.142 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.6268e-01 (8.6471e-01)	Acc@1  75.78 ( 75.10)	Acc@5  91.41 ( 94.63)
03-Mar-22 09:29:25 -  * Acc@1 71.900 Acc@5 93.700
03-Mar-22 09:29:25 - Best acc at epoch 20: 74.04000091552734
03-Mar-22 09:29:25 - Epoch: [21][330/352]	Time  0.110 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.9629e-01 (8.6532e-01)	Acc@1  71.88 ( 75.07)	Acc@5  94.53 ( 94.64)
03-Mar-22 09:29:25 - Epoch: [21][  0/352]	Time  0.371 ( 0.371)	Data  0.218 ( 0.218)	Loss 7.7336e-01 (7.7336e-01)	Acc@1  77.34 ( 77.34)	Acc@5  95.31 ( 95.31)
03-Mar-22 09:29:27 - Epoch: [21][340/352]	Time  0.135 ( 0.147)	Data  0.001 ( 0.003)	Loss 7.3039e-01 (8.6453e-01)	Acc@1  78.12 ( 75.11)	Acc@5  94.53 ( 94.64)
03-Mar-22 09:29:27 - Epoch: [21][ 10/352]	Time  0.167 ( 0.178)	Data  0.002 ( 0.022)	Loss 9.2127e-01 (8.5107e-01)	Acc@1  76.56 ( 75.50)	Acc@5  93.75 ( 94.74)
03-Mar-22 09:29:28 - Epoch: [21][350/352]	Time  0.149 ( 0.147)	Data  0.001 ( 0.003)	Loss 8.0513e-01 (8.6476e-01)	Acc@1  78.12 ( 75.10)	Acc@5  96.88 ( 94.64)
03-Mar-22 09:29:29 - Epoch: [21][ 20/352]	Time  0.133 ( 0.166)	Data  0.003 ( 0.013)	Loss 9.6130e-01 (8.6597e-01)	Acc@1  73.44 ( 75.37)	Acc@5  92.19 ( 94.38)
03-Mar-22 09:29:29 - Test: [ 0/20]	Time  0.361 ( 0.361)	Loss 1.0340e+00 (1.0340e+00)	Acc@1  67.97 ( 67.97)	Acc@5  92.97 ( 92.97)
03-Mar-22 09:29:30 - Test: [10/20]	Time  0.111 ( 0.133)	Loss 7.9253e-01 (9.6260e-01)	Acc@1  77.34 ( 72.59)	Acc@5  95.70 ( 93.29)
03-Mar-22 09:29:30 - Epoch: [21][ 30/352]	Time  0.157 ( 0.161)	Data  0.002 ( 0.009)	Loss 9.1562e-01 (8.7108e-01)	Acc@1  77.34 ( 75.00)	Acc@5  91.41 ( 94.15)
03-Mar-22 09:29:31 -  * Acc@1 73.000 Acc@5 93.340
03-Mar-22 09:29:31 - Best acc at epoch 21: 73.97999572753906
03-Mar-22 09:29:31 - Epoch: [22][  0/352]	Time  0.398 ( 0.398)	Data  0.223 ( 0.223)	Loss 8.2066e-01 (8.2066e-01)	Acc@1  76.56 ( 76.56)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:29:31 - Epoch: [21][ 40/352]	Time  0.149 ( 0.156)	Data  0.002 ( 0.007)	Loss 9.0566e-01 (8.7798e-01)	Acc@1  71.09 ( 74.52)	Acc@5  92.19 ( 94.15)
03-Mar-22 09:29:33 - Epoch: [22][ 10/352]	Time  0.144 ( 0.168)	Data  0.002 ( 0.022)	Loss 1.1677e+00 (8.9726e-01)	Acc@1  61.72 ( 73.79)	Acc@5  90.62 ( 94.11)
03-Mar-22 09:29:33 - Epoch: [21][ 50/352]	Time  0.165 ( 0.156)	Data  0.002 ( 0.006)	Loss 8.9880e-01 (8.7021e-01)	Acc@1  76.56 ( 74.85)	Acc@5  92.97 ( 94.21)
03-Mar-22 09:29:34 - Epoch: [22][ 20/352]	Time  0.147 ( 0.156)	Data  0.002 ( 0.013)	Loss 7.0258e-01 (8.8348e-01)	Acc@1  78.12 ( 75.07)	Acc@5  98.44 ( 94.05)
03-Mar-22 09:29:34 - Epoch: [21][ 60/352]	Time  0.154 ( 0.154)	Data  0.002 ( 0.006)	Loss 6.8335e-01 (8.6330e-01)	Acc@1  79.69 ( 75.14)	Acc@5  96.09 ( 94.30)
03-Mar-22 09:29:36 - Epoch: [22][ 30/352]	Time  0.143 ( 0.152)	Data  0.002 ( 0.009)	Loss 9.7983e-01 (8.6549e-01)	Acc@1  71.88 ( 75.30)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:29:36 - Epoch: [21][ 70/352]	Time  0.156 ( 0.154)	Data  0.002 ( 0.005)	Loss 7.1681e-01 (8.6275e-01)	Acc@1  78.91 ( 74.91)	Acc@5  96.88 ( 94.38)
03-Mar-22 09:29:37 - Epoch: [22][ 40/352]	Time  0.133 ( 0.151)	Data  0.002 ( 0.008)	Loss 1.0003e+00 (8.6605e-01)	Acc@1  71.09 ( 75.21)	Acc@5  92.19 ( 94.59)
03-Mar-22 09:29:37 - Epoch: [21][ 80/352]	Time  0.157 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.2182e-01 (8.6477e-01)	Acc@1  76.56 ( 74.81)	Acc@5  97.66 ( 94.51)
03-Mar-22 09:29:39 - Epoch: [22][ 50/352]	Time  0.136 ( 0.150)	Data  0.002 ( 0.007)	Loss 8.3144e-01 (8.6616e-01)	Acc@1  75.00 ( 74.95)	Acc@5  93.75 ( 94.62)
03-Mar-22 09:29:39 - Epoch: [21][ 90/352]	Time  0.140 ( 0.154)	Data  0.002 ( 0.005)	Loss 8.9361e-01 (8.5605e-01)	Acc@1  76.56 ( 75.07)	Acc@5  92.97 ( 94.64)
03-Mar-22 09:29:40 - Epoch: [22][ 60/352]	Time  0.134 ( 0.149)	Data  0.002 ( 0.006)	Loss 8.4439e-01 (8.6200e-01)	Acc@1  78.91 ( 75.12)	Acc@5  95.31 ( 94.67)
03-Mar-22 09:29:41 - Epoch: [21][100/352]	Time  0.162 ( 0.154)	Data  0.002 ( 0.004)	Loss 7.6210e-01 (8.5486e-01)	Acc@1  77.34 ( 75.12)	Acc@5  98.44 ( 94.71)
03-Mar-22 09:29:42 - Epoch: [22][ 70/352]	Time  0.149 ( 0.149)	Data  0.002 ( 0.005)	Loss 7.3792e-01 (8.5771e-01)	Acc@1  79.69 ( 75.30)	Acc@5  96.88 ( 94.84)
03-Mar-22 09:29:42 - Epoch: [21][110/352]	Time  0.156 ( 0.153)	Data  0.002 ( 0.004)	Loss 6.3349e-01 (8.5698e-01)	Acc@1  82.81 ( 75.18)	Acc@5  96.88 ( 94.66)
03-Mar-22 09:29:43 - Epoch: [22][ 80/352]	Time  0.152 ( 0.148)	Data  0.002 ( 0.005)	Loss 1.0108e+00 (8.6218e-01)	Acc@1  74.22 ( 75.17)	Acc@5  94.53 ( 94.83)
03-Mar-22 09:29:44 - Epoch: [21][120/352]	Time  0.154 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.8850e-01 (8.5989e-01)	Acc@1  74.22 ( 75.19)	Acc@5  97.66 ( 94.67)
03-Mar-22 09:29:44 - Epoch: [22][ 90/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.005)	Loss 9.0197e-01 (8.5630e-01)	Acc@1  67.19 ( 75.29)	Acc@5  96.09 ( 94.88)
03-Mar-22 09:29:45 - Epoch: [21][130/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.8820e-01 (8.5989e-01)	Acc@1  78.12 ( 75.22)	Acc@5  94.53 ( 94.62)
03-Mar-22 09:29:46 - Epoch: [22][100/352]	Time  0.137 ( 0.148)	Data  0.002 ( 0.004)	Loss 1.0994e+00 (8.5917e-01)	Acc@1  73.44 ( 75.25)	Acc@5  88.28 ( 94.84)
03-Mar-22 09:29:47 - Epoch: [21][140/352]	Time  0.146 ( 0.153)	Data  0.003 ( 0.004)	Loss 9.7430e-01 (8.5997e-01)	Acc@1  67.19 ( 75.21)	Acc@5  90.62 ( 94.61)
03-Mar-22 09:29:47 - Epoch: [22][110/352]	Time  0.154 ( 0.148)	Data  0.002 ( 0.004)	Loss 7.8405e-01 (8.5965e-01)	Acc@1  75.00 ( 75.18)	Acc@5  94.53 ( 94.84)
03-Mar-22 09:29:48 - Epoch: [21][150/352]	Time  0.154 ( 0.153)	Data  0.002 ( 0.004)	Loss 9.3104e-01 (8.5860e-01)	Acc@1  75.00 ( 75.29)	Acc@5  93.75 ( 94.65)
03-Mar-22 09:29:49 - Epoch: [22][120/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.4325e-01 (8.5895e-01)	Acc@1  70.31 ( 75.19)	Acc@5  94.53 ( 94.91)
03-Mar-22 09:29:50 - Epoch: [21][160/352]	Time  0.132 ( 0.152)	Data  0.002 ( 0.004)	Loss 9.5790e-01 (8.6323e-01)	Acc@1  73.44 ( 75.22)	Acc@5  94.53 ( 94.59)
03-Mar-22 09:29:50 - Epoch: [22][130/352]	Time  0.135 ( 0.148)	Data  0.002 ( 0.004)	Loss 9.0847e-01 (8.6160e-01)	Acc@1  73.44 ( 75.15)	Acc@5  91.41 ( 94.87)
03-Mar-22 09:29:51 - Epoch: [21][170/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0737e+00 (8.6408e-01)	Acc@1  70.31 ( 75.16)	Acc@5  93.75 ( 94.61)
03-Mar-22 09:29:52 - Epoch: [22][140/352]	Time  0.157 ( 0.149)	Data  0.002 ( 0.004)	Loss 7.3346e-01 (8.5870e-01)	Acc@1  78.91 ( 75.21)	Acc@5  96.88 ( 94.90)
03-Mar-22 09:29:53 - Epoch: [21][180/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.004)	Loss 7.7563e-01 (8.6448e-01)	Acc@1  75.00 ( 75.09)	Acc@5  95.31 ( 94.62)
03-Mar-22 09:29:53 - Epoch: [22][150/352]	Time  0.151 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.2715e-01 (8.5835e-01)	Acc@1  75.00 ( 75.21)	Acc@5  95.31 ( 94.91)
03-Mar-22 09:29:54 - Epoch: [21][190/352]	Time  0.154 ( 0.152)	Data  0.002 ( 0.003)	Loss 7.5317e-01 (8.6360e-01)	Acc@1  76.56 ( 75.11)	Acc@5  96.88 ( 94.63)
03-Mar-22 09:29:55 - Epoch: [22][160/352]	Time  0.151 ( 0.149)	Data  0.003 ( 0.004)	Loss 8.4052e-01 (8.5978e-01)	Acc@1  78.12 ( 75.16)	Acc@5  94.53 ( 94.89)
03-Mar-22 09:29:56 - Epoch: [21][200/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.0105e+00 (8.6333e-01)	Acc@1  75.00 ( 75.12)	Acc@5  92.97 ( 94.64)
03-Mar-22 09:29:56 - Epoch: [22][170/352]	Time  0.152 ( 0.149)	Data  0.002 ( 0.004)	Loss 9.1621e-01 (8.5879e-01)	Acc@1  71.88 ( 75.21)	Acc@5  93.75 ( 94.88)
03-Mar-22 09:29:57 - Epoch: [21][210/352]	Time  0.140 ( 0.152)	Data  0.002 ( 0.003)	Loss 9.7936e-01 (8.6542e-01)	Acc@1  67.97 ( 75.03)	Acc@5  95.31 ( 94.61)
03-Mar-22 09:29:58 - Epoch: [22][180/352]	Time  0.135 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.7955e-01 (8.6133e-01)	Acc@1  76.56 ( 75.11)	Acc@5  94.53 ( 94.85)
03-Mar-22 09:29:59 - Epoch: [21][220/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.3005e-01 (8.6757e-01)	Acc@1  75.78 ( 75.05)	Acc@5  96.09 ( 94.59)
03-Mar-22 09:29:59 - Epoch: [22][190/352]	Time  0.155 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.6342e-01 (8.6268e-01)	Acc@1  73.44 ( 75.11)	Acc@5  89.84 ( 94.81)
03-Mar-22 09:30:00 - Epoch: [21][230/352]	Time  0.153 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.1070e-01 (8.6814e-01)	Acc@1  75.00 ( 75.02)	Acc@5  94.53 ( 94.58)
03-Mar-22 09:30:01 - Epoch: [22][200/352]	Time  0.148 ( 0.148)	Data  0.002 ( 0.003)	Loss 7.0372e-01 (8.6203e-01)	Acc@1  78.12 ( 75.12)	Acc@5  96.88 ( 94.83)
03-Mar-22 09:30:02 - Epoch: [21][240/352]	Time  0.134 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.8843e-01 (8.6962e-01)	Acc@1  74.22 ( 74.97)	Acc@5  97.66 ( 94.58)
03-Mar-22 09:30:02 - Epoch: [22][210/352]	Time  0.145 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.0016e-01 (8.6203e-01)	Acc@1  73.44 ( 75.07)	Acc@5  93.75 ( 94.85)
03-Mar-22 09:30:03 - Epoch: [21][250/352]	Time  0.130 ( 0.151)	Data  0.002 ( 0.003)	Loss 6.6495e-01 (8.6630e-01)	Acc@1  80.47 ( 75.03)	Acc@5  98.44 ( 94.65)
03-Mar-22 09:30:04 - Epoch: [22][220/352]	Time  0.147 ( 0.148)	Data  0.003 ( 0.003)	Loss 1.0184e+00 (8.6094e-01)	Acc@1  70.31 ( 75.08)	Acc@5  92.97 ( 94.86)
03-Mar-22 09:30:04 - Epoch: [21][260/352]	Time  0.147 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.0874e-01 (8.6439e-01)	Acc@1  74.22 ( 75.10)	Acc@5  93.75 ( 94.66)
03-Mar-22 09:30:05 - Epoch: [22][230/352]	Time  0.148 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.2070e-01 (8.6100e-01)	Acc@1  71.09 ( 75.07)	Acc@5  93.75 ( 94.85)
03-Mar-22 09:30:06 - Epoch: [21][270/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0101e+00 (8.6504e-01)	Acc@1  70.31 ( 75.04)	Acc@5  91.41 ( 94.67)
03-Mar-22 09:30:07 - Epoch: [22][240/352]	Time  0.139 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.7418e-01 (8.6093e-01)	Acc@1  70.31 ( 75.02)	Acc@5  92.19 ( 94.86)
03-Mar-22 09:30:07 - Epoch: [21][280/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.2146e-01 (8.6614e-01)	Acc@1  75.00 ( 74.99)	Acc@5  97.66 ( 94.73)
03-Mar-22 09:30:08 - Epoch: [22][250/352]	Time  0.152 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.4634e-01 (8.6019e-01)	Acc@1  76.56 ( 75.08)	Acc@5  95.31 ( 94.86)
03-Mar-22 09:30:09 - Epoch: [21][290/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1857e-01 (8.6712e-01)	Acc@1  79.69 ( 75.01)	Acc@5  94.53 ( 94.69)
03-Mar-22 09:30:10 - Epoch: [22][260/352]	Time  0.136 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.7150e-01 (8.6281e-01)	Acc@1  75.00 ( 75.04)	Acc@5  91.41 ( 94.83)
03-Mar-22 09:30:10 - Epoch: [21][300/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3713e-01 (8.6664e-01)	Acc@1  76.56 ( 74.96)	Acc@5  94.53 ( 94.74)
03-Mar-22 09:30:11 - Epoch: [22][270/352]	Time  0.153 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.0756e-01 (8.6313e-01)	Acc@1  75.78 ( 75.04)	Acc@5  93.75 ( 94.83)
03-Mar-22 09:30:12 - Epoch: [21][310/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0530e+00 (8.6947e-01)	Acc@1  70.31 ( 74.89)	Acc@5  92.19 ( 94.70)
03-Mar-22 09:30:13 - Epoch: [22][280/352]	Time  0.156 ( 0.148)	Data  0.003 ( 0.003)	Loss 8.1424e-01 (8.6175e-01)	Acc@1  79.69 ( 75.09)	Acc@5  93.75 ( 94.84)
03-Mar-22 09:30:13 - Epoch: [21][320/352]	Time  0.128 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.9957e-01 (8.7033e-01)	Acc@1  76.56 ( 74.86)	Acc@5  96.09 ( 94.70)
03-Mar-22 09:30:14 - Epoch: [22][290/352]	Time  0.148 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.3282e-01 (8.6054e-01)	Acc@1  74.22 ( 75.10)	Acc@5  95.31 ( 94.85)
03-Mar-22 09:30:15 - Epoch: [21][330/352]	Time  0.147 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1918e-01 (8.7198e-01)	Acc@1  78.91 ( 74.81)	Acc@5  94.53 ( 94.68)
03-Mar-22 09:30:16 - Epoch: [22][300/352]	Time  0.140 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.1351e-01 (8.6025e-01)	Acc@1  77.34 ( 75.11)	Acc@5  94.53 ( 94.83)
03-Mar-22 09:30:16 - Epoch: [21][340/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.3034e-01 (8.7104e-01)	Acc@1  77.34 ( 74.80)	Acc@5  96.88 ( 94.69)
03-Mar-22 09:30:17 - Epoch: [22][310/352]	Time  0.150 ( 0.148)	Data  0.002 ( 0.003)	Loss 8.8513e-01 (8.6038e-01)	Acc@1  77.34 ( 75.13)	Acc@5  94.53 ( 94.82)
03-Mar-22 09:30:18 - Epoch: [21][350/352]	Time  0.148 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0412e+00 (8.7035e-01)	Acc@1  67.19 ( 74.79)	Acc@5  92.19 ( 94.71)
03-Mar-22 09:30:18 - Epoch: [22][320/352]	Time  0.118 ( 0.148)	Data  0.002 ( 0.003)	Loss 7.9669e-01 (8.6033e-01)	Acc@1  80.47 ( 75.15)	Acc@5  95.31 ( 94.80)
03-Mar-22 09:30:18 - Test: [ 0/20]	Time  0.359 ( 0.359)	Loss 1.0808e+00 (1.0808e+00)	Acc@1  69.14 ( 69.14)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:30:19 - Test: [10/20]	Time  0.094 ( 0.127)	Loss 8.7151e-01 (9.5311e-01)	Acc@1  72.27 ( 72.44)	Acc@5  95.70 ( 93.36)
03-Mar-22 09:30:20 - Epoch: [22][330/352]	Time  0.133 ( 0.147)	Data  0.001 ( 0.003)	Loss 9.2514e-01 (8.5969e-01)	Acc@1  75.00 ( 75.16)	Acc@5  92.97 ( 94.81)
03-Mar-22 09:30:20 -  * Acc@1 72.700 Acc@5 93.400
03-Mar-22 09:30:21 - Best acc at epoch 21: 74.04000091552734
03-Mar-22 09:30:21 - Epoch: [22][340/352]	Time  0.106 ( 0.146)	Data  0.002 ( 0.003)	Loss 1.0083e+00 (8.6141e-01)	Acc@1  73.44 ( 75.14)	Acc@5  91.41 ( 94.76)
03-Mar-22 09:30:21 - Epoch: [22][  0/352]	Time  0.390 ( 0.390)	Data  0.242 ( 0.242)	Loss 9.5159e-01 (9.5159e-01)	Acc@1  76.56 ( 76.56)	Acc@5  90.62 ( 90.62)
03-Mar-22 09:30:22 - Epoch: [22][350/352]	Time  0.136 ( 0.146)	Data  0.001 ( 0.003)	Loss 9.3999e-01 (8.6230e-01)	Acc@1  72.66 ( 75.13)	Acc@5  92.97 ( 94.72)
03-Mar-22 09:30:22 - Epoch: [22][ 10/352]	Time  0.145 ( 0.167)	Data  0.002 ( 0.024)	Loss 8.6034e-01 (8.8857e-01)	Acc@1  72.66 ( 74.36)	Acc@5  96.88 ( 94.18)
03-Mar-22 09:30:23 - Test: [ 0/20]	Time  0.374 ( 0.374)	Loss 9.7895e-01 (9.7895e-01)	Acc@1  71.88 ( 71.88)	Acc@5  92.58 ( 92.58)
03-Mar-22 09:30:24 - Epoch: [22][ 20/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.013)	Loss 7.5429e-01 (8.8471e-01)	Acc@1  79.69 ( 74.63)	Acc@5  95.31 ( 94.53)
03-Mar-22 09:30:24 - Test: [10/20]	Time  0.094 ( 0.124)	Loss 9.0476e-01 (9.4640e-01)	Acc@1  72.66 ( 72.34)	Acc@5  94.92 ( 93.54)
03-Mar-22 09:30:25 -  * Acc@1 72.280 Acc@5 93.620
03-Mar-22 09:30:25 - Best acc at epoch 22: 73.97999572753906
03-Mar-22 09:30:25 - Epoch: [22][ 30/352]	Time  0.118 ( 0.149)	Data  0.002 ( 0.010)	Loss 8.2062e-01 (8.6888e-01)	Acc@1  77.34 ( 74.87)	Acc@5  94.53 ( 94.78)
03-Mar-22 09:30:25 - Epoch: [23][  0/352]	Time  0.374 ( 0.374)	Data  0.228 ( 0.228)	Loss 9.2306e-01 (9.2306e-01)	Acc@1  71.09 ( 71.09)	Acc@5  94.53 ( 94.53)
03-Mar-22 09:30:27 - Epoch: [22][ 40/352]	Time  0.158 ( 0.149)	Data  0.002 ( 0.008)	Loss 8.9937e-01 (8.8859e-01)	Acc@1  72.66 ( 74.60)	Acc@5  96.88 ( 94.47)
03-Mar-22 09:30:27 - Epoch: [23][ 10/352]	Time  0.152 ( 0.170)	Data  0.002 ( 0.023)	Loss 7.8765e-01 (8.8304e-01)	Acc@1  77.34 ( 75.21)	Acc@5  94.53 ( 94.11)
03-Mar-22 09:30:28 - Epoch: [23][ 20/352]	Time  0.141 ( 0.156)	Data  0.002 ( 0.013)	Loss 7.8644e-01 (8.7558e-01)	Acc@1  74.22 ( 74.37)	Acc@5  96.88 ( 94.68)
03-Mar-22 09:30:28 - Epoch: [22][ 50/352]	Time  0.151 ( 0.150)	Data  0.003 ( 0.007)	Loss 7.6245e-01 (8.7392e-01)	Acc@1  75.78 ( 74.97)	Acc@5  97.66 ( 94.81)
03-Mar-22 09:30:30 - Epoch: [23][ 30/352]	Time  0.151 ( 0.155)	Data  0.002 ( 0.009)	Loss 9.9355e-01 (8.6637e-01)	Acc@1  73.44 ( 74.80)	Acc@5  93.75 ( 94.88)
03-Mar-22 09:30:30 - Epoch: [22][ 60/352]	Time  0.150 ( 0.150)	Data  0.003 ( 0.006)	Loss 9.2775e-01 (8.7750e-01)	Acc@1  78.12 ( 75.12)	Acc@5  96.09 ( 94.72)
03-Mar-22 09:30:31 - Epoch: [23][ 40/352]	Time  0.137 ( 0.153)	Data  0.002 ( 0.008)	Loss 9.0498e-01 (8.6798e-01)	Acc@1  68.75 ( 74.89)	Acc@5  92.97 ( 94.72)
03-Mar-22 09:30:31 - Epoch: [22][ 70/352]	Time  0.154 ( 0.150)	Data  0.002 ( 0.006)	Loss 9.4548e-01 (8.7170e-01)	Acc@1  73.44 ( 75.19)	Acc@5  92.97 ( 94.81)
03-Mar-22 09:30:33 - Epoch: [23][ 50/352]	Time  0.134 ( 0.151)	Data  0.002 ( 0.007)	Loss 9.5192e-01 (8.6931e-01)	Acc@1  71.88 ( 74.92)	Acc@5  92.97 ( 94.68)
03-Mar-22 09:30:33 - Epoch: [22][ 80/352]	Time  0.158 ( 0.151)	Data  0.002 ( 0.005)	Loss 8.2285e-01 (8.6755e-01)	Acc@1  74.22 ( 75.23)	Acc@5  95.31 ( 94.81)
03-Mar-22 09:30:34 - Epoch: [23][ 60/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.006)	Loss 8.1515e-01 (8.6199e-01)	Acc@1  80.47 ( 75.08)	Acc@5  93.75 ( 94.81)
03-Mar-22 09:30:34 - Epoch: [22][ 90/352]	Time  0.148 ( 0.150)	Data  0.002 ( 0.005)	Loss 9.4838e-01 (8.6256e-01)	Acc@1  71.88 ( 75.46)	Acc@5  92.97 ( 94.84)
03-Mar-22 09:30:36 - Epoch: [23][ 70/352]	Time  0.134 ( 0.150)	Data  0.002 ( 0.005)	Loss 8.2953e-01 (8.6261e-01)	Acc@1  75.78 ( 74.97)	Acc@5  96.09 ( 94.82)
03-Mar-22 09:30:36 - Epoch: [22][100/352]	Time  0.172 ( 0.150)	Data  0.003 ( 0.005)	Loss 8.8901e-01 (8.6875e-01)	Acc@1  80.47 ( 75.32)	Acc@5  94.53 ( 94.71)
03-Mar-22 09:30:37 - Epoch: [23][ 80/352]	Time  0.147 ( 0.149)	Data  0.002 ( 0.005)	Loss 8.7312e-01 (8.6690e-01)	Acc@1  73.44 ( 74.74)	Acc@5  95.31 ( 94.82)
03-Mar-22 09:30:37 - Epoch: [22][110/352]	Time  0.145 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.6322e-01 (8.6683e-01)	Acc@1  76.56 ( 75.44)	Acc@5  92.97 ( 94.71)
03-Mar-22 09:30:38 - Epoch: [23][ 90/352]	Time  0.134 ( 0.148)	Data  0.002 ( 0.005)	Loss 1.0618e+00 (8.7639e-01)	Acc@1  67.97 ( 74.49)	Acc@5  90.62 ( 94.69)
03-Mar-22 09:30:39 - Epoch: [22][120/352]	Time  0.156 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.0017e+00 (8.6642e-01)	Acc@1  74.22 ( 75.46)	Acc@5  92.97 ( 94.71)
03-Mar-22 09:30:40 - Epoch: [23][100/352]	Time  0.138 ( 0.148)	Data  0.002 ( 0.004)	Loss 7.4615e-01 (8.7068e-01)	Acc@1  78.91 ( 74.78)	Acc@5  97.66 ( 94.76)
03-Mar-22 09:30:40 - Epoch: [22][130/352]	Time  0.152 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.7823e-01 (8.7036e-01)	Acc@1  74.22 ( 75.32)	Acc@5  92.19 ( 94.70)
03-Mar-22 09:30:41 - Epoch: [23][110/352]	Time  0.139 ( 0.148)	Data  0.002 ( 0.004)	Loss 7.0499e-01 (8.6774e-01)	Acc@1  78.91 ( 74.93)	Acc@5  98.44 ( 94.80)
03-Mar-22 09:30:42 - Epoch: [22][140/352]	Time  0.157 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.1720e-01 (8.6744e-01)	Acc@1  75.78 ( 75.37)	Acc@5  93.75 ( 94.74)
03-Mar-22 09:30:43 - Epoch: [23][120/352]	Time  0.152 ( 0.148)	Data  0.002 ( 0.004)	Loss 7.7609e-01 (8.6892e-01)	Acc@1  77.34 ( 74.88)	Acc@5  93.75 ( 94.79)
03-Mar-22 09:30:43 - Epoch: [22][150/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.1073e-01 (8.6659e-01)	Acc@1  70.31 ( 75.32)	Acc@5  94.53 ( 94.75)
03-Mar-22 09:30:44 - Epoch: [23][130/352]	Time  0.144 ( 0.148)	Data  0.003 ( 0.004)	Loss 9.7439e-01 (8.6216e-01)	Acc@1  70.31 ( 75.07)	Acc@5  96.09 ( 94.90)
03-Mar-22 09:30:45 - Epoch: [22][160/352]	Time  0.151 ( 0.151)	Data  0.002 ( 0.004)	Loss 7.0769e-01 (8.6004e-01)	Acc@1  78.91 ( 75.54)	Acc@5  96.88 ( 94.83)
03-Mar-22 09:30:46 - Epoch: [23][140/352]	Time  0.144 ( 0.147)	Data  0.002 ( 0.004)	Loss 7.4583e-01 (8.6149e-01)	Acc@1  82.03 ( 75.02)	Acc@5  96.88 ( 94.91)
03-Mar-22 09:30:46 - Epoch: [22][170/352]	Time  0.157 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.0143e+00 (8.6154e-01)	Acc@1  68.75 ( 75.42)	Acc@5  92.97 ( 94.80)
03-Mar-22 09:30:47 - Epoch: [23][150/352]	Time  0.154 ( 0.147)	Data  0.002 ( 0.004)	Loss 6.8811e-01 (8.6368e-01)	Acc@1  78.91 ( 75.03)	Acc@5  96.09 ( 94.83)
03-Mar-22 09:30:48 - Epoch: [22][180/352]	Time  0.165 ( 0.151)	Data  0.002 ( 0.004)	Loss 9.1894e-01 (8.6076e-01)	Acc@1  75.00 ( 75.45)	Acc@5  95.31 ( 94.82)
03-Mar-22 09:30:49 - Epoch: [23][160/352]	Time  0.145 ( 0.147)	Data  0.002 ( 0.004)	Loss 8.6345e-01 (8.6046e-01)	Acc@1  74.22 ( 75.05)	Acc@5  92.19 ( 94.81)
03-Mar-22 09:30:49 - Epoch: [22][190/352]	Time  0.158 ( 0.151)	Data  0.003 ( 0.004)	Loss 8.1887e-01 (8.6028e-01)	Acc@1  75.00 ( 75.46)	Acc@5  95.31 ( 94.79)
03-Mar-22 09:30:50 - Epoch: [23][170/352]	Time  0.127 ( 0.147)	Data  0.002 ( 0.004)	Loss 7.3783e-01 (8.6065e-01)	Acc@1  78.91 ( 75.03)	Acc@5  96.88 ( 94.84)
03-Mar-22 09:30:51 - Epoch: [22][200/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.004)	Loss 8.4414e-01 (8.6232e-01)	Acc@1  72.66 ( 75.38)	Acc@5  94.53 ( 94.75)
03-Mar-22 09:30:52 - Epoch: [23][180/352]	Time  0.145 ( 0.147)	Data  0.002 ( 0.003)	Loss 1.0122e+00 (8.6277e-01)	Acc@1  72.66 ( 74.95)	Acc@5  93.75 ( 94.82)
03-Mar-22 09:30:52 - Epoch: [22][210/352]	Time  0.151 ( 0.151)	Data  0.003 ( 0.003)	Loss 9.3500e-01 (8.6117e-01)	Acc@1  72.66 ( 75.43)	Acc@5  97.66 ( 94.77)
03-Mar-22 09:30:53 - Epoch: [23][190/352]	Time  0.140 ( 0.147)	Data  0.002 ( 0.003)	Loss 9.9764e-01 (8.6487e-01)	Acc@1  70.31 ( 74.89)	Acc@5  91.41 ( 94.81)
03-Mar-22 09:30:54 - Epoch: [22][220/352]	Time  0.162 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.3417e-01 (8.6130e-01)	Acc@1  77.34 ( 75.42)	Acc@5  96.88 ( 94.78)
03-Mar-22 09:30:54 - Epoch: [23][200/352]	Time  0.139 ( 0.147)	Data  0.002 ( 0.003)	Loss 7.4597e-01 (8.6666e-01)	Acc@1  78.12 ( 74.89)	Acc@5  97.66 ( 94.75)
03-Mar-22 09:30:55 - Epoch: [22][230/352]	Time  0.131 ( 0.151)	Data  0.002 ( 0.003)	Loss 8.1783e-01 (8.6138e-01)	Acc@1  75.00 ( 75.42)	Acc@5  95.31 ( 94.78)
03-Mar-22 09:30:56 - Epoch: [23][210/352]	Time  0.140 ( 0.147)	Data  0.002 ( 0.003)	Loss 7.7749e-01 (8.6500e-01)	Acc@1  78.91 ( 74.97)	Acc@5  94.53 ( 94.72)
03-Mar-22 09:30:57 - Epoch: [22][240/352]	Time  0.155 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0718e+00 (8.6445e-01)	Acc@1  64.84 ( 75.32)	Acc@5  93.75 ( 94.79)
03-Mar-22 09:30:57 - Epoch: [23][220/352]	Time  0.140 ( 0.147)	Data  0.002 ( 0.003)	Loss 8.1269e-01 (8.6449e-01)	Acc@1  75.78 ( 74.96)	Acc@5  95.31 ( 94.72)
03-Mar-22 09:32:40 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:32:40 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:32:40 - match all modules defined in bit_config: False
03-Mar-22 09:32:40 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:32:45 - Epoch: [0][  0/352]	Time  0.407 ( 0.407)	Data  0.230 ( 0.230)	Loss 3.0511e-01 (3.0511e-01)	Acc@1  91.41 ( 91.41)	Acc@5  98.44 ( 98.44)
03-Mar-22 09:32:46 - Epoch: [0][ 10/352]	Time  0.128 ( 0.153)	Data  0.002 ( 0.023)	Loss 2.8913e-01 (3.0537e-01)	Acc@1  92.97 ( 91.62)	Acc@5  99.22 ( 99.36)
03-Mar-22 09:32:47 - Epoch: [0][ 20/352]	Time  0.134 ( 0.141)	Data  0.002 ( 0.013)	Loss 3.8745e-01 (2.9305e-01)	Acc@1  87.50 ( 91.85)	Acc@5  98.44 ( 99.59)
03-Mar-22 09:32:49 - Epoch: [0][ 30/352]	Time  0.139 ( 0.138)	Data  0.002 ( 0.010)	Loss 3.3961e-01 (2.7593e-01)	Acc@1  88.28 ( 92.14)	Acc@5 100.00 ( 99.65)
03-Mar-22 09:32:50 - Epoch: [0][ 40/352]	Time  0.133 ( 0.137)	Data  0.002 ( 0.008)	Loss 2.5080e-01 (2.6919e-01)	Acc@1  92.19 ( 92.07)	Acc@5 100.00 ( 99.73)
03-Mar-22 09:32:51 - Epoch: [0][ 50/352]	Time  0.140 ( 0.136)	Data  0.002 ( 0.007)	Loss 1.6434e-01 (2.6213e-01)	Acc@1  95.31 ( 92.17)	Acc@5 100.00 ( 99.77)
03-Mar-22 09:32:53 - Epoch: [0][ 60/352]	Time  0.126 ( 0.136)	Data  0.003 ( 0.006)	Loss 2.4038e-01 (2.5864e-01)	Acc@1  91.41 ( 92.16)	Acc@5 100.00 ( 99.78)
03-Mar-22 09:32:59 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:32:59 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:32:59 - match all modules defined in bit_config: False
03-Mar-22 09:32:59 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:33:03 - Epoch: [0][  0/352]	Time  0.439 ( 0.439)	Data  0.260 ( 0.260)	Loss 3.2302e-01 (3.2302e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
03-Mar-22 09:33:05 - Epoch: [0][ 10/352]	Time  0.127 ( 0.166)	Data  0.002 ( 0.026)	Loss 2.8748e-01 (2.9797e-01)	Acc@1  90.62 ( 91.34)	Acc@5 100.00 ( 99.72)
03-Mar-22 09:33:06 - Epoch: [0][ 20/352]	Time  0.124 ( 0.153)	Data  0.002 ( 0.014)	Loss 2.4996e-01 (2.8716e-01)	Acc@1  92.19 ( 91.63)	Acc@5 100.00 ( 99.67)
03-Mar-22 09:33:07 - Epoch: [0][ 30/352]	Time  0.142 ( 0.146)	Data  0.002 ( 0.010)	Loss 2.2613e-01 (2.7712e-01)	Acc@1  93.75 ( 91.78)	Acc@5 100.00 ( 99.70)
03-Mar-22 09:33:08 - Epoch: [0][ 40/352]	Time  0.124 ( 0.141)	Data  0.002 ( 0.008)	Loss 2.7349e-01 (2.6950e-01)	Acc@1  89.84 ( 92.05)	Acc@5  99.22 ( 99.70)
03-Mar-22 09:33:10 - Epoch: [0][ 50/352]	Time  0.123 ( 0.139)	Data  0.003 ( 0.007)	Loss 2.0644e-01 (2.6620e-01)	Acc@1  94.53 ( 92.23)	Acc@5 100.00 ( 99.71)
03-Mar-22 09:33:10 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:33:10 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:33:10 - match all modules defined in bit_config: False
03-Mar-22 09:33:10 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:33:11 - Epoch: [0][ 60/352]	Time  0.125 ( 0.138)	Data  0.003 ( 0.006)	Loss 1.9474e-01 (2.5711e-01)	Acc@1  93.75 ( 92.35)	Acc@5 100.00 ( 99.72)
03-Mar-22 09:33:12 - Epoch: [0][ 70/352]	Time  0.128 ( 0.136)	Data  0.002 ( 0.006)	Loss 2.7776e-01 (2.5185e-01)	Acc@1  93.75 ( 92.46)	Acc@5  99.22 ( 99.72)
03-Mar-22 09:33:14 - Epoch: [0][ 80/352]	Time  0.129 ( 0.135)	Data  0.002 ( 0.006)	Loss 1.7303e-01 (2.4546e-01)	Acc@1  96.09 ( 92.70)	Acc@5 100.00 ( 99.72)
03-Mar-22 09:33:15 - Epoch: [0][ 90/352]	Time  0.140 ( 0.134)	Data  0.002 ( 0.005)	Loss 1.4778e-01 (2.4315e-01)	Acc@1  94.53 ( 92.66)	Acc@5 100.00 ( 99.73)
03-Mar-22 09:33:15 - Epoch: [0][  0/352]	Time  0.456 ( 0.456)	Data  0.223 ( 0.223)	Loss 3.4624e-01 (3.4624e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
03-Mar-22 09:33:16 - Epoch: [0][ 10/352]	Time  0.119 ( 0.163)	Data  0.002 ( 0.022)	Loss 2.8381e-01 (2.9310e-01)	Acc@1  91.41 ( 92.19)	Acc@5  99.22 ( 99.50)
03-Mar-22 09:34:35 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:34:35 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:34:35 - match all modules defined in bit_config: False
03-Mar-22 09:34:35 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:34:40 - Epoch: [0][  0/352]	Time  0.456 ( 0.456)	Data  0.252 ( 0.252)	Loss 2.7826e-01 (2.7826e-01)	Acc@1  94.53 ( 94.53)	Acc@5  99.22 ( 99.22)
03-Mar-22 09:34:41 - Epoch: [0][ 10/352]	Time  0.116 ( 0.155)	Data  0.002 ( 0.025)	Loss 1.9750e-01 (2.6128e-01)	Acc@1  96.09 ( 92.90)	Acc@5 100.00 ( 99.79)
03-Mar-22 09:34:42 - Epoch: [0][ 20/352]	Time  0.124 ( 0.140)	Data  0.002 ( 0.014)	Loss 2.1885e-01 (2.6429e-01)	Acc@1  92.97 ( 92.52)	Acc@5 100.00 ( 99.70)
03-Mar-22 09:34:43 - Epoch: [0][ 30/352]	Time  0.127 ( 0.131)	Data  0.003 ( 0.010)	Loss 2.0099e-01 (2.6117e-01)	Acc@1  94.53 ( 92.34)	Acc@5 100.00 ( 99.75)
03-Mar-22 09:34:45 - Epoch: [0][ 40/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.008)	Loss 2.7462e-01 (2.6284e-01)	Acc@1  91.41 ( 92.17)	Acc@5 100.00 ( 99.70)
03-Mar-22 09:34:46 - Epoch: [0][ 50/352]	Time  0.127 ( 0.129)	Data  0.003 ( 0.007)	Loss 2.0681e-01 (2.5539e-01)	Acc@1  94.53 ( 92.43)	Acc@5  99.22 ( 99.69)
03-Mar-22 09:34:47 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 09:34:47 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 09:34:47 - match all modules defined in bit_config: False
03-Mar-22 09:34:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 09:34:47 - Epoch: [0][ 60/352]	Time  0.130 ( 0.129)	Data  0.003 ( 0.006)	Loss 3.0912e-01 (2.4658e-01)	Acc@1  92.19 ( 92.65)	Acc@5  98.44 ( 99.72)
03-Mar-22 09:34:48 - Epoch: [0][ 70/352]	Time  0.136 ( 0.129)	Data  0.003 ( 0.006)	Loss 2.1660e-01 (2.4340e-01)	Acc@1  94.53 ( 92.75)	Acc@5 100.00 ( 99.71)
03-Mar-22 09:34:50 - Epoch: [0][ 80/352]	Time  0.155 ( 0.129)	Data  0.003 ( 0.005)	Loss 2.6344e-01 (2.4256e-01)	Acc@1  91.41 ( 92.72)	Acc@5 100.00 ( 99.74)
03-Mar-22 09:34:51 - Epoch: [0][ 90/352]	Time  0.103 ( 0.129)	Data  0.004 ( 0.005)	Loss 3.2199e-01 (2.4129e-01)	Acc@1  88.28 ( 92.73)	Acc@5 100.00 ( 99.75)
03-Mar-22 09:34:51 - Epoch: [0][  0/352]	Time  0.406 ( 0.406)	Data  0.219 ( 0.219)	Loss 2.5951e-01 (2.5951e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
03-Mar-22 09:34:52 - Epoch: [0][100/352]	Time  0.128 ( 0.129)	Data  0.003 ( 0.005)	Loss 2.5989e-01 (2.4187e-01)	Acc@1  90.62 ( 92.69)	Acc@5 100.00 ( 99.78)
03-Mar-22 09:34:52 - Epoch: [0][ 10/352]	Time  0.124 ( 0.139)	Data  0.002 ( 0.022)	Loss 3.6868e-01 (2.8737e-01)	Acc@1  85.94 ( 91.05)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:34:54 - Epoch: [0][110/352]	Time  0.119 ( 0.129)	Data  0.002 ( 0.005)	Loss 3.0044e-01 (2.4048e-01)	Acc@1  88.28 ( 92.72)	Acc@5 100.00 ( 99.78)
03-Mar-22 09:34:54 - Epoch: [0][ 20/352]	Time  0.107 ( 0.132)	Data  0.002 ( 0.013)	Loss 3.4879e-01 (2.8253e-01)	Acc@1  89.84 ( 91.48)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:34:55 - Epoch: [0][120/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.4756e-01 (2.3822e-01)	Acc@1  92.97 ( 92.80)	Acc@5 100.00 ( 99.79)
03-Mar-22 09:34:55 - Epoch: [0][ 30/352]	Time  0.126 ( 0.130)	Data  0.003 ( 0.009)	Loss 2.3900e-01 (2.7931e-01)	Acc@1  92.97 ( 91.51)	Acc@5  99.22 ( 99.82)
03-Mar-22 09:34:56 - Epoch: [0][130/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.8770e-01 (2.3730e-01)	Acc@1  92.97 ( 92.77)	Acc@5  99.22 ( 99.79)
03-Mar-22 09:34:56 - Epoch: [0][ 40/352]	Time  0.128 ( 0.130)	Data  0.003 ( 0.008)	Loss 1.9676e-01 (2.7383e-01)	Acc@1  95.31 ( 91.62)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:34:57 - Epoch: [0][140/352]	Time  0.129 ( 0.127)	Data  0.002 ( 0.004)	Loss 2.4497e-01 (2.3577e-01)	Acc@1  94.53 ( 92.87)	Acc@5 100.00 ( 99.78)
03-Mar-22 09:34:57 - Epoch: [0][ 50/352]	Time  0.128 ( 0.129)	Data  0.003 ( 0.007)	Loss 1.7405e-01 (2.7074e-01)	Acc@1  95.31 ( 91.71)	Acc@5 100.00 ( 99.80)
03-Mar-22 09:34:59 - Epoch: [0][150/352]	Time  0.124 ( 0.128)	Data  0.003 ( 0.004)	Loss 2.6332e-01 (2.3332e-01)	Acc@1  92.19 ( 92.94)	Acc@5  99.22 ( 99.79)
03-Mar-22 09:34:59 - Epoch: [0][ 60/352]	Time  0.127 ( 0.129)	Data  0.002 ( 0.006)	Loss 1.8156e-01 (2.6358e-01)	Acc@1  95.31 ( 91.97)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:35:00 - Epoch: [0][160/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.5320e-01 (2.3071e-01)	Acc@1  96.88 ( 93.03)	Acc@5 100.00 ( 99.80)
03-Mar-22 09:35:00 - Epoch: [0][ 70/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.4427e-01 (2.5729e-01)	Acc@1  95.31 ( 92.07)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:01 - Epoch: [0][170/352]	Time  0.098 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.4294e-01 (2.2866e-01)	Acc@1  95.31 ( 93.08)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:35:01 - Epoch: [0][ 80/352]	Time  0.116 ( 0.128)	Data  0.002 ( 0.005)	Loss 2.3166e-01 (2.4893e-01)	Acc@1  92.97 ( 92.37)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:02 - Epoch: [0][180/352]	Time  0.132 ( 0.126)	Data  0.003 ( 0.004)	Loss 1.8212e-01 (2.2700e-01)	Acc@1  95.31 ( 93.13)	Acc@5  99.22 ( 99.81)
03-Mar-22 09:35:02 - Epoch: [0][ 90/352]	Time  0.115 ( 0.128)	Data  0.002 ( 0.005)	Loss 3.2173e-01 (2.4647e-01)	Acc@1  89.84 ( 92.48)	Acc@5  99.22 ( 99.83)
03-Mar-22 09:35:03 - Epoch: [0][190/352]	Time  0.156 ( 0.126)	Data  0.003 ( 0.004)	Loss 2.7061e-01 (2.2670e-01)	Acc@1  89.84 ( 93.10)	Acc@5  99.22 ( 99.81)
03-Mar-22 09:35:04 - Epoch: [0][100/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.005)	Loss 1.9928e-01 (2.4109e-01)	Acc@1  94.53 ( 92.65)	Acc@5  99.22 ( 99.82)
03-Mar-22 09:35:05 - Epoch: [0][200/352]	Time  0.113 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.4097e-01 (2.2576e-01)	Acc@1  96.88 ( 93.13)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:35:05 - Epoch: [0][110/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.004)	Loss 2.0982e-01 (2.3944e-01)	Acc@1  92.97 ( 92.71)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:06 - Epoch: [0][210/352]	Time  0.132 ( 0.126)	Data  0.003 ( 0.004)	Loss 2.6606e-01 (2.2607e-01)	Acc@1  91.41 ( 93.08)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:35:06 - Epoch: [0][120/352]	Time  0.130 ( 0.128)	Data  0.002 ( 0.004)	Loss 2.0398e-01 (2.3489e-01)	Acc@1  92.97 ( 92.87)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:07 - Epoch: [0][220/352]	Time  0.127 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.6473e-01 (2.2471e-01)	Acc@1  96.09 ( 93.14)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:35:07 - Epoch: [0][130/352]	Time  0.136 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.6994e-01 (2.3245e-01)	Acc@1  94.53 ( 92.93)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:08 - Epoch: [0][230/352]	Time  0.131 ( 0.126)	Data  0.003 ( 0.003)	Loss 1.9906e-01 (2.2431e-01)	Acc@1  93.75 ( 93.15)	Acc@5  99.22 ( 99.81)
03-Mar-22 09:35:09 - Epoch: [0][140/352]	Time  0.128 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.9147e-01 (2.3201e-01)	Acc@1  95.31 ( 92.94)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:10 - Epoch: [0][240/352]	Time  0.127 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.1192e-01 (2.2366e-01)	Acc@1  97.66 ( 93.16)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:10 - Epoch: [0][150/352]	Time  0.128 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.9188e-01 (2.3112e-01)	Acc@1  92.19 ( 92.92)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:11 - Epoch: [0][250/352]	Time  0.102 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.4844e-01 (2.2181e-01)	Acc@1  98.44 ( 93.25)	Acc@5  99.22 ( 99.82)
03-Mar-22 09:35:11 - Epoch: [0][160/352]	Time  0.133 ( 0.128)	Data  0.003 ( 0.004)	Loss 1.7374e-01 (2.2876e-01)	Acc@1  93.75 ( 93.01)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:12 - Epoch: [0][260/352]	Time  0.128 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.2560e-01 (2.2071e-01)	Acc@1  98.44 ( 93.25)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:13 - Epoch: [0][170/352]	Time  0.135 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.5004e-01 (2.2581e-01)	Acc@1  96.09 ( 93.09)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:13 - Epoch: [0][270/352]	Time  0.123 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.0959e-01 (2.2002e-01)	Acc@1  96.09 ( 93.25)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:14 - Epoch: [0][180/352]	Time  0.110 ( 0.127)	Data  0.003 ( 0.004)	Loss 1.6422e-01 (2.2439e-01)	Acc@1  96.09 ( 93.13)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:15 - Epoch: [0][280/352]	Time  0.129 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.3436e-01 (2.2040e-01)	Acc@1  91.41 ( 93.24)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:15 - Epoch: [0][190/352]	Time  0.129 ( 0.127)	Data  0.002 ( 0.004)	Loss 2.8992e-01 (2.2297e-01)	Acc@1  89.84 ( 93.17)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:16 - Epoch: [0][290/352]	Time  0.128 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.2926e-01 (2.1899e-01)	Acc@1  96.88 ( 93.28)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:16 - Epoch: [0][200/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.6853e-01 (2.2238e-01)	Acc@1  95.31 ( 93.19)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:17 - Epoch: [0][300/352]	Time  0.131 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.3662e-01 (2.1815e-01)	Acc@1  91.41 ( 93.29)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:18 - Epoch: [0][210/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.8862e-01 (2.2136e-01)	Acc@1  95.31 ( 93.19)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:18 - Epoch: [0][310/352]	Time  0.143 ( 0.126)	Data  0.003 ( 0.003)	Loss 1.5673e-01 (2.1700e-01)	Acc@1  94.53 ( 93.33)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:19 - Epoch: [0][220/352]	Time  0.129 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.6918e-01 (2.2167e-01)	Acc@1  90.62 ( 93.16)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:20 - Epoch: [0][320/352]	Time  0.133 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.1022e-01 (2.1698e-01)	Acc@1  93.75 ( 93.32)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:20 - Epoch: [0][230/352]	Time  0.130 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.6418e-01 (2.2040e-01)	Acc@1  95.31 ( 93.20)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:21 - Epoch: [0][330/352]	Time  0.119 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.4490e-01 (2.1614e-01)	Acc@1  92.97 ( 93.36)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:21 - Epoch: [0][240/352]	Time  0.128 ( 0.127)	Data  0.003 ( 0.003)	Loss 1.3041e-01 (2.1855e-01)	Acc@1  96.88 ( 93.24)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:22 - Epoch: [0][340/352]	Time  0.116 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.6803e-01 (2.1575e-01)	Acc@1  95.31 ( 93.34)	Acc@5  99.22 ( 99.83)
03-Mar-22 09:35:23 - Epoch: [0][250/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.3051e-01 (2.1840e-01)	Acc@1  90.62 ( 93.25)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:24 - Epoch: [0][350/352]	Time  0.132 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.4933e-01 (2.1539e-01)	Acc@1  90.62 ( 93.34)	Acc@5  99.22 ( 99.82)
03-Mar-22 09:35:24 - Epoch: [0][260/352]	Time  0.109 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.2865e-01 (2.1781e-01)	Acc@1  93.75 ( 93.27)	Acc@5  99.22 ( 99.84)
03-Mar-22 09:35:24 - Test: [ 0/20]	Time  0.376 ( 0.376)	Loss 1.7135e-01 (1.7135e-01)	Acc@1  94.92 ( 94.92)	Acc@5 100.00 (100.00)
03-Mar-22 09:35:25 - Test: [10/20]	Time  0.073 ( 0.104)	Loss 1.6158e-01 (1.7530e-01)	Acc@1  96.09 ( 94.74)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:35:25 - Epoch: [0][270/352]	Time  0.111 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.2405e-01 (2.1748e-01)	Acc@1  98.44 ( 93.25)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:26 -  * Acc@1 94.220 Acc@5 99.880
03-Mar-22 09:35:26 - Best acc at epoch 0: 94.22000122070312
03-Mar-22 09:35:26 - Epoch: [1][  0/352]	Time  0.374 ( 0.374)	Data  0.259 ( 0.259)	Loss 2.0222e-01 (2.0222e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:35:26 - Epoch: [0][280/352]	Time  0.113 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.9184e-01 (2.1703e-01)	Acc@1  89.06 ( 93.26)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:27 - Epoch: [1][ 10/352]	Time  0.106 ( 0.140)	Data  0.002 ( 0.025)	Loss 2.8557e-01 (2.0250e-01)	Acc@1  89.84 ( 93.61)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:35:27 - Epoch: [0][290/352]	Time  0.109 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.9697e-01 (2.1623e-01)	Acc@1  92.97 ( 93.28)	Acc@5  99.22 ( 99.83)
03-Mar-22 09:35:29 - Epoch: [1][ 20/352]	Time  0.135 ( 0.134)	Data  0.003 ( 0.014)	Loss 1.3317e-01 (1.9681e-01)	Acc@1  94.53 ( 93.49)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:35:29 - Epoch: [0][300/352]	Time  0.128 ( 0.126)	Data  0.003 ( 0.003)	Loss 2.0645e-01 (2.1578e-01)	Acc@1  94.53 ( 93.28)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:30 - Epoch: [1][ 30/352]	Time  0.117 ( 0.130)	Data  0.002 ( 0.010)	Loss 1.7041e-01 (1.8667e-01)	Acc@1  96.09 ( 93.93)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:35:30 - Epoch: [0][310/352]	Time  0.130 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.8824e-01 (2.1538e-01)	Acc@1  95.31 ( 93.28)	Acc@5  99.22 ( 99.83)
03-Mar-22 09:35:31 - Epoch: [1][ 40/352]	Time  0.136 ( 0.130)	Data  0.002 ( 0.008)	Loss 1.4980e-01 (1.8526e-01)	Acc@1  96.88 ( 94.04)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:35:31 - Epoch: [0][320/352]	Time  0.104 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.9011e-01 (2.1510e-01)	Acc@1  91.41 ( 93.27)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:32 - Epoch: [1][ 50/352]	Time  0.127 ( 0.130)	Data  0.002 ( 0.007)	Loss 1.3970e-01 (1.8798e-01)	Acc@1  96.88 ( 93.93)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:35:32 - Epoch: [0][330/352]	Time  0.127 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.7506e-01 (2.1456e-01)	Acc@1  93.75 ( 93.27)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:34 - Epoch: [0][340/352]	Time  0.102 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.0557e-01 (2.1342e-01)	Acc@1  92.19 ( 93.31)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:34 - Epoch: [1][ 60/352]	Time  0.112 ( 0.129)	Data  0.002 ( 0.006)	Loss 1.1275e-01 (1.8655e-01)	Acc@1  98.44 ( 94.06)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:35:35 - Epoch: [0][350/352]	Time  0.124 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.3390e-01 (2.1277e-01)	Acc@1  90.62 ( 93.32)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:35 - Epoch: [1][ 70/352]	Time  0.115 ( 0.129)	Data  0.002 ( 0.006)	Loss 1.5056e-01 (1.8449e-01)	Acc@1  94.53 ( 94.07)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:36 - Test: [ 0/20]	Time  0.383 ( 0.383)	Loss 1.6732e-01 (1.6732e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:35:36 - Epoch: [1][ 80/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.005)	Loss 1.8011e-01 (1.8460e-01)	Acc@1  94.53 ( 94.03)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:36 - Test: [10/20]	Time  0.085 ( 0.109)	Loss 1.3199e-01 (1.7087e-01)	Acc@1  96.88 ( 94.85)	Acc@5  99.61 ( 99.79)
03-Mar-22 09:35:37 -  * Acc@1 94.520 Acc@5 99.820
03-Mar-22 09:35:37 - Best acc at epoch 0: 94.5199966430664
03-Mar-22 09:35:37 - Epoch: [1][ 90/352]	Time  0.100 ( 0.127)	Data  0.002 ( 0.005)	Loss 2.5077e-01 (1.8549e-01)	Acc@1  89.84 ( 93.98)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:35:37 - Epoch: [1][  0/352]	Time  0.336 ( 0.336)	Data  0.218 ( 0.218)	Loss 1.9114e-01 (1.9114e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:35:39 - Epoch: [1][100/352]	Time  0.133 ( 0.127)	Data  0.003 ( 0.005)	Loss 1.3415e-01 (1.8580e-01)	Acc@1  96.09 ( 93.97)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:35:39 - Epoch: [1][ 10/352]	Time  0.128 ( 0.137)	Data  0.002 ( 0.021)	Loss 2.0399e-01 (1.8671e-01)	Acc@1  92.19 ( 93.54)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:35:40 - Epoch: [1][110/352]	Time  0.106 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.6270e-01 (1.8648e-01)	Acc@1  94.53 ( 93.91)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:35:40 - Epoch: [1][ 20/352]	Time  0.125 ( 0.128)	Data  0.002 ( 0.012)	Loss 1.8194e-01 (1.8819e-01)	Acc@1  94.53 ( 93.75)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:35:41 - Epoch: [1][120/352]	Time  0.112 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.5624e-01 (1.9029e-01)	Acc@1  96.09 ( 93.77)	Acc@5  99.22 ( 99.85)
03-Mar-22 09:35:41 - Epoch: [1][ 30/352]	Time  0.103 ( 0.126)	Data  0.001 ( 0.009)	Loss 1.6378e-01 (1.9646e-01)	Acc@1  96.09 ( 93.60)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:42 - Epoch: [1][130/352]	Time  0.110 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.9212e-01 (1.8843e-01)	Acc@1  93.75 ( 93.85)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:35:42 - Epoch: [1][ 40/352]	Time  0.104 ( 0.124)	Data  0.002 ( 0.007)	Loss 1.8031e-01 (1.8888e-01)	Acc@1  94.53 ( 93.90)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:35:43 - Epoch: [1][ 50/352]	Time  0.110 ( 0.121)	Data  0.003 ( 0.006)	Loss 2.2284e-01 (1.8615e-01)	Acc@1  92.19 ( 94.00)	Acc@5  99.22 ( 99.85)
03-Mar-22 09:35:43 - Epoch: [1][140/352]	Time  0.128 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.8906e-01 (1.8945e-01)	Acc@1  96.09 ( 93.86)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:35:44 - Epoch: [1][ 60/352]	Time  0.118 ( 0.119)	Data  0.002 ( 0.005)	Loss 1.9681e-01 (1.8656e-01)	Acc@1  93.75 ( 93.97)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:45 - Epoch: [1][150/352]	Time  0.131 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.4730e-01 (1.8777e-01)	Acc@1  96.88 ( 93.94)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:46 - Epoch: [1][ 70/352]	Time  0.105 ( 0.119)	Data  0.002 ( 0.005)	Loss 2.1392e-01 (1.8666e-01)	Acc@1  91.41 ( 93.93)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:46 - Epoch: [1][160/352]	Time  0.123 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.9135e-01 (1.8789e-01)	Acc@1  93.75 ( 93.95)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:47 - Epoch: [1][ 80/352]	Time  0.112 ( 0.119)	Data  0.002 ( 0.005)	Loss 1.8131e-01 (1.8777e-01)	Acc@1  96.09 ( 93.89)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:47 - Epoch: [1][170/352]	Time  0.136 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.5259e-01 (1.8769e-01)	Acc@1  96.09 ( 93.97)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:48 - Epoch: [1][ 90/352]	Time  0.131 ( 0.120)	Data  0.002 ( 0.004)	Loss 1.6149e-01 (1.8654e-01)	Acc@1  96.88 ( 94.02)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:48 - Epoch: [1][180/352]	Time  0.133 ( 0.125)	Data  0.003 ( 0.004)	Loss 1.7630e-01 (1.8783e-01)	Acc@1  93.75 ( 93.94)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:49 - Epoch: [1][100/352]	Time  0.115 ( 0.120)	Data  0.002 ( 0.004)	Loss 2.2304e-01 (1.8896e-01)	Acc@1  92.19 ( 93.93)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:50 - Epoch: [1][190/352]	Time  0.130 ( 0.125)	Data  0.002 ( 0.004)	Loss 2.5379e-01 (1.8738e-01)	Acc@1  95.31 ( 93.98)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:51 - Epoch: [1][110/352]	Time  0.106 ( 0.120)	Data  0.002 ( 0.004)	Loss 1.5288e-01 (1.9012e-01)	Acc@1  95.31 ( 93.93)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:51 - Epoch: [1][200/352]	Time  0.124 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.1079e-01 (1.8784e-01)	Acc@1  93.75 ( 93.96)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:52 - Epoch: [1][120/352]	Time  0.111 ( 0.120)	Data  0.002 ( 0.004)	Loss 1.8448e-01 (1.9129e-01)	Acc@1  95.31 ( 93.90)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:52 - Epoch: [1][210/352]	Time  0.113 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.2494e-01 (1.8622e-01)	Acc@1  96.88 ( 94.03)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:53 - Epoch: [1][130/352]	Time  0.127 ( 0.120)	Data  0.002 ( 0.004)	Loss 1.3262e-01 (1.9130e-01)	Acc@1  95.31 ( 93.91)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:53 - Epoch: [1][220/352]	Time  0.128 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.4970e-01 (1.8613e-01)	Acc@1  97.66 ( 94.04)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:54 - Epoch: [1][140/352]	Time  0.108 ( 0.119)	Data  0.002 ( 0.004)	Loss 2.3119e-01 (1.9043e-01)	Acc@1  90.62 ( 93.97)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:35:55 - Epoch: [1][230/352]	Time  0.130 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.6306e-01 (1.8576e-01)	Acc@1  94.53 ( 94.06)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:55 - Epoch: [1][150/352]	Time  0.115 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.5076e-01 (1.9094e-01)	Acc@1  96.88 ( 93.95)	Acc@5  99.22 ( 99.83)
03-Mar-22 09:35:56 - Epoch: [1][240/352]	Time  0.131 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.8908e-01 (1.8593e-01)	Acc@1  92.97 ( 94.04)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:35:56 - Epoch: [1][160/352]	Time  0.125 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.0930e-01 (1.9151e-01)	Acc@1  96.88 ( 93.91)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:35:57 - Epoch: [1][250/352]	Time  0.128 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.5239e-01 (1.8590e-01)	Acc@1  96.88 ( 94.02)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:35:58 - Epoch: [1][170/352]	Time  0.126 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.5563e-01 (1.9089e-01)	Acc@1  95.31 ( 93.92)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:35:58 - Epoch: [1][260/352]	Time  0.115 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.4590e-01 (1.8573e-01)	Acc@1  95.31 ( 94.04)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:35:59 - Epoch: [1][180/352]	Time  0.153 ( 0.120)	Data  0.002 ( 0.003)	Loss 1.9112e-01 (1.9037e-01)	Acc@1  93.75 ( 93.94)	Acc@5  99.22 ( 99.81)
03-Mar-22 09:36:00 - Epoch: [1][270/352]	Time  0.130 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.8497e-01 (1.8523e-01)	Acc@1  92.97 ( 94.06)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:00 - Epoch: [1][190/352]	Time  0.125 ( 0.120)	Data  0.002 ( 0.003)	Loss 1.6685e-01 (1.8887e-01)	Acc@1  95.31 ( 93.98)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:36:01 - Epoch: [1][280/352]	Time  0.109 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.0686e-01 (1.8560e-01)	Acc@1  94.53 ( 94.05)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:01 - Epoch: [1][200/352]	Time  0.130 ( 0.120)	Data  0.002 ( 0.003)	Loss 7.9481e-02 (1.8761e-01)	Acc@1  98.44 ( 94.05)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:36:02 - Epoch: [1][290/352]	Time  0.133 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.2825e-01 (1.8596e-01)	Acc@1  94.53 ( 94.05)	Acc@5  99.22 ( 99.86)
03-Mar-22 09:36:03 - Epoch: [1][210/352]	Time  0.128 ( 0.120)	Data  0.002 ( 0.003)	Loss 1.7633e-01 (1.8717e-01)	Acc@1  94.53 ( 94.05)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:04 - Epoch: [1][300/352]	Time  0.132 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.2411e-01 (1.8533e-01)	Acc@1  94.53 ( 94.09)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:04 - Epoch: [1][220/352]	Time  0.105 ( 0.120)	Data  0.002 ( 0.003)	Loss 2.6872e-01 (1.8824e-01)	Acc@1  90.62 ( 94.00)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:05 - Epoch: [1][310/352]	Time  0.130 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.0622e-01 (1.8490e-01)	Acc@1  98.44 ( 94.12)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:05 - Epoch: [1][230/352]	Time  0.125 ( 0.120)	Data  0.002 ( 0.003)	Loss 2.0169e-01 (1.8827e-01)	Acc@1  95.31 ( 94.02)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:06 - Epoch: [1][320/352]	Time  0.130 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.3324e-01 (1.8482e-01)	Acc@1  90.62 ( 94.11)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:06 - Epoch: [1][240/352]	Time  0.110 ( 0.120)	Data  0.002 ( 0.003)	Loss 2.0931e-01 (1.8735e-01)	Acc@1  92.19 ( 94.03)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:07 - Epoch: [1][330/352]	Time  0.131 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.3872e-01 (1.8409e-01)	Acc@1  95.31 ( 94.13)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:07 - Epoch: [1][250/352]	Time  0.118 ( 0.121)	Data  0.002 ( 0.003)	Loss 1.5512e-01 (1.8690e-01)	Acc@1  95.31 ( 94.05)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:08 - Epoch: [1][340/352]	Time  0.129 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.7831e-01 (1.8382e-01)	Acc@1  93.75 ( 94.13)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:09 - Epoch: [1][260/352]	Time  0.127 ( 0.121)	Data  0.002 ( 0.003)	Loss 2.9248e-01 (1.8675e-01)	Acc@1  92.19 ( 94.04)	Acc@5  99.22 ( 99.84)
03-Mar-22 09:36:10 - Epoch: [1][350/352]	Time  0.132 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.2992e-01 (1.8360e-01)	Acc@1  95.31 ( 94.16)	Acc@5  99.22 ( 99.86)
03-Mar-22 09:36:10 - Epoch: [1][270/352]	Time  0.127 ( 0.121)	Data  0.002 ( 0.003)	Loss 2.6084e-01 (1.8725e-01)	Acc@1  89.84 ( 94.01)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:10 - Test: [ 0/20]	Time  0.336 ( 0.336)	Loss 1.6500e-01 (1.6500e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.61 ( 99.61)
03-Mar-22 09:36:11 - Epoch: [1][280/352]	Time  0.094 ( 0.120)	Data  0.001 ( 0.003)	Loss 2.2780e-01 (1.8678e-01)	Acc@1  92.97 ( 94.04)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:11 - Test: [10/20]	Time  0.070 ( 0.098)	Loss 1.4541e-01 (1.7036e-01)	Acc@1  94.53 ( 94.32)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:36:12 -  * Acc@1 94.100 Acc@5 99.900
03-Mar-22 09:36:12 - Best acc at epoch 1: 94.22000122070312
03-Mar-22 09:36:12 - Epoch: [1][290/352]	Time  0.106 ( 0.120)	Data  0.002 ( 0.003)	Loss 1.2788e-01 (1.8583e-01)	Acc@1  96.09 ( 94.07)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:12 - Epoch: [2][  0/352]	Time  0.347 ( 0.347)	Data  0.247 ( 0.247)	Loss 1.6319e-01 (1.6319e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:36:13 - Epoch: [1][300/352]	Time  0.123 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.5321e-01 (1.8627e-01)	Acc@1  96.09 ( 94.07)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:13 - Epoch: [2][ 10/352]	Time  0.128 ( 0.148)	Data  0.002 ( 0.025)	Loss 1.5791e-01 (1.6075e-01)	Acc@1  93.75 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:36:14 - Epoch: [1][310/352]	Time  0.096 ( 0.119)	Data  0.002 ( 0.003)	Loss 2.8420e-01 (1.8608e-01)	Acc@1  89.84 ( 94.09)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:15 - Epoch: [2][ 20/352]	Time  0.127 ( 0.138)	Data  0.002 ( 0.014)	Loss 1.9580e-01 (1.7161e-01)	Acc@1  93.75 ( 94.42)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:36:15 - Epoch: [1][320/352]	Time  0.127 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.8760e-01 (1.8637e-01)	Acc@1  93.75 ( 94.07)	Acc@5 100.00 ( 99.84)
03-Mar-22 09:36:16 - Epoch: [2][ 30/352]	Time  0.129 ( 0.135)	Data  0.002 ( 0.010)	Loss 1.6972e-01 (1.7942e-01)	Acc@1  94.53 ( 94.23)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:36:16 - Epoch: [1][330/352]	Time  0.126 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.7158e-01 (1.8652e-01)	Acc@1  94.53 ( 94.06)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:36:17 - Epoch: [2][ 40/352]	Time  0.111 ( 0.133)	Data  0.002 ( 0.008)	Loss 1.9196e-01 (1.7592e-01)	Acc@1  95.31 ( 94.49)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:18 - Epoch: [1][340/352]	Time  0.122 ( 0.119)	Data  0.002 ( 0.003)	Loss 2.7443e-01 (1.8592e-01)	Acc@1  90.62 ( 94.07)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:36:19 - Epoch: [2][ 50/352]	Time  0.128 ( 0.133)	Data  0.002 ( 0.007)	Loss 2.3959e-01 (1.7317e-01)	Acc@1  90.62 ( 94.50)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:19 - Epoch: [1][350/352]	Time  0.158 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.8315e-01 (1.8533e-01)	Acc@1  94.53 ( 94.08)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:36:19 - Test: [ 0/20]	Time  0.305 ( 0.305)	Loss 1.3811e-01 (1.3811e-01)	Acc@1  95.70 ( 95.70)	Acc@5 100.00 (100.00)
03-Mar-22 09:36:20 - Epoch: [2][ 60/352]	Time  0.132 ( 0.132)	Data  0.002 ( 0.006)	Loss 1.4772e-01 (1.7457e-01)	Acc@1  94.53 ( 94.48)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:20 - Test: [10/20]	Time  0.070 ( 0.094)	Loss 1.7333e-01 (1.6132e-01)	Acc@1  94.14 ( 94.82)	Acc@5  99.61 ( 99.86)
03-Mar-22 09:36:21 -  * Acc@1 94.520 Acc@5 99.880
03-Mar-22 09:36:21 - Best acc at epoch 1: 94.5199966430664
03-Mar-22 09:36:21 - Epoch: [2][ 70/352]	Time  0.099 ( 0.130)	Data  0.002 ( 0.006)	Loss 1.4175e-01 (1.7729e-01)	Acc@1  96.88 ( 94.43)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:21 - Epoch: [2][  0/352]	Time  0.343 ( 0.343)	Data  0.228 ( 0.228)	Loss 1.0681e-01 (1.0681e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
03-Mar-22 09:36:22 - Epoch: [2][ 10/352]	Time  0.096 ( 0.139)	Data  0.002 ( 0.023)	Loss 1.4397e-01 (1.5639e-01)	Acc@1  95.31 ( 94.46)	Acc@5 100.00 (100.00)
03-Mar-22 09:36:22 - Epoch: [2][ 80/352]	Time  0.139 ( 0.130)	Data  0.002 ( 0.005)	Loss 2.6705e-01 (1.7757e-01)	Acc@1  89.06 ( 94.30)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:24 - Epoch: [2][ 20/352]	Time  0.108 ( 0.129)	Data  0.002 ( 0.013)	Loss 2.1768e-01 (1.5995e-01)	Acc@1  94.53 ( 94.79)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:36:24 - Epoch: [2][ 90/352]	Time  0.135 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.5866e-01 (1.7747e-01)	Acc@1  94.53 ( 94.30)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:25 - Epoch: [2][ 30/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.009)	Loss 1.7946e-01 (1.6818e-01)	Acc@1  93.75 ( 94.43)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:36:25 - Epoch: [2][100/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.005)	Loss 2.0339e-01 (1.7798e-01)	Acc@1  93.75 ( 94.34)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:26 - Epoch: [2][ 40/352]	Time  0.129 ( 0.128)	Data  0.003 ( 0.008)	Loss 1.2893e-01 (1.6421e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:36:26 - Epoch: [2][110/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.5288e-01 (1.8038e-01)	Acc@1  91.41 ( 94.24)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:27 - Epoch: [2][ 50/352]	Time  0.129 ( 0.129)	Data  0.003 ( 0.007)	Loss 1.7044e-01 (1.6414e-01)	Acc@1  94.53 ( 94.78)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:36:27 - Epoch: [2][120/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.3456e-01 (1.7977e-01)	Acc@1  97.66 ( 94.29)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:29 - Epoch: [2][ 60/352]	Time  0.127 ( 0.128)	Data  0.002 ( 0.006)	Loss 1.9967e-01 (1.6815e-01)	Acc@1  94.53 ( 94.60)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:36:29 - Epoch: [2][130/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.2644e-01 (1.7945e-01)	Acc@1  95.31 ( 94.31)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:30 - Epoch: [2][ 70/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.005)	Loss 1.7677e-01 (1.6777e-01)	Acc@1  94.53 ( 94.63)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:36:30 - Epoch: [2][140/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.2623e-01 (1.7858e-01)	Acc@1  93.75 ( 94.33)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:31 - Epoch: [2][ 80/352]	Time  0.127 ( 0.127)	Data  0.003 ( 0.005)	Loss 1.3589e-01 (1.6940e-01)	Acc@1  95.31 ( 94.58)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:31 - Epoch: [2][150/352]	Time  0.139 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.9972e-01 (1.7738e-01)	Acc@1  94.53 ( 94.39)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:32 - Epoch: [2][ 90/352]	Time  0.129 ( 0.126)	Data  0.002 ( 0.005)	Loss 1.5632e-01 (1.7326e-01)	Acc@1  95.31 ( 94.45)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:33 - Epoch: [2][160/352]	Time  0.111 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.2722e-01 (1.7786e-01)	Acc@1  89.84 ( 94.34)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:33 - Epoch: [2][100/352]	Time  0.106 ( 0.126)	Data  0.002 ( 0.004)	Loss 2.5250e-01 (1.7457e-01)	Acc@1  92.97 ( 94.40)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:34 - Epoch: [2][170/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.4392e-01 (1.7570e-01)	Acc@1  94.53 ( 94.41)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:35 - Epoch: [2][110/352]	Time  0.123 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.8492e-01 (1.7332e-01)	Acc@1  92.97 ( 94.42)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:35 - Epoch: [2][180/352]	Time  0.116 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.1564e-01 (1.7694e-01)	Acc@1  92.97 ( 94.35)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:36 - Epoch: [2][120/352]	Time  0.126 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.1107e-01 (1.7254e-01)	Acc@1  96.09 ( 94.42)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:36 - Epoch: [2][190/352]	Time  0.133 ( 0.128)	Data  0.002 ( 0.004)	Loss 2.6246e-01 (1.7772e-01)	Acc@1  91.41 ( 94.33)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:37 - Epoch: [2][130/352]	Time  0.126 ( 0.126)	Data  0.002 ( 0.004)	Loss 2.5117e-01 (1.6955e-01)	Acc@1  92.19 ( 94.53)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:38 - Epoch: [2][200/352]	Time  0.142 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6984e-01 (1.7806e-01)	Acc@1  94.53 ( 94.33)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:38 - Epoch: [2][140/352]	Time  0.124 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.6219e-01 (1.7191e-01)	Acc@1  93.75 ( 94.40)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:39 - Epoch: [2][210/352]	Time  0.133 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.5118e-01 (1.7755e-01)	Acc@1  92.97 ( 94.33)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:40 - Epoch: [2][150/352]	Time  0.127 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.5133e-01 (1.7122e-01)	Acc@1  95.31 ( 94.44)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:40 - Epoch: [2][220/352]	Time  0.106 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.9439e-01 (1.7699e-01)	Acc@1  94.53 ( 94.35)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:41 - Epoch: [2][160/352]	Time  0.123 ( 0.125)	Data  0.003 ( 0.004)	Loss 1.8407e-01 (1.7256e-01)	Acc@1  94.53 ( 94.42)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:36:41 - Epoch: [2][230/352]	Time  0.133 ( 0.128)	Data  0.003 ( 0.003)	Loss 2.1462e-01 (1.7749e-01)	Acc@1  90.62 ( 94.31)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:42 - Epoch: [2][170/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.9183e-01 (1.7322e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:43 - Epoch: [2][240/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.8171e-01 (1.7794e-01)	Acc@1  93.75 ( 94.28)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:43 - Epoch: [2][180/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6721e-01 (1.7178e-01)	Acc@1  93.75 ( 94.45)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:44 - Epoch: [2][250/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.3250e-01 (1.7832e-01)	Acc@1  91.41 ( 94.28)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:44 - Epoch: [2][190/352]	Time  0.122 ( 0.124)	Data  0.002 ( 0.003)	Loss 2.9448e-01 (1.7202e-01)	Acc@1  91.41 ( 94.45)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:45 - Epoch: [2][260/352]	Time  0.111 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.2541e-01 (1.7820e-01)	Acc@1  92.19 ( 94.27)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:36:46 - Epoch: [2][200/352]	Time  0.109 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6637e-01 (1.7258e-01)	Acc@1  96.09 ( 94.42)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:46 - Epoch: [2][270/352]	Time  0.130 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.0332e-01 (1.7773e-01)	Acc@1  96.09 ( 94.28)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:47 - Epoch: [2][210/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.003)	Loss 2.2250e-01 (1.7251e-01)	Acc@1  89.84 ( 94.40)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:48 - Epoch: [2][280/352]	Time  0.132 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.8748e-01 (1.7657e-01)	Acc@1  93.75 ( 94.31)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:48 - Epoch: [2][220/352]	Time  0.129 ( 0.124)	Data  0.003 ( 0.003)	Loss 2.0760e-01 (1.7250e-01)	Acc@1  92.97 ( 94.39)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:36:49 - Epoch: [2][290/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.3660e-01 (1.7583e-01)	Acc@1  95.31 ( 94.33)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:49 - Epoch: [2][230/352]	Time  0.154 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.0453e-01 (1.7187e-01)	Acc@1  96.88 ( 94.42)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:50 - Epoch: [2][300/352]	Time  0.132 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.3280e-01 (1.7632e-01)	Acc@1  96.88 ( 94.31)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:51 - Epoch: [2][240/352]	Time  0.150 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.3203e-01 (1.7294e-01)	Acc@1  96.09 ( 94.38)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:51 - Epoch: [2][310/352]	Time  0.133 ( 0.127)	Data  0.002 ( 0.003)	Loss 8.2781e-02 (1.7594e-01)	Acc@1  98.44 ( 94.33)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:52 - Epoch: [2][250/352]	Time  0.124 ( 0.124)	Data  0.003 ( 0.003)	Loss 1.4926e-01 (1.7306e-01)	Acc@1  94.53 ( 94.37)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:53 - Epoch: [2][320/352]	Time  0.108 ( 0.127)	Data  0.002 ( 0.003)	Loss 8.2724e-02 (1.7516e-01)	Acc@1  96.88 ( 94.34)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:53 - Epoch: [2][260/352]	Time  0.127 ( 0.123)	Data  0.002 ( 0.003)	Loss 1.8306e-01 (1.7313e-01)	Acc@1  94.53 ( 94.38)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:54 - Epoch: [2][330/352]	Time  0.132 ( 0.127)	Data  0.003 ( 0.003)	Loss 1.1504e-01 (1.7424e-01)	Acc@1  95.31 ( 94.38)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:36:54 - Epoch: [2][270/352]	Time  0.121 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.3786e-01 (1.7250e-01)	Acc@1  94.53 ( 94.41)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:55 - Epoch: [2][340/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.003)	Loss 9.5746e-02 (1.7396e-01)	Acc@1  97.66 ( 94.40)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:36:56 - Epoch: [2][280/352]	Time  0.103 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.2755e-01 (1.7248e-01)	Acc@1  96.09 ( 94.41)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:36:57 - Epoch: [2][350/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.6857e-01 (1.7389e-01)	Acc@1  91.41 ( 94.39)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:36:57 - Epoch: [2][290/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.003)	Loss 2.6282e-01 (1.7255e-01)	Acc@1  90.62 ( 94.40)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:57 - Test: [ 0/20]	Time  0.350 ( 0.350)	Loss 1.4981e-01 (1.4981e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:36:58 - Epoch: [2][300/352]	Time  0.123 ( 0.123)	Data  0.002 ( 0.003)	Loss 1.3547e-01 (1.7180e-01)	Acc@1  96.88 ( 94.42)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:58 - Test: [10/20]	Time  0.071 ( 0.102)	Loss 1.4596e-01 (1.6966e-01)	Acc@1  94.92 ( 94.35)	Acc@5 100.00 ( 99.79)
03-Mar-22 09:36:59 -  * Acc@1 94.420 Acc@5 99.820
03-Mar-22 09:36:59 - Best acc at epoch 2: 94.41999816894531
03-Mar-22 09:36:59 - Epoch: [2][310/352]	Time  0.097 ( 0.123)	Data  0.002 ( 0.003)	Loss 2.5862e-01 (1.7244e-01)	Acc@1  90.62 ( 94.40)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:36:59 - Epoch: [3][  0/352]	Time  0.408 ( 0.408)	Data  0.277 ( 0.277)	Loss 2.6469e-01 (2.6469e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
03-Mar-22 09:37:00 - Epoch: [2][320/352]	Time  0.093 ( 0.122)	Data  0.002 ( 0.003)	Loss 2.2493e-01 (1.7326e-01)	Acc@1  91.41 ( 94.38)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:37:00 - Epoch: [3][ 10/352]	Time  0.127 ( 0.153)	Data  0.002 ( 0.027)	Loss 1.9362e-01 (1.6538e-01)	Acc@1  92.97 ( 94.67)	Acc@5  99.22 ( 99.79)
03-Mar-22 09:37:01 - Epoch: [2][330/352]	Time  0.092 ( 0.121)	Data  0.002 ( 0.003)	Loss 2.3921e-01 (1.7351e-01)	Acc@1  92.19 ( 94.37)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:02 - Epoch: [3][ 20/352]	Time  0.133 ( 0.141)	Data  0.002 ( 0.015)	Loss 1.9685e-01 (1.7229e-01)	Acc@1  94.53 ( 94.64)	Acc@5  99.22 ( 99.78)
03-Mar-22 09:37:02 - Epoch: [2][340/352]	Time  0.128 ( 0.121)	Data  0.002 ( 0.003)	Loss 1.9667e-01 (1.7361e-01)	Acc@1  95.31 ( 94.36)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:03 - Epoch: [3][ 30/352]	Time  0.130 ( 0.139)	Data  0.002 ( 0.011)	Loss 1.1427e-01 (1.7034e-01)	Acc@1  96.88 ( 94.63)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:37:03 - Epoch: [2][350/352]	Time  0.109 ( 0.121)	Data  0.002 ( 0.003)	Loss 1.6177e-01 (1.7291e-01)	Acc@1  96.09 ( 94.40)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:04 - Test: [ 0/20]	Time  0.327 ( 0.327)	Loss 1.6191e-01 (1.6191e-01)	Acc@1  95.31 ( 95.31)	Acc@5  99.61 ( 99.61)
03-Mar-22 09:37:04 - Epoch: [3][ 40/352]	Time  0.128 ( 0.136)	Data  0.002 ( 0.009)	Loss 2.0995e-01 (1.7261e-01)	Acc@1  92.97 ( 94.44)	Acc@5  99.22 ( 99.83)
03-Mar-22 09:37:05 - Test: [10/20]	Time  0.091 ( 0.098)	Loss 1.4004e-01 (1.7232e-01)	Acc@1  96.09 ( 94.21)	Acc@5 100.00 ( 99.75)
03-Mar-22 09:37:05 -  * Acc@1 94.200 Acc@5 99.800
03-Mar-22 09:37:05 - Best acc at epoch 2: 94.5199966430664
03-Mar-22 09:37:06 - Epoch: [3][ 50/352]	Time  0.113 ( 0.134)	Data  0.003 ( 0.008)	Loss 2.1274e-01 (1.7319e-01)	Acc@1  92.19 ( 94.47)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:37:06 - Epoch: [3][  0/352]	Time  0.350 ( 0.350)	Data  0.240 ( 0.240)	Loss 9.5441e-02 (9.5441e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
03-Mar-22 09:37:07 - Epoch: [3][ 60/352]	Time  0.117 ( 0.131)	Data  0.002 ( 0.007)	Loss 1.3446e-01 (1.7303e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:37:07 - Epoch: [3][ 10/352]	Time  0.154 ( 0.142)	Data  0.002 ( 0.024)	Loss 1.3266e-01 (1.5552e-01)	Acc@1  94.53 ( 94.67)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:37:08 - Epoch: [3][ 70/352]	Time  0.142 ( 0.131)	Data  0.002 ( 0.006)	Loss 9.5662e-02 (1.7006e-01)	Acc@1  96.09 ( 94.49)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:37:08 - Epoch: [3][ 20/352]	Time  0.127 ( 0.135)	Data  0.002 ( 0.013)	Loss 9.4512e-02 (1.6919e-01)	Acc@1  96.88 ( 94.53)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:09 - Epoch: [3][ 80/352]	Time  0.132 ( 0.131)	Data  0.003 ( 0.006)	Loss 2.3449e-01 (1.7168e-01)	Acc@1  91.41 ( 94.43)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:37:10 - Epoch: [3][ 30/352]	Time  0.127 ( 0.134)	Data  0.002 ( 0.010)	Loss 1.6414e-01 (1.6754e-01)	Acc@1  92.97 ( 94.71)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:11 - Epoch: [3][ 90/352]	Time  0.130 ( 0.131)	Data  0.002 ( 0.005)	Loss 9.6528e-02 (1.6855e-01)	Acc@1  98.44 ( 94.56)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:11 - Epoch: [3][ 40/352]	Time  0.130 ( 0.132)	Data  0.002 ( 0.008)	Loss 2.0341e-01 (1.6456e-01)	Acc@1  91.41 ( 94.72)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:12 - Epoch: [3][100/352]	Time  0.144 ( 0.131)	Data  0.003 ( 0.005)	Loss 2.0880e-01 (1.6693e-01)	Acc@1  92.19 ( 94.63)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:12 - Epoch: [3][ 50/352]	Time  0.131 ( 0.131)	Data  0.002 ( 0.007)	Loss 1.9912e-01 (1.7056e-01)	Acc@1  93.75 ( 94.42)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:13 - Epoch: [3][110/352]	Time  0.142 ( 0.131)	Data  0.002 ( 0.005)	Loss 1.8849e-01 (1.6569e-01)	Acc@1  96.88 ( 94.67)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:37:13 - Epoch: [3][ 60/352]	Time  0.133 ( 0.131)	Data  0.002 ( 0.006)	Loss 2.4888e-01 (1.6915e-01)	Acc@1  90.62 ( 94.43)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:15 - Epoch: [3][120/352]	Time  0.132 ( 0.131)	Data  0.003 ( 0.005)	Loss 1.4943e-01 (1.6410e-01)	Acc@1  92.97 ( 94.77)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:15 - Epoch: [3][ 70/352]	Time  0.111 ( 0.130)	Data  0.002 ( 0.006)	Loss 1.4429e-01 (1.6762e-01)	Acc@1  94.53 ( 94.54)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:37:16 - Epoch: [3][ 80/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.005)	Loss 8.3712e-02 (1.6549e-01)	Acc@1  98.44 ( 94.67)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:16 - Epoch: [3][130/352]	Time  0.109 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.0984e-01 (1.6281e-01)	Acc@1  92.97 ( 94.82)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:17 - Epoch: [3][ 90/352]	Time  0.128 ( 0.128)	Data  0.002 ( 0.005)	Loss 1.5166e-01 (1.6579e-01)	Acc@1  93.75 ( 94.60)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:17 - Epoch: [3][140/352]	Time  0.141 ( 0.130)	Data  0.002 ( 0.004)	Loss 3.0223e-01 (1.6557e-01)	Acc@1  89.84 ( 94.72)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:37:18 - Epoch: [3][100/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.005)	Loss 1.3858e-01 (1.6422e-01)	Acc@1  93.75 ( 94.64)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:18 - Epoch: [3][150/352]	Time  0.133 ( 0.130)	Data  0.003 ( 0.004)	Loss 2.1390e-01 (1.6487e-01)	Acc@1  93.75 ( 94.72)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:37:19 - Epoch: [3][110/352]	Time  0.128 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.6268e-01 (1.6447e-01)	Acc@1  93.75 ( 94.62)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:37:20 - Epoch: [3][160/352]	Time  0.148 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.2855e-01 (1.6422e-01)	Acc@1  96.09 ( 94.76)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:37:21 - Epoch: [3][120/352]	Time  0.129 ( 0.126)	Data  0.002 ( 0.004)	Loss 2.0216e-01 (1.6530e-01)	Acc@1  93.75 ( 94.60)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:37:21 - Epoch: [3][170/352]	Time  0.130 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.6061e-01 (1.6374e-01)	Acc@1  95.31 ( 94.77)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:22 - Epoch: [3][130/352]	Time  0.103 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.5736e-01 (1.6439e-01)	Acc@1  95.31 ( 94.62)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:37:22 - Epoch: [3][180/352]	Time  0.128 ( 0.130)	Data  0.001 ( 0.004)	Loss 2.3081e-01 (1.6444e-01)	Acc@1  91.41 ( 94.75)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:23 - Epoch: [3][140/352]	Time  0.129 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.0750e-01 (1.6349e-01)	Acc@1  97.66 ( 94.68)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:24 - Epoch: [3][190/352]	Time  0.109 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.3443e-01 (1.6573e-01)	Acc@1  96.88 ( 94.70)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:24 - Epoch: [3][150/352]	Time  0.131 ( 0.126)	Data  0.003 ( 0.004)	Loss 1.9928e-01 (1.6396e-01)	Acc@1  92.97 ( 94.62)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:25 - Epoch: [3][200/352]	Time  0.125 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.5230e-01 (1.6721e-01)	Acc@1  93.75 ( 94.62)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:26 - Epoch: [3][160/352]	Time  0.128 ( 0.126)	Data  0.003 ( 0.004)	Loss 2.0077e-01 (1.6452e-01)	Acc@1  92.97 ( 94.62)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:26 - Epoch: [3][210/352]	Time  0.130 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.5352e-01 (1.6768e-01)	Acc@1  94.53 ( 94.60)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:27 - Epoch: [3][170/352]	Time  0.117 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.8692e-01 (1.6481e-01)	Acc@1  92.97 ( 94.63)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:27 - Epoch: [3][220/352]	Time  0.126 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.3877e-01 (1.6686e-01)	Acc@1  96.88 ( 94.64)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:28 - Epoch: [3][180/352]	Time  0.127 ( 0.126)	Data  0.002 ( 0.004)	Loss 2.2957e-01 (1.6503e-01)	Acc@1  93.75 ( 94.60)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:29 - Epoch: [3][230/352]	Time  0.108 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.8450e-01 (1.6670e-01)	Acc@1  94.53 ( 94.64)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:29 - Epoch: [3][190/352]	Time  0.142 ( 0.126)	Data  0.003 ( 0.004)	Loss 1.1824e-01 (1.6434e-01)	Acc@1  96.88 ( 94.63)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:30 - Epoch: [3][240/352]	Time  0.122 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.3108e-01 (1.6580e-01)	Acc@1  96.09 ( 94.68)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:31 - Epoch: [3][200/352]	Time  0.128 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.3805e-01 (1.6458e-01)	Acc@1  96.88 ( 94.64)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:31 - Epoch: [3][250/352]	Time  0.123 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.6985e-01 (1.6581e-01)	Acc@1  92.97 ( 94.67)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:32 - Epoch: [3][210/352]	Time  0.132 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.1352e-01 (1.6463e-01)	Acc@1  96.09 ( 94.64)	Acc@5  98.44 ( 99.88)
03-Mar-22 09:37:33 - Epoch: [3][260/352]	Time  0.124 ( 0.130)	Data  0.003 ( 0.003)	Loss 1.6665e-01 (1.6581e-01)	Acc@1  95.31 ( 94.68)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:33 - Epoch: [3][220/352]	Time  0.109 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.6724e-01 (1.6399e-01)	Acc@1  94.53 ( 94.65)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:37:34 - Epoch: [3][270/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1338e-01 (1.6519e-01)	Acc@1  95.31 ( 94.69)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:35 - Epoch: [3][230/352]	Time  0.137 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.4411e-01 (1.6464e-01)	Acc@1  93.75 ( 94.63)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:35 - Epoch: [3][280/352]	Time  0.135 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4548e-01 (1.6390e-01)	Acc@1  96.09 ( 94.74)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:36 - Epoch: [3][240/352]	Time  0.136 ( 0.126)	Data  0.003 ( 0.003)	Loss 1.6627e-01 (1.6508e-01)	Acc@1  93.75 ( 94.61)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:36 - Epoch: [3][290/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7909e-01 (1.6368e-01)	Acc@1  93.75 ( 94.75)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:37 - Epoch: [3][250/352]	Time  0.142 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.3612e-01 (1.6571e-01)	Acc@1  96.88 ( 94.59)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:38 - Epoch: [3][300/352]	Time  0.111 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.0070e-01 (1.6411e-01)	Acc@1  93.75 ( 94.75)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:38 - Epoch: [3][260/352]	Time  0.127 ( 0.127)	Data  0.003 ( 0.003)	Loss 1.9608e-01 (1.6549e-01)	Acc@1  93.75 ( 94.59)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:37:39 - Epoch: [3][310/352]	Time  0.114 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.6400e-01 (1.6419e-01)	Acc@1  95.31 ( 94.74)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:40 - Epoch: [3][270/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.0011e-01 (1.6652e-01)	Acc@1  92.19 ( 94.55)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:37:40 - Epoch: [3][320/352]	Time  0.109 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3187e-01 (1.6355e-01)	Acc@1  94.53 ( 94.77)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:41 - Epoch: [3][280/352]	Time  0.136 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.9062e-01 (1.6686e-01)	Acc@1  94.53 ( 94.51)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:41 - Epoch: [3][330/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4351e-01 (1.6359e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:42 - Epoch: [3][290/352]	Time  0.120 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.8649e-01 (1.6700e-01)	Acc@1  92.97 ( 94.52)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:43 - Epoch: [3][340/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.6666e-01 (1.6406e-01)	Acc@1  96.09 ( 94.71)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:43 - Epoch: [3][300/352]	Time  0.111 ( 0.126)	Data  0.002 ( 0.003)	Loss 8.9872e-02 (1.6662e-01)	Acc@1  99.22 ( 94.54)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:44 - Epoch: [3][350/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0296e-01 (1.6387e-01)	Acc@1  97.66 ( 94.71)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:44 - Test: [ 0/20]	Time  0.341 ( 0.341)	Loss 1.5193e-01 (1.5193e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:37:45 - Epoch: [3][310/352]	Time  0.122 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.7018e-01 (1.6716e-01)	Acc@1  93.75 ( 94.51)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:45 - Test: [10/20]	Time  0.092 ( 0.103)	Loss 1.6250e-01 (1.6769e-01)	Acc@1  94.92 ( 94.21)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:37:46 -  * Acc@1 94.400 Acc@5 99.840
03-Mar-22 09:37:46 - Best acc at epoch 3: 94.41999816894531
03-Mar-22 09:37:46 - Epoch: [3][320/352]	Time  0.148 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.3533e-01 (1.6734e-01)	Acc@1  91.41 ( 94.50)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:46 - Epoch: [4][  0/352]	Time  0.356 ( 0.356)	Data  0.245 ( 0.245)	Loss 1.1441e-01 (1.1441e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
03-Mar-22 09:37:47 - Epoch: [3][330/352]	Time  0.105 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.0172e-01 (1.6662e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:48 - Epoch: [4][ 10/352]	Time  0.130 ( 0.150)	Data  0.003 ( 0.024)	Loss 2.3802e-01 (1.7684e-01)	Acc@1  91.41 ( 94.03)	Acc@5  99.22 ( 99.72)
03-Mar-22 09:37:48 - Epoch: [3][340/352]	Time  0.127 ( 0.126)	Data  0.003 ( 0.003)	Loss 1.8797e-01 (1.6638e-01)	Acc@1  96.09 ( 94.56)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:37:49 - Epoch: [4][ 20/352]	Time  0.125 ( 0.141)	Data  0.002 ( 0.014)	Loss 1.4966e-01 (1.6138e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:37:50 - Epoch: [3][350/352]	Time  0.123 ( 0.126)	Data  0.003 ( 0.003)	Loss 1.4421e-01 (1.6641e-01)	Acc@1  93.75 ( 94.55)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:50 - Test: [ 0/20]	Time  0.333 ( 0.333)	Loss 1.7690e-01 (1.7690e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:37:50 - Epoch: [4][ 30/352]	Time  0.109 ( 0.136)	Data  0.002 ( 0.010)	Loss 1.7271e-01 (1.5623e-01)	Acc@1  96.09 ( 95.06)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:37:51 - Test: [10/20]	Time  0.089 ( 0.102)	Loss 1.3970e-01 (1.7114e-01)	Acc@1  94.53 ( 94.42)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:37:52 - Epoch: [4][ 40/352]	Time  0.127 ( 0.134)	Data  0.002 ( 0.008)	Loss 1.1740e-01 (1.4989e-01)	Acc@1  96.88 ( 95.26)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:52 -  * Acc@1 94.380 Acc@5 99.900
03-Mar-22 09:37:52 - Best acc at epoch 3: 94.5199966430664
03-Mar-22 09:37:52 - Epoch: [4][  0/352]	Time  0.349 ( 0.349)	Data  0.224 ( 0.224)	Loss 1.1853e-01 (1.1853e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
03-Mar-22 09:37:53 - Epoch: [4][ 50/352]	Time  0.126 ( 0.131)	Data  0.002 ( 0.007)	Loss 1.6225e-01 (1.5286e-01)	Acc@1  94.53 ( 95.21)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:53 - Epoch: [4][ 10/352]	Time  0.131 ( 0.141)	Data  0.002 ( 0.022)	Loss 1.4596e-01 (1.5076e-01)	Acc@1  96.09 ( 94.74)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:37:54 - Epoch: [4][ 60/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.006)	Loss 1.8388e-01 (1.5369e-01)	Acc@1  95.31 ( 95.17)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:37:55 - Epoch: [4][ 20/352]	Time  0.126 ( 0.131)	Data  0.003 ( 0.013)	Loss 1.0174e-01 (1.5433e-01)	Acc@1  96.88 ( 94.75)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:55 - Epoch: [4][ 70/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.006)	Loss 2.6094e-01 (1.5545e-01)	Acc@1  92.97 ( 95.16)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:56 - Epoch: [4][ 30/352]	Time  0.130 ( 0.128)	Data  0.002 ( 0.009)	Loss 1.4971e-01 (1.5481e-01)	Acc@1  96.09 ( 94.71)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:37:57 - Epoch: [4][ 80/352]	Time  0.138 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.0344e-01 (1.5617e-01)	Acc@1  96.09 ( 95.11)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:57 - Epoch: [4][ 40/352]	Time  0.130 ( 0.125)	Data  0.002 ( 0.008)	Loss 2.0691e-01 (1.5666e-01)	Acc@1  92.97 ( 94.74)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:37:58 - Epoch: [4][ 90/352]	Time  0.115 ( 0.129)	Data  0.002 ( 0.005)	Loss 7.6910e-02 (1.5508e-01)	Acc@1  97.66 ( 95.06)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:58 - Epoch: [4][ 50/352]	Time  0.125 ( 0.123)	Data  0.003 ( 0.007)	Loss 1.1835e-01 (1.5709e-01)	Acc@1  96.09 ( 94.76)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:37:59 - Epoch: [4][100/352]	Time  0.138 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.4493e-01 (1.5349e-01)	Acc@1  93.75 ( 95.07)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:37:59 - Epoch: [4][ 60/352]	Time  0.110 ( 0.123)	Data  0.003 ( 0.006)	Loss 2.3014e-01 (1.5800e-01)	Acc@1  91.41 ( 94.81)	Acc@5  99.22 ( 99.86)
03-Mar-22 09:38:00 - Epoch: [4][110/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.7750e-01 (1.5312e-01)	Acc@1  92.97 ( 95.08)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:01 - Epoch: [4][ 70/352]	Time  0.109 ( 0.122)	Data  0.002 ( 0.005)	Loss 1.3837e-01 (1.5845e-01)	Acc@1  94.53 ( 94.71)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:02 - Epoch: [4][120/352]	Time  0.135 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.1537e-01 (1.5368e-01)	Acc@1  96.88 ( 95.07)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:02 - Epoch: [4][ 80/352]	Time  0.131 ( 0.122)	Data  0.002 ( 0.005)	Loss 1.7887e-01 (1.5794e-01)	Acc@1  96.09 ( 94.83)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:03 - Epoch: [4][130/352]	Time  0.132 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.7363e-01 (1.5442e-01)	Acc@1  92.97 ( 95.01)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:03 - Epoch: [4][ 90/352]	Time  0.129 ( 0.123)	Data  0.003 ( 0.005)	Loss 1.6384e-01 (1.5694e-01)	Acc@1  94.53 ( 94.82)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:04 - Epoch: [4][140/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.0562e-01 (1.5541e-01)	Acc@1  97.66 ( 94.96)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:04 - Epoch: [4][100/352]	Time  0.128 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.4020e-01 (1.5670e-01)	Acc@1  93.75 ( 94.82)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:05 - Epoch: [4][110/352]	Time  0.100 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.9687e-01 (1.5691e-01)	Acc@1  92.19 ( 94.81)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:06 - Epoch: [4][150/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.7073e-01 (1.5587e-01)	Acc@1  93.75 ( 94.92)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:07 - Epoch: [4][120/352]	Time  0.141 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.3085e-01 (1.6041e-01)	Acc@1  95.31 ( 94.65)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:07 - Epoch: [4][160/352]	Time  0.128 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.3363e-01 (1.5594e-01)	Acc@1  91.41 ( 94.95)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:38:08 - Epoch: [4][130/352]	Time  0.130 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.3715e-01 (1.6037e-01)	Acc@1  95.31 ( 94.64)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:08 - Epoch: [4][170/352]	Time  0.131 ( 0.130)	Data  0.003 ( 0.004)	Loss 7.3720e-02 (1.5555e-01)	Acc@1  96.88 ( 94.93)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:09 - Epoch: [4][140/352]	Time  0.121 ( 0.123)	Data  0.002 ( 0.004)	Loss 2.0560e-01 (1.6147e-01)	Acc@1  94.53 ( 94.62)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:38:10 - Epoch: [4][180/352]	Time  0.109 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.1279e-01 (1.5664e-01)	Acc@1  96.88 ( 94.92)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:10 - Epoch: [4][150/352]	Time  0.111 ( 0.123)	Data  0.002 ( 0.004)	Loss 2.1568e-01 (1.6229e-01)	Acc@1  91.41 ( 94.61)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:38:11 - Epoch: [4][190/352]	Time  0.128 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.4320e-01 (1.5792e-01)	Acc@1  93.75 ( 94.87)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:12 - Epoch: [4][160/352]	Time  0.137 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.5992e-01 (1.6235e-01)	Acc@1  96.09 ( 94.62)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:38:12 - Epoch: [4][200/352]	Time  0.130 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.6239e-01 (1.5815e-01)	Acc@1  95.31 ( 94.89)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:13 - Epoch: [4][170/352]	Time  0.113 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.2743e-01 (1.6268e-01)	Acc@1  96.09 ( 94.59)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:13 - Epoch: [4][210/352]	Time  0.122 ( 0.130)	Data  0.003 ( 0.003)	Loss 1.5960e-01 (1.5800e-01)	Acc@1  93.75 ( 94.86)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:14 - Epoch: [4][180/352]	Time  0.136 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6419e-01 (1.6366e-01)	Acc@1  95.31 ( 94.57)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:15 - Epoch: [4][220/352]	Time  0.132 ( 0.130)	Data  0.003 ( 0.003)	Loss 2.7856e-01 (1.5969e-01)	Acc@1  90.62 ( 94.82)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:38:16 - Epoch: [4][190/352]	Time  0.140 ( 0.125)	Data  0.003 ( 0.003)	Loss 2.4513e-01 (1.6419e-01)	Acc@1  93.75 ( 94.58)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:38:16 - Epoch: [4][230/352]	Time  0.097 ( 0.130)	Data  0.001 ( 0.003)	Loss 1.5102e-01 (1.6090e-01)	Acc@1  94.53 ( 94.80)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:17 - Epoch: [4][200/352]	Time  0.136 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.2304e-01 (1.6356e-01)	Acc@1  96.88 ( 94.61)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:17 - Epoch: [4][240/352]	Time  0.136 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3037e-01 (1.6200e-01)	Acc@1  95.31 ( 94.77)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:18 - Epoch: [4][210/352]	Time  0.121 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.1378e-01 (1.6347e-01)	Acc@1  93.75 ( 94.60)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:19 - Epoch: [4][250/352]	Time  0.109 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4775e-01 (1.6225e-01)	Acc@1  93.75 ( 94.75)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:19 - Epoch: [4][220/352]	Time  0.106 ( 0.124)	Data  0.002 ( 0.003)	Loss 2.0019e-01 (1.6318e-01)	Acc@1  91.41 ( 94.62)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:20 - Epoch: [4][260/352]	Time  0.135 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1366e-01 (1.6249e-01)	Acc@1  95.31 ( 94.70)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:21 - Epoch: [4][230/352]	Time  0.123 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.7565e-01 (1.6278e-01)	Acc@1  94.53 ( 94.64)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:21 - Epoch: [4][270/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0423e-01 (1.6178e-01)	Acc@1  96.88 ( 94.72)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:22 - Epoch: [4][240/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.003)	Loss 8.4067e-02 (1.6221e-01)	Acc@1  99.22 ( 94.67)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:22 - Epoch: [4][280/352]	Time  0.135 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.6038e-01 (1.6178e-01)	Acc@1  93.75 ( 94.70)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:23 - Epoch: [4][250/352]	Time  0.116 ( 0.124)	Data  0.002 ( 0.003)	Loss 8.2782e-02 (1.6199e-01)	Acc@1  96.88 ( 94.67)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:24 - Epoch: [4][290/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1411e-01 (1.6168e-01)	Acc@1  96.09 ( 94.69)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:24 - Epoch: [4][260/352]	Time  0.102 ( 0.124)	Data  0.001 ( 0.003)	Loss 2.9601e-01 (1.6229e-01)	Acc@1  89.84 ( 94.64)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:25 - Epoch: [4][300/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5984e-01 (1.6252e-01)	Acc@1  94.53 ( 94.67)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:25 - Epoch: [4][270/352]	Time  0.103 ( 0.123)	Data  0.002 ( 0.003)	Loss 1.3394e-01 (1.6250e-01)	Acc@1  96.09 ( 94.63)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:26 - Epoch: [4][310/352]	Time  0.145 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.2600e-01 (1.6263e-01)	Acc@1  96.88 ( 94.66)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:26 - Epoch: [4][280/352]	Time  0.107 ( 0.123)	Data  0.002 ( 0.003)	Loss 1.3377e-01 (1.6208e-01)	Acc@1  96.09 ( 94.63)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:28 - Epoch: [4][320/352]	Time  0.102 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0725e-01 (1.6245e-01)	Acc@1  96.88 ( 94.67)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:28 - Epoch: [4][290/352]	Time  0.115 ( 0.123)	Data  0.002 ( 0.003)	Loss 1.6450e-01 (1.6282e-01)	Acc@1  94.53 ( 94.59)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:29 - Epoch: [4][330/352]	Time  0.115 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3044e-01 (1.6256e-01)	Acc@1  96.88 ( 94.66)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:29 - Epoch: [4][300/352]	Time  0.103 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.1775e-01 (1.6369e-01)	Acc@1  96.88 ( 94.57)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:30 - Epoch: [4][340/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5708e-01 (1.6216e-01)	Acc@1  93.75 ( 94.67)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:30 - Epoch: [4][310/352]	Time  0.124 ( 0.123)	Data  0.002 ( 0.003)	Loss 1.6178e-01 (1.6351e-01)	Acc@1  93.75 ( 94.56)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:31 - Epoch: [4][320/352]	Time  0.123 ( 0.123)	Data  0.002 ( 0.003)	Loss 5.9187e-02 (1.6322e-01)	Acc@1  99.22 ( 94.60)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:31 - Epoch: [4][350/352]	Time  0.108 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5690e-01 (1.6237e-01)	Acc@1  96.09 ( 94.67)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:32 - Test: [ 0/20]	Time  0.356 ( 0.356)	Loss 1.7736e-01 (1.7736e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:38:32 - Epoch: [4][330/352]	Time  0.092 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.9389e-01 (1.6320e-01)	Acc@1  93.75 ( 94.60)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:33 - Test: [10/20]	Time  0.071 ( 0.102)	Loss 1.2909e-01 (1.6111e-01)	Acc@1  96.48 ( 94.99)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:38:33 - Epoch: [4][340/352]	Time  0.092 ( 0.121)	Data  0.002 ( 0.003)	Loss 1.2349e-01 (1.6307e-01)	Acc@1  95.31 ( 94.61)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:33 -  * Acc@1 94.780 Acc@5 99.860
03-Mar-22 09:38:33 - Best acc at epoch 4: 94.77999877929688
03-Mar-22 09:38:34 - Epoch: [5][  0/352]	Time  0.340 ( 0.340)	Data  0.226 ( 0.226)	Loss 1.4162e-01 (1.4162e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:38:34 - Epoch: [4][350/352]	Time  0.101 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.3037e-01 (1.6231e-01)	Acc@1  94.53 ( 94.64)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:38:35 - Test: [ 0/20]	Time  0.322 ( 0.322)	Loss 1.7307e-01 (1.7307e-01)	Acc@1  93.36 ( 93.36)	Acc@5 100.00 (100.00)
03-Mar-22 09:38:35 - Epoch: [5][ 10/352]	Time  0.130 ( 0.138)	Data  0.002 ( 0.022)	Loss 1.0685e-01 (1.6494e-01)	Acc@1  98.44 ( 94.60)	Acc@5  99.22 ( 99.72)
03-Mar-22 09:38:36 - Test: [10/20]	Time  0.087 ( 0.098)	Loss 1.6222e-01 (1.6356e-01)	Acc@1  92.97 ( 94.39)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:36 - Epoch: [5][ 20/352]	Time  0.127 ( 0.133)	Data  0.002 ( 0.013)	Loss 1.6888e-01 (1.7281e-01)	Acc@1  94.53 ( 94.20)	Acc@5 100.00 ( 99.78)
03-Mar-22 09:38:36 -  * Acc@1 94.340 Acc@5 99.920
03-Mar-22 09:38:36 - Best acc at epoch 4: 94.5199966430664
03-Mar-22 09:38:37 - Epoch: [5][  0/352]	Time  0.354 ( 0.354)	Data  0.237 ( 0.237)	Loss 1.7563e-01 (1.7563e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
03-Mar-22 09:38:37 - Epoch: [5][ 30/352]	Time  0.121 ( 0.128)	Data  0.002 ( 0.009)	Loss 1.0905e-01 (1.6675e-01)	Acc@1  97.66 ( 94.41)	Acc@5  99.22 ( 99.82)
03-Mar-22 09:38:38 - Epoch: [5][ 10/352]	Time  0.128 ( 0.144)	Data  0.002 ( 0.023)	Loss 1.4214e-01 (1.5393e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:38:39 - Epoch: [5][ 40/352]	Time  0.149 ( 0.129)	Data  0.002 ( 0.008)	Loss 1.0632e-01 (1.6089e-01)	Acc@1  96.88 ( 94.74)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:38:39 - Epoch: [5][ 20/352]	Time  0.136 ( 0.137)	Data  0.002 ( 0.013)	Loss 2.6260e-01 (1.6609e-01)	Acc@1  90.62 ( 94.23)	Acc@5  99.22 ( 99.81)
03-Mar-22 09:38:40 - Epoch: [5][ 50/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.007)	Loss 1.1993e-01 (1.5808e-01)	Acc@1  96.88 ( 94.94)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:41 - Epoch: [5][ 30/352]	Time  0.140 ( 0.134)	Data  0.003 ( 0.010)	Loss 1.6171e-01 (1.5898e-01)	Acc@1  94.53 ( 94.78)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:38:41 - Epoch: [5][ 60/352]	Time  0.108 ( 0.129)	Data  0.002 ( 0.006)	Loss 1.1397e-01 (1.5845e-01)	Acc@1  96.88 ( 94.94)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:38:42 - Epoch: [5][ 40/352]	Time  0.135 ( 0.132)	Data  0.002 ( 0.008)	Loss 7.8852e-02 (1.5519e-01)	Acc@1  98.44 ( 94.80)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:38:43 - Epoch: [5][ 70/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.005)	Loss 2.7760e-01 (1.6008e-01)	Acc@1  90.62 ( 94.85)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:43 - Epoch: [5][ 50/352]	Time  0.136 ( 0.133)	Data  0.002 ( 0.007)	Loss 1.8932e-01 (1.5908e-01)	Acc@1  94.53 ( 94.81)	Acc@5 100.00 ( 99.83)
03-Mar-22 09:38:44 - Epoch: [5][ 80/352]	Time  0.152 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.5681e-01 (1.6114e-01)	Acc@1  94.53 ( 94.76)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:44 - Epoch: [5][ 60/352]	Time  0.124 ( 0.132)	Data  0.002 ( 0.006)	Loss 1.4079e-01 (1.6082e-01)	Acc@1  96.09 ( 94.70)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:38:45 - Epoch: [5][ 90/352]	Time  0.132 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.4539e-01 (1.5956e-01)	Acc@1  93.75 ( 94.73)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:46 - Epoch: [5][ 70/352]	Time  0.136 ( 0.132)	Data  0.002 ( 0.006)	Loss 1.8405e-01 (1.6176e-01)	Acc@1  92.19 ( 94.66)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:47 - Epoch: [5][100/352]	Time  0.110 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.0520e-01 (1.5992e-01)	Acc@1  96.09 ( 94.70)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:47 - Epoch: [5][ 80/352]	Time  0.140 ( 0.132)	Data  0.002 ( 0.005)	Loss 2.1151e-01 (1.5989e-01)	Acc@1  92.19 ( 94.68)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:48 - Epoch: [5][110/352]	Time  0.127 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.7520e-01 (1.5966e-01)	Acc@1  93.75 ( 94.68)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:48 - Epoch: [5][ 90/352]	Time  0.137 ( 0.132)	Data  0.002 ( 0.005)	Loss 8.3122e-02 (1.5833e-01)	Acc@1  96.88 ( 94.81)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:38:49 - Epoch: [5][120/352]	Time  0.126 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.2916e-01 (1.6034e-01)	Acc@1  95.31 ( 94.69)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:50 - Epoch: [5][100/352]	Time  0.149 ( 0.131)	Data  0.002 ( 0.005)	Loss 1.0695e-01 (1.5783e-01)	Acc@1  96.88 ( 94.82)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:50 - Epoch: [5][130/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.3240e-01 (1.5784e-01)	Acc@1  97.66 ( 94.85)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:38:51 - Epoch: [5][110/352]	Time  0.137 ( 0.131)	Data  0.003 ( 0.004)	Loss 1.3801e-01 (1.5964e-01)	Acc@1  96.88 ( 94.75)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:52 - Epoch: [5][140/352]	Time  0.130 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.6372e-01 (1.5984e-01)	Acc@1  90.62 ( 94.73)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:52 - Epoch: [5][120/352]	Time  0.119 ( 0.132)	Data  0.002 ( 0.004)	Loss 2.5389e-01 (1.6050e-01)	Acc@1  92.97 ( 94.73)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:53 - Epoch: [5][150/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.1975e-01 (1.5854e-01)	Acc@1  96.09 ( 94.77)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:54 - Epoch: [5][130/352]	Time  0.125 ( 0.131)	Data  0.002 ( 0.004)	Loss 1.0196e-01 (1.6065e-01)	Acc@1  96.09 ( 94.72)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:38:54 - Epoch: [5][160/352]	Time  0.137 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.9128e-01 (1.5947e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:55 - Epoch: [5][140/352]	Time  0.149 ( 0.131)	Data  0.002 ( 0.004)	Loss 2.3729e-01 (1.6198e-01)	Acc@1  92.19 ( 94.68)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:56 - Epoch: [5][170/352]	Time  0.115 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.2061e-01 (1.6046e-01)	Acc@1  92.19 ( 94.76)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:38:56 - Epoch: [5][150/352]	Time  0.135 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.6519e-01 (1.6126e-01)	Acc@1  94.53 ( 94.72)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:57 - Epoch: [5][180/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.3254e-01 (1.6062e-01)	Acc@1  92.97 ( 94.75)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:57 - Epoch: [5][160/352]	Time  0.114 ( 0.131)	Data  0.002 ( 0.004)	Loss 2.0088e-01 (1.6120e-01)	Acc@1  92.19 ( 94.71)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:38:58 - Epoch: [5][190/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4632e-01 (1.6090e-01)	Acc@1  96.88 ( 94.72)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:59 - Epoch: [5][170/352]	Time  0.135 ( 0.131)	Data  0.003 ( 0.004)	Loss 1.1669e-01 (1.6100e-01)	Acc@1  98.44 ( 94.74)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:38:59 - Epoch: [5][200/352]	Time  0.130 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.4442e-01 (1.6058e-01)	Acc@1  96.09 ( 94.71)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:00 - Epoch: [5][180/352]	Time  0.114 ( 0.131)	Data  0.002 ( 0.004)	Loss 1.9763e-01 (1.6050e-01)	Acc@1  92.19 ( 94.74)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:01 - Epoch: [5][210/352]	Time  0.109 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4196e-01 (1.5988e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:01 - Epoch: [5][190/352]	Time  0.136 ( 0.131)	Data  0.003 ( 0.003)	Loss 2.1552e-01 (1.6082e-01)	Acc@1  94.53 ( 94.72)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:39:02 - Epoch: [5][220/352]	Time  0.128 ( 0.128)	Data  0.003 ( 0.003)	Loss 2.7248e-01 (1.6071e-01)	Acc@1  91.41 ( 94.72)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:03 - Epoch: [5][200/352]	Time  0.116 ( 0.130)	Data  0.001 ( 0.003)	Loss 6.6137e-02 (1.6014e-01)	Acc@1  98.44 ( 94.75)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:03 - Epoch: [5][230/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.5925e-01 (1.6122e-01)	Acc@1  91.41 ( 94.69)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:39:04 - Epoch: [5][210/352]	Time  0.139 ( 0.130)	Data  0.003 ( 0.003)	Loss 1.8849e-01 (1.5987e-01)	Acc@1  92.97 ( 94.75)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:05 - Epoch: [5][240/352]	Time  0.142 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.6180e-01 (1.6080e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:05 - Epoch: [5][220/352]	Time  0.140 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.2135e-01 (1.6015e-01)	Acc@1  96.88 ( 94.74)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:06 - Epoch: [5][250/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7257e-01 (1.6101e-01)	Acc@1  94.53 ( 94.69)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:06 - Epoch: [5][230/352]	Time  0.148 ( 0.130)	Data  0.002 ( 0.003)	Loss 2.3851e-01 (1.6016e-01)	Acc@1  91.41 ( 94.75)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:07 - Epoch: [5][260/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.9428e-01 (1.6099e-01)	Acc@1  92.97 ( 94.69)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:08 - Epoch: [5][240/352]	Time  0.154 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.3747e-01 (1.5932e-01)	Acc@1  96.09 ( 94.76)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:08 - Epoch: [5][270/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7321e-01 (1.6065e-01)	Acc@1  93.75 ( 94.71)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:39:09 - Epoch: [5][250/352]	Time  0.140 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.4822e-01 (1.5953e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:10 - Epoch: [5][280/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0699e-01 (1.6027e-01)	Acc@1  96.09 ( 94.72)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:10 - Epoch: [5][260/352]	Time  0.112 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5452e-01 (1.5836e-01)	Acc@1  94.53 ( 94.82)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:11 - Epoch: [5][290/352]	Time  0.127 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.6636e-01 (1.6057e-01)	Acc@1  94.53 ( 94.71)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:11 - Epoch: [5][270/352]	Time  0.137 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1703e-01 (1.5859e-01)	Acc@1  97.66 ( 94.84)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:12 - Epoch: [5][300/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7311e-01 (1.6073e-01)	Acc@1  92.97 ( 94.70)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:39:13 - Epoch: [5][280/352]	Time  0.139 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.2128e-01 (1.5920e-01)	Acc@1  92.97 ( 94.78)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:14 - Epoch: [5][310/352]	Time  0.134 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3878e-01 (1.6038e-01)	Acc@1  94.53 ( 94.69)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:14 - Epoch: [5][290/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0590e-01 (1.5921e-01)	Acc@1  96.88 ( 94.75)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:15 - Epoch: [5][320/352]	Time  0.139 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3506e-01 (1.6054e-01)	Acc@1  97.66 ( 94.70)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:15 - Epoch: [5][300/352]	Time  0.136 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4131e-01 (1.5952e-01)	Acc@1  95.31 ( 94.77)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:16 - Epoch: [5][330/352]	Time  0.104 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.3008e-01 (1.6075e-01)	Acc@1  91.41 ( 94.69)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:17 - Epoch: [5][310/352]	Time  0.138 ( 0.130)	Data  0.003 ( 0.003)	Loss 1.7321e-01 (1.6085e-01)	Acc@1  93.75 ( 94.73)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:17 - Epoch: [5][340/352]	Time  0.140 ( 0.129)	Data  0.001 ( 0.003)	Loss 1.8369e-01 (1.6120e-01)	Acc@1  93.75 ( 94.68)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:18 - Epoch: [5][320/352]	Time  0.139 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7632e-01 (1.6082e-01)	Acc@1  93.75 ( 94.73)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:19 - Epoch: [5][350/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4019e-01 (1.6050e-01)	Acc@1  94.53 ( 94.69)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:39:19 - Epoch: [5][330/352]	Time  0.125 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.0108e-01 (1.6134e-01)	Acc@1  96.88 ( 94.72)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:19 - Test: [ 0/20]	Time  0.323 ( 0.323)	Loss 1.7837e-01 (1.7837e-01)	Acc@1  94.53 ( 94.53)	Acc@5  99.61 ( 99.61)
03-Mar-22 09:39:20 - Test: [10/20]	Time  0.072 ( 0.098)	Loss 1.7533e-01 (1.6922e-01)	Acc@1  94.92 ( 94.64)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:20 - Epoch: [5][340/352]	Time  0.161 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1426e-01 (1.6131e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:21 -  * Acc@1 94.660 Acc@5 99.880
03-Mar-22 09:39:21 - Best acc at epoch 5: 94.77999877929688
03-Mar-22 09:39:21 - Epoch: [6][  0/352]	Time  0.341 ( 0.341)	Data  0.241 ( 0.241)	Loss 1.5468e-01 (1.5468e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:39:22 - Epoch: [5][350/352]	Time  0.100 ( 0.129)	Data  0.001 ( 0.003)	Loss 1.9660e-01 (1.6146e-01)	Acc@1  92.97 ( 94.74)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:22 - Test: [ 0/20]	Time  0.309 ( 0.309)	Loss 1.3605e-01 (1.3605e-01)	Acc@1  94.92 ( 94.92)	Acc@5 100.00 (100.00)
03-Mar-22 09:39:22 - Epoch: [6][ 10/352]	Time  0.134 ( 0.144)	Data  0.003 ( 0.024)	Loss 1.6643e-01 (1.7830e-01)	Acc@1  95.31 ( 94.39)	Acc@5  99.22 ( 99.86)
03-Mar-22 09:39:23 - Test: [10/20]	Time  0.069 ( 0.098)	Loss 1.4809e-01 (1.6415e-01)	Acc@1  94.53 ( 94.42)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:24 -  * Acc@1 94.300 Acc@5 99.880
03-Mar-22 09:39:24 - Best acc at epoch 5: 94.5199966430664
03-Mar-22 09:39:24 - Epoch: [6][ 20/352]	Time  0.137 ( 0.134)	Data  0.002 ( 0.014)	Loss 1.2918e-01 (1.6959e-01)	Acc@1  96.09 ( 94.79)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:24 - Epoch: [6][  0/352]	Time  0.371 ( 0.371)	Data  0.241 ( 0.241)	Loss 1.3188e-01 (1.3188e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:39:25 - Epoch: [6][ 30/352]	Time  0.133 ( 0.134)	Data  0.002 ( 0.010)	Loss 2.0074e-01 (1.6299e-01)	Acc@1  92.97 ( 94.91)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:39:25 - Epoch: [6][ 10/352]	Time  0.138 ( 0.153)	Data  0.002 ( 0.024)	Loss 1.3683e-01 (1.4527e-01)	Acc@1  96.09 ( 95.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:39:26 - Epoch: [6][ 40/352]	Time  0.145 ( 0.130)	Data  0.002 ( 0.008)	Loss 1.1852e-01 (1.6422e-01)	Acc@1  96.88 ( 94.66)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:39:27 - Epoch: [6][ 20/352]	Time  0.127 ( 0.141)	Data  0.002 ( 0.014)	Loss 1.4570e-01 (1.4319e-01)	Acc@1  95.31 ( 95.46)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:39:28 - Epoch: [6][ 50/352]	Time  0.145 ( 0.132)	Data  0.003 ( 0.007)	Loss 2.1169e-01 (1.6347e-01)	Acc@1  93.75 ( 94.55)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:39:28 - Epoch: [6][ 30/352]	Time  0.145 ( 0.139)	Data  0.002 ( 0.010)	Loss 1.2816e-01 (1.3922e-01)	Acc@1  95.31 ( 95.49)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:39:29 - Epoch: [6][ 60/352]	Time  0.131 ( 0.131)	Data  0.002 ( 0.006)	Loss 2.5048e-01 (1.6166e-01)	Acc@1  90.62 ( 94.60)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:29 - Epoch: [6][ 40/352]	Time  0.152 ( 0.138)	Data  0.003 ( 0.008)	Loss 2.0290e-01 (1.4947e-01)	Acc@1  92.97 ( 95.06)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:39:30 - Epoch: [6][ 70/352]	Time  0.141 ( 0.131)	Data  0.002 ( 0.006)	Loss 2.1056e-01 (1.5865e-01)	Acc@1  92.19 ( 94.67)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:31 - Epoch: [6][ 50/352]	Time  0.128 ( 0.136)	Data  0.002 ( 0.007)	Loss 1.8019e-01 (1.5336e-01)	Acc@1  94.53 ( 94.88)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:39:31 - Epoch: [6][ 80/352]	Time  0.110 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.5500e-01 (1.5853e-01)	Acc@1  93.75 ( 94.63)	Acc@5  99.22 ( 99.94)
03-Mar-22 09:39:32 - Epoch: [6][ 60/352]	Time  0.103 ( 0.133)	Data  0.002 ( 0.006)	Loss 1.4540e-01 (1.5446e-01)	Acc@1  95.31 ( 94.84)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:39:33 - Epoch: [6][ 90/352]	Time  0.124 ( 0.130)	Data  0.001 ( 0.005)	Loss 1.7124e-01 (1.5908e-01)	Acc@1  94.53 ( 94.65)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:33 - Epoch: [6][ 70/352]	Time  0.096 ( 0.131)	Data  0.002 ( 0.006)	Loss 1.7363e-01 (1.5419e-01)	Acc@1  93.75 ( 94.85)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:39:34 - Epoch: [6][100/352]	Time  0.112 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.1415e-01 (1.5798e-01)	Acc@1  94.53 ( 94.67)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:34 - Epoch: [6][ 80/352]	Time  0.112 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.4799e-01 (1.5479e-01)	Acc@1  96.88 ( 94.83)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:39:35 - Epoch: [6][110/352]	Time  0.141 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.1940e-01 (1.5777e-01)	Acc@1  94.53 ( 94.62)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:35 - Epoch: [6][ 90/352]	Time  0.124 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.6681e-01 (1.5537e-01)	Acc@1  93.75 ( 94.81)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:39:37 - Epoch: [6][120/352]	Time  0.144 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.3022e-01 (1.5649e-01)	Acc@1  94.53 ( 94.72)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:37 - Epoch: [6][100/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.005)	Loss 2.5584e-01 (1.5867e-01)	Acc@1  91.41 ( 94.72)	Acc@5  99.22 ( 99.86)
03-Mar-22 09:39:38 - Epoch: [6][130/352]	Time  0.148 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.6851e-01 (1.5783e-01)	Acc@1  95.31 ( 94.66)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:38 - Epoch: [6][110/352]	Time  0.105 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.0346e-01 (1.5891e-01)	Acc@1  96.88 ( 94.70)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:39:39 - Epoch: [6][120/352]	Time  0.127 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.8285e-01 (1.5875e-01)	Acc@1  92.97 ( 94.71)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:39:39 - Epoch: [6][140/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.1690e-01 (1.5779e-01)	Acc@1  96.88 ( 94.67)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:40 - Epoch: [6][130/352]	Time  0.127 ( 0.128)	Data  0.002 ( 0.004)	Loss 2.2728e-01 (1.5884e-01)	Acc@1  95.31 ( 94.73)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:39:41 - Epoch: [6][150/352]	Time  0.141 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.3443e-01 (1.5694e-01)	Acc@1  96.09 ( 94.71)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:42 - Epoch: [6][140/352]	Time  0.127 ( 0.128)	Data  0.003 ( 0.004)	Loss 1.3976e-01 (1.5810e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:42 - Epoch: [6][160/352]	Time  0.139 ( 0.130)	Data  0.003 ( 0.004)	Loss 2.4405e-01 (1.5752e-01)	Acc@1  93.75 ( 94.68)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:43 - Epoch: [6][150/352]	Time  0.128 ( 0.128)	Data  0.002 ( 0.004)	Loss 8.9253e-02 (1.5815e-01)	Acc@1  96.09 ( 94.73)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:39:43 - Epoch: [6][170/352]	Time  0.142 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.2309e-01 (1.5703e-01)	Acc@1  92.19 ( 94.72)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:39:44 - Epoch: [6][160/352]	Time  0.118 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.6292e-01 (1.5768e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:39:44 - Epoch: [6][180/352]	Time  0.132 ( 0.131)	Data  0.003 ( 0.004)	Loss 1.8122e-01 (1.5833e-01)	Acc@1  92.97 ( 94.66)	Acc@5  99.22 ( 99.94)
03-Mar-22 09:39:45 - Epoch: [6][170/352]	Time  0.124 ( 0.127)	Data  0.002 ( 0.004)	Loss 9.3485e-02 (1.5576e-01)	Acc@1  97.66 ( 94.84)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:39:46 - Epoch: [6][190/352]	Time  0.137 ( 0.131)	Data  0.002 ( 0.004)	Loss 1.3114e-01 (1.5668e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:47 - Epoch: [6][180/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.2450e-01 (1.5470e-01)	Acc@1  96.09 ( 94.86)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:39:47 - Epoch: [6][200/352]	Time  0.101 ( 0.131)	Data  0.002 ( 0.004)	Loss 1.7893e-01 (1.5557e-01)	Acc@1  94.53 ( 94.75)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:39:48 - Epoch: [6][190/352]	Time  0.108 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.3045e-01 (1.5442e-01)	Acc@1  93.75 ( 94.88)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:48 - Epoch: [6][210/352]	Time  0.132 ( 0.131)	Data  0.002 ( 0.003)	Loss 2.0293e-01 (1.5408e-01)	Acc@1  92.19 ( 94.83)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:49 - Epoch: [6][200/352]	Time  0.104 ( 0.127)	Data  0.001 ( 0.003)	Loss 1.9295e-01 (1.5492e-01)	Acc@1  92.97 ( 94.87)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:50 - Epoch: [6][220/352]	Time  0.112 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.3835e-01 (1.5473e-01)	Acc@1  96.88 ( 94.82)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:50 - Epoch: [6][210/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.6500e-01 (1.5568e-01)	Acc@1  94.53 ( 94.87)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:51 - Epoch: [6][230/352]	Time  0.110 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.1775e-01 (1.5508e-01)	Acc@1  95.31 ( 94.80)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:52 - Epoch: [6][220/352]	Time  0.110 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.4895e-01 (1.5636e-01)	Acc@1  91.41 ( 94.84)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:52 - Epoch: [6][240/352]	Time  0.135 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.7564e-01 (1.5653e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:53 - Epoch: [6][230/352]	Time  0.132 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.3050e-01 (1.5616e-01)	Acc@1  96.88 ( 94.84)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:54 - Epoch: [6][250/352]	Time  0.129 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.8256e-01 (1.5688e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:54 - Epoch: [6][240/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.4140e-01 (1.5602e-01)	Acc@1  92.19 ( 94.85)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:55 - Epoch: [6][260/352]	Time  0.134 ( 0.131)	Data  0.003 ( 0.003)	Loss 1.5259e-01 (1.5657e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:55 - Epoch: [6][250/352]	Time  0.105 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.6777e-01 (1.5599e-01)	Acc@1  93.75 ( 94.87)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:56 - Epoch: [6][270/352]	Time  0.131 ( 0.131)	Data  0.002 ( 0.003)	Loss 2.2077e-01 (1.5623e-01)	Acc@1  93.75 ( 94.78)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:57 - Epoch: [6][260/352]	Time  0.128 ( 0.126)	Data  0.001 ( 0.003)	Loss 2.5071e-01 (1.5597e-01)	Acc@1  91.41 ( 94.87)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:39:58 - Epoch: [6][280/352]	Time  0.112 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.5435e-01 (1.5572e-01)	Acc@1  92.19 ( 94.78)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:39:58 - Epoch: [6][270/352]	Time  0.110 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.2660e-01 (1.5664e-01)	Acc@1  96.09 ( 94.86)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:39:59 - Epoch: [6][290/352]	Time  0.141 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.7686e-01 (1.5613e-01)	Acc@1  94.53 ( 94.78)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:39:59 - Epoch: [6][280/352]	Time  0.107 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.5520e-01 (1.5756e-01)	Acc@1  95.31 ( 94.84)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:40:00 - Epoch: [6][300/352]	Time  0.123 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.0020e-01 (1.5532e-01)	Acc@1  97.66 ( 94.81)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:00 - Epoch: [6][290/352]	Time  0.128 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.3880e-01 (1.5728e-01)	Acc@1  95.31 ( 94.86)	Acc@5  98.44 ( 99.89)
03-Mar-22 09:40:02 - Epoch: [6][310/352]	Time  0.136 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.4182e-01 (1.5584e-01)	Acc@1  96.09 ( 94.79)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:02 - Epoch: [6][300/352]	Time  0.125 ( 0.126)	Data  0.002 ( 0.003)	Loss 1.4779e-01 (1.5789e-01)	Acc@1  95.31 ( 94.82)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:40:03 - Epoch: [6][320/352]	Time  0.124 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.6396e-01 (1.5634e-01)	Acc@1  94.53 ( 94.78)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:03 - Epoch: [6][310/352]	Time  0.132 ( 0.126)	Data  0.003 ( 0.003)	Loss 1.2582e-01 (1.5853e-01)	Acc@1  97.66 ( 94.82)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:40:04 - Epoch: [6][330/352]	Time  0.145 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.6013e-01 (1.5674e-01)	Acc@1  94.53 ( 94.75)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:04 - Epoch: [6][320/352]	Time  0.127 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.0761e-01 (1.5754e-01)	Acc@1  92.97 ( 94.84)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:40:05 - Epoch: [6][330/352]	Time  0.133 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.0460e-01 (1.5810e-01)	Acc@1  92.19 ( 94.80)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:40:05 - Epoch: [6][340/352]	Time  0.133 ( 0.131)	Data  0.003 ( 0.003)	Loss 1.3074e-01 (1.5752e-01)	Acc@1  96.09 ( 94.73)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:07 - Epoch: [6][350/352]	Time  0.132 ( 0.131)	Data  0.002 ( 0.003)	Loss 1.9634e-01 (1.5789e-01)	Acc@1  92.97 ( 94.74)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:07 - Epoch: [6][340/352]	Time  0.130 ( 0.126)	Data  0.003 ( 0.003)	Loss 2.6831e-01 (1.5812e-01)	Acc@1  89.06 ( 94.79)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:40:07 - Test: [ 0/20]	Time  0.342 ( 0.342)	Loss 1.4421e-01 (1.4421e-01)	Acc@1  95.70 ( 95.70)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:08 - Epoch: [6][350/352]	Time  0.146 ( 0.126)	Data  0.003 ( 0.003)	Loss 2.0355e-01 (1.5800e-01)	Acc@1  94.53 ( 94.79)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:40:08 - Test: [10/20]	Time  0.071 ( 0.095)	Loss 1.5601e-01 (1.7426e-01)	Acc@1  94.92 ( 94.28)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:40:08 - Test: [ 0/20]	Time  0.295 ( 0.295)	Loss 1.6076e-01 (1.6076e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:09 -  * Acc@1 94.480 Acc@5 99.840
03-Mar-22 09:40:09 - Best acc at epoch 6: 94.77999877929688
03-Mar-22 09:40:09 - Test: [10/20]	Time  0.073 ( 0.095)	Loss 1.4419e-01 (1.7500e-01)	Acc@1  94.92 ( 94.07)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:40:09 - Epoch: [7][  0/352]	Time  0.336 ( 0.336)	Data  0.237 ( 0.237)	Loss 1.5831e-01 (1.5831e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:10 -  * Acc@1 93.900 Acc@5 99.860
03-Mar-22 09:40:10 - Best acc at epoch 6: 94.5199966430664
03-Mar-22 09:40:10 - Epoch: [7][  0/352]	Time  0.354 ( 0.354)	Data  0.247 ( 0.247)	Loss 2.1995e-01 (2.1995e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:10 - Epoch: [7][ 10/352]	Time  0.125 ( 0.140)	Data  0.002 ( 0.024)	Loss 1.9116e-01 (1.6715e-01)	Acc@1  90.62 ( 94.82)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:11 - Epoch: [7][ 10/352]	Time  0.109 ( 0.148)	Data  0.002 ( 0.024)	Loss 1.4878e-01 (1.6818e-01)	Acc@1  94.53 ( 93.96)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:12 - Epoch: [7][ 20/352]	Time  0.133 ( 0.135)	Data  0.002 ( 0.014)	Loss 1.5970e-01 (1.6300e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:13 - Epoch: [7][ 20/352]	Time  0.132 ( 0.140)	Data  0.002 ( 0.014)	Loss 2.4325e-01 (1.6589e-01)	Acc@1  95.31 ( 94.61)	Acc@5  99.22 ( 99.96)
03-Mar-22 09:40:13 - Epoch: [7][ 30/352]	Time  0.133 ( 0.134)	Data  0.002 ( 0.010)	Loss 2.3479e-01 (1.6076e-01)	Acc@1  92.97 ( 94.88)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:14 - Epoch: [7][ 30/352]	Time  0.139 ( 0.138)	Data  0.002 ( 0.010)	Loss 1.5528e-01 (1.6471e-01)	Acc@1  93.75 ( 94.71)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:14 - Epoch: [7][ 40/352]	Time  0.126 ( 0.134)	Data  0.002 ( 0.008)	Loss 1.6376e-01 (1.5376e-01)	Acc@1  94.53 ( 95.14)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:40:15 - Epoch: [7][ 40/352]	Time  0.153 ( 0.138)	Data  0.002 ( 0.008)	Loss 2.3056e-01 (1.6611e-01)	Acc@1  90.62 ( 94.65)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:40:16 - Epoch: [7][ 50/352]	Time  0.128 ( 0.133)	Data  0.002 ( 0.007)	Loss 1.1713e-01 (1.5324e-01)	Acc@1  96.88 ( 95.10)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:40:17 - Epoch: [7][ 50/352]	Time  0.128 ( 0.136)	Data  0.002 ( 0.007)	Loss 1.1372e-01 (1.5963e-01)	Acc@1  97.66 ( 94.85)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:40:17 - Epoch: [7][ 60/352]	Time  0.129 ( 0.132)	Data  0.002 ( 0.006)	Loss 8.3344e-02 (1.5275e-01)	Acc@1  97.66 ( 95.09)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:40:18 - Epoch: [7][ 60/352]	Time  0.138 ( 0.135)	Data  0.003 ( 0.006)	Loss 1.5226e-01 (1.6315e-01)	Acc@1  95.31 ( 94.63)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:18 - Epoch: [7][ 70/352]	Time  0.132 ( 0.131)	Data  0.002 ( 0.006)	Loss 9.8223e-02 (1.5100e-01)	Acc@1  96.09 ( 95.14)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:40:19 - Epoch: [7][ 70/352]	Time  0.115 ( 0.133)	Data  0.002 ( 0.006)	Loss 1.6032e-01 (1.6147e-01)	Acc@1  96.09 ( 94.74)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:40:19 - Epoch: [7][ 80/352]	Time  0.129 ( 0.131)	Data  0.002 ( 0.005)	Loss 1.4579e-01 (1.5113e-01)	Acc@1  92.97 ( 95.05)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:21 - Epoch: [7][ 80/352]	Time  0.152 ( 0.135)	Data  0.002 ( 0.005)	Loss 1.3138e-01 (1.6078e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:21 - Epoch: [7][ 90/352]	Time  0.130 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.7410e-01 (1.5303e-01)	Acc@1  93.75 ( 95.04)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:22 - Epoch: [7][100/352]	Time  0.126 ( 0.130)	Data  0.002 ( 0.005)	Loss 2.0997e-01 (1.5451e-01)	Acc@1  92.97 ( 94.98)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:22 - Epoch: [7][ 90/352]	Time  0.136 ( 0.135)	Data  0.002 ( 0.005)	Loss 2.1819e-01 (1.6203e-01)	Acc@1  92.19 ( 94.64)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:23 - Epoch: [7][110/352]	Time  0.127 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.0785e-01 (1.5554e-01)	Acc@1  92.97 ( 94.91)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:23 - Epoch: [7][100/352]	Time  0.110 ( 0.134)	Data  0.002 ( 0.005)	Loss 1.2718e-01 (1.5917e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:25 - Epoch: [7][120/352]	Time  0.128 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.2228e-01 (1.5624e-01)	Acc@1  96.09 ( 94.92)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:40:25 - Epoch: [7][110/352]	Time  0.130 ( 0.133)	Data  0.002 ( 0.005)	Loss 1.0310e-01 (1.5802e-01)	Acc@1  97.66 ( 94.82)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:26 - Epoch: [7][130/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.2827e-01 (1.5771e-01)	Acc@1  92.19 ( 94.84)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:40:26 - Epoch: [7][120/352]	Time  0.115 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.5325e-01 (1.5797e-01)	Acc@1  96.88 ( 94.87)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:27 - Epoch: [7][140/352]	Time  0.127 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.1284e-01 (1.5696e-01)	Acc@1  96.88 ( 94.87)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:40:27 - Epoch: [7][130/352]	Time  0.129 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.7756e-01 (1.5808e-01)	Acc@1  93.75 ( 94.79)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:40:28 - Epoch: [7][150/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.9895e-01 (1.5717e-01)	Acc@1  92.19 ( 94.87)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:40:29 - Epoch: [7][140/352]	Time  0.126 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.5413e-01 (1.5705e-01)	Acc@1  92.97 ( 94.81)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:40:30 - Epoch: [7][160/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.3571e-01 (1.5590e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:30 - Epoch: [7][150/352]	Time  0.126 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.2641e-01 (1.5771e-01)	Acc@1  96.09 ( 94.78)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:31 - Epoch: [7][170/352]	Time  0.108 ( 0.129)	Data  0.001 ( 0.004)	Loss 1.9905e-01 (1.5577e-01)	Acc@1  95.31 ( 94.94)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:40:31 - Epoch: [7][160/352]	Time  0.123 ( 0.131)	Data  0.002 ( 0.004)	Loss 1.5811e-01 (1.5744e-01)	Acc@1  94.53 ( 94.79)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:32 - Epoch: [7][180/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.6735e-01 (1.5494e-01)	Acc@1  92.97 ( 94.93)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:32 - Epoch: [7][170/352]	Time  0.127 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.2565e-01 (1.5745e-01)	Acc@1  96.09 ( 94.82)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:40:33 - Epoch: [7][190/352]	Time  0.124 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.1855e-01 (1.5499e-01)	Acc@1  95.31 ( 94.92)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:33 - Epoch: [7][180/352]	Time  0.104 ( 0.130)	Data  0.002 ( 0.004)	Loss 6.1273e-02 (1.5678e-01)	Acc@1  97.66 ( 94.82)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:35 - Epoch: [7][200/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6299e-01 (1.5530e-01)	Acc@1  95.31 ( 94.90)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:40:35 - Epoch: [7][190/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.004)	Loss 2.1534e-01 (1.5768e-01)	Acc@1  94.53 ( 94.80)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:40:36 - Epoch: [7][210/352]	Time  0.126 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6473e-01 (1.5461e-01)	Acc@1  95.31 ( 94.93)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:40:36 - Epoch: [7][200/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.4164e-01 (1.5713e-01)	Acc@1  92.19 ( 94.80)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:40:37 - Epoch: [7][210/352]	Time  0.114 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.9608e-01 (1.5711e-01)	Acc@1  90.62 ( 94.79)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:40:37 - Epoch: [7][220/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4517e-01 (1.5439e-01)	Acc@1  96.09 ( 94.92)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:40:38 - Epoch: [7][220/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0892e-01 (1.5658e-01)	Acc@1  96.88 ( 94.82)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:40:38 - Epoch: [7][230/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.5082e-01 (1.5428e-01)	Acc@1  96.88 ( 94.95)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:40 - Epoch: [7][230/352]	Time  0.121 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5889e-01 (1.5662e-01)	Acc@1  95.31 ( 94.82)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:40 - Epoch: [7][240/352]	Time  0.126 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.3333e-01 (1.5324e-01)	Acc@1  93.75 ( 95.00)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:41 - Epoch: [7][240/352]	Time  0.126 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6251e-01 (1.5626e-01)	Acc@1  94.53 ( 94.80)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:41 - Epoch: [7][250/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.3730e-01 (1.5445e-01)	Acc@1  89.84 ( 94.99)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:42 - Epoch: [7][250/352]	Time  0.133 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.1159e-01 (1.5554e-01)	Acc@1  96.09 ( 94.83)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:40:42 - Epoch: [7][260/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.0329e-01 (1.5449e-01)	Acc@1  96.88 ( 94.99)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:43 - Epoch: [7][260/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.8663e-01 (1.5587e-01)	Acc@1  96.09 ( 94.82)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:40:43 - Epoch: [7][270/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.3667e-01 (1.5518e-01)	Acc@1  96.09 ( 94.95)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:45 - Epoch: [7][270/352]	Time  0.106 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.7518e-01 (1.5505e-01)	Acc@1  93.75 ( 94.83)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:45 - Epoch: [7][280/352]	Time  0.143 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4704e-01 (1.5585e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:46 - Epoch: [7][280/352]	Time  0.130 ( 0.128)	Data  0.003 ( 0.003)	Loss 1.5273e-01 (1.5480e-01)	Acc@1  96.09 ( 94.86)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:46 - Epoch: [7][290/352]	Time  0.110 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.6229e-01 (1.5600e-01)	Acc@1  96.09 ( 94.94)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:47 - Epoch: [7][290/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4664e-01 (1.5487e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:47 - Epoch: [7][300/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.5762e-01 (1.5705e-01)	Acc@1  96.09 ( 94.90)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:48 - Epoch: [7][310/352]	Time  0.127 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.2929e-01 (1.5688e-01)	Acc@1  96.09 ( 94.90)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:49 - Epoch: [7][300/352]	Time  0.140 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.5756e-01 (1.5474e-01)	Acc@1  96.09 ( 94.88)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:50 - Epoch: [7][320/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.5318e-01 (1.5630e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:50 - Epoch: [7][310/352]	Time  0.127 ( 0.128)	Data  0.002 ( 0.003)	Loss 6.1593e-02 (1.5403e-01)	Acc@1  99.22 ( 94.91)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:51 - Epoch: [7][330/352]	Time  0.113 ( 0.127)	Data  0.002 ( 0.003)	Loss 9.3286e-02 (1.5618e-01)	Acc@1  97.66 ( 94.94)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:51 - Epoch: [7][320/352]	Time  0.125 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4167e-01 (1.5352e-01)	Acc@1  96.09 ( 94.95)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:40:52 - Epoch: [7][340/352]	Time  0.108 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.1378e-01 (1.5615e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:52 - Epoch: [7][330/352]	Time  0.116 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.0363e-01 (1.5339e-01)	Acc@1  92.97 ( 94.94)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:53 - Epoch: [7][350/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.2960e-01 (1.5609e-01)	Acc@1  93.75 ( 94.94)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:53 - Epoch: [7][340/352]	Time  0.128 ( 0.128)	Data  0.003 ( 0.003)	Loss 2.1479e-01 (1.5346e-01)	Acc@1  91.41 ( 94.94)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:40:54 - Test: [ 0/20]	Time  0.345 ( 0.345)	Loss 1.5364e-01 (1.5364e-01)	Acc@1  94.92 ( 94.92)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:55 - Epoch: [7][350/352]	Time  0.117 ( 0.127)	Data  0.002 ( 0.003)	Loss 2.1007e-01 (1.5397e-01)	Acc@1  94.53 ( 94.92)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:40:55 - Test: [10/20]	Time  0.071 ( 0.098)	Loss 1.7148e-01 (1.7248e-01)	Acc@1  94.14 ( 94.07)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:40:55 - Test: [ 0/20]	Time  0.322 ( 0.322)	Loss 1.7999e-01 (1.7999e-01)	Acc@1  94.14 ( 94.14)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:55 -  * Acc@1 94.200 Acc@5 99.900
03-Mar-22 09:40:55 - Best acc at epoch 7: 94.77999877929688
03-Mar-22 09:40:56 - Test: [10/20]	Time  0.070 ( 0.096)	Loss 1.5121e-01 (1.6458e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:56 - Epoch: [8][  0/352]	Time  0.349 ( 0.349)	Data  0.249 ( 0.249)	Loss 1.6488e-01 (1.6488e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:57 -  * Acc@1 94.480 Acc@5 99.960
03-Mar-22 09:40:57 - Best acc at epoch 7: 94.5199966430664
03-Mar-22 09:40:57 - Epoch: [8][  0/352]	Time  0.337 ( 0.337)	Data  0.230 ( 0.230)	Loss 1.5276e-01 (1.5276e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:57 - Epoch: [8][ 10/352]	Time  0.131 ( 0.141)	Data  0.002 ( 0.025)	Loss 9.7751e-02 (1.5948e-01)	Acc@1  96.88 ( 94.46)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:40:58 - Epoch: [8][ 10/352]	Time  0.138 ( 0.145)	Data  0.002 ( 0.023)	Loss 1.5515e-01 (1.5038e-01)	Acc@1  94.53 ( 94.96)	Acc@5 100.00 (100.00)
03-Mar-22 09:40:58 - Epoch: [8][ 20/352]	Time  0.128 ( 0.135)	Data  0.002 ( 0.014)	Loss 1.1713e-01 (1.6624e-01)	Acc@1  96.09 ( 94.35)	Acc@5 100.00 ( 99.78)
03-Mar-22 09:41:00 - Epoch: [8][ 20/352]	Time  0.117 ( 0.139)	Data  0.002 ( 0.013)	Loss 1.7396e-01 (1.5420e-01)	Acc@1  93.75 ( 94.94)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:00 - Epoch: [8][ 30/352]	Time  0.128 ( 0.134)	Data  0.002 ( 0.010)	Loss 1.0268e-01 (1.5572e-01)	Acc@1  97.66 ( 94.81)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:41:01 - Epoch: [8][ 30/352]	Time  0.140 ( 0.133)	Data  0.002 ( 0.009)	Loss 1.4628e-01 (1.6303e-01)	Acc@1  94.53 ( 94.48)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:01 - Epoch: [8][ 40/352]	Time  0.137 ( 0.133)	Data  0.002 ( 0.008)	Loss 1.5672e-01 (1.5219e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:41:02 - Epoch: [8][ 40/352]	Time  0.145 ( 0.133)	Data  0.002 ( 0.008)	Loss 1.1976e-01 (1.5693e-01)	Acc@1  96.88 ( 94.72)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:41:02 - Epoch: [8][ 50/352]	Time  0.132 ( 0.133)	Data  0.002 ( 0.007)	Loss 1.3992e-01 (1.5775e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:41:03 - Epoch: [8][ 50/352]	Time  0.137 ( 0.133)	Data  0.003 ( 0.007)	Loss 1.1796e-01 (1.5372e-01)	Acc@1  96.09 ( 94.94)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:04 - Epoch: [8][ 60/352]	Time  0.130 ( 0.132)	Data  0.003 ( 0.006)	Loss 1.4517e-01 (1.5800e-01)	Acc@1  93.75 ( 94.66)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:41:05 - Epoch: [8][ 60/352]	Time  0.136 ( 0.133)	Data  0.002 ( 0.006)	Loss 1.6797e-01 (1.5271e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:05 - Epoch: [8][ 70/352]	Time  0.142 ( 0.132)	Data  0.002 ( 0.006)	Loss 9.2937e-02 (1.5360e-01)	Acc@1  96.09 ( 94.82)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:06 - Epoch: [8][ 70/352]	Time  0.137 ( 0.133)	Data  0.003 ( 0.005)	Loss 9.2528e-02 (1.5272e-01)	Acc@1  96.88 ( 94.97)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:06 - Epoch: [8][ 80/352]	Time  0.133 ( 0.132)	Data  0.002 ( 0.005)	Loss 1.2340e-01 (1.5268e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:07 - Epoch: [8][ 80/352]	Time  0.137 ( 0.134)	Data  0.002 ( 0.005)	Loss 2.2690e-01 (1.5355e-01)	Acc@1  92.97 ( 94.93)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:08 - Epoch: [8][ 90/352]	Time  0.143 ( 0.132)	Data  0.002 ( 0.005)	Loss 1.4193e-01 (1.5206e-01)	Acc@1  94.53 ( 94.92)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:09 - Epoch: [8][ 90/352]	Time  0.139 ( 0.134)	Data  0.003 ( 0.005)	Loss 1.4307e-01 (1.5467e-01)	Acc@1  95.31 ( 94.88)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:09 - Epoch: [8][100/352]	Time  0.133 ( 0.132)	Data  0.002 ( 0.005)	Loss 9.3774e-02 (1.4953e-01)	Acc@1  97.66 ( 95.03)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:10 - Epoch: [8][100/352]	Time  0.121 ( 0.134)	Data  0.002 ( 0.005)	Loss 1.1374e-01 (1.5424e-01)	Acc@1  96.88 ( 94.93)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:10 - Epoch: [8][110/352]	Time  0.129 ( 0.132)	Data  0.003 ( 0.005)	Loss 2.6759e-01 (1.5135e-01)	Acc@1  92.19 ( 94.99)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:41:11 - Epoch: [8][110/352]	Time  0.141 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.0806e-01 (1.5320e-01)	Acc@1  96.88 ( 95.02)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:12 - Epoch: [8][120/352]	Time  0.142 ( 0.132)	Data  0.003 ( 0.004)	Loss 1.4263e-01 (1.5171e-01)	Acc@1  97.66 ( 95.00)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:41:13 - Epoch: [8][120/352]	Time  0.135 ( 0.133)	Data  0.002 ( 0.004)	Loss 2.2024e-01 (1.5349e-01)	Acc@1  92.97 ( 94.99)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:13 - Epoch: [8][130/352]	Time  0.144 ( 0.132)	Data  0.003 ( 0.004)	Loss 1.4609e-01 (1.5198e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:14 - Epoch: [8][130/352]	Time  0.141 ( 0.133)	Data  0.002 ( 0.004)	Loss 8.6783e-02 (1.5439e-01)	Acc@1  98.44 ( 94.95)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:14 - Epoch: [8][140/352]	Time  0.131 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.3784e-01 (1.5258e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:41:15 - Epoch: [8][140/352]	Time  0.139 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.6990e-01 (1.5317e-01)	Acc@1  93.75 ( 95.00)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:16 - Epoch: [8][150/352]	Time  0.134 ( 0.132)	Data  0.003 ( 0.004)	Loss 2.3429e-01 (1.5251e-01)	Acc@1  92.19 ( 94.93)	Acc@5  98.44 ( 99.88)
03-Mar-22 09:41:17 - Epoch: [8][150/352]	Time  0.139 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.7908e-01 (1.5422e-01)	Acc@1  94.53 ( 94.97)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:17 - Epoch: [8][160/352]	Time  0.131 ( 0.133)	Data  0.003 ( 0.004)	Loss 1.5870e-01 (1.5245e-01)	Acc@1  94.53 ( 94.94)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:41:18 - Epoch: [8][160/352]	Time  0.138 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.6920e-01 (1.5225e-01)	Acc@1  94.53 ( 95.01)	Acc@5  99.22 ( 99.95)
03-Mar-22 09:41:18 - Epoch: [8][170/352]	Time  0.133 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.4814e-01 (1.5164e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:41:19 - Epoch: [8][170/352]	Time  0.142 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.4511e-01 (1.5118e-01)	Acc@1  95.31 ( 95.03)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:19 - Epoch: [8][180/352]	Time  0.125 ( 0.133)	Data  0.002 ( 0.004)	Loss 1.9104e-01 (1.5143e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:41:21 - Epoch: [8][180/352]	Time  0.138 ( 0.133)	Data  0.003 ( 0.004)	Loss 1.7146e-01 (1.5338e-01)	Acc@1  93.75 ( 94.94)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:21 - Epoch: [8][190/352]	Time  0.119 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.7493e-01 (1.5147e-01)	Acc@1  92.97 ( 95.01)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:22 - Epoch: [8][190/352]	Time  0.145 ( 0.133)	Data  0.003 ( 0.004)	Loss 2.1272e-01 (1.5311e-01)	Acc@1  91.41 ( 94.97)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:22 - Epoch: [8][200/352]	Time  0.133 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.9054e-01 (1.5157e-01)	Acc@1  94.53 ( 95.05)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:23 - Epoch: [8][200/352]	Time  0.137 ( 0.133)	Data  0.003 ( 0.003)	Loss 1.6728e-01 (1.5204e-01)	Acc@1  92.97 ( 95.01)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:23 - Epoch: [8][210/352]	Time  0.136 ( 0.132)	Data  0.003 ( 0.004)	Loss 1.2427e-01 (1.5112e-01)	Acc@1  95.31 ( 95.05)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:24 - Epoch: [8][210/352]	Time  0.136 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.4450e-01 (1.5228e-01)	Acc@1  94.53 ( 95.00)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:25 - Epoch: [8][220/352]	Time  0.132 ( 0.132)	Data  0.002 ( 0.004)	Loss 1.1368e-01 (1.5009e-01)	Acc@1  96.09 ( 95.09)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:26 - Epoch: [8][220/352]	Time  0.131 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.3492e-01 (1.5213e-01)	Acc@1  96.09 ( 95.02)	Acc@5  99.22 ( 99.94)
03-Mar-22 09:41:26 - Epoch: [8][230/352]	Time  0.101 ( 0.132)	Data  0.002 ( 0.003)	Loss 2.0255e-01 (1.4955e-01)	Acc@1  94.53 ( 95.12)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:27 - Epoch: [8][230/352]	Time  0.136 ( 0.132)	Data  0.002 ( 0.003)	Loss 2.1576e-01 (1.5242e-01)	Acc@1  91.41 ( 94.97)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:27 - Epoch: [8][240/352]	Time  0.100 ( 0.131)	Data  0.001 ( 0.003)	Loss 1.3986e-01 (1.4948e-01)	Acc@1  95.31 ( 95.13)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:28 - Epoch: [8][250/352]	Time  0.097 ( 0.131)	Data  0.002 ( 0.003)	Loss 2.3550e-01 (1.5074e-01)	Acc@1  93.75 ( 95.09)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:28 - Epoch: [8][240/352]	Time  0.131 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.1560e-01 (1.5270e-01)	Acc@1  97.66 ( 94.95)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:41:30 - Epoch: [8][260/352]	Time  0.134 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.0896e-01 (1.5055e-01)	Acc@1  96.88 ( 95.09)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:30 - Epoch: [8][250/352]	Time  0.116 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.4239e-01 (1.5233e-01)	Acc@1  96.88 ( 94.96)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:31 - Epoch: [8][270/352]	Time  0.139 ( 0.130)	Data  0.003 ( 0.003)	Loss 1.8741e-01 (1.5099e-01)	Acc@1  92.97 ( 95.05)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:31 - Epoch: [8][260/352]	Time  0.159 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.6368e-01 (1.5233e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:32 - Epoch: [8][280/352]	Time  0.129 ( 0.130)	Data  0.003 ( 0.003)	Loss 2.2938e-01 (1.5215e-01)	Acc@1  89.84 ( 95.01)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:32 - Epoch: [8][270/352]	Time  0.136 ( 0.132)	Data  0.003 ( 0.003)	Loss 1.8779e-01 (1.5278e-01)	Acc@1  92.97 ( 94.96)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:33 - Epoch: [8][290/352]	Time  0.132 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.1903e-01 (1.5276e-01)	Acc@1  96.88 ( 94.97)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:34 - Epoch: [8][280/352]	Time  0.129 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.2419e-01 (1.5206e-01)	Acc@1  96.09 ( 94.97)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:35 - Epoch: [8][300/352]	Time  0.114 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.4499e-01 (1.5342e-01)	Acc@1  95.31 ( 94.94)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:35 - Epoch: [8][290/352]	Time  0.116 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.4782e-01 (1.5144e-01)	Acc@1  96.09 ( 95.00)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:36 - Epoch: [8][310/352]	Time  0.132 ( 0.130)	Data  0.002 ( 0.003)	Loss 2.7774e-01 (1.5419e-01)	Acc@1  92.19 ( 94.93)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:41:36 - Epoch: [8][300/352]	Time  0.141 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.3371e-01 (1.5208e-01)	Acc@1  96.09 ( 95.00)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:37 - Epoch: [8][320/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.003)	Loss 2.0967e-01 (1.5406e-01)	Acc@1  92.97 ( 94.93)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:38 - Epoch: [8][310/352]	Time  0.143 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.8102e-01 (1.5209e-01)	Acc@1  92.97 ( 94.99)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:39 - Epoch: [8][330/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.3152e-01 (1.5454e-01)	Acc@1  96.09 ( 94.89)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:39 - Epoch: [8][320/352]	Time  0.140 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.4624e-01 (1.5194e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:40 - Epoch: [8][340/352]	Time  0.134 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.5202e-01 (1.5515e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:40 - Epoch: [8][330/352]	Time  0.142 ( 0.132)	Data  0.003 ( 0.003)	Loss 1.8924e-01 (1.5239e-01)	Acc@1  94.53 ( 94.96)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:41:41 - Epoch: [8][350/352]	Time  0.129 ( 0.130)	Data  0.002 ( 0.003)	Loss 1.0199e-01 (1.5581e-01)	Acc@1  96.88 ( 94.83)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:42 - Epoch: [8][340/352]	Time  0.106 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.2792e-01 (1.5301e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:42 - Test: [ 0/20]	Time  0.338 ( 0.338)	Loss 1.3805e-01 (1.3805e-01)	Acc@1  95.70 ( 95.70)	Acc@5 100.00 (100.00)
03-Mar-22 09:41:43 - Test: [10/20]	Time  0.079 ( 0.100)	Loss 1.6201e-01 (1.6393e-01)	Acc@1  94.14 ( 94.46)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:41:43 - Epoch: [8][350/352]	Time  0.152 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.4009e-01 (1.5329e-01)	Acc@1  94.53 ( 94.91)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:43 -  * Acc@1 94.620 Acc@5 99.920
03-Mar-22 09:41:43 - Best acc at epoch 8: 94.77999877929688
03-Mar-22 09:41:44 - Test: [ 0/20]	Time  0.318 ( 0.318)	Loss 1.7306e-01 (1.7306e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.61 ( 99.61)
03-Mar-22 09:41:44 - Epoch: [9][  0/352]	Time  0.355 ( 0.355)	Data  0.227 ( 0.227)	Loss 2.2384e-01 (2.2384e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
03-Mar-22 09:41:44 - Test: [10/20]	Time  0.069 ( 0.094)	Loss 1.6276e-01 (1.8128e-01)	Acc@1  94.92 ( 93.93)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:45 - Epoch: [9][ 10/352]	Time  0.126 ( 0.142)	Data  0.002 ( 0.022)	Loss 7.9740e-02 (1.6088e-01)	Acc@1  96.88 ( 94.60)	Acc@5 100.00 (100.00)
03-Mar-22 09:41:45 -  * Acc@1 94.220 Acc@5 99.840
03-Mar-22 09:41:45 - Best acc at epoch 8: 94.5199966430664
03-Mar-22 09:41:45 - Epoch: [9][  0/352]	Time  0.358 ( 0.358)	Data  0.245 ( 0.245)	Loss 1.5315e-01 (1.5315e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:41:46 - Epoch: [9][ 20/352]	Time  0.139 ( 0.134)	Data  0.002 ( 0.013)	Loss 1.2968e-01 (1.5046e-01)	Acc@1  96.88 ( 95.05)	Acc@5 100.00 (100.00)
03-Mar-22 09:41:47 - Epoch: [9][ 10/352]	Time  0.123 ( 0.138)	Data  0.002 ( 0.024)	Loss 2.2673e-01 (1.3913e-01)	Acc@1  93.75 ( 95.81)	Acc@5 100.00 (100.00)
03-Mar-22 09:41:47 - Epoch: [9][ 30/352]	Time  0.130 ( 0.131)	Data  0.002 ( 0.010)	Loss 1.4005e-01 (1.4984e-01)	Acc@1  96.09 ( 95.11)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:41:48 - Epoch: [9][ 20/352]	Time  0.121 ( 0.131)	Data  0.002 ( 0.014)	Loss 1.7300e-01 (1.5592e-01)	Acc@1  94.53 ( 94.79)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:41:49 - Epoch: [9][ 40/352]	Time  0.142 ( 0.129)	Data  0.002 ( 0.008)	Loss 1.5003e-01 (1.6018e-01)	Acc@1  93.75 ( 94.70)	Acc@5 100.00 ( 99.98)
03-Mar-22 09:41:49 - Epoch: [9][ 30/352]	Time  0.111 ( 0.129)	Data  0.002 ( 0.010)	Loss 1.6326e-01 (1.6076e-01)	Acc@1  94.53 ( 94.68)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:41:50 - Epoch: [9][ 50/352]	Time  0.110 ( 0.129)	Data  0.002 ( 0.007)	Loss 1.9384e-01 (1.6494e-01)	Acc@1  94.53 ( 94.49)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:41:50 - Epoch: [9][ 40/352]	Time  0.104 ( 0.128)	Data  0.002 ( 0.008)	Loss 1.7911e-01 (1.5715e-01)	Acc@1  95.31 ( 94.91)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:51 - Epoch: [9][ 60/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.006)	Loss 1.7455e-01 (1.6787e-01)	Acc@1  93.75 ( 94.40)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:41:51 - Epoch: [9][ 50/352]	Time  0.105 ( 0.126)	Data  0.002 ( 0.007)	Loss 1.6713e-01 (1.5246e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:41:53 - Epoch: [9][ 70/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.1061e-01 (1.6448e-01)	Acc@1  94.53 ( 94.54)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:41:53 - Epoch: [9][ 60/352]	Time  0.127 ( 0.125)	Data  0.003 ( 0.006)	Loss 1.2110e-01 (1.4921e-01)	Acc@1  96.09 ( 94.95)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:54 - Epoch: [9][ 70/352]	Time  0.125 ( 0.123)	Data  0.002 ( 0.006)	Loss 1.9256e-01 (1.4950e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:54 - Epoch: [9][ 80/352]	Time  0.138 ( 0.130)	Data  0.002 ( 0.005)	Loss 2.1794e-01 (1.6785e-01)	Acc@1  92.19 ( 94.43)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:55 - Epoch: [9][ 80/352]	Time  0.127 ( 0.122)	Data  0.002 ( 0.005)	Loss 8.3054e-02 (1.4886e-01)	Acc@1  96.88 ( 94.97)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:55 - Epoch: [9][ 90/352]	Time  0.127 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.6364e-01 (1.6351e-01)	Acc@1  94.53 ( 94.56)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:56 - Epoch: [9][ 90/352]	Time  0.110 ( 0.121)	Data  0.002 ( 0.005)	Loss 1.2034e-01 (1.4930e-01)	Acc@1  94.53 ( 94.96)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:56 - Epoch: [9][100/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.005)	Loss 9.1220e-02 (1.6249e-01)	Acc@1  97.66 ( 94.57)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:57 - Epoch: [9][100/352]	Time  0.108 ( 0.120)	Data  0.002 ( 0.005)	Loss 2.2425e-01 (1.4976e-01)	Acc@1  90.62 ( 94.96)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:41:58 - Epoch: [9][110/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.1918e-01 (1.5775e-01)	Acc@1  95.31 ( 94.74)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:41:58 - Epoch: [9][110/352]	Time  0.103 ( 0.119)	Data  0.002 ( 0.004)	Loss 1.7433e-01 (1.5288e-01)	Acc@1  93.75 ( 94.80)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:41:59 - Epoch: [9][120/352]	Time  0.142 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.0634e-01 (1.5513e-01)	Acc@1  96.09 ( 94.80)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:41:59 - Epoch: [9][120/352]	Time  0.125 ( 0.119)	Data  0.002 ( 0.004)	Loss 1.5416e-01 (1.5156e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:00 - Epoch: [9][130/352]	Time  0.132 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.5264e-01 (1.5568e-01)	Acc@1  95.31 ( 94.80)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:42:01 - Epoch: [9][130/352]	Time  0.127 ( 0.119)	Data  0.002 ( 0.004)	Loss 1.2391e-01 (1.4989e-01)	Acc@1  94.53 ( 94.92)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:02 - Epoch: [9][140/352]	Time  0.142 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.1283e-01 (1.5835e-01)	Acc@1  95.31 ( 94.66)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:42:02 - Epoch: [9][140/352]	Time  0.123 ( 0.119)	Data  0.002 ( 0.004)	Loss 1.8291e-01 (1.5110e-01)	Acc@1  92.19 ( 94.86)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:03 - Epoch: [9][150/352]	Time  0.113 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.6418e-01 (1.5747e-01)	Acc@1  94.53 ( 94.67)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:42:03 - Epoch: [9][150/352]	Time  0.122 ( 0.119)	Data  0.002 ( 0.004)	Loss 7.3123e-02 (1.5092e-01)	Acc@1 100.00 ( 94.90)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:04 - Epoch: [9][160/352]	Time  0.103 ( 0.118)	Data  0.002 ( 0.004)	Loss 1.9607e-01 (1.5121e-01)	Acc@1  92.97 ( 94.92)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:42:04 - Epoch: [9][160/352]	Time  0.132 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.6019e-01 (1.5663e-01)	Acc@1  95.31 ( 94.70)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:42:05 - Epoch: [9][170/352]	Time  0.104 ( 0.118)	Data  0.002 ( 0.004)	Loss 1.5160e-01 (1.5075e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:05 - Epoch: [9][170/352]	Time  0.125 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.0528e-01 (1.5647e-01)	Acc@1  96.09 ( 94.74)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:42:06 - Epoch: [9][180/352]	Time  0.105 ( 0.118)	Data  0.002 ( 0.003)	Loss 8.9636e-02 (1.5113e-01)	Acc@1  98.44 ( 95.01)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:07 - Epoch: [9][180/352]	Time  0.126 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.3478e-01 (1.5693e-01)	Acc@1  89.06 ( 94.71)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:42:08 - Epoch: [9][190/352]	Time  0.124 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.1667e-01 (1.5104e-01)	Acc@1  96.09 ( 95.02)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:08 - Epoch: [9][190/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.004)	Loss 7.6267e-02 (1.5603e-01)	Acc@1  98.44 ( 94.78)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:42:09 - Epoch: [9][200/352]	Time  0.103 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.1045e-01 (1.5016e-01)	Acc@1  96.88 ( 95.05)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:09 - Epoch: [9][200/352]	Time  0.117 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4573e-01 (1.5538e-01)	Acc@1  94.53 ( 94.82)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:10 - Epoch: [9][210/352]	Time  0.115 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.5270e-01 (1.5107e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:11 - Epoch: [9][210/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4393e-01 (1.5503e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:11 - Epoch: [9][220/352]	Time  0.127 ( 0.117)	Data  0.002 ( 0.003)	Loss 2.5567e-01 (1.5222e-01)	Acc@1  93.75 ( 94.97)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:42:12 - Epoch: [9][220/352]	Time  0.128 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.2196e-01 (1.5465e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:12 - Epoch: [9][230/352]	Time  0.128 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.1196e-01 (1.5203e-01)	Acc@1  96.88 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:13 - Epoch: [9][230/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4853e-01 (1.5503e-01)	Acc@1  95.31 ( 94.84)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:13 - Epoch: [9][240/352]	Time  0.117 ( 0.117)	Data  0.002 ( 0.003)	Loss 1.3118e-01 (1.5202e-01)	Acc@1  95.31 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:14 - Epoch: [9][240/352]	Time  0.126 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.8271e-01 (1.5479e-01)	Acc@1  92.19 ( 94.83)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:14 - Epoch: [9][250/352]	Time  0.103 ( 0.117)	Data  0.002 ( 0.003)	Loss 1.2271e-01 (1.5228e-01)	Acc@1  95.31 ( 94.95)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:16 - Epoch: [9][260/352]	Time  0.105 ( 0.117)	Data  0.002 ( 0.003)	Loss 1.3456e-01 (1.5221e-01)	Acc@1  95.31 ( 94.94)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:16 - Epoch: [9][250/352]	Time  0.124 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.3085e-01 (1.5424e-01)	Acc@1  94.53 ( 94.86)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:42:17 - Epoch: [9][270/352]	Time  0.127 ( 0.117)	Data  0.002 ( 0.003)	Loss 1.0578e-01 (1.5205e-01)	Acc@1  95.31 ( 94.94)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:17 - Epoch: [9][260/352]	Time  0.128 ( 0.128)	Data  0.002 ( 0.003)	Loss 8.4686e-02 (1.5369e-01)	Acc@1  96.09 ( 94.88)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:18 - Epoch: [9][280/352]	Time  0.127 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.5161e-01 (1.5234e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:18 - Epoch: [9][270/352]	Time  0.138 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6271e-01 (1.5313e-01)	Acc@1  94.53 ( 94.92)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:42:19 - Epoch: [9][290/352]	Time  0.123 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.1830e-01 (1.5205e-01)	Acc@1  94.53 ( 94.94)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:19 - Epoch: [9][280/352]	Time  0.125 ( 0.128)	Data  0.002 ( 0.003)	Loss 9.5841e-02 (1.5285e-01)	Acc@1  96.88 ( 94.92)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:21 - Epoch: [9][300/352]	Time  0.126 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.5257e-01 (1.5171e-01)	Acc@1  94.53 ( 94.98)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:21 - Epoch: [9][290/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.003)	Loss 9.3955e-02 (1.5338e-01)	Acc@1  97.66 ( 94.91)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:22 - Epoch: [9][310/352]	Time  0.127 ( 0.118)	Data  0.002 ( 0.003)	Loss 2.0719e-01 (1.5138e-01)	Acc@1  91.41 ( 94.98)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:22 - Epoch: [9][300/352]	Time  0.121 ( 0.128)	Data  0.003 ( 0.003)	Loss 1.1554e-01 (1.5407e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:23 - Epoch: [9][320/352]	Time  0.127 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.3782e-01 (1.5110e-01)	Acc@1  95.31 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:23 - Epoch: [9][310/352]	Time  0.110 ( 0.128)	Data  0.002 ( 0.003)	Loss 7.7226e-02 (1.5371e-01)	Acc@1  99.22 ( 94.89)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:24 - Epoch: [9][330/352]	Time  0.105 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.3822e-01 (1.5147e-01)	Acc@1  96.09 ( 94.97)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:24 - Epoch: [9][320/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.5142e-01 (1.5361e-01)	Acc@1  95.31 ( 94.89)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:25 - Epoch: [9][340/352]	Time  0.127 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.3797e-01 (1.5201e-01)	Acc@1  96.09 ( 94.95)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:26 - Epoch: [9][330/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4000e-01 (1.5328e-01)	Acc@1  96.09 ( 94.92)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:27 - Epoch: [9][350/352]	Time  0.149 ( 0.119)	Data  0.002 ( 0.003)	Loss 1.5381e-01 (1.5229e-01)	Acc@1  95.31 ( 94.92)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:27 - Epoch: [9][340/352]	Time  0.124 ( 0.128)	Data  0.003 ( 0.003)	Loss 1.2485e-01 (1.5345e-01)	Acc@1  96.09 ( 94.92)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:27 - Test: [ 0/20]	Time  0.332 ( 0.332)	Loss 1.1332e-01 (1.1332e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:42:28 - Epoch: [9][350/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.5902e-01 (1.5255e-01)	Acc@1  94.53 ( 94.95)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:28 - Test: [10/20]	Time  0.069 ( 0.094)	Loss 1.5917e-01 (1.6144e-01)	Acc@1  94.53 ( 94.71)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:42:29 - Test: [ 0/20]	Time  0.331 ( 0.331)	Loss 1.6685e-01 (1.6685e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:42:29 -  * Acc@1 94.620 Acc@5 99.880
03-Mar-22 09:42:29 - Best acc at epoch 9: 94.6199951171875
03-Mar-22 09:42:29 - Epoch: [10][  0/352]	Time  0.326 ( 0.326)	Data  0.228 ( 0.228)	Loss 1.1477e-01 (1.1477e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
03-Mar-22 09:42:29 - Test: [10/20]	Time  0.070 ( 0.097)	Loss 1.9133e-01 (1.7629e-01)	Acc@1  92.58 ( 94.18)	Acc@5 100.00 ( 99.82)
03-Mar-22 09:42:30 -  * Acc@1 93.940 Acc@5 99.880
03-Mar-22 09:42:30 - Best acc at epoch 9: 94.77999877929688
03-Mar-22 09:42:30 - Epoch: [10][ 10/352]	Time  0.108 ( 0.134)	Data  0.002 ( 0.022)	Loss 1.2837e-01 (1.3623e-01)	Acc@1  95.31 ( 95.67)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:42:31 - Epoch: [10][  0/352]	Time  0.343 ( 0.343)	Data  0.235 ( 0.235)	Loss 8.4682e-02 (8.4682e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
03-Mar-22 09:42:31 - Epoch: [10][ 20/352]	Time  0.130 ( 0.127)	Data  0.002 ( 0.013)	Loss 2.2446e-01 (1.4876e-01)	Acc@1  94.53 ( 95.35)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:32 - Epoch: [10][ 10/352]	Time  0.125 ( 0.148)	Data  0.003 ( 0.024)	Loss 1.3123e-01 (1.4427e-01)	Acc@1  96.09 ( 95.67)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:42:33 - Epoch: [10][ 30/352]	Time  0.137 ( 0.125)	Data  0.002 ( 0.009)	Loss 1.5981e-01 (1.4915e-01)	Acc@1  93.75 ( 95.11)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:42:33 - Epoch: [10][ 20/352]	Time  0.132 ( 0.140)	Data  0.004 ( 0.014)	Loss 1.4014e-01 (1.4224e-01)	Acc@1  94.53 ( 95.28)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:42:34 - Epoch: [10][ 40/352]	Time  0.142 ( 0.127)	Data  0.002 ( 0.008)	Loss 1.2497e-01 (1.4972e-01)	Acc@1  96.88 ( 95.14)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:34 - Epoch: [10][ 30/352]	Time  0.111 ( 0.136)	Data  0.003 ( 0.010)	Loss 9.1864e-02 (1.4215e-01)	Acc@1  97.66 ( 95.21)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:35 - Epoch: [10][ 50/352]	Time  0.129 ( 0.126)	Data  0.002 ( 0.007)	Loss 2.1644e-01 (1.5059e-01)	Acc@1  92.97 ( 95.21)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:42:36 - Epoch: [10][ 40/352]	Time  0.116 ( 0.133)	Data  0.002 ( 0.008)	Loss 1.5603e-01 (1.4463e-01)	Acc@1  95.31 ( 95.29)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:42:37 - Epoch: [10][ 60/352]	Time  0.132 ( 0.127)	Data  0.002 ( 0.006)	Loss 1.5058e-01 (1.5159e-01)	Acc@1  94.53 ( 95.07)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:42:37 - Epoch: [10][ 50/352]	Time  0.114 ( 0.132)	Data  0.002 ( 0.007)	Loss 1.5602e-01 (1.4902e-01)	Acc@1  93.75 ( 95.10)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:42:38 - Epoch: [10][ 70/352]	Time  0.131 ( 0.126)	Data  0.002 ( 0.005)	Loss 1.1792e-01 (1.4900e-01)	Acc@1  95.31 ( 95.08)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:42:38 - Epoch: [10][ 60/352]	Time  0.131 ( 0.131)	Data  0.002 ( 0.006)	Loss 1.9703e-01 (1.5171e-01)	Acc@1  93.75 ( 95.06)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:42:39 - Epoch: [10][ 80/352]	Time  0.122 ( 0.126)	Data  0.002 ( 0.005)	Loss 8.8985e-02 (1.5090e-01)	Acc@1  96.88 ( 95.01)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:39 - Epoch: [10][ 70/352]	Time  0.134 ( 0.131)	Data  0.002 ( 0.006)	Loss 1.2132e-01 (1.4995e-01)	Acc@1  95.31 ( 95.09)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:42:40 - Epoch: [10][ 90/352]	Time  0.144 ( 0.127)	Data  0.002 ( 0.005)	Loss 1.4111e-01 (1.5021e-01)	Acc@1  94.53 ( 95.08)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:41 - Epoch: [10][ 80/352]	Time  0.134 ( 0.131)	Data  0.004 ( 0.005)	Loss 1.0344e-01 (1.5130e-01)	Acc@1  96.88 ( 95.01)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:42:42 - Epoch: [10][100/352]	Time  0.136 ( 0.127)	Data  0.003 ( 0.004)	Loss 1.3552e-01 (1.5111e-01)	Acc@1  95.31 ( 95.06)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:42 - Epoch: [10][ 90/352]	Time  0.129 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.5740e-01 (1.5005e-01)	Acc@1  95.31 ( 95.08)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:42:43 - Epoch: [10][110/352]	Time  0.124 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.1075e-01 (1.5038e-01)	Acc@1  96.09 ( 95.06)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:43 - Epoch: [10][100/352]	Time  0.117 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.5835e-01 (1.4961e-01)	Acc@1  96.09 ( 95.10)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:44 - Epoch: [10][120/352]	Time  0.108 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.7899e-01 (1.4930e-01)	Acc@1  91.41 ( 95.07)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:45 - Epoch: [10][110/352]	Time  0.130 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.8629e-01 (1.4797e-01)	Acc@1  92.97 ( 95.11)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:45 - Epoch: [10][130/352]	Time  0.137 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.9018e-01 (1.4925e-01)	Acc@1  92.19 ( 95.07)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:46 - Epoch: [10][120/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.8505e-01 (1.5089e-01)	Acc@1  93.75 ( 95.03)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:47 - Epoch: [10][140/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.4565e-01 (1.4980e-01)	Acc@1  95.31 ( 95.04)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:47 - Epoch: [10][130/352]	Time  0.126 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.5432e-01 (1.5172e-01)	Acc@1  94.53 ( 95.03)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:48 - Epoch: [10][150/352]	Time  0.101 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.2006e-01 (1.5014e-01)	Acc@1  96.09 ( 95.00)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:49 - Epoch: [10][140/352]	Time  0.127 ( 0.130)	Data  0.002 ( 0.004)	Loss 6.1017e-02 (1.5158e-01)	Acc@1  99.22 ( 95.07)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:49 - Epoch: [10][160/352]	Time  0.130 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.4591e-01 (1.4794e-01)	Acc@1  93.75 ( 95.07)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:42:50 - Epoch: [10][150/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.9306e-01 (1.5280e-01)	Acc@1  92.97 ( 95.02)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:42:50 - Epoch: [10][170/352]	Time  0.130 ( 0.126)	Data  0.003 ( 0.004)	Loss 1.0526e-01 (1.4927e-01)	Acc@1  97.66 ( 95.01)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:51 - Epoch: [10][160/352]	Time  0.109 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.6166e-01 (1.5385e-01)	Acc@1  92.19 ( 94.94)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:52 - Epoch: [10][180/352]	Time  0.141 ( 0.126)	Data  0.002 ( 0.004)	Loss 1.6871e-01 (1.4908e-01)	Acc@1  92.97 ( 95.00)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:42:52 - Epoch: [10][170/352]	Time  0.130 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.2343e-01 (1.5438e-01)	Acc@1  94.53 ( 94.90)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:42:53 - Epoch: [10][190/352]	Time  0.128 ( 0.126)	Data  0.002 ( 0.003)	Loss 2.5503e-01 (1.4922e-01)	Acc@1  91.41 ( 95.01)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:54 - Epoch: [10][180/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.7679e-01 (1.5536e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:42:54 - Epoch: [10][200/352]	Time  0.141 ( 0.127)	Data  0.003 ( 0.003)	Loss 1.7589e-01 (1.4974e-01)	Acc@1  93.75 ( 95.02)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:55 - Epoch: [10][190/352]	Time  0.128 ( 0.129)	Data  0.003 ( 0.004)	Loss 2.6120e-01 (1.5623e-01)	Acc@1  90.62 ( 94.80)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:42:56 - Epoch: [10][210/352]	Time  0.125 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.8064e-01 (1.5043e-01)	Acc@1  94.53 ( 95.00)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:56 - Epoch: [10][200/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.1607e-01 (1.5544e-01)	Acc@1  97.66 ( 94.83)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:42:57 - Epoch: [10][220/352]	Time  0.141 ( 0.127)	Data  0.002 ( 0.003)	Loss 7.1261e-02 (1.5039e-01)	Acc@1  98.44 ( 95.02)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:42:57 - Epoch: [10][210/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.9048e-01 (1.5566e-01)	Acc@1  93.75 ( 94.82)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:42:58 - Epoch: [10][230/352]	Time  0.129 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.9764e-01 (1.4948e-01)	Acc@1  92.97 ( 95.04)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:42:59 - Epoch: [10][220/352]	Time  0.114 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3528e-01 (1.5505e-01)	Acc@1  96.09 ( 94.85)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:43:00 - Epoch: [10][240/352]	Time  0.125 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.7168e-01 (1.4905e-01)	Acc@1  93.75 ( 95.03)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:00 - Epoch: [10][230/352]	Time  0.126 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5452e-01 (1.5483e-01)	Acc@1  95.31 ( 94.88)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:01 - Epoch: [10][250/352]	Time  0.156 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.3575e-01 (1.4922e-01)	Acc@1  94.53 ( 95.04)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:43:01 - Epoch: [10][240/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7155e-01 (1.5397e-01)	Acc@1  96.09 ( 94.92)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:43:02 - Epoch: [10][260/352]	Time  0.140 ( 0.128)	Data  0.002 ( 0.003)	Loss 3.2478e-01 (1.4911e-01)	Acc@1  87.50 ( 95.04)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:03 - Epoch: [10][250/352]	Time  0.144 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3723e-01 (1.5289e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:43:03 - Epoch: [10][270/352]	Time  0.127 ( 0.128)	Data  0.002 ( 0.003)	Loss 2.2002e-01 (1.4842e-01)	Acc@1  94.53 ( 95.09)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:43:04 - Epoch: [10][260/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3291e-01 (1.5329e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:05 - Epoch: [10][280/352]	Time  0.128 ( 0.127)	Data  0.003 ( 0.003)	Loss 1.4643e-01 (1.4843e-01)	Acc@1  93.75 ( 95.08)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:05 - Epoch: [10][270/352]	Time  0.137 ( 0.129)	Data  0.002 ( 0.003)	Loss 8.4052e-02 (1.5296e-01)	Acc@1  96.88 ( 94.97)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:43:06 - Epoch: [10][290/352]	Time  0.126 ( 0.127)	Data  0.003 ( 0.003)	Loss 1.2858e-01 (1.4799e-01)	Acc@1  97.66 ( 95.10)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:06 - Epoch: [10][280/352]	Time  0.130 ( 0.129)	Data  0.003 ( 0.003)	Loss 2.6253e-01 (1.5354e-01)	Acc@1  90.62 ( 94.94)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:43:07 - Epoch: [10][300/352]	Time  0.127 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.1753e-01 (1.4835e-01)	Acc@1  96.88 ( 95.08)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:08 - Epoch: [10][290/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5063e-01 (1.5351e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:43:08 - Epoch: [10][310/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.8426e-01 (1.4900e-01)	Acc@1  94.53 ( 95.05)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:43:09 - Epoch: [10][300/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0541e-01 (1.5287e-01)	Acc@1  96.88 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:10 - Epoch: [10][320/352]	Time  0.122 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.5618e-01 (1.4970e-01)	Acc@1  92.97 ( 95.02)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:10 - Epoch: [10][310/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0239e-01 (1.5272e-01)	Acc@1  97.66 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:11 - Epoch: [10][330/352]	Time  0.103 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.5327e-01 (1.4933e-01)	Acc@1  94.53 ( 95.02)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:12 - Epoch: [10][320/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4866e-01 (1.5313e-01)	Acc@1  96.09 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:12 - Epoch: [10][340/352]	Time  0.129 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.3848e-01 (1.4911e-01)	Acc@1  94.53 ( 95.02)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:43:13 - Epoch: [10][330/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0386e-01 (1.5265e-01)	Acc@1  97.66 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:13 - Epoch: [10][350/352]	Time  0.159 ( 0.127)	Data  0.002 ( 0.003)	Loss 1.7695e-01 (1.4824e-01)	Acc@1  95.31 ( 95.05)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:14 - Test: [ 0/20]	Time  0.353 ( 0.353)	Loss 1.5037e-01 (1.5037e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:43:14 - Epoch: [10][340/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0049e-01 (1.5209e-01)	Acc@1  97.66 ( 94.99)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:15 - Test: [10/20]	Time  0.068 ( 0.097)	Loss 1.5574e-01 (1.7737e-01)	Acc@1  95.31 ( 94.32)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:15 - Epoch: [10][350/352]	Time  0.126 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.1924e-01 (1.5239e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:15 -  * Acc@1 94.420 Acc@5 99.880
03-Mar-22 09:43:15 - Best acc at epoch 10: 94.6199951171875
03-Mar-22 09:43:16 - Epoch: [11][  0/352]	Time  0.333 ( 0.333)	Data  0.234 ( 0.234)	Loss 1.7163e-01 (1.7163e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.22 ( 99.22)
03-Mar-22 09:43:16 - Test: [ 0/20]	Time  0.334 ( 0.334)	Loss 1.3412e-01 (1.3412e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:43:17 - Test: [10/20]	Time  0.088 ( 0.102)	Loss 1.7238e-01 (1.6821e-01)	Acc@1  93.36 ( 93.96)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:43:17 - Epoch: [11][ 10/352]	Time  0.103 ( 0.128)	Data  0.002 ( 0.023)	Loss 1.3694e-01 (1.5379e-01)	Acc@1  95.31 ( 94.53)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:43:18 -  * Acc@1 94.040 Acc@5 99.900
03-Mar-22 09:43:18 - Best acc at epoch 10: 94.77999877929688
03-Mar-22 09:43:18 - Epoch: [11][  0/352]	Time  0.356 ( 0.356)	Data  0.246 ( 0.246)	Loss 1.3721e-01 (1.3721e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:43:18 - Epoch: [11][ 20/352]	Time  0.094 ( 0.126)	Data  0.002 ( 0.013)	Loss 1.5191e-01 (1.4870e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:43:19 - Epoch: [11][ 10/352]	Time  0.127 ( 0.140)	Data  0.002 ( 0.024)	Loss 9.0639e-02 (1.3136e-01)	Acc@1  97.66 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:43:19 - Epoch: [11][ 30/352]	Time  0.124 ( 0.121)	Data  0.002 ( 0.010)	Loss 1.7543e-01 (1.4433e-01)	Acc@1  91.41 ( 94.93)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:20 - Epoch: [11][ 20/352]	Time  0.106 ( 0.130)	Data  0.002 ( 0.014)	Loss 1.0570e-01 (1.3835e-01)	Acc@1  96.09 ( 95.39)	Acc@5 100.00 (100.00)
03-Mar-22 09:43:20 - Epoch: [11][ 40/352]	Time  0.125 ( 0.120)	Data  0.002 ( 0.008)	Loss 1.4082e-01 (1.4374e-01)	Acc@1  94.53 ( 95.05)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:43:22 - Epoch: [11][ 30/352]	Time  0.149 ( 0.129)	Data  0.003 ( 0.010)	Loss 1.0840e-01 (1.3012e-01)	Acc@1  95.31 ( 95.77)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:43:22 - Epoch: [11][ 50/352]	Time  0.128 ( 0.121)	Data  0.003 ( 0.007)	Loss 1.6912e-01 (1.4842e-01)	Acc@1  93.75 ( 94.99)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:43:23 - Epoch: [11][ 60/352]	Time  0.122 ( 0.122)	Data  0.002 ( 0.006)	Loss 1.1308e-01 (1.4622e-01)	Acc@1  96.09 ( 95.06)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:43:23 - Epoch: [11][ 40/352]	Time  0.138 ( 0.130)	Data  0.004 ( 0.008)	Loss 1.3318e-01 (1.4082e-01)	Acc@1  96.09 ( 95.43)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:43:24 - Epoch: [11][ 70/352]	Time  0.115 ( 0.123)	Data  0.002 ( 0.005)	Loss 1.2445e-01 (1.4660e-01)	Acc@1  96.09 ( 95.02)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:43:24 - Epoch: [11][ 50/352]	Time  0.142 ( 0.130)	Data  0.003 ( 0.007)	Loss 6.1197e-02 (1.4446e-01)	Acc@1  98.44 ( 95.34)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:25 - Epoch: [11][ 80/352]	Time  0.111 ( 0.123)	Data  0.002 ( 0.005)	Loss 1.4638e-01 (1.4398e-01)	Acc@1  95.31 ( 95.04)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:43:26 - Epoch: [11][ 60/352]	Time  0.129 ( 0.130)	Data  0.002 ( 0.006)	Loss 1.0244e-01 (1.4237e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:27 - Epoch: [11][ 90/352]	Time  0.125 ( 0.123)	Data  0.002 ( 0.005)	Loss 9.6768e-02 (1.4234e-01)	Acc@1  98.44 ( 95.11)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:27 - Epoch: [11][ 70/352]	Time  0.140 ( 0.130)	Data  0.002 ( 0.006)	Loss 1.3266e-01 (1.4216e-01)	Acc@1  96.09 ( 95.38)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:28 - Epoch: [11][100/352]	Time  0.126 ( 0.123)	Data  0.002 ( 0.005)	Loss 1.9074e-01 (1.4338e-01)	Acc@1  92.97 ( 95.10)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:28 - Epoch: [11][ 80/352]	Time  0.119 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.1076e-01 (1.4225e-01)	Acc@1  96.88 ( 95.37)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:43:29 - Epoch: [11][110/352]	Time  0.126 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.6680e-01 (1.4718e-01)	Acc@1  96.09 ( 94.97)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:29 - Epoch: [11][ 90/352]	Time  0.131 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.4392e-01 (1.4478e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:43:30 - Epoch: [11][120/352]	Time  0.131 ( 0.123)	Data  0.003 ( 0.004)	Loss 9.8470e-02 (1.4944e-01)	Acc@1  96.88 ( 94.89)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:31 - Epoch: [11][100/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.7233e-01 (1.4588e-01)	Acc@1  95.31 ( 95.28)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:43:32 - Epoch: [11][130/352]	Time  0.126 ( 0.123)	Data  0.002 ( 0.004)	Loss 6.9633e-02 (1.4631e-01)	Acc@1  98.44 ( 95.06)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:43:32 - Epoch: [11][110/352]	Time  0.109 ( 0.129)	Data  0.002 ( 0.005)	Loss 6.9310e-02 (1.4526e-01)	Acc@1  99.22 ( 95.28)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:43:33 - Epoch: [11][140/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.4939e-01 (1.4781e-01)	Acc@1  96.09 ( 95.02)	Acc@5  99.22 ( 99.95)
03-Mar-22 09:43:33 - Epoch: [11][120/352]	Time  0.107 ( 0.128)	Data  0.002 ( 0.004)	Loss 2.5590e-01 (1.4792e-01)	Acc@1  93.75 ( 95.21)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:43:34 - Epoch: [11][150/352]	Time  0.112 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.2410e-01 (1.4799e-01)	Acc@1  94.53 ( 95.00)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:34 - Epoch: [11][130/352]	Time  0.109 ( 0.128)	Data  0.002 ( 0.004)	Loss 2.4101e-01 (1.4807e-01)	Acc@1  90.62 ( 95.16)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:35 - Epoch: [11][160/352]	Time  0.129 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.7892e-01 (1.4820e-01)	Acc@1  93.75 ( 94.99)	Acc@5  99.22 ( 99.95)
03-Mar-22 09:43:36 - Epoch: [11][140/352]	Time  0.130 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.3282e-01 (1.4829e-01)	Acc@1  94.53 ( 95.11)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:37 - Epoch: [11][170/352]	Time  0.127 ( 0.124)	Data  0.003 ( 0.004)	Loss 6.3098e-02 (1.4858e-01)	Acc@1  99.22 ( 94.98)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:37 - Epoch: [11][150/352]	Time  0.132 ( 0.128)	Data  0.003 ( 0.004)	Loss 7.8503e-02 (1.4934e-01)	Acc@1  97.66 ( 95.06)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:43:38 - Epoch: [11][180/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.2175e-01 (1.4849e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:38 - Epoch: [11][160/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.9056e-01 (1.4920e-01)	Acc@1  92.97 ( 95.06)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:43:39 - Epoch: [11][190/352]	Time  0.129 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.0667e-01 (1.4862e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:40 - Epoch: [11][170/352]	Time  0.143 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.7584e-01 (1.5021e-01)	Acc@1  92.97 ( 95.01)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:40 - Epoch: [11][200/352]	Time  0.133 ( 0.124)	Data  0.003 ( 0.003)	Loss 8.0840e-02 (1.4875e-01)	Acc@1  96.09 ( 95.01)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:41 - Epoch: [11][180/352]	Time  0.124 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.6290e-01 (1.5087e-01)	Acc@1  96.09 ( 94.98)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:43:42 - Epoch: [11][210/352]	Time  0.108 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.8356e-01 (1.4879e-01)	Acc@1  94.53 ( 95.01)	Acc@5  99.22 ( 99.95)
03-Mar-22 09:43:42 - Epoch: [11][190/352]	Time  0.129 ( 0.128)	Data  0.003 ( 0.004)	Loss 2.1165e-01 (1.5136e-01)	Acc@1  94.53 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:43 - Epoch: [11][220/352]	Time  0.128 ( 0.124)	Data  0.003 ( 0.003)	Loss 8.3641e-02 (1.4746e-01)	Acc@1  98.44 ( 95.07)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:43 - Epoch: [11][200/352]	Time  0.134 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.9149e-01 (1.5195e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:44 - Epoch: [11][230/352]	Time  0.129 ( 0.124)	Data  0.003 ( 0.003)	Loss 1.6247e-01 (1.4747e-01)	Acc@1  93.75 ( 95.07)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:45 - Epoch: [11][210/352]	Time  0.134 ( 0.128)	Data  0.003 ( 0.004)	Loss 1.1288e-01 (1.5177e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:45 - Epoch: [11][240/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.003)	Loss 2.1056e-01 (1.4776e-01)	Acc@1  94.53 ( 95.05)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:46 - Epoch: [11][220/352]	Time  0.149 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.0617e-01 (1.5232e-01)	Acc@1  97.66 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:47 - Epoch: [11][250/352]	Time  0.125 ( 0.124)	Data  0.002 ( 0.003)	Loss 3.0318e-01 (1.4789e-01)	Acc@1  90.62 ( 95.07)	Acc@5  99.22 ( 99.95)
03-Mar-22 09:43:47 - Epoch: [11][230/352]	Time  0.147 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1849e-01 (1.5295e-01)	Acc@1  96.88 ( 94.94)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:43:48 - Epoch: [11][260/352]	Time  0.151 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.3820e-01 (1.4738e-01)	Acc@1  95.31 ( 95.09)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:49 - Epoch: [11][240/352]	Time  0.128 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4927e-01 (1.5296e-01)	Acc@1  96.09 ( 94.94)	Acc@5  99.22 ( 99.89)
03-Mar-22 09:43:49 - Epoch: [11][270/352]	Time  0.129 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6661e-01 (1.4835e-01)	Acc@1  93.75 ( 95.05)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:50 - Epoch: [11][250/352]	Time  0.128 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.2253e-01 (1.5152e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:50 - Epoch: [11][280/352]	Time  0.122 ( 0.124)	Data  0.002 ( 0.003)	Loss 9.4226e-02 (1.4824e-01)	Acc@1  96.88 ( 95.05)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:51 - Epoch: [11][260/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.3257e-01 (1.5189e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:52 - Epoch: [11][290/352]	Time  0.128 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.5730e-01 (1.4885e-01)	Acc@1  96.09 ( 95.04)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:53 - Epoch: [11][270/352]	Time  0.148 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.3982e-01 (1.5166e-01)	Acc@1  96.09 ( 94.98)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:53 - Epoch: [11][300/352]	Time  0.128 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.1108e-01 (1.4929e-01)	Acc@1  92.19 ( 95.03)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:43:54 - Epoch: [11][280/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0867e-01 (1.5210e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:54 - Epoch: [11][310/352]	Time  0.125 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.3814e-01 (1.5007e-01)	Acc@1  95.31 ( 95.00)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:55 - Epoch: [11][290/352]	Time  0.106 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1276e-01 (1.5153e-01)	Acc@1  95.31 ( 94.99)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:56 - Epoch: [11][320/352]	Time  0.132 ( 0.125)	Data  0.003 ( 0.003)	Loss 9.1789e-02 (1.5051e-01)	Acc@1  97.66 ( 94.97)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:57 - Epoch: [11][300/352]	Time  0.116 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.1983e-01 (1.5194e-01)	Acc@1  92.19 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:43:57 - Epoch: [11][330/352]	Time  0.127 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.3197e-01 (1.5100e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:58 - Epoch: [11][310/352]	Time  0.132 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.9708e-01 (1.5258e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:43:58 - Epoch: [11][340/352]	Time  0.130 ( 0.125)	Data  0.003 ( 0.003)	Loss 1.1738e-01 (1.5089e-01)	Acc@1  95.31 ( 94.96)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:43:59 - Epoch: [11][320/352]	Time  0.137 ( 0.129)	Data  0.003 ( 0.003)	Loss 9.9619e-02 (1.5230e-01)	Acc@1  96.88 ( 94.96)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:43:59 - Epoch: [11][350/352]	Time  0.129 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.2608e-01 (1.5132e-01)	Acc@1  94.53 ( 94.96)	Acc@5  98.44 ( 99.94)
03-Mar-22 09:44:00 - Test: [ 0/20]	Time  0.331 ( 0.331)	Loss 1.3612e-01 (1.3612e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:00 - Epoch: [11][330/352]	Time  0.118 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7027e-01 (1.5183e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:01 - Test: [10/20]	Time  0.071 ( 0.105)	Loss 1.2440e-01 (1.5514e-01)	Acc@1  94.14 ( 95.03)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:44:01 -  * Acc@1 94.660 Acc@5 99.920
03-Mar-22 09:44:01 - Best acc at epoch 11: 94.65999603271484
03-Mar-22 09:44:02 - Epoch: [11][340/352]	Time  0.108 ( 0.129)	Data  0.003 ( 0.003)	Loss 2.0298e-01 (1.5168e-01)	Acc@1  92.97 ( 94.98)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:44:02 - Epoch: [12][  0/352]	Time  0.328 ( 0.328)	Data  0.221 ( 0.221)	Loss 1.0938e-01 (1.0938e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:03 - Epoch: [11][350/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0353e-01 (1.5201e-01)	Acc@1  97.66 ( 94.98)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:03 - Epoch: [12][ 10/352]	Time  0.128 ( 0.136)	Data  0.002 ( 0.022)	Loss 1.6716e-01 (1.4033e-01)	Acc@1  95.31 ( 95.10)	Acc@5  99.22 ( 99.86)
03-Mar-22 09:44:03 - Test: [ 0/20]	Time  0.349 ( 0.349)	Loss 1.4611e-01 (1.4611e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:04 - Epoch: [12][ 20/352]	Time  0.097 ( 0.123)	Data  0.002 ( 0.013)	Loss 2.2583e-01 (1.3867e-01)	Acc@1  94.53 ( 95.50)	Acc@5  99.22 ( 99.85)
03-Mar-22 09:44:04 - Test: [10/20]	Time  0.078 ( 0.101)	Loss 1.6991e-01 (1.6162e-01)	Acc@1  92.97 ( 94.64)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:44:05 -  * Acc@1 94.500 Acc@5 99.900
03-Mar-22 09:44:05 - Best acc at epoch 11: 94.77999877929688
03-Mar-22 09:44:05 - Epoch: [12][ 30/352]	Time  0.121 ( 0.121)	Data  0.003 ( 0.009)	Loss 2.1118e-01 (1.4796e-01)	Acc@1  92.19 ( 95.19)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:44:05 - Epoch: [12][  0/352]	Time  0.340 ( 0.340)	Data  0.231 ( 0.231)	Loss 1.6780e-01 (1.6780e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:06 - Epoch: [12][ 40/352]	Time  0.120 ( 0.117)	Data  0.002 ( 0.007)	Loss 1.3134e-01 (1.4987e-01)	Acc@1  93.75 ( 94.99)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:06 - Epoch: [12][ 10/352]	Time  0.125 ( 0.136)	Data  0.002 ( 0.023)	Loss 2.0767e-01 (1.5788e-01)	Acc@1  92.19 ( 94.32)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:07 - Epoch: [12][ 50/352]	Time  0.128 ( 0.118)	Data  0.002 ( 0.006)	Loss 9.5632e-02 (1.4316e-01)	Acc@1  96.09 ( 95.28)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:08 - Epoch: [12][ 20/352]	Time  0.114 ( 0.127)	Data  0.002 ( 0.013)	Loss 2.0473e-01 (1.5935e-01)	Acc@1  91.41 ( 94.20)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:44:09 - Epoch: [12][ 60/352]	Time  0.107 ( 0.119)	Data  0.002 ( 0.006)	Loss 1.4154e-01 (1.4240e-01)	Acc@1  94.53 ( 95.26)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:09 - Epoch: [12][ 30/352]	Time  0.128 ( 0.127)	Data  0.002 ( 0.009)	Loss 1.9344e-01 (1.6111e-01)	Acc@1  93.75 ( 94.23)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:10 - Epoch: [12][ 70/352]	Time  0.131 ( 0.121)	Data  0.002 ( 0.005)	Loss 1.3719e-01 (1.4302e-01)	Acc@1  95.31 ( 95.20)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:10 - Epoch: [12][ 40/352]	Time  0.153 ( 0.128)	Data  0.002 ( 0.008)	Loss 2.4222e-01 (1.5839e-01)	Acc@1  92.19 ( 94.40)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:44:11 - Epoch: [12][ 80/352]	Time  0.126 ( 0.121)	Data  0.002 ( 0.005)	Loss 6.7027e-02 (1.4133e-01)	Acc@1  96.88 ( 95.24)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:12 - Epoch: [12][ 50/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.007)	Loss 1.8901e-01 (1.5791e-01)	Acc@1  94.53 ( 94.50)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:44:12 - Epoch: [12][ 90/352]	Time  0.124 ( 0.121)	Data  0.002 ( 0.005)	Loss 1.4954e-01 (1.4203e-01)	Acc@1  95.31 ( 95.18)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:13 - Epoch: [12][ 60/352]	Time  0.137 ( 0.128)	Data  0.002 ( 0.006)	Loss 1.9855e-01 (1.5725e-01)	Acc@1  94.53 ( 94.56)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:44:14 - Epoch: [12][100/352]	Time  0.151 ( 0.122)	Data  0.003 ( 0.004)	Loss 1.6579e-01 (1.4403e-01)	Acc@1  95.31 ( 95.17)	Acc@5  99.22 ( 99.88)
03-Mar-22 09:44:14 - Epoch: [12][ 70/352]	Time  0.131 ( 0.127)	Data  0.002 ( 0.005)	Loss 1.1220e-01 (1.5817e-01)	Acc@1  95.31 ( 94.53)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:15 - Epoch: [12][110/352]	Time  0.141 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.4597e-01 (1.4479e-01)	Acc@1  95.31 ( 95.12)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:15 - Epoch: [12][ 80/352]	Time  0.119 ( 0.126)	Data  0.003 ( 0.005)	Loss 1.3545e-01 (1.5771e-01)	Acc@1  97.66 ( 94.55)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:16 - Epoch: [12][120/352]	Time  0.128 ( 0.124)	Data  0.003 ( 0.004)	Loss 1.1981e-01 (1.4431e-01)	Acc@1  96.88 ( 95.11)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:17 - Epoch: [12][ 90/352]	Time  0.109 ( 0.126)	Data  0.002 ( 0.005)	Loss 1.7535e-01 (1.5905e-01)	Acc@1  94.53 ( 94.58)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:18 - Epoch: [12][130/352]	Time  0.125 ( 0.124)	Data  0.002 ( 0.004)	Loss 2.2420e-01 (1.4604e-01)	Acc@1  92.19 ( 95.07)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:18 - Epoch: [12][100/352]	Time  0.156 ( 0.127)	Data  0.002 ( 0.005)	Loss 1.1889e-01 (1.5710e-01)	Acc@1  96.88 ( 94.66)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:19 - Epoch: [12][140/352]	Time  0.121 ( 0.124)	Data  0.003 ( 0.004)	Loss 1.3186e-01 (1.4680e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:19 - Epoch: [12][110/352]	Time  0.133 ( 0.127)	Data  0.002 ( 0.004)	Loss 1.5918e-01 (1.5625e-01)	Acc@1  96.88 ( 94.81)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:44:20 - Epoch: [12][150/352]	Time  0.109 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.4736e-01 (1.4666e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:20 - Epoch: [12][120/352]	Time  0.143 ( 0.127)	Data  0.002 ( 0.004)	Loss 2.3096e-01 (1.5723e-01)	Acc@1  89.84 ( 94.81)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:44:21 - Epoch: [12][160/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.004)	Loss 2.1645e-01 (1.4699e-01)	Acc@1  93.75 ( 95.00)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:22 - Epoch: [12][130/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.7466e-01 (1.5707e-01)	Acc@1  95.31 ( 94.79)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:44:23 - Epoch: [12][170/352]	Time  0.106 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.0857e-01 (1.4718e-01)	Acc@1  97.66 ( 95.00)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:44:23 - Epoch: [12][140/352]	Time  0.113 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.2018e-01 (1.5778e-01)	Acc@1  94.53 ( 94.76)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:44:24 - Epoch: [12][180/352]	Time  0.125 ( 0.124)	Data  0.003 ( 0.004)	Loss 6.5958e-02 (1.4613e-01)	Acc@1  98.44 ( 95.05)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:44:24 - Epoch: [12][150/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.6655e-01 (1.5564e-01)	Acc@1  92.19 ( 94.81)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:44:25 - Epoch: [12][190/352]	Time  0.126 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.3192e-01 (1.4630e-01)	Acc@1  96.09 ( 95.06)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:26 - Epoch: [12][160/352]	Time  0.133 ( 0.128)	Data  0.003 ( 0.004)	Loss 7.2617e-02 (1.5549e-01)	Acc@1  98.44 ( 94.75)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:44:26 - Epoch: [12][200/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.9582e-01 (1.4729e-01)	Acc@1  92.19 ( 95.02)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:27 - Epoch: [12][170/352]	Time  0.120 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.2052e-01 (1.5523e-01)	Acc@1  94.53 ( 94.74)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:44:27 - Epoch: [12][210/352]	Time  0.111 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.9338e-01 (1.4665e-01)	Acc@1  94.53 ( 95.05)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:28 - Epoch: [12][180/352]	Time  0.134 ( 0.128)	Data  0.002 ( 0.004)	Loss 8.7277e-02 (1.5369e-01)	Acc@1  97.66 ( 94.79)	Acc@5 100.00 ( 99.87)
03-Mar-22 09:44:29 - Epoch: [12][220/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.3648e-01 (1.4754e-01)	Acc@1  96.88 ( 95.05)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:29 - Epoch: [12][190/352]	Time  0.128 ( 0.128)	Data  0.002 ( 0.004)	Loss 1.4561e-01 (1.5288e-01)	Acc@1  94.53 ( 94.82)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:30 - Epoch: [12][230/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.4543e-01 (1.4752e-01)	Acc@1  93.75 ( 95.02)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:31 - Epoch: [12][200/352]	Time  0.139 ( 0.128)	Data  0.003 ( 0.003)	Loss 1.0548e-01 (1.5192e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:31 - Epoch: [12][240/352]	Time  0.105 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6424e-01 (1.4798e-01)	Acc@1  93.75 ( 95.02)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:44:32 - Epoch: [12][210/352]	Time  0.127 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4013e-01 (1.5114e-01)	Acc@1  94.53 ( 94.91)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:32 - Epoch: [12][250/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.0305e-01 (1.4733e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:33 - Epoch: [12][220/352]	Time  0.126 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.1332e-01 (1.5112e-01)	Acc@1  97.66 ( 94.93)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:34 - Epoch: [12][260/352]	Time  0.135 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.5143e-01 (1.4824e-01)	Acc@1  95.31 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:35 - Epoch: [12][230/352]	Time  0.113 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.1206e-01 (1.5063e-01)	Acc@1  95.31 ( 94.95)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:35 - Epoch: [12][270/352]	Time  0.122 ( 0.124)	Data  0.003 ( 0.003)	Loss 1.3216e-01 (1.4806e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:36 - Epoch: [12][240/352]	Time  0.133 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.9580e-01 (1.5085e-01)	Acc@1  93.75 ( 94.95)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:36 - Epoch: [12][280/352]	Time  0.132 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.3020e-01 (1.4726e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:37 - Epoch: [12][250/352]	Time  0.110 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.4629e-01 (1.5163e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:37 - Epoch: [12][290/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.003)	Loss 8.0525e-02 (1.4799e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:38 - Epoch: [12][260/352]	Time  0.144 ( 0.128)	Data  0.003 ( 0.003)	Loss 9.1379e-02 (1.5050e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:39 - Epoch: [12][300/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.003)	Loss 5.6630e-02 (1.4760e-01)	Acc@1  99.22 ( 94.99)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:44:40 - Epoch: [12][270/352]	Time  0.132 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6827e-01 (1.5096e-01)	Acc@1  94.53 ( 94.96)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:40 - Epoch: [12][310/352]	Time  0.113 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.7138e-01 (1.4844e-01)	Acc@1  93.75 ( 94.98)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:41 - Epoch: [12][280/352]	Time  0.109 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.6814e-01 (1.5054e-01)	Acc@1  92.97 ( 94.97)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:41 - Epoch: [12][320/352]	Time  0.108 ( 0.124)	Data  0.002 ( 0.003)	Loss 2.2055e-01 (1.4880e-01)	Acc@1  93.75 ( 94.97)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:42 - Epoch: [12][330/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.1684e-01 (1.4831e-01)	Acc@1  96.88 ( 95.00)	Acc@5  99.22 ( 99.90)
03-Mar-22 09:44:42 - Epoch: [12][290/352]	Time  0.135 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5484e-01 (1.5030e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:44 - Epoch: [12][340/352]	Time  0.123 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.7939e-01 (1.4818e-01)	Acc@1  93.75 ( 95.00)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:44 - Epoch: [12][300/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.2814e-01 (1.5006e-01)	Acc@1  96.88 ( 94.99)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:45 - Epoch: [12][350/352]	Time  0.132 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.4952e-01 (1.4824e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:45 - Epoch: [12][310/352]	Time  0.132 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.8616e-01 (1.5045e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:46 - Test: [ 0/20]	Time  0.329 ( 0.329)	Loss 1.3573e-01 (1.3573e-01)	Acc@1  95.70 ( 95.70)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:46 - Epoch: [12][320/352]	Time  0.098 ( 0.128)	Data  0.001 ( 0.003)	Loss 2.2409e-01 (1.5096e-01)	Acc@1  92.97 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:46 - Test: [10/20]	Time  0.070 ( 0.098)	Loss 1.6061e-01 (1.6195e-01)	Acc@1  93.36 ( 94.74)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:44:47 -  * Acc@1 94.540 Acc@5 99.920
03-Mar-22 09:44:47 - Best acc at epoch 12: 94.65999603271484
03-Mar-22 09:44:47 - Epoch: [12][330/352]	Time  0.100 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.0859e-01 (1.5062e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:47 - Epoch: [13][  0/352]	Time  0.333 ( 0.333)	Data  0.236 ( 0.236)	Loss 1.0573e-01 (1.0573e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:49 - Epoch: [13][ 10/352]	Time  0.136 ( 0.139)	Data  0.002 ( 0.023)	Loss 1.5792e-01 (1.3916e-01)	Acc@1  96.09 ( 95.45)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:49 - Epoch: [12][340/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.2585e-01 (1.5058e-01)	Acc@1  95.31 ( 94.99)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:50 - Epoch: [13][ 20/352]	Time  0.103 ( 0.130)	Data  0.002 ( 0.013)	Loss 1.4589e-01 (1.4216e-01)	Acc@1  95.31 ( 95.28)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:44:50 - Epoch: [12][350/352]	Time  0.130 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.3477e-01 (1.5095e-01)	Acc@1  94.53 ( 94.96)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:44:51 - Test: [ 0/20]	Time  0.347 ( 0.347)	Loss 1.2547e-01 (1.2547e-01)	Acc@1  94.14 ( 94.14)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:51 - Epoch: [13][ 30/352]	Time  0.109 ( 0.125)	Data  0.002 ( 0.010)	Loss 1.5794e-01 (1.4820e-01)	Acc@1  94.53 ( 95.19)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:44:51 - Test: [10/20]	Time  0.072 ( 0.105)	Loss 1.8625e-01 (1.6511e-01)	Acc@1  94.14 ( 94.57)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:44:52 -  * Acc@1 94.540 Acc@5 99.860
03-Mar-22 09:44:52 - Best acc at epoch 12: 94.77999877929688
03-Mar-22 09:44:52 - Epoch: [13][ 40/352]	Time  0.125 ( 0.123)	Data  0.002 ( 0.008)	Loss 1.2796e-01 (1.4474e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:44:53 - Epoch: [13][  0/352]	Time  0.406 ( 0.406)	Data  0.267 ( 0.267)	Loss 2.2211e-01 (2.2211e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
03-Mar-22 09:44:53 - Epoch: [13][ 50/352]	Time  0.111 ( 0.121)	Data  0.002 ( 0.007)	Loss 1.3452e-01 (1.4462e-01)	Acc@1  96.88 ( 95.40)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:44:54 - Epoch: [13][ 10/352]	Time  0.130 ( 0.147)	Data  0.002 ( 0.026)	Loss 1.8006e-01 (1.6067e-01)	Acc@1  93.75 ( 94.53)	Acc@5 100.00 ( 99.79)
03-Mar-22 09:44:54 - Epoch: [13][ 60/352]	Time  0.101 ( 0.121)	Data  0.002 ( 0.006)	Loss 1.8143e-01 (1.4544e-01)	Acc@1  93.75 ( 95.36)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:44:55 - Epoch: [13][ 20/352]	Time  0.136 ( 0.140)	Data  0.002 ( 0.015)	Loss 9.7454e-02 (1.4708e-01)	Acc@1  96.09 ( 95.05)	Acc@5 100.00 ( 99.81)
03-Mar-22 09:44:56 - Epoch: [13][ 70/352]	Time  0.142 ( 0.120)	Data  0.002 ( 0.005)	Loss 1.2262e-01 (1.4682e-01)	Acc@1  95.31 ( 95.35)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:44:56 - Epoch: [13][ 30/352]	Time  0.130 ( 0.137)	Data  0.002 ( 0.011)	Loss 1.0354e-01 (1.4332e-01)	Acc@1  97.66 ( 95.06)	Acc@5 100.00 ( 99.85)
03-Mar-22 09:44:57 - Epoch: [13][ 80/352]	Time  0.127 ( 0.120)	Data  0.002 ( 0.005)	Loss 1.4486e-01 (1.4721e-01)	Acc@1  93.75 ( 95.28)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:44:58 - Epoch: [13][ 40/352]	Time  0.143 ( 0.137)	Data  0.003 ( 0.009)	Loss 7.9813e-02 (1.3876e-01)	Acc@1  96.09 ( 95.16)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:44:58 - Epoch: [13][ 90/352]	Time  0.139 ( 0.120)	Data  0.002 ( 0.005)	Loss 2.0733e-01 (1.4918e-01)	Acc@1  90.62 ( 95.14)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:44:59 - Epoch: [13][ 50/352]	Time  0.135 ( 0.136)	Data  0.002 ( 0.008)	Loss 1.2063e-01 (1.3654e-01)	Acc@1  96.09 ( 95.39)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:44:59 - Epoch: [13][100/352]	Time  0.128 ( 0.121)	Data  0.002 ( 0.004)	Loss 2.0513e-01 (1.5238e-01)	Acc@1  91.41 ( 95.05)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:00 - Epoch: [13][ 60/352]	Time  0.141 ( 0.136)	Data  0.003 ( 0.007)	Loss 1.8613e-01 (1.3743e-01)	Acc@1  91.41 ( 95.31)	Acc@5  99.22 ( 99.87)
03-Mar-22 09:45:01 - Epoch: [13][110/352]	Time  0.133 ( 0.121)	Data  0.003 ( 0.004)	Loss 1.9849e-01 (1.5247e-01)	Acc@1  95.31 ( 95.06)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:02 - Epoch: [13][ 70/352]	Time  0.133 ( 0.136)	Data  0.003 ( 0.006)	Loss 7.6453e-02 (1.3826e-01)	Acc@1  98.44 ( 95.29)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:45:02 - Epoch: [13][120/352]	Time  0.112 ( 0.122)	Data  0.002 ( 0.004)	Loss 1.1018e-01 (1.5071e-01)	Acc@1  96.09 ( 95.12)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:03 - Epoch: [13][130/352]	Time  0.110 ( 0.122)	Data  0.002 ( 0.004)	Loss 1.6073e-01 (1.4851e-01)	Acc@1  93.75 ( 95.18)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:03 - Epoch: [13][ 80/352]	Time  0.138 ( 0.136)	Data  0.002 ( 0.006)	Loss 8.3173e-02 (1.3714e-01)	Acc@1  97.66 ( 95.33)	Acc@5 100.00 ( 99.88)
03-Mar-22 09:45:04 - Epoch: [13][140/352]	Time  0.131 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.0499e-01 (1.4758e-01)	Acc@1  96.88 ( 95.21)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:05 - Epoch: [13][ 90/352]	Time  0.133 ( 0.136)	Data  0.002 ( 0.006)	Loss 9.1183e-02 (1.3409e-01)	Acc@1  97.66 ( 95.42)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:45:06 - Epoch: [13][150/352]	Time  0.109 ( 0.123)	Data  0.002 ( 0.004)	Loss 5.8049e-02 (1.4585e-01)	Acc@1  98.44 ( 95.29)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:06 - Epoch: [13][100/352]	Time  0.113 ( 0.136)	Data  0.002 ( 0.005)	Loss 7.8762e-02 (1.3422e-01)	Acc@1  97.66 ( 95.46)	Acc@5 100.00 ( 99.90)
03-Mar-22 09:45:07 - Epoch: [13][160/352]	Time  0.129 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.2532e-01 (1.4602e-01)	Acc@1  96.09 ( 95.25)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:07 - Epoch: [13][110/352]	Time  0.134 ( 0.136)	Data  0.003 ( 0.005)	Loss 1.1961e-01 (1.3520e-01)	Acc@1  97.66 ( 95.47)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:08 - Epoch: [13][170/352]	Time  0.126 ( 0.123)	Data  0.002 ( 0.004)	Loss 1.4642e-01 (1.4688e-01)	Acc@1  96.88 ( 95.22)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:09 - Epoch: [13][120/352]	Time  0.099 ( 0.136)	Data  0.002 ( 0.005)	Loss 1.8634e-01 (1.3648e-01)	Acc@1  92.19 ( 95.44)	Acc@5  99.22 ( 99.91)
03-Mar-22 09:45:09 - Epoch: [13][180/352]	Time  0.138 ( 0.124)	Data  0.003 ( 0.004)	Loss 1.0392e-01 (1.4677e-01)	Acc@1  96.88 ( 95.24)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:10 - Epoch: [13][130/352]	Time  0.132 ( 0.135)	Data  0.003 ( 0.005)	Loss 1.3691e-01 (1.3759e-01)	Acc@1  94.53 ( 95.35)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:11 - Epoch: [13][190/352]	Time  0.104 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.9772e-01 (1.4622e-01)	Acc@1  96.09 ( 95.28)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:11 - Epoch: [13][140/352]	Time  0.136 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.1027e-01 (1.3773e-01)	Acc@1  96.09 ( 95.32)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:12 - Epoch: [13][200/352]	Time  0.131 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.0162e-01 (1.4538e-01)	Acc@1  98.44 ( 95.28)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:13 - Epoch: [13][150/352]	Time  0.132 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.5934e-01 (1.3985e-01)	Acc@1  92.97 ( 95.22)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:13 - Epoch: [13][210/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.0539e-01 (1.4544e-01)	Acc@1  97.66 ( 95.26)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:14 - Epoch: [13][160/352]	Time  0.134 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.5652e-01 (1.4071e-01)	Acc@1  94.53 ( 95.20)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:15 - Epoch: [13][220/352]	Time  0.134 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6909e-01 (1.4602e-01)	Acc@1  94.53 ( 95.25)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:15 - Epoch: [13][170/352]	Time  0.123 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.1294e-01 (1.4042e-01)	Acc@1  95.31 ( 95.23)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:45:16 - Epoch: [13][230/352]	Time  0.153 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6212e-01 (1.4609e-01)	Acc@1  94.53 ( 95.27)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:17 - Epoch: [13][180/352]	Time  0.130 ( 0.135)	Data  0.003 ( 0.004)	Loss 8.9137e-02 (1.4008e-01)	Acc@1  97.66 ( 95.24)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:17 - Epoch: [13][240/352]	Time  0.146 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.3866e-01 (1.4569e-01)	Acc@1  95.31 ( 95.30)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:18 - Epoch: [13][190/352]	Time  0.138 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.5772e-01 (1.4023e-01)	Acc@1  94.53 ( 95.25)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:18 - Epoch: [13][250/352]	Time  0.137 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.4778e-01 (1.4610e-01)	Acc@1  92.97 ( 95.25)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:19 - Epoch: [13][200/352]	Time  0.143 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.4173e-01 (1.4122e-01)	Acc@1  94.53 ( 95.22)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:20 - Epoch: [13][260/352]	Time  0.126 ( 0.125)	Data  0.002 ( 0.003)	Loss 2.3378e-01 (1.4645e-01)	Acc@1  94.53 ( 95.24)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:21 - Epoch: [13][210/352]	Time  0.140 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.2084e-01 (1.4111e-01)	Acc@1  96.88 ( 95.25)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:21 - Epoch: [13][270/352]	Time  0.125 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.0705e-01 (1.4613e-01)	Acc@1  96.88 ( 95.26)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:22 - Epoch: [13][220/352]	Time  0.127 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.1863e-01 (1.4174e-01)	Acc@1  96.09 ( 95.21)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:45:22 - Epoch: [13][280/352]	Time  0.125 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.3913e-01 (1.4554e-01)	Acc@1  95.31 ( 95.26)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:23 - Epoch: [13][230/352]	Time  0.136 ( 0.134)	Data  0.002 ( 0.004)	Loss 1.6058e-01 (1.4242e-01)	Acc@1  94.53 ( 95.21)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:23 - Epoch: [13][290/352]	Time  0.125 ( 0.124)	Data  0.003 ( 0.003)	Loss 1.5633e-01 (1.4548e-01)	Acc@1  93.75 ( 95.27)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:24 - Epoch: [13][300/352]	Time  0.110 ( 0.124)	Data  0.002 ( 0.003)	Loss 3.4839e-01 (1.4700e-01)	Acc@1  86.72 ( 95.20)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:25 - Epoch: [13][240/352]	Time  0.110 ( 0.134)	Data  0.002 ( 0.004)	Loss 1.7794e-01 (1.4347e-01)	Acc@1  93.75 ( 95.16)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:26 - Epoch: [13][310/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.6145e-01 (1.4697e-01)	Acc@1  94.53 ( 95.18)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:26 - Epoch: [13][250/352]	Time  0.128 ( 0.134)	Data  0.002 ( 0.004)	Loss 1.4358e-01 (1.4343e-01)	Acc@1  96.09 ( 95.18)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:27 - Epoch: [13][320/352]	Time  0.126 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.1425e-01 (1.4639e-01)	Acc@1  96.88 ( 95.22)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:27 - Epoch: [13][260/352]	Time  0.124 ( 0.134)	Data  0.002 ( 0.004)	Loss 1.5606e-01 (1.4237e-01)	Acc@1  96.09 ( 95.24)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:28 - Epoch: [13][330/352]	Time  0.125 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.0787e-01 (1.4517e-01)	Acc@1  96.88 ( 95.25)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:45:28 - Epoch: [13][270/352]	Time  0.144 ( 0.134)	Data  0.002 ( 0.004)	Loss 8.4358e-02 (1.4360e-01)	Acc@1  97.66 ( 95.20)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:29 - Epoch: [13][340/352]	Time  0.130 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.2351e-01 (1.4510e-01)	Acc@1  96.09 ( 95.23)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:30 - Epoch: [13][280/352]	Time  0.132 ( 0.134)	Data  0.003 ( 0.003)	Loss 6.6440e-02 (1.4354e-01)	Acc@1  99.22 ( 95.21)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:31 - Epoch: [13][350/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.1877e-01 (1.4513e-01)	Acc@1  97.66 ( 95.24)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:45:31 - Epoch: [13][290/352]	Time  0.099 ( 0.134)	Data  0.002 ( 0.003)	Loss 1.3647e-01 (1.4360e-01)	Acc@1  95.31 ( 95.19)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:45:31 - Test: [ 0/20]	Time  0.323 ( 0.323)	Loss 1.3446e-01 (1.3446e-01)	Acc@1  94.92 ( 94.92)	Acc@5 100.00 (100.00)
03-Mar-22 09:45:32 - Test: [10/20]	Time  0.069 ( 0.093)	Loss 1.5294e-01 (1.6112e-01)	Acc@1  94.92 ( 94.78)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:32 - Epoch: [13][300/352]	Time  0.134 ( 0.133)	Data  0.002 ( 0.003)	Loss 1.3692e-01 (1.4411e-01)	Acc@1  97.66 ( 95.20)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:32 -  * Acc@1 94.640 Acc@5 99.940
03-Mar-22 09:45:32 - Best acc at epoch 13: 94.65999603271484
03-Mar-22 09:45:33 - Epoch: [14][  0/352]	Time  0.333 ( 0.333)	Data  0.226 ( 0.226)	Loss 1.8403e-01 (1.8403e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 09:45:33 - Epoch: [13][310/352]	Time  0.127 ( 0.133)	Data  0.002 ( 0.003)	Loss 1.5723e-01 (1.4395e-01)	Acc@1  93.75 ( 95.22)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:34 - Epoch: [14][ 10/352]	Time  0.126 ( 0.145)	Data  0.002 ( 0.023)	Loss 1.5553e-01 (1.3901e-01)	Acc@1  94.53 ( 95.24)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:45:35 - Epoch: [13][320/352]	Time  0.114 ( 0.132)	Data  0.003 ( 0.003)	Loss 9.5389e-02 (1.4427e-01)	Acc@1  98.44 ( 95.21)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:35 - Epoch: [14][ 20/352]	Time  0.145 ( 0.136)	Data  0.003 ( 0.013)	Loss 7.7351e-02 (1.3279e-01)	Acc@1  96.88 ( 95.65)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:36 - Epoch: [13][330/352]	Time  0.132 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.3914e-01 (1.4470e-01)	Acc@1  96.09 ( 95.21)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:37 - Epoch: [14][ 30/352]	Time  0.144 ( 0.136)	Data  0.003 ( 0.010)	Loss 1.0730e-01 (1.3389e-01)	Acc@1  97.66 ( 95.74)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:45:37 - Epoch: [13][340/352]	Time  0.129 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.6165e-01 (1.4551e-01)	Acc@1  94.53 ( 95.19)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:38 - Epoch: [14][ 40/352]	Time  0.131 ( 0.134)	Data  0.002 ( 0.008)	Loss 1.3767e-01 (1.3144e-01)	Acc@1  96.09 ( 95.77)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:38 - Epoch: [13][350/352]	Time  0.130 ( 0.132)	Data  0.002 ( 0.003)	Loss 1.5476e-01 (1.4576e-01)	Acc@1  96.09 ( 95.18)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:39 - Test: [ 0/20]	Time  0.319 ( 0.319)	Loss 1.6447e-01 (1.6447e-01)	Acc@1  94.53 ( 94.53)	Acc@5  99.61 ( 99.61)
03-Mar-22 09:45:39 - Epoch: [14][ 50/352]	Time  0.124 ( 0.133)	Data  0.002 ( 0.007)	Loss 8.7521e-02 (1.3006e-01)	Acc@1  98.44 ( 95.86)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:40 - Test: [10/20]	Time  0.086 ( 0.099)	Loss 1.4485e-01 (1.6678e-01)	Acc@1  95.70 ( 94.39)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:45:41 - Epoch: [14][ 60/352]	Time  0.123 ( 0.132)	Data  0.002 ( 0.006)	Loss 1.0945e-01 (1.3070e-01)	Acc@1  96.09 ( 95.77)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:41 -  * Acc@1 94.280 Acc@5 99.920
03-Mar-22 09:45:41 - Best acc at epoch 13: 94.77999877929688
03-Mar-22 09:45:41 - Epoch: [14][  0/352]	Time  0.350 ( 0.350)	Data  0.241 ( 0.241)	Loss 2.5042e-01 (2.5042e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
03-Mar-22 09:45:42 - Epoch: [14][ 70/352]	Time  0.135 ( 0.131)	Data  0.003 ( 0.006)	Loss 2.2352e-01 (1.3322e-01)	Acc@1  92.19 ( 95.70)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:45:42 - Epoch: [14][ 10/352]	Time  0.129 ( 0.142)	Data  0.002 ( 0.024)	Loss 1.8261e-01 (1.6064e-01)	Acc@1  93.75 ( 93.89)	Acc@5 100.00 (100.00)
03-Mar-22 09:45:43 - Epoch: [14][ 80/352]	Time  0.131 ( 0.131)	Data  0.002 ( 0.005)	Loss 1.1817e-01 (1.3555e-01)	Acc@1  96.09 ( 95.57)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:44 - Epoch: [14][ 20/352]	Time  0.129 ( 0.136)	Data  0.002 ( 0.014)	Loss 1.0143e-01 (1.5310e-01)	Acc@1  96.88 ( 94.61)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:45:44 - Epoch: [14][ 90/352]	Time  0.130 ( 0.130)	Data  0.002 ( 0.005)	Loss 1.4078e-01 (1.3645e-01)	Acc@1  94.53 ( 95.52)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:45:45 - Epoch: [14][ 30/352]	Time  0.132 ( 0.134)	Data  0.002 ( 0.010)	Loss 1.6798e-01 (1.5329e-01)	Acc@1  94.53 ( 94.68)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:45:46 - Epoch: [14][100/352]	Time  0.110 ( 0.130)	Data  0.003 ( 0.005)	Loss 1.4707e-01 (1.3875e-01)	Acc@1  93.75 ( 95.45)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:46 - Epoch: [14][ 40/352]	Time  0.113 ( 0.132)	Data  0.002 ( 0.008)	Loss 1.3882e-01 (1.5044e-01)	Acc@1  96.09 ( 94.87)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:45:47 - Epoch: [14][110/352]	Time  0.137 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.3584e-01 (1.3982e-01)	Acc@1  96.09 ( 95.43)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:47 - Epoch: [14][ 50/352]	Time  0.113 ( 0.131)	Data  0.002 ( 0.007)	Loss 1.7995e-01 (1.5192e-01)	Acc@1  93.75 ( 94.76)	Acc@5 100.00 ( 99.97)
03-Mar-22 09:45:48 - Epoch: [14][120/352]	Time  0.135 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.3009e-01 (1.3995e-01)	Acc@1  92.97 ( 95.34)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:49 - Epoch: [14][ 60/352]	Time  0.127 ( 0.130)	Data  0.003 ( 0.006)	Loss 1.5515e-01 (1.5322e-01)	Acc@1  94.53 ( 94.79)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:45:49 - Epoch: [14][130/352]	Time  0.124 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.3094e-01 (1.4042e-01)	Acc@1  94.53 ( 95.36)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:50 - Epoch: [14][ 70/352]	Time  0.127 ( 0.129)	Data  0.002 ( 0.006)	Loss 2.4879e-01 (1.5639e-01)	Acc@1  92.19 ( 94.69)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:45:51 - Epoch: [14][140/352]	Time  0.103 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.5372e-01 (1.4267e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:51 - Epoch: [14][ 80/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.005)	Loss 1.5672e-01 (1.5388e-01)	Acc@1  95.31 ( 94.81)	Acc@5 100.00 ( 99.95)
03-Mar-22 09:45:52 - Epoch: [14][150/352]	Time  0.150 ( 0.129)	Data  0.002 ( 0.004)	Loss 9.5474e-02 (1.4343e-01)	Acc@1  96.09 ( 95.31)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:52 - Epoch: [14][ 90/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.005)	Loss 2.0206e-01 (1.5224e-01)	Acc@1  95.31 ( 94.88)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:45:53 - Epoch: [14][160/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.004)	Loss 9.7943e-02 (1.4376e-01)	Acc@1  96.09 ( 95.27)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:54 - Epoch: [14][100/352]	Time  0.133 ( 0.130)	Data  0.003 ( 0.005)	Loss 1.2250e-01 (1.5194e-01)	Acc@1  96.09 ( 94.87)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:45:55 - Epoch: [14][170/352]	Time  0.107 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.2769e-01 (1.4345e-01)	Acc@1  97.66 ( 95.33)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:45:55 - Epoch: [14][110/352]	Time  0.131 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.3899e-01 (1.5202e-01)	Acc@1  95.31 ( 94.88)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:56 - Epoch: [14][180/352]	Time  0.132 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.4099e-01 (1.4408e-01)	Acc@1  96.09 ( 95.30)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:56 - Epoch: [14][120/352]	Time  0.110 ( 0.129)	Data  0.002 ( 0.004)	Loss 2.7462e-01 (1.5442e-01)	Acc@1  91.41 ( 94.81)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:45:57 - Epoch: [14][190/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.3407e-01 (1.4487e-01)	Acc@1  96.09 ( 95.27)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:58 - Epoch: [14][130/352]	Time  0.132 ( 0.130)	Data  0.003 ( 0.004)	Loss 1.4872e-01 (1.5322e-01)	Acc@1  93.75 ( 94.85)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:45:58 - Epoch: [14][200/352]	Time  0.113 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.8424e-01 (1.4515e-01)	Acc@1  96.09 ( 95.28)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:45:59 - Epoch: [14][140/352]	Time  0.130 ( 0.130)	Data  0.002 ( 0.004)	Loss 1.3384e-01 (1.5321e-01)	Acc@1  92.97 ( 94.86)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:00 - Epoch: [14][210/352]	Time  0.116 ( 0.129)	Data  0.002 ( 0.003)	Loss 8.7557e-02 (1.4517e-01)	Acc@1  98.44 ( 95.24)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:00 - Epoch: [14][150/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.2640e-01 (1.5163e-01)	Acc@1  96.09 ( 94.90)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:01 - Epoch: [14][220/352]	Time  0.127 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7774e-01 (1.4490e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:01 - Epoch: [14][160/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.004)	Loss 9.6842e-02 (1.5028e-01)	Acc@1  97.66 ( 94.99)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:02 - Epoch: [14][230/352]	Time  0.131 ( 0.129)	Data  0.003 ( 0.003)	Loss 7.3600e-02 (1.4541e-01)	Acc@1  96.88 ( 95.19)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:03 - Epoch: [14][170/352]	Time  0.134 ( 0.129)	Data  0.003 ( 0.004)	Loss 1.7247e-01 (1.4943e-01)	Acc@1  94.53 ( 94.99)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:04 - Epoch: [14][240/352]	Time  0.129 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.8026e-01 (1.4696e-01)	Acc@1  93.75 ( 95.12)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:46:04 - Epoch: [14][180/352]	Time  0.134 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.7086e-01 (1.4970e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:05 - Epoch: [14][250/352]	Time  0.142 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7417e-01 (1.4828e-01)	Acc@1  94.53 ( 95.08)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:46:05 - Epoch: [14][190/352]	Time  0.134 ( 0.129)	Data  0.003 ( 0.004)	Loss 8.5629e-02 (1.4879e-01)	Acc@1  96.88 ( 95.04)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:06 - Epoch: [14][260/352]	Time  0.122 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.6216e-01 (1.4856e-01)	Acc@1  96.09 ( 95.07)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:07 - Epoch: [14][200/352]	Time  0.126 ( 0.129)	Data  0.002 ( 0.004)	Loss 1.6554e-01 (1.5035e-01)	Acc@1  92.97 ( 94.95)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:46:08 - Epoch: [14][270/352]	Time  0.124 ( 0.129)	Data  0.003 ( 0.003)	Loss 2.2099e-01 (1.4951e-01)	Acc@1  92.97 ( 95.02)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:08 - Epoch: [14][210/352]	Time  0.132 ( 0.129)	Data  0.003 ( 0.004)	Loss 2.5951e-01 (1.5046e-01)	Acc@1  92.97 ( 94.96)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:09 - Epoch: [14][280/352]	Time  0.132 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.0046e-01 (1.4864e-01)	Acc@1  96.09 ( 95.05)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:09 - Epoch: [14][220/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.6640e-01 (1.5034e-01)	Acc@1  91.41 ( 94.94)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:10 - Epoch: [14][290/352]	Time  0.138 ( 0.129)	Data  0.004 ( 0.003)	Loss 1.9902e-01 (1.4869e-01)	Acc@1  92.97 ( 95.02)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:10 - Epoch: [14][230/352]	Time  0.142 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.2616e-01 (1.4930e-01)	Acc@1  96.09 ( 95.00)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:11 - Epoch: [14][300/352]	Time  0.129 ( 0.129)	Data  0.004 ( 0.003)	Loss 2.2263e-01 (1.4901e-01)	Acc@1  92.97 ( 95.01)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:12 - Epoch: [14][240/352]	Time  0.133 ( 0.129)	Data  0.003 ( 0.003)	Loss 1.3959e-01 (1.4960e-01)	Acc@1  96.88 ( 95.01)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:13 - Epoch: [14][310/352]	Time  0.134 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.4783e-01 (1.4846e-01)	Acc@1  96.88 ( 95.06)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:13 - Epoch: [14][250/352]	Time  0.130 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7150e-01 (1.4926e-01)	Acc@1  94.53 ( 95.02)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:14 - Epoch: [14][320/352]	Time  0.135 ( 0.129)	Data  0.004 ( 0.003)	Loss 1.9826e-01 (1.4825e-01)	Acc@1  93.75 ( 95.07)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:14 - Epoch: [14][260/352]	Time  0.133 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1942e-01 (1.4883e-01)	Acc@1  95.31 ( 95.04)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:15 - Epoch: [14][330/352]	Time  0.140 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1651e-01 (1.4771e-01)	Acc@1  97.66 ( 95.09)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:16 - Epoch: [14][270/352]	Time  0.112 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5455e-01 (1.4870e-01)	Acc@1  92.19 ( 95.02)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:17 - Epoch: [14][340/352]	Time  0.160 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.5691e-01 (1.4736e-01)	Acc@1  93.75 ( 95.10)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:17 - Epoch: [14][280/352]	Time  0.146 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.1240e-01 (1.4877e-01)	Acc@1  97.66 ( 95.03)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:18 - Epoch: [14][350/352]	Time  0.137 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.3958e-01 (1.4750e-01)	Acc@1  93.75 ( 95.09)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:18 - Epoch: [14][290/352]	Time  0.102 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.7203e-01 (1.4883e-01)	Acc@1  93.75 ( 95.03)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:18 - Test: [ 0/20]	Time  0.333 ( 0.333)	Loss 1.1318e-01 (1.1318e-01)	Acc@1  96.48 ( 96.48)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:19 - Test: [10/20]	Time  0.071 ( 0.101)	Loss 1.6651e-01 (1.6571e-01)	Acc@1  92.58 ( 94.35)	Acc@5 100.00 ( 99.96)
03-Mar-22 09:46:19 - Epoch: [14][300/352]	Time  0.125 ( 0.129)	Data  0.002 ( 0.003)	Loss 2.0098e-01 (1.4874e-01)	Acc@1  89.84 ( 95.02)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:20 -  * Acc@1 94.600 Acc@5 99.940
03-Mar-22 09:46:20 - Best acc at epoch 14: 94.65999603271484
03-Mar-22 09:46:20 - Epoch: [15][  0/352]	Time  0.341 ( 0.341)	Data  0.240 ( 0.240)	Loss 2.1297e-01 (2.1297e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:21 - Epoch: [14][310/352]	Time  0.107 ( 0.129)	Data  0.002 ( 0.003)	Loss 1.8862e-01 (1.4869e-01)	Acc@1  91.41 ( 95.02)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:21 - Epoch: [15][ 10/352]	Time  0.124 ( 0.141)	Data  0.002 ( 0.023)	Loss 2.1727e-01 (1.6731e-01)	Acc@1  94.53 ( 94.82)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:22 - Epoch: [14][320/352]	Time  0.112 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.0619e-01 (1.4830e-01)	Acc@1  94.53 ( 95.04)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:23 - Epoch: [15][ 20/352]	Time  0.155 ( 0.131)	Data  0.003 ( 0.013)	Loss 9.8965e-02 (1.4734e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:23 - Epoch: [14][330/352]	Time  0.131 ( 0.128)	Data  0.002 ( 0.003)	Loss 1.1946e-01 (1.4815e-01)	Acc@1  97.66 ( 95.06)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:24 - Epoch: [15][ 30/352]	Time  0.129 ( 0.129)	Data  0.002 ( 0.010)	Loss 9.7826e-02 (1.3930e-01)	Acc@1  97.66 ( 95.39)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:24 - Epoch: [14][340/352]	Time  0.128 ( 0.128)	Data  0.003 ( 0.003)	Loss 1.3094e-01 (1.4806e-01)	Acc@1  93.75 ( 95.03)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:25 - Epoch: [15][ 40/352]	Time  0.108 ( 0.127)	Data  0.002 ( 0.008)	Loss 2.4232e-01 (1.4162e-01)	Acc@1  93.75 ( 95.31)	Acc@5 100.00 ( 99.98)
03-Mar-22 09:46:26 - Epoch: [14][350/352]	Time  0.142 ( 0.128)	Data  0.003 ( 0.003)	Loss 1.3590e-01 (1.4788e-01)	Acc@1  96.88 ( 95.04)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:26 - Test: [ 0/20]	Time  0.339 ( 0.339)	Loss 1.4521e-01 (1.4521e-01)	Acc@1  94.92 ( 94.92)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:26 - Epoch: [15][ 50/352]	Time  0.104 ( 0.126)	Data  0.002 ( 0.007)	Loss 1.7027e-01 (1.4486e-01)	Acc@1  94.53 ( 95.14)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:27 - Test: [10/20]	Time  0.073 ( 0.105)	Loss 1.3383e-01 (1.5746e-01)	Acc@1  94.53 ( 94.85)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:28 - Epoch: [15][ 60/352]	Time  0.138 ( 0.125)	Data  0.002 ( 0.006)	Loss 1.3529e-01 (1.3993e-01)	Acc@1  95.31 ( 95.30)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:28 -  * Acc@1 94.760 Acc@5 99.900
03-Mar-22 09:46:28 - Best acc at epoch 14: 94.77999877929688
03-Mar-22 09:46:28 - Epoch: [15][  0/352]	Time  0.351 ( 0.351)	Data  0.238 ( 0.238)	Loss 1.2946e-01 (1.2946e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 09:46:29 - Epoch: [15][ 70/352]	Time  0.128 ( 0.125)	Data  0.002 ( 0.006)	Loss 1.9672e-01 (1.4037e-01)	Acc@1  93.75 ( 95.29)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:30 - Epoch: [15][ 10/352]	Time  0.125 ( 0.144)	Data  0.002 ( 0.024)	Loss 1.5223e-01 (1.3903e-01)	Acc@1  94.53 ( 95.38)	Acc@5 100.00 ( 99.86)
03-Mar-22 09:46:30 - Epoch: [15][ 80/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.005)	Loss 1.9520e-01 (1.4001e-01)	Acc@1  91.41 ( 95.19)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:31 - Epoch: [15][ 20/352]	Time  0.143 ( 0.139)	Data  0.002 ( 0.014)	Loss 9.4987e-02 (1.3179e-01)	Acc@1  96.09 ( 95.35)	Acc@5 100.00 ( 99.89)
03-Mar-22 09:46:31 - Epoch: [15][ 90/352]	Time  0.143 ( 0.125)	Data  0.003 ( 0.005)	Loss 2.1949e-01 (1.4127e-01)	Acc@1  92.97 ( 95.15)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:32 - Epoch: [15][ 30/352]	Time  0.131 ( 0.138)	Data  0.002 ( 0.010)	Loss 1.4941e-01 (1.3567e-01)	Acc@1  93.75 ( 95.31)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:33 - Epoch: [15][100/352]	Time  0.131 ( 0.125)	Data  0.002 ( 0.005)	Loss 2.0563e-01 (1.4123e-01)	Acc@1  94.53 ( 95.20)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:34 - Epoch: [15][ 40/352]	Time  0.139 ( 0.136)	Data  0.003 ( 0.008)	Loss 1.5644e-01 (1.3253e-01)	Acc@1  92.97 ( 95.46)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:34 - Epoch: [15][110/352]	Time  0.126 ( 0.125)	Data  0.002 ( 0.004)	Loss 1.6100e-01 (1.4176e-01)	Acc@1  95.31 ( 95.21)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:35 - Epoch: [15][ 50/352]	Time  0.144 ( 0.135)	Data  0.003 ( 0.007)	Loss 1.0314e-01 (1.3522e-01)	Acc@1  96.88 ( 95.47)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:46:35 - Epoch: [15][120/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.004)	Loss 2.0103e-01 (1.4129e-01)	Acc@1  92.97 ( 95.24)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:36 - Epoch: [15][ 60/352]	Time  0.144 ( 0.134)	Data  0.003 ( 0.006)	Loss 1.7438e-01 (1.3696e-01)	Acc@1  93.75 ( 95.36)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:46:36 - Epoch: [15][130/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.1408e-01 (1.4101e-01)	Acc@1  96.09 ( 95.25)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:37 - Epoch: [15][140/352]	Time  0.131 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.1016e-01 (1.4023e-01)	Acc@1  94.53 ( 95.30)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:38 - Epoch: [15][ 70/352]	Time  0.142 ( 0.135)	Data  0.002 ( 0.006)	Loss 8.9351e-02 (1.3687e-01)	Acc@1  96.88 ( 95.35)	Acc@5 100.00 ( 99.91)
03-Mar-22 09:46:39 - Epoch: [15][150/352]	Time  0.127 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.3730e-01 (1.4205e-01)	Acc@1  97.66 ( 95.24)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:39 - Epoch: [15][ 80/352]	Time  0.142 ( 0.135)	Data  0.002 ( 0.005)	Loss 8.6794e-02 (1.3651e-01)	Acc@1  97.66 ( 95.31)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:40 - Epoch: [15][160/352]	Time  0.125 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.1145e-01 (1.4216e-01)	Acc@1  96.09 ( 95.24)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:40 - Epoch: [15][ 90/352]	Time  0.142 ( 0.136)	Data  0.003 ( 0.005)	Loss 1.0515e-01 (1.3764e-01)	Acc@1  96.88 ( 95.31)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:41 - Epoch: [15][170/352]	Time  0.128 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.3024e-01 (1.4137e-01)	Acc@1  96.09 ( 95.27)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:42 - Epoch: [15][100/352]	Time  0.136 ( 0.136)	Data  0.003 ( 0.005)	Loss 9.5189e-02 (1.3779e-01)	Acc@1  96.09 ( 95.29)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:42 - Epoch: [15][180/352]	Time  0.129 ( 0.124)	Data  0.002 ( 0.004)	Loss 1.8926e-01 (1.4289e-01)	Acc@1  94.53 ( 95.24)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:43 - Epoch: [15][110/352]	Time  0.130 ( 0.137)	Data  0.003 ( 0.005)	Loss 2.1993e-01 (1.3796e-01)	Acc@1  89.06 ( 95.24)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:44 - Epoch: [15][190/352]	Time  0.129 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.5044e-01 (1.4265e-01)	Acc@1  95.31 ( 95.24)	Acc@5 100.00 ( 99.92)
03-Mar-22 09:46:45 - Epoch: [15][120/352]	Time  0.128 ( 0.137)	Data  0.002 ( 0.004)	Loss 1.3320e-01 (1.3759e-01)	Acc@1  94.53 ( 95.22)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:45 - Epoch: [15][200/352]	Time  0.124 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.2279e-01 (1.4212e-01)	Acc@1  94.53 ( 95.25)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:46 - Epoch: [15][130/352]	Time  0.133 ( 0.136)	Data  0.002 ( 0.004)	Loss 1.5781e-01 (1.3884e-01)	Acc@1  94.53 ( 95.21)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:46 - Epoch: [15][210/352]	Time  0.117 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.4426e-01 (1.4206e-01)	Acc@1  96.88 ( 95.23)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:47 - Epoch: [15][140/352]	Time  0.139 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.6292e-01 (1.3896e-01)	Acc@1  92.97 ( 95.20)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:47 - Epoch: [15][220/352]	Time  0.132 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.2118e-01 (1.4153e-01)	Acc@1  93.75 ( 95.26)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:48 - Epoch: [15][150/352]	Time  0.127 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.4676e-01 (1.4106e-01)	Acc@1  96.09 ( 95.17)	Acc@5  99.22 ( 99.92)
03-Mar-22 09:46:49 - Epoch: [15][230/352]	Time  0.111 ( 0.124)	Data  0.002 ( 0.003)	Loss 1.4702e-01 (1.4164e-01)	Acc@1  95.31 ( 95.26)	Acc@5  99.22 ( 99.93)
03-Mar-22 09:46:50 - Epoch: [15][160/352]	Time  0.147 ( 0.135)	Data  0.003 ( 0.004)	Loss 1.4718e-01 (1.4068e-01)	Acc@1  95.31 ( 95.20)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:50 - Epoch: [15][240/352]	Time  0.139 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.4776e-01 (1.4179e-01)	Acc@1  94.53 ( 95.24)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:51 - Epoch: [15][170/352]	Time  0.134 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.3360e-01 (1.4010e-01)	Acc@1  95.31 ( 95.18)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:51 - Epoch: [15][250/352]	Time  0.136 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.2835e-01 (1.4203e-01)	Acc@1  97.66 ( 95.25)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:52 - Epoch: [15][180/352]	Time  0.144 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.5855e-01 (1.3961e-01)	Acc@1  95.31 ( 95.22)	Acc@5 100.00 ( 99.94)
03-Mar-22 09:46:53 - Epoch: [15][260/352]	Time  0.146 ( 0.125)	Data  0.002 ( 0.003)	Loss 1.4791e-01 (1.4278e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.93)
03-Mar-22 09:46:54 - Epoch: [15][190/352]	Time  0.132 ( 0.135)	Data  0.002 ( 0.004)	Loss 1.4916e-01 (1.3792e-01)	Acc@1  93.75 ( 95.27)	Acc@5 100.00 ( 99.94)
03-Mar-22 10:01:14 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:01:14 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:04:39 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:04:39 - => creating PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:04:39 - match all modules defined in bit_config: False
03-Mar-22 10:04:39 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:13:06 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:13:06 - Use GPU: 0 for training
03-Mar-22 10:13:06 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:15:02 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:15:02 - Use GPU: 0 for training
03-Mar-22 10:15:02 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:15:06 - match all modules defined in bit_config: False
03-Mar-22 10:15:06 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:15:27 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:15:27 - Use GPU: 0 for training
03-Mar-22 10:15:27 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:15:31 - match all modules defined in bit_config: False
03-Mar-22 10:15:31 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:17:14 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:17:14 - Use GPU: 0 for training
03-Mar-22 10:17:14 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:17:18 - match all modules defined in bit_config: False
03-Mar-22 10:17:18 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:19:22 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:19:22 - Use GPU: 0 for training
03-Mar-22 10:19:22 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:19:26 - match all modules defined in bit_config: False
03-Mar-22 10:19:26 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:41:56 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:41:56 - Use GPU: 0 for training
03-Mar-22 10:41:56 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:42:00 - match all modules defined in bit_config: False
03-Mar-22 10:42:00 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:43:16 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:43:16 - Use GPU: 0 for training
03-Mar-22 10:43:16 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 10:43:20 - match all modules defined in bit_config: True
03-Mar-22 10:43:20 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:43:20 - Epoch: [0][  0/352]	Time  0.413 ( 0.413)	Data  0.237 ( 0.237)	Loss 3.0795e-01 (3.0795e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
03-Mar-22 10:43:22 - Epoch: [0][ 10/352]	Time  0.098 ( 0.142)	Data  0.001 ( 0.023)	Loss 1.9220e-01 (1.7236e-01)	Acc@1  96.09 ( 94.32)	Acc@5 100.00 (100.00)
03-Mar-22 10:43:23 - Epoch: [0][ 20/352]	Time  0.097 ( 0.121)	Data  0.001 ( 0.013)	Loss 9.7979e-02 (1.4542e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 (100.00)
03-Mar-22 10:43:23 - Epoch: [0][ 30/352]	Time  0.097 ( 0.114)	Data  0.001 ( 0.009)	Loss 1.0826e-01 (1.4135e-01)	Acc@1  96.88 ( 95.09)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:24 - Epoch: [0][ 40/352]	Time  0.096 ( 0.110)	Data  0.001 ( 0.007)	Loss 8.8699e-02 (1.3833e-01)	Acc@1  96.88 ( 95.18)	Acc@5  99.22 ( 99.94)
03-Mar-22 10:43:25 - Epoch: [0][ 50/352]	Time  0.096 ( 0.108)	Data  0.001 ( 0.006)	Loss 1.0920e-01 (1.3333e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.95)
03-Mar-22 10:43:27 - Epoch: [0][ 60/352]	Time  0.131 ( 0.109)	Data  0.002 ( 0.005)	Loss 1.4759e-01 (1.3037e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 ( 99.96)
03-Mar-22 10:43:28 - Epoch: [0][ 70/352]	Time  0.133 ( 0.112)	Data  0.002 ( 0.005)	Loss 9.4575e-02 (1.2611e-01)	Acc@1  96.09 ( 95.35)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:29 - Epoch: [0][ 80/352]	Time  0.131 ( 0.115)	Data  0.002 ( 0.005)	Loss 1.0132e-01 (1.2644e-01)	Acc@1  96.09 ( 95.37)	Acc@5 100.00 ( 99.96)
03-Mar-22 10:43:31 - Epoch: [0][ 90/352]	Time  0.096 ( 0.116)	Data  0.002 ( 0.004)	Loss 8.2958e-02 (1.2348e-01)	Acc@1  95.31 ( 95.52)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:31 - Epoch: [0][100/352]	Time  0.096 ( 0.114)	Data  0.001 ( 0.004)	Loss 5.7821e-02 (1.2140e-01)	Acc@1  98.44 ( 95.60)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:32 - Epoch: [0][110/352]	Time  0.098 ( 0.113)	Data  0.002 ( 0.004)	Loss 1.1381e-01 (1.1959e-01)	Acc@1  94.53 ( 95.69)	Acc@5 100.00 ( 99.96)
03-Mar-22 10:43:33 - Epoch: [0][120/352]	Time  0.096 ( 0.111)	Data  0.001 ( 0.004)	Loss 1.7338e-01 (1.1988e-01)	Acc@1  93.75 ( 95.69)	Acc@5 100.00 ( 99.96)
03-Mar-22 10:43:34 - Epoch: [0][130/352]	Time  0.097 ( 0.110)	Data  0.002 ( 0.004)	Loss 1.2692e-01 (1.1928e-01)	Acc@1  94.53 ( 95.71)	Acc@5 100.00 ( 99.96)
03-Mar-22 10:43:35 - Epoch: [0][140/352]	Time  0.096 ( 0.109)	Data  0.001 ( 0.003)	Loss 7.6918e-02 (1.1740e-01)	Acc@1  96.88 ( 95.80)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:36 - Epoch: [0][150/352]	Time  0.097 ( 0.108)	Data  0.002 ( 0.003)	Loss 9.9368e-02 (1.1762e-01)	Acc@1  95.31 ( 95.76)	Acc@5 100.00 ( 99.96)
03-Mar-22 10:43:37 - Epoch: [0][160/352]	Time  0.097 ( 0.108)	Data  0.001 ( 0.003)	Loss 3.1986e-02 (1.1649e-01)	Acc@1 100.00 ( 95.80)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:38 - Epoch: [0][170/352]	Time  0.096 ( 0.107)	Data  0.002 ( 0.003)	Loss 1.2716e-01 (1.1669e-01)	Acc@1  95.31 ( 95.76)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:39 - Epoch: [0][180/352]	Time  0.096 ( 0.107)	Data  0.001 ( 0.003)	Loss 1.0362e-01 (1.1563e-01)	Acc@1  96.88 ( 95.81)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:40 - Epoch: [0][190/352]	Time  0.097 ( 0.106)	Data  0.001 ( 0.003)	Loss 8.7430e-02 (1.1528e-01)	Acc@1  97.66 ( 95.84)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:41 - Epoch: [0][200/352]	Time  0.096 ( 0.106)	Data  0.001 ( 0.003)	Loss 1.3658e-01 (1.1441e-01)	Acc@1  95.31 ( 95.88)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:42 - Epoch: [0][210/352]	Time  0.096 ( 0.105)	Data  0.002 ( 0.003)	Loss 1.3520e-01 (1.1345e-01)	Acc@1  96.09 ( 95.93)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:43 - Epoch: [0][220/352]	Time  0.096 ( 0.105)	Data  0.001 ( 0.003)	Loss 9.1622e-02 (1.1400e-01)	Acc@1  96.88 ( 95.91)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:44 - Epoch: [0][230/352]	Time  0.097 ( 0.105)	Data  0.001 ( 0.003)	Loss 1.0555e-01 (1.1345e-01)	Acc@1  95.31 ( 95.90)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:45 - Epoch: [0][240/352]	Time  0.101 ( 0.105)	Data  0.001 ( 0.003)	Loss 5.0035e-02 (1.1285e-01)	Acc@1  98.44 ( 95.93)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:46 - Epoch: [0][250/352]	Time  0.097 ( 0.104)	Data  0.002 ( 0.003)	Loss 2.1798e-01 (1.1270e-01)	Acc@1  90.62 ( 95.92)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:47 - Epoch: [0][260/352]	Time  0.097 ( 0.104)	Data  0.001 ( 0.003)	Loss 7.1866e-02 (1.1198e-01)	Acc@1  98.44 ( 95.97)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:48 - Epoch: [0][270/352]	Time  0.100 ( 0.104)	Data  0.002 ( 0.003)	Loss 4.5325e-02 (1.1209e-01)	Acc@1  97.66 ( 95.96)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:49 - Epoch: [0][280/352]	Time  0.100 ( 0.104)	Data  0.002 ( 0.002)	Loss 8.9781e-02 (1.1243e-01)	Acc@1  96.09 ( 95.96)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:50 - Epoch: [0][290/352]	Time  0.097 ( 0.104)	Data  0.002 ( 0.002)	Loss 1.3641e-01 (1.1194e-01)	Acc@1  92.19 ( 95.96)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:51 - Epoch: [0][300/352]	Time  0.094 ( 0.104)	Data  0.001 ( 0.002)	Loss 1.5616e-01 (1.1220e-01)	Acc@1  94.53 ( 95.95)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:52 - Epoch: [0][310/352]	Time  0.097 ( 0.104)	Data  0.001 ( 0.002)	Loss 1.3005e-01 (1.1210e-01)	Acc@1  94.53 ( 95.96)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:53 - Epoch: [0][320/352]	Time  0.096 ( 0.104)	Data  0.001 ( 0.002)	Loss 3.3182e-02 (1.1114e-01)	Acc@1  99.22 ( 96.00)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:54 - Epoch: [0][330/352]	Time  0.097 ( 0.104)	Data  0.001 ( 0.002)	Loss 1.9141e-01 (1.1168e-01)	Acc@1  92.19 ( 95.98)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:55 - Epoch: [0][340/352]	Time  0.096 ( 0.103)	Data  0.001 ( 0.002)	Loss 4.6548e-02 (1.1105e-01)	Acc@1  98.44 ( 96.01)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:56 - Epoch: [0][350/352]	Time  0.096 ( 0.103)	Data  0.001 ( 0.002)	Loss 5.1613e-02 (1.1129e-01)	Acc@1  98.44 ( 96.01)	Acc@5 100.00 ( 99.97)
03-Mar-22 10:43:57 - Test: [ 0/20]	Time  0.416 ( 0.416)	Loss 3.7956e-01 (3.7956e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
03-Mar-22 10:43:58 - Test: [10/20]	Time  0.068 ( 0.104)	Loss 4.1133e-01 (3.6462e-01)	Acc@1  89.45 ( 90.52)	Acc@5  99.22 ( 99.57)
03-Mar-22 10:43:58 -  * Acc@1 90.580 Acc@5 99.560
03-Mar-22 10:43:58 - Best acc at epoch 0: 90.57999420166016
03-Mar-22 10:43:59 - Epoch: [1][  0/352]	Time  0.360 ( 0.360)	Data  0.229 ( 0.229)	Loss 1.1012e-01 (1.1012e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:00 - Epoch: [1][ 10/352]	Time  0.096 ( 0.123)	Data  0.001 ( 0.022)	Loss 6.5734e-02 (9.9683e-02)	Acc@1  97.66 ( 96.38)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:01 - Epoch: [1][ 20/352]	Time  0.096 ( 0.110)	Data  0.001 ( 0.012)	Loss 7.6517e-02 (9.0303e-02)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:02 - Epoch: [1][ 30/352]	Time  0.097 ( 0.110)	Data  0.001 ( 0.009)	Loss 3.4132e-02 (9.5411e-02)	Acc@1  99.22 ( 96.52)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:03 - Epoch: [1][ 40/352]	Time  0.098 ( 0.107)	Data  0.001 ( 0.007)	Loss 1.2667e-01 (9.8297e-02)	Acc@1  96.09 ( 96.46)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:04 - Epoch: [1][ 50/352]	Time  0.100 ( 0.107)	Data  0.002 ( 0.006)	Loss 5.1800e-02 (9.6819e-02)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:05 - Epoch: [1][ 60/352]	Time  0.097 ( 0.107)	Data  0.001 ( 0.005)	Loss 1.0185e-01 (9.9054e-02)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:06 - Epoch: [1][ 70/352]	Time  0.097 ( 0.106)	Data  0.001 ( 0.005)	Loss 9.0441e-02 (1.0038e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:07 - Epoch: [1][ 80/352]	Time  0.096 ( 0.105)	Data  0.001 ( 0.004)	Loss 1.0128e-01 (9.8681e-02)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:08 - Epoch: [1][ 90/352]	Time  0.097 ( 0.104)	Data  0.001 ( 0.004)	Loss 1.0746e-01 (9.8275e-02)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:09 - Epoch: [1][100/352]	Time  0.096 ( 0.103)	Data  0.001 ( 0.004)	Loss 5.0433e-02 (9.7333e-02)	Acc@1  99.22 ( 96.56)	Acc@5 100.00 (100.00)
03-Mar-22 10:44:10 - Epoch: [1][110/352]	Time  0.097 ( 0.103)	Data  0.001 ( 0.004)	Loss 6.5939e-02 (9.6042e-02)	Acc@1  96.09 ( 96.60)	Acc@5 100.00 ( 99.99)
03-Mar-22 10:44:11 - Epoch: [1][120/352]	Time  0.096 ( 0.103)	Data  0.001 ( 0.003)	Loss 1.3300e-01 (9.7863e-02)	Acc@1  95.31 ( 96.55)	Acc@5  99.22 ( 99.99)
03-Mar-22 10:44:12 - Epoch: [1][130/352]	Time  0.097 ( 0.102)	Data  0.001 ( 0.003)	Loss 5.1890e-02 (9.7773e-02)	Acc@1  98.44 ( 96.57)	Acc@5 100.00 ( 99.99)
03-Mar-22 10:51:35 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:51:35 - Use GPU: 0 for training
03-Mar-22 10:51:35 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 10:51:39 - match all modules defined in bit_config: True
03-Mar-22 10:51:39 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 10:51:40 - Epoch: [0][  0/352]	Time  0.408 ( 0.408)	Data  0.226 ( 0.226)	Loss 1.1617e-01 (1.1617e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 10:51:41 - Epoch: [0][ 10/352]	Time  0.095 ( 0.131)	Data  0.001 ( 0.022)	Loss 1.0751e-01 (1.5330e-01)	Acc@1  95.31 ( 94.32)	Acc@5 100.00 (100.00)
03-Mar-22 10:51:54 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 10:51:54 - Use GPU: 0 for training
03-Mar-22 10:51:54 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 10:51:58 - match all modules defined in bit_config: False
03-Mar-22 10:51:58 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ) weight_bit=8, full_precision_flag=False, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ) weight_bit=4, full_precision_flag=False, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:03:39 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:03:39 - Use GPU: 0 for training
03-Mar-22 11:03:39 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:03:43 - match all modules defined in bit_config: False
03-Mar-22 11:05:14 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:05:14 - Use GPU: 0 for training
03-Mar-22 11:05:14 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:05:18 - match all modules defined in bit_config: False
03-Mar-22 11:05:18 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=None, groups=1, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:07:59 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:07:59 - Use GPU: 0 for training
03-Mar-22 11:07:59 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:08:03 - match all modules defined in bit_config: True
03-Mar-22 11:08:03 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:23:10 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:23:10 - Use GPU: 0 for training
03-Mar-22 11:23:10 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:23:14 - match all modules defined in bit_config: True
03-Mar-22 11:23:14 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:24:49 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:24:49 - Use GPU: 0 for training
03-Mar-22 11:24:49 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:24:54 - match all modules defined in bit_config: True
03-Mar-22 11:24:54 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:25:46 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:25:46 - Use GPU: 0 for training
03-Mar-22 11:25:46 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:25:50 - match all modules defined in bit_config: True
03-Mar-22 11:25:50 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:27:33 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:27:33 - Use GPU: 0 for training
03-Mar-22 11:27:33 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:27:37 - match all modules defined in bit_config: True
03-Mar-22 11:27:37 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 11:27:54 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 11:27:54 - Use GPU: 0 for training
03-Mar-22 11:27:54 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 11:27:58 - match all modules defined in bit_config: True
03-Mar-22 11:27:58 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:14:19 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:14:19 - Use GPU: 0 for training
03-Mar-22 13:14:19 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:14:23 - match all modules defined in bit_config: True
03-Mar-22 13:14:23 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:15:04 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:15:04 - Use GPU: 0 for training
03-Mar-22 13:15:04 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:15:08 - match all modules defined in bit_config: True
03-Mar-22 13:15:08 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:15:53 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:15:53 - Use GPU: 0 for training
03-Mar-22 13:15:53 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:15:57 - match all modules defined in bit_config: True
03-Mar-22 13:15:57 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:18:43 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:18:43 - Use GPU: 0 for training
03-Mar-22 13:18:43 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:18:47 - match all modules defined in bit_config: True
03-Mar-22 13:18:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:20:50 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:20:50 - Use GPU: 0 for training
03-Mar-22 13:20:50 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:20:54 - match all modules defined in bit_config: True
03-Mar-22 13:20:54 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:23:51 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:23:51 - Use GPU: 0 for training
03-Mar-22 13:23:51 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:23:55 - match all modules defined in bit_config: True
03-Mar-22 13:23:55 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:25:03 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:25:03 - Use GPU: 0 for training
03-Mar-22 13:25:03 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:25:07 - match all modules defined in bit_config: True
03-Mar-22 13:25:07 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:25:22 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:25:22 - Use GPU: 0 for training
03-Mar-22 13:25:22 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:25:26 - match all modules defined in bit_config: True
03-Mar-22 13:25:26 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:26:08 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:26:08 - Use GPU: 0 for training
03-Mar-22 13:26:08 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:26:12 - match all modules defined in bit_config: True
03-Mar-22 13:26:12 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:26:36 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:26:36 - Use GPU: 0 for training
03-Mar-22 13:26:36 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:26:41 - match all modules defined in bit_config: True
03-Mar-22 13:26:41 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:27:17 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:27:17 - Use GPU: 0 for training
03-Mar-22 13:27:17 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:27:21 - match all modules defined in bit_config: True
03-Mar-22 13:27:21 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:27:22 - Epoch: [0][  0/352]	Time  0.395 ( 0.395)	Data  0.223 ( 0.223)	Loss 2.3707e+00 (2.3707e+00)	Acc@1   5.47 (  5.47)	Acc@5  50.78 ( 50.78)
03-Mar-22 13:27:23 - Epoch: [0][ 10/352]	Time  0.102 ( 0.141)	Data  0.001 ( 0.022)	Loss 2.3235e+00 (2.3473e+00)	Acc@1  10.16 (  9.87)	Acc@5  53.91 ( 49.79)
03-Mar-22 13:27:24 - Epoch: [0][ 20/352]	Time  0.102 ( 0.123)	Data  0.002 ( 0.012)	Loss 2.3226e+00 (2.3478e+00)	Acc@1  14.84 ( 10.27)	Acc@5  51.56 ( 48.74)
03-Mar-22 13:27:25 - Epoch: [0][ 30/352]	Time  0.102 ( 0.123)	Data  0.002 ( 0.009)	Loss 2.3227e+00 (2.3505e+00)	Acc@1  13.28 ( 10.26)	Acc@5  52.34 ( 48.71)
03-Mar-22 13:27:26 - Epoch: [0][ 40/352]	Time  0.127 ( 0.121)	Data  0.002 ( 0.007)	Loss 2.3111e+00 (2.3472e+00)	Acc@1  11.72 ( 10.04)	Acc@5  57.03 ( 49.58)
03-Mar-22 13:27:28 - Epoch: [0][ 50/352]	Time  0.102 ( 0.119)	Data  0.002 ( 0.006)	Loss 2.3633e+00 (2.3476e+00)	Acc@1   4.69 ( 10.16)	Acc@5  49.22 ( 49.59)
03-Mar-22 13:27:29 - Epoch: [0][ 60/352]	Time  0.112 ( 0.117)	Data  0.002 ( 0.005)	Loss 2.3277e+00 (2.3458e+00)	Acc@1  10.94 ( 10.03)	Acc@5  49.22 ( 49.72)
03-Mar-22 13:27:30 - Epoch: [0][ 70/352]	Time  0.102 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.3262e+00 (2.3441e+00)	Acc@1  11.72 ( 10.09)	Acc@5  48.44 ( 49.89)
03-Mar-22 13:27:31 - Epoch: [0][ 80/352]	Time  0.132 ( 0.116)	Data  0.002 ( 0.005)	Loss 2.3191e+00 (2.3429e+00)	Acc@1  14.06 ( 10.20)	Acc@5  50.78 ( 49.87)
03-Mar-22 13:27:32 - Epoch: [0][ 90/352]	Time  0.127 ( 0.117)	Data  0.002 ( 0.004)	Loss 2.3357e+00 (2.3419e+00)	Acc@1  12.50 ( 10.22)	Acc@5  46.88 ( 49.99)
03-Mar-22 13:27:33 - Epoch: [0][100/352]	Time  0.102 ( 0.117)	Data  0.001 ( 0.004)	Loss 2.3543e+00 (2.3416e+00)	Acc@1  10.16 ( 10.09)	Acc@5  47.66 ( 49.98)
03-Mar-22 13:27:34 - Epoch: [0][110/352]	Time  0.104 ( 0.116)	Data  0.001 ( 0.004)	Loss 2.3537e+00 (2.3410e+00)	Acc@1  10.16 ( 10.14)	Acc@5  48.44 ( 50.08)
03-Mar-22 13:27:35 - Epoch: [0][120/352]	Time  0.101 ( 0.116)	Data  0.001 ( 0.004)	Loss 2.3709e+00 (2.3407e+00)	Acc@1  10.94 ( 10.12)	Acc@5  46.09 ( 50.15)
03-Mar-22 13:27:37 - Epoch: [0][130/352]	Time  0.130 ( 0.117)	Data  0.002 ( 0.004)	Loss 2.3600e+00 (2.3405e+00)	Acc@1   7.03 ( 10.08)	Acc@5  47.66 ( 50.16)
03-Mar-22 13:27:38 - Epoch: [0][140/352]	Time  0.102 ( 0.117)	Data  0.001 ( 0.003)	Loss 2.3343e+00 (2.3390e+00)	Acc@1  11.72 ( 10.17)	Acc@5  50.78 ( 50.27)
03-Mar-22 13:27:39 - Epoch: [0][150/352]	Time  0.109 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3481e+00 (2.3389e+00)	Acc@1   9.38 ( 10.14)	Acc@5  47.66 ( 50.24)
03-Mar-22 13:27:40 - Epoch: [0][160/352]	Time  0.101 ( 0.116)	Data  0.001 ( 0.003)	Loss 2.3410e+00 (2.3386e+00)	Acc@1   9.38 ( 10.09)	Acc@5  47.66 ( 50.21)
03-Mar-22 13:27:41 - Epoch: [0][170/352]	Time  0.124 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3263e+00 (2.3382e+00)	Acc@1   7.03 ( 10.02)	Acc@5  52.34 ( 50.25)
03-Mar-22 13:27:42 - Epoch: [0][180/352]	Time  0.100 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3456e+00 (2.3382e+00)	Acc@1   7.81 (  9.91)	Acc@5  51.56 ( 50.27)
03-Mar-22 13:27:44 - Epoch: [0][190/352]	Time  0.130 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3518e+00 (2.3382e+00)	Acc@1  11.72 (  9.95)	Acc@5  43.75 ( 50.19)
03-Mar-22 13:27:45 - Epoch: [0][200/352]	Time  0.129 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3316e+00 (2.3379e+00)	Acc@1  14.06 (  9.93)	Acc@5  52.34 ( 50.19)
03-Mar-22 13:27:46 - Epoch: [0][210/352]	Time  0.104 ( 0.116)	Data  0.001 ( 0.003)	Loss 2.3510e+00 (2.3374e+00)	Acc@1   6.25 (  9.90)	Acc@5  45.31 ( 50.20)
03-Mar-22 13:27:47 - Epoch: [0][220/352]	Time  0.128 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3171e+00 (2.3365e+00)	Acc@1   6.25 (  9.93)	Acc@5  53.91 ( 50.27)
03-Mar-22 13:27:48 - Epoch: [0][230/352]	Time  0.127 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3786e+00 (2.3363e+00)	Acc@1   2.34 (  9.90)	Acc@5  42.19 ( 50.18)
03-Mar-22 13:27:49 - Epoch: [0][240/352]	Time  0.100 ( 0.116)	Data  0.001 ( 0.003)	Loss 2.3132e+00 (2.3361e+00)	Acc@1   7.03 (  9.87)	Acc@5  52.34 ( 50.15)
03-Mar-22 13:27:50 - Epoch: [0][250/352]	Time  0.104 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3590e+00 (2.3360e+00)	Acc@1   7.03 (  9.86)	Acc@5  46.88 ( 50.14)
03-Mar-22 13:27:52 - Epoch: [0][260/352]	Time  0.104 ( 0.116)	Data  0.002 ( 0.003)	Loss 2.3174e+00 (2.3355e+00)	Acc@1  14.06 (  9.88)	Acc@5  52.34 ( 50.16)
03-Mar-22 13:27:53 - Epoch: [0][270/352]	Time  0.123 ( 0.115)	Data  0.002 ( 0.003)	Loss 2.3414e+00 (2.3352e+00)	Acc@1   8.59 (  9.86)	Acc@5  50.00 ( 50.17)
03-Mar-22 13:27:54 - Epoch: [0][280/352]	Time  0.105 ( 0.115)	Data  0.002 ( 0.003)	Loss 2.3237e+00 (2.3349e+00)	Acc@1  10.94 (  9.82)	Acc@5  50.78 ( 50.23)
03-Mar-22 13:27:55 - Epoch: [0][290/352]	Time  0.101 ( 0.115)	Data  0.001 ( 0.003)	Loss 2.3341e+00 (2.3344e+00)	Acc@1   8.59 (  9.87)	Acc@5  53.12 ( 50.25)
03-Mar-22 13:27:56 - Epoch: [0][300/352]	Time  0.104 ( 0.115)	Data  0.001 ( 0.003)	Loss 2.2942e+00 (2.3343e+00)	Acc@1  12.50 (  9.89)	Acc@5  57.03 ( 50.22)
03-Mar-22 13:27:57 - Epoch: [0][310/352]	Time  0.103 ( 0.115)	Data  0.002 ( 0.003)	Loss 2.3285e+00 (2.3339e+00)	Acc@1  16.41 (  9.93)	Acc@5  47.66 ( 50.22)
03-Mar-22 13:27:58 - Epoch: [0][320/352]	Time  0.104 ( 0.115)	Data  0.002 ( 0.003)	Loss 2.3121e+00 (2.3339e+00)	Acc@1  10.94 (  9.92)	Acc@5  50.78 ( 50.15)
03-Mar-22 13:28:00 - Epoch: [0][330/352]	Time  0.127 ( 0.115)	Data  0.002 ( 0.002)	Loss 2.3375e+00 (2.3334e+00)	Acc@1   8.59 (  9.94)	Acc@5  47.66 ( 50.19)
03-Mar-22 13:28:01 - Epoch: [0][340/352]	Time  0.104 ( 0.115)	Data  0.002 ( 0.002)	Loss 2.3282e+00 (2.3332e+00)	Acc@1  14.06 (  9.98)	Acc@5  48.44 ( 50.14)
03-Mar-22 13:28:02 - Epoch: [0][350/352]	Time  0.109 ( 0.115)	Data  0.002 ( 0.002)	Loss 2.2957e+00 (2.3331e+00)	Acc@1  14.84 (  9.96)	Acc@5  58.59 ( 50.11)
03-Mar-22 13:28:03 - Test: [ 0/20]	Time  0.360 ( 0.360)	Loss 2.3250e+00 (2.3250e+00)	Acc@1  10.16 ( 10.16)	Acc@5  50.78 ( 50.78)
03-Mar-22 13:28:03 - Test: [10/20]	Time  0.074 ( 0.100)	Loss 2.3363e+00 (2.3255e+00)	Acc@1   8.98 (  9.98)	Acc@5  46.48 ( 49.08)
03-Mar-22 13:28:04 -  * Acc@1 10.380 Acc@5 49.080
03-Mar-22 13:28:04 - Best acc at epoch 0: 10.380000114440918
03-Mar-22 13:28:05 - Epoch: [1][  0/352]	Time  0.335 ( 0.335)	Data  0.224 ( 0.224)	Loss 2.3433e+00 (2.3433e+00)	Acc@1  10.94 ( 10.94)	Acc@5  42.97 ( 42.97)
03-Mar-22 13:28:06 - Epoch: [1][ 10/352]	Time  0.100 ( 0.122)	Data  0.001 ( 0.022)	Loss 2.3487e+00 (2.3295e+00)	Acc@1   9.38 ( 10.16)	Acc@5  41.41 ( 48.08)
03-Mar-22 13:28:07 - Epoch: [1][ 20/352]	Time  0.101 ( 0.118)	Data  0.001 ( 0.012)	Loss 2.3151e+00 (2.3216e+00)	Acc@1   9.38 (  9.82)	Acc@5  51.56 ( 50.11)
03-Mar-22 13:28:08 - Epoch: [1][ 30/352]	Time  0.105 ( 0.113)	Data  0.002 ( 0.009)	Loss 2.3099e+00 (2.3217e+00)	Acc@1  11.72 ( 10.26)	Acc@5  53.12 ( 49.85)
03-Mar-22 13:28:09 - Epoch: [1][ 40/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.007)	Loss 2.3251e+00 (2.3225e+00)	Acc@1   5.47 ( 10.00)	Acc@5  48.44 ( 49.47)
03-Mar-22 13:28:10 - Epoch: [1][ 50/352]	Time  0.127 ( 0.112)	Data  0.002 ( 0.006)	Loss 2.2889e+00 (2.3223e+00)	Acc@1  11.72 (  9.74)	Acc@5  53.91 ( 49.69)
03-Mar-22 13:28:11 - Epoch: [1][ 60/352]	Time  0.101 ( 0.111)	Data  0.002 ( 0.005)	Loss 2.3354e+00 (2.3233e+00)	Acc@1  10.94 (  9.73)	Acc@5  46.88 ( 49.68)
03-Mar-22 13:28:12 - Epoch: [1][ 70/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.005)	Loss 2.3159e+00 (2.3231e+00)	Acc@1  10.16 (  9.69)	Acc@5  49.22 ( 49.70)
03-Mar-22 13:28:13 - Epoch: [1][ 80/352]	Time  0.104 ( 0.111)	Data  0.002 ( 0.004)	Loss 2.3309e+00 (2.3231e+00)	Acc@1  10.16 (  9.73)	Acc@5  46.88 ( 49.62)
03-Mar-22 13:28:14 - Epoch: [1][ 90/352]	Time  0.104 ( 0.110)	Data  0.002 ( 0.004)	Loss 2.3225e+00 (2.3224e+00)	Acc@1  13.28 (  9.86)	Acc@5  50.00 ( 49.73)
03-Mar-22 13:28:16 - Epoch: [1][100/352]	Time  0.134 ( 0.111)	Data  0.002 ( 0.004)	Loss 2.3198e+00 (2.3220e+00)	Acc@1  10.94 (  9.91)	Acc@5  50.78 ( 49.83)
03-Mar-22 13:28:17 - Epoch: [1][110/352]	Time  0.101 ( 0.111)	Data  0.001 ( 0.004)	Loss 2.3364e+00 (2.3215e+00)	Acc@1   8.59 (  9.91)	Acc@5  46.09 ( 49.96)
03-Mar-22 13:28:18 - Epoch: [1][120/352]	Time  0.124 ( 0.112)	Data  0.002 ( 0.004)	Loss 2.3011e+00 (2.3215e+00)	Acc@1  13.28 (  9.91)	Acc@5  54.69 ( 49.91)
03-Mar-22 13:28:19 - Epoch: [1][130/352]	Time  0.101 ( 0.111)	Data  0.001 ( 0.003)	Loss 2.3121e+00 (2.3213e+00)	Acc@1   9.38 (  9.86)	Acc@5  51.56 ( 49.93)
03-Mar-22 13:28:20 - Epoch: [1][140/352]	Time  0.101 ( 0.111)	Data  0.001 ( 0.003)	Loss 2.3233e+00 (2.3217e+00)	Acc@1  14.06 (  9.88)	Acc@5  44.53 ( 49.73)
03-Mar-22 13:28:21 - Epoch: [1][150/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.2967e+00 (2.3211e+00)	Acc@1  14.84 (  9.85)	Acc@5  49.22 ( 49.82)
03-Mar-22 13:28:22 - Epoch: [1][160/352]	Time  0.124 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.2841e+00 (2.3204e+00)	Acc@1  15.62 (  9.90)	Acc@5  57.81 ( 49.98)
03-Mar-22 13:28:23 - Epoch: [1][170/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3086e+00 (2.3206e+00)	Acc@1  12.50 (  9.90)	Acc@5  47.66 ( 49.86)
03-Mar-22 13:28:24 - Epoch: [1][180/352]	Time  0.104 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3270e+00 (2.3208e+00)	Acc@1   5.47 (  9.91)	Acc@5  46.09 ( 49.83)
03-Mar-22 13:28:25 - Epoch: [1][190/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3303e+00 (2.3206e+00)	Acc@1   6.25 (  9.77)	Acc@5  47.66 ( 49.85)
03-Mar-22 13:28:26 - Epoch: [1][200/352]	Time  0.120 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3357e+00 (2.3205e+00)	Acc@1  10.16 (  9.78)	Acc@5  46.09 ( 49.86)
03-Mar-22 13:28:27 - Epoch: [1][210/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3356e+00 (2.3202e+00)	Acc@1   9.38 (  9.78)	Acc@5  42.19 ( 49.82)
03-Mar-22 13:28:29 - Epoch: [1][220/352]	Time  0.124 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3154e+00 (2.3198e+00)	Acc@1  10.94 (  9.83)	Acc@5  49.22 ( 49.89)
03-Mar-22 13:28:30 - Epoch: [1][230/352]	Time  0.124 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3050e+00 (2.3197e+00)	Acc@1  10.16 (  9.82)	Acc@5  51.56 ( 49.87)
03-Mar-22 13:28:31 - Epoch: [1][240/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3281e+00 (2.3196e+00)	Acc@1  10.94 (  9.81)	Acc@5  41.41 ( 49.85)
03-Mar-22 13:28:32 - Epoch: [1][250/352]	Time  0.106 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.3030e+00 (2.3192e+00)	Acc@1  14.06 (  9.84)	Acc@5  51.56 ( 49.88)
03-Mar-22 13:28:33 - Epoch: [1][260/352]	Time  0.105 ( 0.109)	Data  0.002 ( 0.002)	Loss 2.2995e+00 (2.3193e+00)	Acc@1  10.16 (  9.77)	Acc@5  56.25 ( 49.85)
03-Mar-22 13:28:34 - Epoch: [1][270/352]	Time  0.100 ( 0.109)	Data  0.001 ( 0.002)	Loss 2.3398e+00 (2.3191e+00)	Acc@1  10.16 (  9.79)	Acc@5  40.62 ( 49.85)
03-Mar-22 13:28:35 - Epoch: [1][280/352]	Time  0.127 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3055e+00 (2.3189e+00)	Acc@1  12.50 (  9.83)	Acc@5  50.78 ( 49.89)
03-Mar-22 13:28:36 - Epoch: [1][290/352]	Time  0.128 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3016e+00 (2.3185e+00)	Acc@1  11.72 (  9.86)	Acc@5  53.12 ( 49.94)
03-Mar-22 13:28:37 - Epoch: [1][300/352]	Time  0.105 ( 0.110)	Data  0.001 ( 0.002)	Loss 2.2939e+00 (2.3182e+00)	Acc@1  10.94 (  9.88)	Acc@5  58.59 ( 50.01)
03-Mar-22 13:28:38 - Epoch: [1][310/352]	Time  0.124 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3059e+00 (2.3179e+00)	Acc@1   8.59 (  9.88)	Acc@5  53.12 ( 50.06)
03-Mar-22 13:28:40 - Epoch: [1][320/352]	Time  0.123 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3207e+00 (2.3177e+00)	Acc@1  12.50 (  9.90)	Acc@5  46.09 ( 50.07)
03-Mar-22 13:28:41 - Epoch: [1][330/352]	Time  0.103 ( 0.110)	Data  0.001 ( 0.002)	Loss 2.3355e+00 (2.3175e+00)	Acc@1  10.16 (  9.92)	Acc@5  42.97 ( 50.09)
03-Mar-22 13:28:42 - Epoch: [1][340/352]	Time  0.106 ( 0.110)	Data  0.001 ( 0.002)	Loss 2.3094e+00 (2.3173e+00)	Acc@1   7.81 (  9.95)	Acc@5  50.00 ( 50.12)
03-Mar-22 13:28:43 - Epoch: [1][350/352]	Time  0.124 ( 0.110)	Data  0.001 ( 0.002)	Loss 2.3120e+00 (2.3171e+00)	Acc@1   7.03 (  9.96)	Acc@5  48.44 ( 50.12)
03-Mar-22 13:28:44 - Test: [ 0/20]	Time  0.350 ( 0.350)	Loss 2.3138e+00 (2.3138e+00)	Acc@1  10.16 ( 10.16)	Acc@5  50.78 ( 50.78)
03-Mar-22 13:28:44 - Test: [10/20]	Time  0.076 ( 0.103)	Loss 2.3216e+00 (2.3140e+00)	Acc@1   8.98 (  9.98)	Acc@5  46.48 ( 49.08)
03-Mar-22 13:28:45 -  * Acc@1 10.380 Acc@5 49.080
03-Mar-22 13:28:45 - Best acc at epoch 1: 10.380000114440918
03-Mar-22 13:28:46 - Epoch: [2][  0/352]	Time  0.369 ( 0.369)	Data  0.244 ( 0.244)	Loss 2.3204e+00 (2.3204e+00)	Acc@1   7.81 (  7.81)	Acc@5  46.09 ( 46.09)
03-Mar-22 13:28:47 - Epoch: [2][ 10/352]	Time  0.124 ( 0.136)	Data  0.001 ( 0.024)	Loss 2.3131e+00 (2.3099e+00)	Acc@1   8.59 ( 10.65)	Acc@5  50.78 ( 50.00)
03-Mar-22 13:28:48 - Epoch: [2][ 20/352]	Time  0.100 ( 0.119)	Data  0.001 ( 0.013)	Loss 2.3098e+00 (2.3083e+00)	Acc@1   8.59 ( 10.60)	Acc@5  50.00 ( 50.93)
03-Mar-22 13:28:49 - Epoch: [2][ 30/352]	Time  0.109 ( 0.114)	Data  0.002 ( 0.009)	Loss 2.3017e+00 (2.3088e+00)	Acc@1  10.94 ( 10.38)	Acc@5  51.56 ( 50.68)
03-Mar-22 13:28:50 - Epoch: [2][ 40/352]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.3115e+00 (2.3092e+00)	Acc@1  12.50 ( 10.08)	Acc@5  51.56 ( 50.57)
03-Mar-22 13:28:51 - Epoch: [2][ 50/352]	Time  0.151 ( 0.111)	Data  0.002 ( 0.006)	Loss 2.3316e+00 (2.3107e+00)	Acc@1   7.03 (  9.79)	Acc@5  46.09 ( 50.15)
03-Mar-22 13:28:52 - Epoch: [2][ 60/352]	Time  0.100 ( 0.112)	Data  0.001 ( 0.006)	Loss 2.3223e+00 (2.3119e+00)	Acc@1   8.59 (  9.73)	Acc@5  44.53 ( 49.83)
03-Mar-22 13:28:53 - Epoch: [2][ 70/352]	Time  0.100 ( 0.111)	Data  0.002 ( 0.005)	Loss 2.3126e+00 (2.3114e+00)	Acc@1  10.16 (  9.79)	Acc@5  50.00 ( 49.94)
03-Mar-22 13:28:54 - Epoch: [2][ 80/352]	Time  0.100 ( 0.110)	Data  0.001 ( 0.005)	Loss 2.3114e+00 (2.3120e+00)	Acc@1   5.47 (  9.76)	Acc@5  50.00 ( 49.81)
03-Mar-22 13:28:55 - Epoch: [2][ 90/352]	Time  0.100 ( 0.109)	Data  0.001 ( 0.004)	Loss 2.3012e+00 (2.3118e+00)	Acc@1   8.59 (  9.78)	Acc@5  53.91 ( 49.93)
03-Mar-22 13:28:56 - Epoch: [2][100/352]	Time  0.101 ( 0.108)	Data  0.001 ( 0.004)	Loss 2.3012e+00 (2.3111e+00)	Acc@1  12.50 (  9.92)	Acc@5  56.25 ( 50.25)
03-Mar-22 13:28:57 - Epoch: [2][110/352]	Time  0.103 ( 0.109)	Data  0.002 ( 0.004)	Loss 2.3319e+00 (2.3107e+00)	Acc@1   7.81 (  9.99)	Acc@5  42.19 ( 50.39)
03-Mar-22 13:28:58 - Epoch: [2][120/352]	Time  0.100 ( 0.108)	Data  0.001 ( 0.004)	Loss 2.3158e+00 (2.3111e+00)	Acc@1   9.38 (  9.93)	Acc@5  47.66 ( 50.21)
03-Mar-22 13:28:59 - Epoch: [2][130/352]	Time  0.100 ( 0.108)	Data  0.001 ( 0.003)	Loss 2.2890e+00 (2.3109e+00)	Acc@1   9.38 (  9.94)	Acc@5  57.81 ( 50.26)
03-Mar-22 13:29:00 - Epoch: [2][140/352]	Time  0.124 ( 0.108)	Data  0.002 ( 0.003)	Loss 2.3121e+00 (2.3109e+00)	Acc@1  10.94 (  9.98)	Acc@5  43.75 ( 50.14)
03-Mar-22 13:29:02 - Epoch: [2][150/352]	Time  0.104 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.3107e+00 (2.3110e+00)	Acc@1   7.03 (  9.91)	Acc@5  52.34 ( 50.11)
03-Mar-22 13:29:03 - Epoch: [2][160/352]	Time  0.105 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.2912e+00 (2.3107e+00)	Acc@1  13.28 (  9.94)	Acc@5  57.03 ( 50.21)
03-Mar-22 13:29:04 - Epoch: [2][170/352]	Time  0.124 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.2974e+00 (2.3108e+00)	Acc@1   8.59 (  9.84)	Acc@5  56.25 ( 50.15)
03-Mar-22 13:29:05 - Epoch: [2][180/352]	Time  0.124 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3173e+00 (2.3105e+00)	Acc@1   7.03 (  9.87)	Acc@5  50.00 ( 50.22)
03-Mar-22 13:29:06 - Epoch: [2][190/352]	Time  0.123 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3103e+00 (2.3106e+00)	Acc@1   3.91 (  9.85)	Acc@5  53.12 ( 50.17)
03-Mar-22 13:29:07 - Epoch: [2][200/352]	Time  0.104 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3182e+00 (2.3108e+00)	Acc@1   4.69 (  9.79)	Acc@5  47.66 ( 50.02)
03-Mar-22 13:29:08 - Epoch: [2][210/352]	Time  0.105 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3160e+00 (2.3106e+00)	Acc@1   9.38 (  9.82)	Acc@5  49.22 ( 50.04)
03-Mar-22 13:29:10 - Epoch: [2][220/352]	Time  0.105 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3158e+00 (2.3105e+00)	Acc@1   9.38 (  9.79)	Acc@5  49.22 ( 50.07)
03-Mar-22 13:29:11 - Epoch: [2][230/352]	Time  0.128 ( 0.111)	Data  0.002 ( 0.003)	Loss 2.3336e+00 (2.3104e+00)	Acc@1   7.03 (  9.78)	Acc@5  38.28 ( 50.03)
03-Mar-22 13:29:12 - Epoch: [2][240/352]	Time  0.104 ( 0.111)	Data  0.001 ( 0.003)	Loss 2.3255e+00 (2.3104e+00)	Acc@1   7.81 (  9.81)	Acc@5  39.84 ( 49.99)
03-Mar-22 13:29:13 - Epoch: [2][250/352]	Time  0.123 ( 0.111)	Data  0.002 ( 0.003)	Loss 2.3069e+00 (2.3105e+00)	Acc@1   9.38 (  9.77)	Acc@5  52.34 ( 49.96)
03-Mar-22 13:29:14 - Epoch: [2][260/352]	Time  0.105 ( 0.112)	Data  0.002 ( 0.003)	Loss 2.3171e+00 (2.3106e+00)	Acc@1   7.03 (  9.76)	Acc@5  46.09 ( 49.85)
03-Mar-22 13:29:16 - Epoch: [2][270/352]	Time  0.105 ( 0.112)	Data  0.002 ( 0.003)	Loss 2.2888e+00 (2.3103e+00)	Acc@1  16.41 (  9.83)	Acc@5  54.69 ( 49.95)
03-Mar-22 13:29:17 - Epoch: [2][280/352]	Time  0.124 ( 0.112)	Data  0.002 ( 0.003)	Loss 2.2977e+00 (2.3101e+00)	Acc@1  11.72 (  9.85)	Acc@5  53.91 ( 50.00)
03-Mar-22 13:29:18 - Epoch: [2][290/352]	Time  0.124 ( 0.112)	Data  0.002 ( 0.003)	Loss 2.3020e+00 (2.3100e+00)	Acc@1  12.50 (  9.84)	Acc@5  49.22 ( 50.01)
03-Mar-22 13:29:19 - Epoch: [2][300/352]	Time  0.105 ( 0.112)	Data  0.002 ( 0.003)	Loss 2.2949e+00 (2.3098e+00)	Acc@1   7.81 (  9.83)	Acc@5  55.47 ( 50.08)
03-Mar-22 13:29:20 - Epoch: [2][310/352]	Time  0.105 ( 0.112)	Data  0.002 ( 0.002)	Loss 2.3012e+00 (2.3098e+00)	Acc@1   9.38 (  9.84)	Acc@5  57.03 ( 50.11)
03-Mar-22 13:29:21 - Epoch: [2][320/352]	Time  0.106 ( 0.112)	Data  0.002 ( 0.002)	Loss 2.3228e+00 (2.3097e+00)	Acc@1  10.16 (  9.87)	Acc@5  46.88 ( 50.13)
03-Mar-22 13:29:22 - Epoch: [2][330/352]	Time  0.104 ( 0.112)	Data  0.002 ( 0.002)	Loss 2.3176e+00 (2.3097e+00)	Acc@1  12.50 (  9.89)	Acc@5  46.09 ( 50.09)
03-Mar-22 13:29:24 - Epoch: [2][340/352]	Time  0.123 ( 0.112)	Data  0.002 ( 0.002)	Loss 2.3132e+00 (2.3096e+00)	Acc@1   8.59 (  9.92)	Acc@5  46.09 ( 50.04)
03-Mar-22 13:29:25 - Epoch: [2][350/352]	Time  0.100 ( 0.112)	Data  0.001 ( 0.002)	Loss 2.3029e+00 (2.3094e+00)	Acc@1   7.03 (  9.96)	Acc@5  51.56 ( 50.11)
03-Mar-22 13:29:25 - Test: [ 0/20]	Time  0.331 ( 0.331)	Loss 2.3081e+00 (2.3081e+00)	Acc@1  10.16 ( 10.16)	Acc@5  50.78 ( 50.78)
03-Mar-22 13:29:26 - Test: [10/20]	Time  0.073 ( 0.097)	Loss 2.3136e+00 (2.3082e+00)	Acc@1   8.98 (  9.98)	Acc@5  46.48 ( 49.08)
03-Mar-22 13:29:27 -  * Acc@1 10.380 Acc@5 49.080
03-Mar-22 13:29:27 - Best acc at epoch 2: 10.380000114440918
03-Mar-22 13:29:27 - Epoch: [3][  0/352]	Time  0.367 ( 0.367)	Data  0.245 ( 0.245)	Loss 2.3198e+00 (2.3198e+00)	Acc@1   7.03 (  7.03)	Acc@5  42.19 ( 42.19)
03-Mar-22 13:29:28 - Epoch: [3][ 10/352]	Time  0.101 ( 0.136)	Data  0.001 ( 0.024)	Loss 2.3098e+00 (2.3072e+00)	Acc@1  14.06 ( 10.01)	Acc@5  46.09 ( 50.43)
03-Mar-22 13:29:29 - Epoch: [3][ 20/352]	Time  0.101 ( 0.121)	Data  0.001 ( 0.013)	Loss 2.3093e+00 (2.3084e+00)	Acc@1   7.81 (  9.04)	Acc@5  48.44 ( 49.26)
03-Mar-22 13:29:30 - Epoch: [3][ 30/352]	Time  0.102 ( 0.121)	Data  0.002 ( 0.010)	Loss 2.2903e+00 (2.3074e+00)	Acc@1   9.38 (  9.10)	Acc@5  59.38 ( 49.72)
03-Mar-22 13:29:32 - Epoch: [3][ 40/352]	Time  0.119 ( 0.119)	Data  0.002 ( 0.008)	Loss 2.3086e+00 (2.3060e+00)	Acc@1   9.38 (  9.53)	Acc@5  50.00 ( 50.46)
03-Mar-22 13:29:33 - Epoch: [3][ 50/352]	Time  0.120 ( 0.116)	Data  0.002 ( 0.006)	Loss 2.3176e+00 (2.3067e+00)	Acc@1  10.16 (  9.36)	Acc@5  45.31 ( 50.20)
03-Mar-22 13:29:34 - Epoch: [3][ 60/352]	Time  0.101 ( 0.114)	Data  0.001 ( 0.006)	Loss 2.3067e+00 (2.3057e+00)	Acc@1  12.50 (  9.80)	Acc@5  50.78 ( 50.60)
03-Mar-22 13:29:35 - Epoch: [3][ 70/352]	Time  0.101 ( 0.112)	Data  0.001 ( 0.005)	Loss 2.3047e+00 (2.3064e+00)	Acc@1  11.72 (  9.66)	Acc@5  52.34 ( 50.25)
03-Mar-22 13:29:36 - Epoch: [3][ 80/352]	Time  0.101 ( 0.111)	Data  0.001 ( 0.005)	Loss 2.3159e+00 (2.3068e+00)	Acc@1  10.94 (  9.64)	Acc@5  45.31 ( 49.86)
03-Mar-22 13:29:37 - Epoch: [3][ 90/352]	Time  0.101 ( 0.110)	Data  0.001 ( 0.004)	Loss 2.3148e+00 (2.3068e+00)	Acc@1   8.59 (  9.65)	Acc@5  44.53 ( 49.97)
03-Mar-22 13:29:38 - Epoch: [3][100/352]	Time  0.103 ( 0.110)	Data  0.002 ( 0.004)	Loss 2.3092e+00 (2.3071e+00)	Acc@1   9.38 (  9.50)	Acc@5  48.44 ( 49.65)
03-Mar-22 13:29:39 - Epoch: [3][110/352]	Time  0.119 ( 0.110)	Data  0.002 ( 0.004)	Loss 2.3107e+00 (2.3073e+00)	Acc@1  12.50 (  9.54)	Acc@5  46.09 ( 49.65)
03-Mar-22 13:29:40 - Epoch: [3][120/352]	Time  0.122 ( 0.110)	Data  0.002 ( 0.004)	Loss 2.2961e+00 (2.3076e+00)	Acc@1   9.38 (  9.56)	Acc@5  57.81 ( 49.54)
03-Mar-22 13:29:41 - Epoch: [3][130/352]	Time  0.102 ( 0.109)	Data  0.001 ( 0.003)	Loss 2.3084e+00 (2.3074e+00)	Acc@1   7.81 (  9.55)	Acc@5  48.44 ( 49.65)
03-Mar-22 13:29:42 - Epoch: [3][140/352]	Time  0.101 ( 0.109)	Data  0.001 ( 0.003)	Loss 2.3080e+00 (2.3075e+00)	Acc@1  11.72 (  9.60)	Acc@5  50.00 ( 49.61)
03-Mar-22 13:29:43 - Epoch: [3][150/352]	Time  0.101 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.2967e+00 (2.3074e+00)	Acc@1  13.28 (  9.58)	Acc@5  55.47 ( 49.65)
03-Mar-22 13:29:44 - Epoch: [3][160/352]	Time  0.112 ( 0.108)	Data  0.002 ( 0.003)	Loss 2.3074e+00 (2.3073e+00)	Acc@1   9.38 (  9.56)	Acc@5  50.00 ( 49.58)
03-Mar-22 13:29:45 - Epoch: [3][170/352]	Time  0.122 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.3125e+00 (2.3070e+00)	Acc@1   7.81 (  9.57)	Acc@5  48.44 ( 49.74)
03-Mar-22 13:29:47 - Epoch: [3][180/352]	Time  0.104 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3140e+00 (2.3070e+00)	Acc@1   4.69 (  9.51)	Acc@5  46.88 ( 49.73)
03-Mar-22 13:29:48 - Epoch: [3][190/352]	Time  0.104 ( 0.110)	Data  0.002 ( 0.003)	Loss 2.3097e+00 (2.3071e+00)	Acc@1   7.81 (  9.58)	Acc@5  48.44 ( 49.63)
03-Mar-22 13:29:49 - Epoch: [3][200/352]	Time  0.101 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.3034e+00 (2.3069e+00)	Acc@1  10.94 (  9.64)	Acc@5  48.44 ( 49.63)
03-Mar-22 13:29:50 - Epoch: [3][210/352]	Time  0.101 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.2973e+00 (2.3070e+00)	Acc@1   8.59 (  9.67)	Acc@5  59.38 ( 49.59)
03-Mar-22 13:29:51 - Epoch: [3][220/352]	Time  0.103 ( 0.110)	Data  0.001 ( 0.003)	Loss 2.2955e+00 (2.3069e+00)	Acc@1  17.19 (  9.72)	Acc@5  57.03 ( 49.71)
03-Mar-22 13:29:52 - Epoch: [3][230/352]	Time  0.101 ( 0.109)	Data  0.001 ( 0.003)	Loss 2.3055e+00 (2.3068e+00)	Acc@1   6.25 (  9.71)	Acc@5  53.12 ( 49.73)
03-Mar-22 13:29:53 - Epoch: [3][240/352]	Time  0.123 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.3139e+00 (2.3065e+00)	Acc@1   7.81 (  9.76)	Acc@5  42.97 ( 49.85)
03-Mar-22 13:29:54 - Epoch: [3][250/352]	Time  0.101 ( 0.109)	Data  0.001 ( 0.003)	Loss 2.2938e+00 (2.3064e+00)	Acc@1  13.28 (  9.87)	Acc@5  54.69 ( 49.89)
03-Mar-22 13:29:55 - Epoch: [3][260/352]	Time  0.102 ( 0.109)	Data  0.001 ( 0.003)	Loss 2.2977e+00 (2.3063e+00)	Acc@1   9.38 (  9.92)	Acc@5  53.12 ( 49.91)
03-Mar-22 13:29:56 - Epoch: [3][270/352]	Time  0.124 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.2982e+00 (2.3062e+00)	Acc@1  12.50 (  9.92)	Acc@5  51.56 ( 49.90)
03-Mar-22 13:29:57 - Epoch: [3][280/352]	Time  0.128 ( 0.109)	Data  0.002 ( 0.003)	Loss 2.3047e+00 (2.3061e+00)	Acc@1   7.03 (  9.92)	Acc@5  51.56 ( 49.98)
03-Mar-22 13:29:59 - Epoch: [3][290/352]	Time  0.107 ( 0.110)	Data  0.001 ( 0.002)	Loss 2.3020e+00 (2.3059e+00)	Acc@1  12.50 (  9.96)	Acc@5  50.78 ( 49.99)
03-Mar-22 13:30:00 - Epoch: [3][300/352]	Time  0.123 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3123e+00 (2.3059e+00)	Acc@1   7.81 (  9.94)	Acc@5  49.22 ( 50.01)
03-Mar-22 13:30:01 - Epoch: [3][310/352]	Time  0.101 ( 0.110)	Data  0.001 ( 0.002)	Loss 2.3133e+00 (2.3060e+00)	Acc@1   9.38 (  9.94)	Acc@5  47.66 ( 49.95)
03-Mar-22 13:30:02 - Epoch: [3][320/352]	Time  0.129 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3017e+00 (2.3058e+00)	Acc@1   6.25 (  9.93)	Acc@5  52.34 ( 50.02)
03-Mar-22 13:30:03 - Epoch: [3][330/352]	Time  0.129 ( 0.110)	Data  0.002 ( 0.002)	Loss 2.3023e+00 (2.3058e+00)	Acc@1  10.94 (  9.95)	Acc@5  52.34 ( 50.07)
03-Mar-22 13:30:04 - Epoch: [3][340/352]	Time  0.122 ( 0.111)	Data  0.002 ( 0.002)	Loss 2.3012e+00 (2.3058e+00)	Acc@1  13.28 (  9.97)	Acc@5  46.88 ( 50.06)
03-Mar-22 13:30:05 - Epoch: [3][350/352]	Time  0.104 ( 0.111)	Data  0.001 ( 0.002)	Loss 2.2938e+00 (2.3057e+00)	Acc@1  10.16 (  9.99)	Acc@5  58.59 ( 50.09)
03-Mar-22 13:30:06 - Test: [ 0/20]	Time  0.338 ( 0.338)	Loss 2.3052e+00 (2.3052e+00)	Acc@1  10.16 ( 10.16)	Acc@5  50.78 ( 50.78)
03-Mar-22 13:30:07 - Test: [10/20]	Time  0.076 ( 0.100)	Loss 2.3092e+00 (2.3053e+00)	Acc@1   8.98 ( 10.09)	Acc@5  46.48 ( 49.08)
03-Mar-22 13:30:08 -  * Acc@1 10.420 Acc@5 49.080
03-Mar-22 13:30:08 - Best acc at epoch 3: 10.420000076293945
03-Mar-22 13:30:08 - Epoch: [4][  0/352]	Time  0.336 ( 0.336)	Data  0.233 ( 0.233)	Loss 2.3134e+00 (2.3134e+00)	Acc@1   9.38 (  9.38)	Acc@5  42.19 ( 42.19)
03-Mar-22 13:30:09 - Epoch: [4][ 10/352]	Time  0.105 ( 0.134)	Data  0.002 ( 0.023)	Loss 2.3054e+00 (2.3067e+00)	Acc@1   7.81 (  8.59)	Acc@5  50.00 ( 47.66)
03-Mar-22 13:30:10 - Epoch: [4][ 20/352]	Time  0.126 ( 0.125)	Data  0.002 ( 0.013)	Loss 2.2986e+00 (2.3049e+00)	Acc@1   8.59 (  8.93)	Acc@5  57.81 ( 49.93)
03-Mar-22 13:30:11 - Epoch: [4][ 30/352]	Time  0.121 ( 0.123)	Data  0.002 ( 0.009)	Loss 2.3090e+00 (2.3043e+00)	Acc@1   7.03 (  9.35)	Acc@5  46.88 ( 50.15)
03-Mar-22 13:30:13 - Epoch: [4][ 40/352]	Time  0.124 ( 0.121)	Data  0.001 ( 0.007)	Loss 2.3090e+00 (2.3040e+00)	Acc@1   5.47 (  9.13)	Acc@5  48.44 ( 50.51)
03-Mar-22 13:30:14 - Epoch: [4][ 50/352]	Time  0.105 ( 0.120)	Data  0.002 ( 0.006)	Loss 2.3060e+00 (2.3051e+00)	Acc@1  10.16 (  9.38)	Acc@5  49.22 ( 49.60)
03-Mar-22 13:30:15 - Epoch: [4][ 60/352]	Time  0.128 ( 0.119)	Data  0.002 ( 0.006)	Loss 2.3138e+00 (2.3055e+00)	Acc@1  10.94 (  9.63)	Acc@5  42.97 ( 49.31)
03-Mar-22 13:30:16 - Epoch: [4][ 70/352]	Time  0.123 ( 0.119)	Data  0.002 ( 0.005)	Loss 2.3047e+00 (2.3056e+00)	Acc@1   8.59 (  9.47)	Acc@5  51.56 ( 49.27)
03-Mar-22 13:30:17 - Epoch: [4][ 80/352]	Time  0.127 ( 0.119)	Data  0.002 ( 0.005)	Loss 2.3079e+00 (2.3055e+00)	Acc@1  12.50 (  9.63)	Acc@5  48.44 ( 49.30)
03-Mar-22 13:30:18 - Epoch: [4][ 90/352]	Time  0.100 ( 0.117)	Data  0.001 ( 0.004)	Loss 2.3033e+00 (2.3053e+00)	Acc@1  10.94 (  9.61)	Acc@5  50.00 ( 49.45)
03-Mar-22 13:30:19 - Epoch: [4][100/352]	Time  0.125 ( 0.117)	Data  0.002 ( 0.004)	Loss 2.3008e+00 (2.3049e+00)	Acc@1   7.81 (  9.82)	Acc@5  54.69 ( 49.63)
03-Mar-22 13:30:21 - Epoch: [4][110/352]	Time  0.105 ( 0.117)	Data  0.002 ( 0.004)	Loss 2.3050e+00 (2.3049e+00)	Acc@1   9.38 (  9.87)	Acc@5  46.88 ( 49.66)
03-Mar-22 13:30:22 - Epoch: [4][120/352]	Time  0.100 ( 0.117)	Data  0.001 ( 0.004)	Loss 2.2973e+00 (2.3046e+00)	Acc@1  12.50 (  9.95)	Acc@5  54.69 ( 49.85)
03-Mar-22 13:30:23 - Epoch: [4][130/352]	Time  0.100 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3146e+00 (2.3047e+00)	Acc@1   7.81 ( 10.03)	Acc@5  42.19 ( 49.82)
03-Mar-22 13:30:24 - Epoch: [4][140/352]	Time  0.104 ( 0.114)	Data  0.001 ( 0.003)	Loss 2.3058e+00 (2.3046e+00)	Acc@1   7.81 (  9.88)	Acc@5  53.12 ( 49.96)
03-Mar-22 13:30:25 - Epoch: [4][150/352]	Time  0.116 ( 0.114)	Data  0.001 ( 0.003)	Loss 2.3010e+00 (2.3046e+00)	Acc@1  10.16 (  9.90)	Acc@5  47.66 ( 49.82)
03-Mar-22 13:30:26 - Epoch: [4][160/352]	Time  0.100 ( 0.114)	Data  0.001 ( 0.003)	Loss 2.3048e+00 (2.3043e+00)	Acc@1  12.50 ( 10.06)	Acc@5  49.22 ( 50.00)
03-Mar-22 13:30:27 - Epoch: [4][170/352]	Time  0.112 ( 0.113)	Data  0.001 ( 0.003)	Loss 2.3046e+00 (2.3045e+00)	Acc@1   7.81 (  9.91)	Acc@5  52.34 ( 49.85)
03-Mar-22 13:30:28 - Epoch: [4][180/352]	Time  0.116 ( 0.113)	Data  0.002 ( 0.003)	Loss 2.2976e+00 (2.3043e+00)	Acc@1   8.59 (  9.96)	Acc@5  49.22 ( 49.96)
03-Mar-22 13:30:29 - Epoch: [4][190/352]	Time  0.122 ( 0.112)	Data  0.002 ( 0.003)	Loss 2.2990e+00 (2.3043e+00)	Acc@1   9.38 (  9.94)	Acc@5  54.69 ( 49.95)
03-Mar-22 13:30:30 - Epoch: [4][200/352]	Time  0.105 ( 0.112)	Data  0.001 ( 0.003)	Loss 2.2987e+00 (2.3041e+00)	Acc@1  10.16 ( 10.02)	Acc@5  53.12 ( 50.08)
03-Mar-22 13:30:31 - Epoch: [4][210/352]	Time  0.105 ( 0.112)	Data  0.001 ( 0.003)	Loss 2.3026e+00 (2.3041e+00)	Acc@1  10.94 ( 10.02)	Acc@5  53.12 ( 50.13)
03-Mar-22 13:30:33 - Epoch: [4][220/352]	Time  0.128 ( 0.113)	Data  0.002 ( 0.003)	Loss 2.2990e+00 (2.3041e+00)	Acc@1  13.28 ( 10.11)	Acc@5  54.69 ( 50.09)
03-Mar-22 13:30:34 - Epoch: [4][230/352]	Time  0.104 ( 0.113)	Data  0.002 ( 0.003)	Loss 2.3010e+00 (2.3040e+00)	Acc@1  11.72 ( 10.16)	Acc@5  51.56 ( 50.15)
03-Mar-22 13:30:35 - Epoch: [4][240/352]	Time  0.103 ( 0.113)	Data  0.001 ( 0.003)	Loss 2.2984e+00 (2.3039e+00)	Acc@1  13.28 ( 10.19)	Acc@5  53.91 ( 50.20)
03-Mar-22 13:30:36 - Epoch: [4][250/352]	Time  0.104 ( 0.113)	Data  0.002 ( 0.003)	Loss 2.3041e+00 (2.3039e+00)	Acc@1   8.59 ( 10.21)	Acc@5  50.00 ( 50.18)
03-Mar-22 13:30:37 - Epoch: [4][260/352]	Time  0.124 ( 0.113)	Data  0.002 ( 0.003)	Loss 2.3061e+00 (2.3039e+00)	Acc@1  12.50 ( 10.25)	Acc@5  46.88 ( 50.16)
03-Mar-22 13:30:38 - Epoch: [4][270/352]	Time  0.100 ( 0.113)	Data  0.001 ( 0.003)	Loss 2.3029e+00 (2.3039e+00)	Acc@1  10.94 ( 10.25)	Acc@5  48.44 ( 50.13)
03-Mar-22 13:30:39 - Epoch: [4][280/352]	Time  0.100 ( 0.113)	Data  0.001 ( 0.003)	Loss 2.3068e+00 (2.3039e+00)	Acc@1  11.72 ( 10.30)	Acc@5  47.66 ( 50.15)
03-Mar-22 13:47:51 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:47:51 - Use GPU: 0 for training
03-Mar-22 13:47:51 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:47:55 - match all modules defined in bit_config: False
03-Mar-22 13:47:55 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:49:10 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:49:10 - Use GPU: 0 for training
03-Mar-22 13:49:10 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:49:14 - match all modules defined in bit_config: False
03-Mar-22 13:49:14 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:50:32 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:50:32 - Use GPU: 0 for training
03-Mar-22 13:50:32 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:50:36 - match all modules defined in bit_config: True
03-Mar-22 13:50:36 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  )
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    )
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:54:04 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:54:04 - Use GPU: 0 for training
03-Mar-22 13:54:04 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:54:09 - match all modules defined in bit_config: True
03-Mar-22 13:54:36 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:54:36 - Use GPU: 0 for training
03-Mar-22 13:54:36 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:54:41 - match all modules defined in bit_config: True
03-Mar-22 13:54:41 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 13:56:29 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:56:29 - Use GPU: 0 for training
03-Mar-22 13:56:29 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:56:33 - match all modules defined in bit_config: True
03-Mar-22 13:57:56 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 13:57:56 - Use GPU: 0 for training
03-Mar-22 13:57:56 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 13:58:00 - match all modules defined in bit_config: True
03-Mar-22 13:58:00 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:02:30 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:02:30 - Use GPU: 0 for training
03-Mar-22 14:02:30 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:02:34 - match all modules defined in bit_config: True
03-Mar-22 14:02:34 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:06:43 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:06:43 - Use GPU: 0 for training
03-Mar-22 14:06:43 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:06:47 - match all modules defined in bit_config: True
03-Mar-22 14:06:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:09:16 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:09:16 - Use GPU: 0 for training
03-Mar-22 14:09:16 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:09:20 - match all modules defined in bit_config: True
03-Mar-22 14:09:20 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:09:53 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:09:53 - Use GPU: 0 for training
03-Mar-22 14:09:53 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:09:57 - match all modules defined in bit_config: True
03-Mar-22 14:09:57 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:12:56 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:12:56 - Use GPU: 0 for training
03-Mar-22 14:12:56 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:13:00 - match all modules defined in bit_config: True
03-Mar-22 14:13:00 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:15:32 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:15:32 - Use GPU: 0 for training
03-Mar-22 14:15:32 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 14:15:36 - match all modules defined in bit_config: True
03-Mar-22 14:15:36 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:15:37 - Epoch: [0][  0/352]	Time  0.555 ( 0.555)	Data  0.216 ( 0.216)	Loss 2.1393e-01 (2.1393e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 14:15:56 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:15:56 - Use GPU: 0 for training
03-Mar-22 14:15:56 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 14:16:00 - match all modules defined in bit_config: True
03-Mar-22 14:16:00 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:20:13 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:20:13 - Use GPU: 0 for training
03-Mar-22 14:20:13 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 14:20:38 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:20:38 - Use GPU: 0 for training
03-Mar-22 14:20:38 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:20:42 - match all modules defined in bit_config: True
03-Mar-22 14:20:42 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:24:07 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:24:07 - Use GPU: 0 for training
03-Mar-22 14:24:07 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:24:11 - match all modules defined in bit_config: True
03-Mar-22 14:24:11 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:25:32 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:25:32 - Use GPU: 0 for training
03-Mar-22 14:25:32 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:25:36 - match all modules defined in bit_config: True
03-Mar-22 14:25:36 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:44:58 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:44:58 - Use GPU: 0 for training
03-Mar-22 14:44:58 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:45:02 - match all modules defined in bit_config: True
03-Mar-22 14:45:02 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:47:02 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:47:02 - Use GPU: 0 for training
03-Mar-22 14:47:02 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:47:06 - match all modules defined in bit_config: True
03-Mar-22 14:47:06 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:50:41 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:50:41 - Use GPU: 0 for training
03-Mar-22 14:50:41 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 14:50:45 - match all modules defined in bit_config: True
03-Mar-22 14:50:45 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:54:08 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:54:08 - Use GPU: 0 for training
03-Mar-22 14:54:08 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:54:12 - match all modules defined in bit_config: True
03-Mar-22 14:54:12 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:55:17 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:55:17 - Use GPU: 0 for training
03-Mar-22 14:55:17 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 14:55:21 - match all modules defined in bit_config: True
03-Mar-22 14:55:21 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 14:55:45 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 14:55:45 - Use GPU: 0 for training
03-Mar-22 14:55:45 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 14:55:49 - match all modules defined in bit_config: True
03-Mar-22 14:55:49 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:07:49 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:07:49 - Use GPU: 0 for training
03-Mar-22 15:07:49 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 15:07:53 - match all modules defined in bit_config: True
03-Mar-22 15:07:53 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:09:18 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:09:18 - Use GPU: 0 for training
03-Mar-22 15:09:18 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:09:22 - match all modules defined in bit_config: True
03-Mar-22 15:09:22 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:17:12 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:17:12 - Use GPU: 0 for training
03-Mar-22 15:17:12 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 15:17:16 - match all modules defined in bit_config: True
03-Mar-22 15:17:16 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:18:24 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:18:24 - Use GPU: 0 for training
03-Mar-22 15:18:24 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 15:18:28 - match all modules defined in bit_config: True
03-Mar-22 15:18:28 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:18:29 - Epoch: [0][  0/352]	Time  0.378 ( 0.378)	Data  0.217 ( 0.217)	Loss 1.5242e-01 (1.5242e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
03-Mar-22 15:18:30 - Epoch: [0][ 10/352]	Time  0.094 ( 0.124)	Data  0.001 ( 0.021)	Loss 1.3767e-01 (1.5941e-01)	Acc@1  93.75 ( 94.11)	Acc@5 100.00 (100.00)
03-Mar-22 15:20:01 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:20:01 - Use GPU: 0 for training
03-Mar-22 15:20:01 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:20:05 - match all modules defined in bit_config: True
03-Mar-22 15:20:05 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:20:40 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:20:40 - Use GPU: 0 for training
03-Mar-22 15:20:40 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:20:44 - match all modules defined in bit_config: True
03-Mar-22 15:20:44 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:21:30 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:21:30 - Use GPU: 0 for training
03-Mar-22 15:21:30 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:21:34 - match all modules defined in bit_config: True
03-Mar-22 15:21:34 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:22:20 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:22:20 - Use GPU: 0 for training
03-Mar-22 15:22:20 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:22:24 - match all modules defined in bit_config: True
03-Mar-22 15:22:24 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:26:18 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:26:18 - Use GPU: 0 for training
03-Mar-22 15:26:18 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:26:22 - match all modules defined in bit_config: True
03-Mar-22 15:26:22 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:32:42 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:32:42 - Use GPU: 0 for training
03-Mar-22 15:32:42 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 15:32:47 - match all modules defined in bit_config: True
03-Mar-22 15:32:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-channel-wise=False, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:58:45 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:58:45 - Use GPU: 0 for training
03-Mar-22 15:58:45 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 15:58:49 - match all modules defined in bit_config: True
03-Mar-22 15:58:49 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:58:50 - Epoch: [0][  0/352]	Time  0.420 ( 0.420)	Data  0.244 ( 0.244)	Loss 2.3715e-01 (2.3715e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.22 ( 99.22)
03-Mar-22 15:58:51 - Epoch: [0][ 10/352]	Time  0.098 ( 0.136)	Data  0.002 ( 0.024)	Loss 1.2559e-01 (1.4926e-01)	Acc@1  96.09 ( 94.82)	Acc@5 100.00 ( 99.93)
03-Mar-22 15:58:52 - Epoch: [0][ 20/352]	Time  0.117 ( 0.119)	Data  0.002 ( 0.013)	Loss 2.7271e-01 (1.5694e-01)	Acc@1  92.97 ( 94.79)	Acc@5 100.00 ( 99.96)
03-Mar-22 15:58:53 - Epoch: [0][ 30/352]	Time  0.093 ( 0.114)	Data  0.001 ( 0.010)	Loss 1.3964e-01 (1.4792e-01)	Acc@1  94.53 ( 94.88)	Acc@5 100.00 ( 99.97)
03-Mar-22 15:58:54 - Epoch: [0][ 40/352]	Time  0.117 ( 0.112)	Data  0.002 ( 0.008)	Loss 1.1712e-01 (1.4101e-01)	Acc@1  96.88 ( 95.08)	Acc@5 100.00 ( 99.96)
03-Mar-22 15:58:55 - Epoch: [0][ 50/352]	Time  0.118 ( 0.111)	Data  0.002 ( 0.007)	Loss 1.4252e-01 (1.3894e-01)	Acc@1  94.53 ( 95.13)	Acc@5 100.00 ( 99.95)
03-Mar-22 15:58:56 - Epoch: [0][ 60/352]	Time  0.117 ( 0.111)	Data  0.002 ( 0.006)	Loss 1.1157e-01 (1.3731e-01)	Acc@1  97.66 ( 95.22)	Acc@5 100.00 ( 99.96)
03-Mar-22 15:58:57 - Epoch: [0][ 70/352]	Time  0.094 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.5617e-02 (1.3291e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.97)
03-Mar-22 15:59:35 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 15:59:35 - Use GPU: 0 for training
03-Mar-22 15:59:35 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 15:59:39 - match all modules defined in bit_config: True
03-Mar-22 15:59:39 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 15:59:39 - Epoch: [0][  0/352]	Time  0.426 ( 0.426)	Data  0.236 ( 0.236)	Loss 3.0758e-01 (3.0758e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
03-Mar-22 15:59:41 - Epoch: [0][ 10/352]	Time  0.122 ( 0.137)	Data  0.002 ( 0.023)	Loss 1.3989e-01 (1.7335e-01)	Acc@1  94.53 ( 94.18)	Acc@5 100.00 ( 99.86)
03-Mar-22 15:59:42 - Epoch: [0][ 20/352]	Time  0.096 ( 0.121)	Data  0.002 ( 0.013)	Loss 1.1212e-01 (1.5339e-01)	Acc@1  96.88 ( 94.72)	Acc@5 100.00 ( 99.93)
03-Mar-22 15:59:43 - Epoch: [0][ 30/352]	Time  0.121 ( 0.118)	Data  0.002 ( 0.009)	Loss 6.7367e-02 (1.4284e-01)	Acc@1  96.88 ( 95.09)	Acc@5 100.00 ( 99.95)
03-Mar-22 15:59:44 - Epoch: [0][ 40/352]	Time  0.096 ( 0.113)	Data  0.002 ( 0.007)	Loss 1.3325e-01 (1.3959e-01)	Acc@1  97.66 ( 95.45)	Acc@5 100.00 ( 99.94)
03-Mar-22 16:00:40 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:00:40 - Use GPU: 0 for training
03-Mar-22 16:00:40 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:00:44 - match all modules defined in bit_config: True
03-Mar-22 16:00:44 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:01:27 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:01:27 - Use GPU: 0 for training
03-Mar-22 16:01:27 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:01:31 - match all modules defined in bit_config: True
03-Mar-22 16:01:31 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:23:03 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:23:03 - Use GPU: 0 for training
03-Mar-22 16:23:03 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 16:23:07 - match all modules defined in bit_config: True
03-Mar-22 16:23:07 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:29:43 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:29:43 - Use GPU: 0 for training
03-Mar-22 16:29:43 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 16:29:47 - match all modules defined in bit_config: True
03-Mar-22 16:29:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:37:24 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:37:24 - Use GPU: 0 for training
03-Mar-22 16:37:24 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:37:28 - match all modules defined in bit_config: True
03-Mar-22 16:37:28 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:37:28 - Epoch: [0][  0/352]	Time  0.417 ( 0.417)	Data  0.225 ( 0.225)	Loss 1.5334e-01 (1.5334e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 16:37:29 - Epoch: [0][ 10/352]	Time  0.121 ( 0.132)	Data  0.002 ( 0.022)	Loss 5.2840e-02 (1.3998e-01)	Acc@1  97.66 ( 95.10)	Acc@5 100.00 ( 99.93)
03-Mar-22 16:37:30 - Epoch: [0][ 20/352]	Time  0.096 ( 0.116)	Data  0.002 ( 0.012)	Loss 1.3331e-01 (1.4151e-01)	Acc@1  93.75 ( 95.09)	Acc@5 100.00 ( 99.93)
03-Mar-22 16:37:54 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:37:54 - Use GPU: 0 for training
03-Mar-22 16:37:54 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:37:58 - match all modules defined in bit_config: True
03-Mar-22 16:37:58 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:37:58 - Epoch: [0][  0/352]	Time  0.456 ( 0.456)	Data  0.259 ( 0.259)	Loss 9.4047e-02 (9.4047e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
03-Mar-22 16:38:00 - Epoch: [0][ 10/352]	Time  0.096 ( 0.136)	Data  0.002 ( 0.025)	Loss 1.5748e-01 (1.6084e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 (100.00)
03-Mar-22 16:38:01 - Epoch: [0][ 20/352]	Time  0.095 ( 0.120)	Data  0.002 ( 0.014)	Loss 1.1596e-01 (1.3742e-01)	Acc@1  95.31 ( 95.20)	Acc@5 100.00 ( 99.96)
03-Mar-22 16:38:02 - Epoch: [0][ 30/352]	Time  0.118 ( 0.115)	Data  0.002 ( 0.010)	Loss 1.1495e-01 (1.3294e-01)	Acc@1  95.31 ( 95.34)	Acc@5 100.00 ( 99.95)
03-Mar-22 16:38:03 - Epoch: [0][ 40/352]	Time  0.117 ( 0.114)	Data  0.002 ( 0.008)	Loss 1.6831e-01 (1.2695e-01)	Acc@1  93.75 ( 95.56)	Acc@5 100.00 ( 99.96)
03-Mar-22 16:39:18 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:39:18 - Use GPU: 0 for training
03-Mar-22 16:39:18 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:39:22 - match all modules defined in bit_config: True
03-Mar-22 16:39:22 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:39:23 - Epoch: [0][  0/352]	Time  0.397 ( 0.397)	Data  0.225 ( 0.225)	Loss 1.0740e-01 (1.0740e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
03-Mar-22 16:39:24 - Epoch: [0][ 10/352]	Time  0.097 ( 0.126)	Data  0.001 ( 0.022)	Loss 1.2862e-01 (1.1954e-01)	Acc@1  95.31 ( 95.45)	Acc@5 100.00 (100.00)
03-Mar-22 16:39:25 - Epoch: [0][ 20/352]	Time  0.097 ( 0.112)	Data  0.001 ( 0.012)	Loss 1.6124e-01 (1.1588e-01)	Acc@1  94.53 ( 95.76)	Acc@5 100.00 ( 99.96)
03-Mar-22 16:39:26 - Epoch: [0][ 30/352]	Time  0.126 ( 0.108)	Data  0.002 ( 0.009)	Loss 1.6326e-01 (1.1690e-01)	Acc@1  95.31 ( 95.87)	Acc@5 100.00 ( 99.97)
03-Mar-22 16:39:27 - Epoch: [0][ 40/352]	Time  0.096 ( 0.106)	Data  0.001 ( 0.007)	Loss 7.3781e-02 (1.1647e-01)	Acc@1  96.88 ( 95.81)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:28 - Epoch: [0][ 50/352]	Time  0.141 ( 0.111)	Data  0.002 ( 0.006)	Loss 1.0817e-01 (1.1825e-01)	Acc@1  96.88 ( 95.86)	Acc@5 100.00 ( 99.97)
03-Mar-22 16:39:29 - Epoch: [0][ 60/352]	Time  0.096 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2395e-02 (1.1464e-01)	Acc@1  96.88 ( 95.94)	Acc@5 100.00 ( 99.97)
03-Mar-22 16:39:30 - Epoch: [0][ 70/352]	Time  0.096 ( 0.107)	Data  0.001 ( 0.005)	Loss 1.2106e-01 (1.1419e-01)	Acc@1  95.31 ( 95.99)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:31 - Epoch: [0][ 80/352]	Time  0.096 ( 0.107)	Data  0.001 ( 0.004)	Loss 1.8384e-01 (1.1417e-01)	Acc@1  96.88 ( 96.02)	Acc@5  99.22 ( 99.97)
03-Mar-22 16:39:32 - Epoch: [0][ 90/352]	Time  0.096 ( 0.105)	Data  0.001 ( 0.004)	Loss 1.0539e-01 (1.1272e-01)	Acc@1  96.88 ( 96.04)	Acc@5 100.00 ( 99.97)
03-Mar-22 16:39:33 - Epoch: [0][100/352]	Time  0.101 ( 0.105)	Data  0.002 ( 0.004)	Loss 7.8795e-02 (1.1301e-01)	Acc@1  97.66 ( 96.02)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:34 - Epoch: [0][110/352]	Time  0.096 ( 0.104)	Data  0.001 ( 0.004)	Loss 7.0538e-02 (1.1230e-01)	Acc@1  97.66 ( 96.09)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:35 - Epoch: [0][120/352]	Time  0.142 ( 0.106)	Data  0.002 ( 0.003)	Loss 3.9086e-02 (1.1034e-01)	Acc@1  99.22 ( 96.18)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:36 - Epoch: [0][130/352]	Time  0.141 ( 0.108)	Data  0.002 ( 0.003)	Loss 1.1819e-01 (1.0862e-01)	Acc@1  96.88 ( 96.21)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:38 - Epoch: [0][140/352]	Time  0.142 ( 0.110)	Data  0.002 ( 0.003)	Loss 1.0071e-01 (1.0924e-01)	Acc@1  94.53 ( 96.17)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:39 - Epoch: [0][150/352]	Time  0.118 ( 0.111)	Data  0.002 ( 0.003)	Loss 5.0714e-02 (1.0858e-01)	Acc@1  98.44 ( 96.20)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:40 - Epoch: [0][160/352]	Time  0.146 ( 0.113)	Data  0.002 ( 0.003)	Loss 9.0117e-02 (1.0828e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:42 - Epoch: [0][170/352]	Time  0.118 ( 0.114)	Data  0.002 ( 0.003)	Loss 6.8622e-02 (1.0844e-01)	Acc@1  96.88 ( 96.21)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:43 - Epoch: [0][180/352]	Time  0.118 ( 0.115)	Data  0.002 ( 0.003)	Loss 9.0309e-02 (1.0771e-01)	Acc@1  98.44 ( 96.24)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:44 - Epoch: [0][190/352]	Time  0.145 ( 0.116)	Data  0.002 ( 0.003)	Loss 5.8610e-02 (1.0748e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:46 - Epoch: [0][200/352]	Time  0.144 ( 0.117)	Data  0.002 ( 0.003)	Loss 7.4623e-02 (1.0767e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:47 - Epoch: [0][210/352]	Time  0.118 ( 0.118)	Data  0.002 ( 0.003)	Loss 1.5186e-01 (1.0756e-01)	Acc@1  92.97 ( 96.20)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:48 - Epoch: [0][220/352]	Time  0.142 ( 0.118)	Data  0.002 ( 0.003)	Loss 9.7963e-02 (1.0704e-01)	Acc@1  96.88 ( 96.21)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:49 - Epoch: [0][230/352]	Time  0.102 ( 0.118)	Data  0.002 ( 0.003)	Loss 6.3965e-02 (1.0807e-01)	Acc@1  96.09 ( 96.18)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:50 - Epoch: [0][240/352]	Time  0.098 ( 0.117)	Data  0.001 ( 0.003)	Loss 8.2589e-02 (1.0879e-01)	Acc@1  96.88 ( 96.16)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:51 - Epoch: [0][250/352]	Time  0.096 ( 0.116)	Data  0.001 ( 0.003)	Loss 1.9599e-01 (1.0877e-01)	Acc@1  91.41 ( 96.16)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:52 - Epoch: [0][260/352]	Time  0.096 ( 0.116)	Data  0.001 ( 0.003)	Loss 7.7076e-02 (1.0869e-01)	Acc@1  97.66 ( 96.17)	Acc@5 100.00 ( 99.99)
03-Mar-22 16:39:53 - Epoch: [0][270/352]	Time  0.096 ( 0.115)	Data  0.001 ( 0.003)	Loss 9.0574e-02 (1.0852e-01)	Acc@1  98.44 ( 96.18)	Acc@5 100.00 ( 99.99)
03-Mar-22 16:39:54 - Epoch: [0][280/352]	Time  0.096 ( 0.115)	Data  0.001 ( 0.003)	Loss 1.3253e-01 (1.0865e-01)	Acc@1  96.09 ( 96.17)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:55 - Epoch: [0][290/352]	Time  0.123 ( 0.114)	Data  0.002 ( 0.002)	Loss 9.8870e-02 (1.0906e-01)	Acc@1  95.31 ( 96.15)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:56 - Epoch: [0][300/352]	Time  0.122 ( 0.114)	Data  0.002 ( 0.002)	Loss 9.7022e-02 (1.0812e-01)	Acc@1  95.31 ( 96.19)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:57 - Epoch: [0][310/352]	Time  0.096 ( 0.113)	Data  0.001 ( 0.002)	Loss 1.0084e-01 (1.0753e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:39:58 - Epoch: [0][320/352]	Time  0.096 ( 0.113)	Data  0.001 ( 0.002)	Loss 1.2287e-01 (1.0766e-01)	Acc@1  94.53 ( 96.20)	Acc@5 100.00 ( 99.99)
03-Mar-22 16:39:59 - Epoch: [0][330/352]	Time  0.096 ( 0.112)	Data  0.001 ( 0.002)	Loss 8.0304e-02 (1.0743e-01)	Acc@1  98.44 ( 96.21)	Acc@5 100.00 ( 99.99)
03-Mar-22 16:40:00 - Epoch: [0][340/352]	Time  0.119 ( 0.112)	Data  0.002 ( 0.002)	Loss 8.2080e-02 (1.0729e-01)	Acc@1  96.09 ( 96.21)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:40:01 - Epoch: [0][350/352]	Time  0.095 ( 0.112)	Data  0.001 ( 0.002)	Loss 1.5755e-01 (1.0824e-01)	Acc@1  94.53 ( 96.18)	Acc@5 100.00 ( 99.98)
03-Mar-22 16:40:02 - Test: [ 0/20]	Time  0.378 ( 0.378)	Loss 3.9787e-01 (3.9787e-01)	Acc@1  89.84 ( 89.84)	Acc@5  98.83 ( 98.83)
03-Mar-22 16:40:03 - Test: [10/20]	Time  0.081 ( 0.106)	Loss 4.6640e-01 (3.6020e-01)	Acc@1  88.28 ( 90.45)	Acc@5  99.61 ( 99.47)
03-Mar-22 16:40:04 -  * Acc@1 90.420 Acc@5 99.500
03-Mar-22 16:40:04 - Best acc at epoch 0: 90.41999816894531
03-Mar-22 16:40:04 - Epoch: [1][  0/352]	Time  0.328 ( 0.328)	Data  0.223 ( 0.223)	Loss 1.4667e-01 (1.4667e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 16:40:05 - Epoch: [1][ 10/352]	Time  0.096 ( 0.120)	Data  0.001 ( 0.022)	Loss 1.7281e-01 (1.0933e-01)	Acc@1  95.31 ( 96.09)	Acc@5 100.00 (100.00)
03-Mar-22 16:40:06 - Epoch: [1][ 20/352]	Time  0.096 ( 0.108)	Data  0.001 ( 0.012)	Loss 7.6673e-02 (1.0637e-01)	Acc@1  97.66 ( 96.39)	Acc@5 100.00 ( 99.96)
03-Mar-22 16:40:07 - Epoch: [1][ 30/352]	Time  0.096 ( 0.106)	Data  0.001 ( 0.009)	Loss 1.0099e-01 (1.0798e-01)	Acc@1  96.09 ( 96.19)	Acc@5 100.00 ( 99.97)
03-Mar-22 16:40:46 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:40:46 - Use GPU: 0 for training
03-Mar-22 16:40:46 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:40:50 - match all modules defined in bit_config: True
03-Mar-22 16:40:50 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:40:50 - Epoch: [0][  0/352]	Time  0.469 ( 0.469)	Data  0.258 ( 0.258)	Loss 1.7980e-01 (1.7980e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
03-Mar-22 16:40:51 - Epoch: [0][ 10/352]	Time  0.095 ( 0.137)	Data  0.001 ( 0.025)	Loss 1.1492e-01 (1.4237e-01)	Acc@1  96.09 ( 95.10)	Acc@5 100.00 ( 99.93)
03-Mar-22 16:40:52 - Epoch: [0][ 20/352]	Time  0.112 ( 0.123)	Data  0.002 ( 0.014)	Loss 1.4314e-01 (1.3299e-01)	Acc@1  92.19 ( 95.31)	Acc@5 100.00 ( 99.93)
03-Mar-22 16:40:53 - Epoch: [0][ 30/352]	Time  0.096 ( 0.114)	Data  0.001 ( 0.010)	Loss 1.4230e-01 (1.2686e-01)	Acc@1  96.09 ( 95.46)	Acc@5 100.00 ( 99.95)
03-Mar-22 16:40:54 - Epoch: [0][ 40/352]	Time  0.095 ( 0.110)	Data  0.002 ( 0.008)	Loss 1.3704e-01 (1.2093e-01)	Acc@1  94.53 ( 95.66)	Acc@5 100.00 ( 99.96)
03-Mar-22 16:42:43 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:42:43 - Use GPU: 0 for training
03-Mar-22 16:42:43 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:42:47 - match all modules defined in bit_config: True
03-Mar-22 16:42:47 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 16:43:48 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 16:43:48 - Use GPU: 0 for training
03-Mar-22 16:43:48 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 16:43:52 - match all modules defined in bit_config: True
03-Mar-22 16:43:52 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:14:02 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:14:02 - Use GPU: 0 for training
03-Mar-22 18:14:02 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:14:06 - match all modules defined in bit_config: True
03-Mar-22 18:14:06 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:15:47 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:15:47 - Use GPU: 0 for training
03-Mar-22 18:15:47 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:15:51 - match all modules defined in bit_config: True
03-Mar-22 18:15:51 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:23:08 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:23:08 - Use GPU: 0 for training
03-Mar-22 18:23:08 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:23:12 - match all modules defined in bit_config: True
03-Mar-22 18:23:12 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:29:13 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:29:13 - Use GPU: 0 for training
03-Mar-22 18:29:13 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:29:17 - match all modules defined in bit_config: True
03-Mar-22 18:29:17 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:32:28 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:32:28 - Use GPU: 0 for training
03-Mar-22 18:32:28 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:32:32 - match all modules defined in bit_config: True
03-Mar-22 18:32:32 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:46:44 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:46:44 - Use GPU: 0 for training
03-Mar-22 18:46:44 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:46:48 - match all modules defined in bit_config: True
03-Mar-22 18:46:48 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:47:30 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:47:30 - Use GPU: 0 for training
03-Mar-22 18:47:30 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:47:34 - match all modules defined in bit_config: True
03-Mar-22 18:47:34 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:48:05 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:48:05 - Use GPU: 0 for training
03-Mar-22 18:48:05 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:48:09 - match all modules defined in bit_config: True
03-Mar-22 18:48:09 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:54:44 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:54:44 - Use GPU: 0 for training
03-Mar-22 18:54:44 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:54:48 - match all modules defined in bit_config: True
03-Mar-22 18:54:48 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 18:55:12 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 18:55:12 - Use GPU: 0 for training
03-Mar-22 18:55:12 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 18:55:16 - match all modules defined in bit_config: True
03-Mar-22 18:55:16 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 19:00:10 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 19:00:10 - Use GPU: 0 for training
03-Mar-22 19:00:10 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 19:00:14 - match all modules defined in bit_config: True
03-Mar-22 19:00:14 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 19:02:36 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 19:02:36 - Use GPU: 0 for training
03-Mar-22 19:02:36 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 19:02:40 - match all modules defined in bit_config: True
03-Mar-22 19:02:40 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 19:04:34 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 19:04:34 - Use GPU: 0 for training
03-Mar-22 19:04:34 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 19:04:39 - match all modules defined in bit_config: True
03-Mar-22 19:04:39 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 19:05:48 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 19:05:48 - Use GPU: 0 for training
03-Mar-22 19:05:48 - => using pre-trained PyTorchCV model 'resnet20_unfold'
03-Mar-22 19:05:52 - match all modules defined in bit_config: True
03-Mar-22 19:05:52 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 19:09:24 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 19:09:24 - Use GPU: 0 for training
03-Mar-22 19:09:24 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 19:09:28 - match all modules defined in bit_config: True
03-Mar-22 19:09:28 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
03-Mar-22 19:09:28 - Epoch: [0][  0/352]	Time  0.426 ( 0.426)	Data  0.261 ( 0.261)	Loss 1.9919e-01 (1.9919e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
03-Mar-22 19:10:21 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
03-Mar-22 19:10:21 - Use GPU: 0 for training
03-Mar-22 19:10:21 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
03-Mar-22 19:10:26 - match all modules defined in bit_config: True
03-Mar-22 19:10:26 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 03:48:01 - Epoch: [0][  0/352]	Time 31054.939 (31054.939)	Data  0.219 ( 0.219)	Loss 2.4985e-01 (2.4985e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
04-Mar-22 03:48:10 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 03:48:10 - Use GPU: 0 for training
04-Mar-22 03:48:10 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
04-Mar-22 03:48:14 - match all modules defined in bit_config: True
04-Mar-22 03:48:14 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 03:48:29 - Epoch: [0][  0/352]	Time 15.282 (15.282)	Data  0.256 ( 0.256)	Loss 1.9154e-01 (1.9154e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
04-Mar-22 03:48:42 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 03:48:42 - Use GPU: 0 for training
04-Mar-22 03:48:42 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 03:48:47 - match all modules defined in bit_config: True
04-Mar-22 03:48:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 03:49:36 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 03:49:36 - Use GPU: 0 for training
04-Mar-22 03:49:36 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 03:49:40 - match all modules defined in bit_config: True
04-Mar-22 03:49:40 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 03:50:43 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 03:50:43 - Use GPU: 0 for training
04-Mar-22 03:50:43 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 03:50:47 - match all modules defined in bit_config: True
04-Mar-22 03:50:47 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 03:52:45 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 03:52:45 - Use GPU: 0 for training
04-Mar-22 03:52:45 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 03:52:49 - match all modules defined in bit_config: True
04-Mar-22 03:52:49 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 03:58:33 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 03:58:33 - Use GPU: 0 for training
04-Mar-22 03:58:33 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 03:58:37 - match all modules defined in bit_config: True
04-Mar-22 03:58:37 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:41:07 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:41:07 - Use GPU: 0 for training
04-Mar-22 05:41:07 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 05:41:11 - match all modules defined in bit_config: True
04-Mar-22 05:41:11 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:42:27 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:42:27 - Use GPU: 0 for training
04-Mar-22 05:42:27 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 05:42:31 - match all modules defined in bit_config: True
04-Mar-22 05:42:31 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:43:58 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:43:58 - Use GPU: 0 for training
04-Mar-22 05:43:58 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
04-Mar-22 05:44:02 - match all modules defined in bit_config: True
04-Mar-22 05:44:02 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:44:24 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:44:24 - Use GPU: 0 for training
04-Mar-22 05:44:24 - => using pre-trained PyTorchCV model 'resnet20_cifar10'
04-Mar-22 05:44:28 - match all modules defined in bit_config: True
04-Mar-22 05:44:28 - Q_ResNet20(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_convbn): (QuantBnConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_convbn): (QuantBnConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn1): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_convbn2): (QuantBnConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:45:03 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:45:03 - Use GPU: 0 for training
04-Mar-22 05:45:03 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 05:45:07 - match all modules defined in bit_config: True
04-Mar-22 05:45:07 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:48:09 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:48:09 - Use GPU: 0 for training
04-Mar-22 05:48:09 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 05:48:13 - match all modules defined in bit_config: True
04-Mar-22 05:48:13 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:50:28 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:50:28 - Use GPU: 0 for training
04-Mar-22 05:50:28 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 05:50:32 - match all modules defined in bit_config: True
04-Mar-22 05:50:32 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 05:59:57 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 05:59:57 - Use GPU: 0 for training
04-Mar-22 05:59:57 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 06:00:01 - match all modules defined in bit_config: True
04-Mar-22 06:00:01 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=4, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 07:07:30 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 07:07:30 - Use GPU: 0 for training
04-Mar-22 07:07:30 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 07:07:34 - match all modules defined in bit_config: True
04-Mar-22 07:07:34 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_act_int4): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
04-Mar-22 07:10:37 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
04-Mar-22 07:10:37 - Use GPU: 0 for training
04-Mar-22 07:10:37 - => using pre-trained PyTorchCV model 'resnet20_unfold'
04-Mar-22 07:10:41 - match all modules defined in bit_config: True
04-Mar-22 07:10:41 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_conv_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
07-Mar-22 02:40:35 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 02:40:35 - Use GPU: 0 for training
07-Mar-22 02:40:35 - => using pre-trained PyTorchCV model 'resnet20_unfold'
07-Mar-22 02:40:39 - match all modules defined in bit_config: True
07-Mar-22 02:40:39 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_conv_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
07-Mar-22 02:45:48 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=128, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 02:45:48 - Use GPU: 0 for training
07-Mar-22 02:45:48 - => using pre-trained PyTorchCV model 'resnet20_unfold'
07-Mar-22 02:45:52 - match all modules defined in bit_config: True
07-Mar-22 02:45:52 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_conv_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
07-Mar-22 02:45:53 - Epoch: [0][  0/352]	Time  0.490 ( 0.490)	Data  0.253 ( 0.253)	Loss 8.2595e-01 (8.2595e-01)	Acc@1  78.91 ( 78.91)	Acc@5  97.66 ( 97.66)
07-Mar-22 02:45:54 - Epoch: [0][ 10/352]	Time  0.142 ( 0.192)	Data  0.002 ( 0.025)	Loss 3.8731e-01 (4.6000e-01)	Acc@1  87.50 ( 85.65)	Acc@5  98.44 ( 99.22)
07-Mar-22 02:45:56 - Epoch: [0][ 20/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.014)	Loss 1.2888e-01 (3.7600e-01)	Acc@1  94.53 ( 87.69)	Acc@5 100.00 ( 99.52)
07-Mar-22 02:45:57 - Epoch: [0][ 30/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.010)	Loss 1.1737e-01 (3.2863e-01)	Acc@1  96.88 ( 89.34)	Acc@5 100.00 ( 99.55)
07-Mar-22 02:45:59 - Epoch: [0][ 40/352]	Time  0.164 ( 0.170)	Data  0.002 ( 0.008)	Loss 3.1755e-01 (2.9590e-01)	Acc@1  91.41 ( 90.28)	Acc@5 100.00 ( 99.64)
07-Mar-22 02:46:01 - Epoch: [0][ 50/352]	Time  0.164 ( 0.170)	Data  0.003 ( 0.007)	Loss 2.2173e-01 (2.7683e-01)	Acc@1  92.97 ( 90.85)	Acc@5 100.00 ( 99.66)
07-Mar-22 02:46:02 - Epoch: [0][ 60/352]	Time  0.163 ( 0.169)	Data  0.002 ( 0.006)	Loss 1.7123e-01 (2.6655e-01)	Acc@1  94.53 ( 91.10)	Acc@5 100.00 ( 99.68)
07-Mar-22 02:46:04 - Epoch: [0][ 70/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.006)	Loss 1.3919e-01 (2.5656e-01)	Acc@1  94.53 ( 91.35)	Acc@5 100.00 ( 99.71)
07-Mar-22 02:46:06 - Epoch: [0][ 80/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.005)	Loss 2.6992e-01 (2.5027e-01)	Acc@1  92.97 ( 91.64)	Acc@5  98.44 ( 99.72)
07-Mar-22 02:46:07 - Epoch: [0][ 90/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.2298e-01 (2.4145e-01)	Acc@1  96.88 ( 91.98)	Acc@5 100.00 ( 99.75)
07-Mar-22 02:46:09 - Epoch: [0][100/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.005)	Loss 2.0295e-01 (2.3432e-01)	Acc@1  93.75 ( 92.22)	Acc@5 100.00 ( 99.78)
07-Mar-22 02:46:11 - Epoch: [0][110/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2132e-01 (2.2823e-01)	Acc@1  94.53 ( 92.36)	Acc@5 100.00 ( 99.80)
07-Mar-22 02:46:13 - Epoch: [0][120/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 2.1244e-01 (2.2574e-01)	Acc@1  91.41 ( 92.39)	Acc@5 100.00 ( 99.79)
07-Mar-22 02:46:14 - Epoch: [0][130/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 2.2002e-01 (2.2222e-01)	Acc@1  92.97 ( 92.52)	Acc@5 100.00 ( 99.80)
07-Mar-22 02:46:16 - Epoch: [0][140/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.0033e-01 (2.1765e-01)	Acc@1  96.88 ( 92.68)	Acc@5 100.00 ( 99.82)
07-Mar-22 02:46:17 - Epoch: [0][150/352]	Time  0.164 ( 0.168)	Data  0.003 ( 0.004)	Loss 1.2052e-01 (2.1499e-01)	Acc@1  95.31 ( 92.80)	Acc@5 100.00 ( 99.82)
07-Mar-22 02:46:19 - Epoch: [0][160/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.5771e-02 (2.1216e-01)	Acc@1  95.31 ( 92.87)	Acc@5 100.00 ( 99.83)
07-Mar-22 02:46:21 - Epoch: [0][170/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.1442e-01 (2.0901e-01)	Acc@1  95.31 ( 92.98)	Acc@5 100.00 ( 99.83)
07-Mar-22 02:46:22 - Epoch: [0][180/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.7575e-01 (2.0692e-01)	Acc@1  92.19 ( 93.02)	Acc@5 100.00 ( 99.84)
07-Mar-22 02:46:24 - Epoch: [0][190/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 2.1886e-01 (2.0438e-01)	Acc@1  90.62 ( 93.10)	Acc@5 100.00 ( 99.84)
07-Mar-22 02:46:26 - Epoch: [0][200/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3268e-01 (2.0215e-01)	Acc@1  93.75 ( 93.15)	Acc@5 100.00 ( 99.85)
07-Mar-22 02:46:28 - Epoch: [0][210/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.5958e-01 (1.9984e-01)	Acc@1  92.19 ( 93.19)	Acc@5 100.00 ( 99.86)
07-Mar-22 02:46:29 - Epoch: [0][220/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.2911e-02 (1.9679e-01)	Acc@1  96.88 ( 93.28)	Acc@5 100.00 ( 99.87)
07-Mar-22 02:46:31 - Epoch: [0][230/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.8943e-01 (1.9418e-01)	Acc@1  93.75 ( 93.38)	Acc@5 100.00 ( 99.87)
07-Mar-22 02:46:32 - Epoch: [0][240/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3046e-01 (1.9172e-01)	Acc@1  95.31 ( 93.47)	Acc@5 100.00 ( 99.87)
07-Mar-22 02:46:34 - Epoch: [0][250/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 2.5677e-01 (1.9072e-01)	Acc@1  89.84 ( 93.49)	Acc@5 100.00 ( 99.87)
07-Mar-22 02:46:36 - Epoch: [0][260/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 2.8975e-01 (1.9055e-01)	Acc@1  92.19 ( 93.51)	Acc@5  99.22 ( 99.87)
07-Mar-22 02:46:38 - Epoch: [0][270/352]	Time  0.189 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.0808e-01 (1.8967e-01)	Acc@1  94.53 ( 93.55)	Acc@5 100.00 ( 99.88)
07-Mar-22 02:46:39 - Epoch: [0][280/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.4452e-01 (1.8854e-01)	Acc@1  92.19 ( 93.59)	Acc@5 100.00 ( 99.87)
07-Mar-22 02:46:41 - Epoch: [0][290/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0465e-01 (1.8684e-01)	Acc@1  96.09 ( 93.63)	Acc@5 100.00 ( 99.88)
07-Mar-22 02:46:43 - Epoch: [0][300/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.9037e-01 (1.8550e-01)	Acc@1  95.31 ( 93.70)	Acc@5 100.00 ( 99.88)
07-Mar-22 02:46:44 - Epoch: [0][310/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4708e-01 (1.8407e-01)	Acc@1  92.97 ( 93.72)	Acc@5 100.00 ( 99.89)
07-Mar-22 02:46:46 - Epoch: [0][320/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.6437e-01 (1.8361e-01)	Acc@1  95.31 ( 93.74)	Acc@5 100.00 ( 99.89)
07-Mar-22 02:46:48 - Epoch: [0][330/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1320e-01 (1.8237e-01)	Acc@1  93.75 ( 93.77)	Acc@5 100.00 ( 99.89)
07-Mar-22 02:46:49 - Epoch: [0][340/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.9593e-01 (1.8152e-01)	Acc@1  93.75 ( 93.81)	Acc@5  99.22 ( 99.89)
07-Mar-22 02:46:51 - Epoch: [0][350/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.7175e-02 (1.8029e-01)	Acc@1  96.88 ( 93.84)	Acc@5 100.00 ( 99.89)
07-Mar-22 02:46:52 - Test: [ 0/20]	Time  0.395 ( 0.395)	Loss 4.2695e-01 (4.2695e-01)	Acc@1  88.67 ( 88.67)	Acc@5  98.83 ( 98.83)
07-Mar-22 02:46:53 - Test: [10/20]	Time  0.098 ( 0.128)	Loss 4.3807e-01 (4.5339e-01)	Acc@1  87.11 ( 86.97)	Acc@5  99.22 ( 99.15)
07-Mar-22 02:46:54 -  * Acc@1 87.140 Acc@5 99.080
07-Mar-22 02:46:54 - Best acc at epoch 0: 87.13999938964844
07-Mar-22 02:46:54 - Epoch: [1][  0/352]	Time  0.384 ( 0.384)	Data  0.236 ( 0.236)	Loss 7.4646e-02 (7.4646e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 02:46:56 - Epoch: [1][ 10/352]	Time  0.143 ( 0.179)	Data  0.002 ( 0.023)	Loss 1.1887e-01 (1.3918e-01)	Acc@1  96.09 ( 95.60)	Acc@5  99.22 ( 99.93)
07-Mar-22 02:46:57 - Epoch: [1][ 20/352]	Time  0.143 ( 0.162)	Data  0.002 ( 0.013)	Loss 1.3256e-01 (1.3540e-01)	Acc@1  93.75 ( 95.39)	Acc@5 100.00 ( 99.93)
07-Mar-22 02:46:59 - Epoch: [1][ 30/352]	Time  0.143 ( 0.156)	Data  0.002 ( 0.009)	Loss 1.5539e-01 (1.4496e-01)	Acc@1  96.09 ( 95.09)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:00 - Epoch: [1][ 40/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.008)	Loss 1.5425e-01 (1.4440e-01)	Acc@1  94.53 ( 95.05)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:01 - Epoch: [1][ 50/352]	Time  0.143 ( 0.151)	Data  0.002 ( 0.006)	Loss 1.0162e-01 (1.4722e-01)	Acc@1  96.88 ( 94.85)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:47:03 - Epoch: [1][ 60/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.006)	Loss 1.3014e-01 (1.4332e-01)	Acc@1  95.31 ( 94.99)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:04 - Epoch: [1][ 70/352]	Time  0.144 ( 0.149)	Data  0.002 ( 0.005)	Loss 2.2432e-01 (1.4386e-01)	Acc@1  92.19 ( 94.91)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:06 - Epoch: [1][ 80/352]	Time  0.143 ( 0.148)	Data  0.002 ( 0.005)	Loss 1.7453e-01 (1.4575e-01)	Acc@1  95.31 ( 94.93)	Acc@5  99.22 ( 99.95)
07-Mar-22 02:47:07 - Epoch: [1][ 90/352]	Time  0.164 ( 0.150)	Data  0.002 ( 0.004)	Loss 6.8779e-02 (1.4585e-01)	Acc@1  96.88 ( 94.97)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:09 - Epoch: [1][100/352]	Time  0.163 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0185e-01 (1.4194e-01)	Acc@1  96.88 ( 95.10)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:11 - Epoch: [1][110/352]	Time  0.171 ( 0.153)	Data  0.001 ( 0.004)	Loss 9.7210e-02 (1.4237e-01)	Acc@1  98.44 ( 95.14)	Acc@5 100.00 ( 99.94)
07-Mar-22 02:47:12 - Epoch: [1][120/352]	Time  0.167 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.7816e-01 (1.4209e-01)	Acc@1  93.75 ( 95.13)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:14 - Epoch: [1][130/352]	Time  0.165 ( 0.155)	Data  0.002 ( 0.004)	Loss 1.2834e-01 (1.4211e-01)	Acc@1  94.53 ( 95.10)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:16 - Epoch: [1][140/352]	Time  0.166 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.1398e-01 (1.4075e-01)	Acc@1  94.53 ( 95.15)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:17 - Epoch: [1][150/352]	Time  0.167 ( 0.156)	Data  0.001 ( 0.003)	Loss 1.8824e-01 (1.4058e-01)	Acc@1  92.97 ( 95.14)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:19 - Epoch: [1][160/352]	Time  0.167 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.7880e-01 (1.3869e-01)	Acc@1  92.97 ( 95.18)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:21 - Epoch: [1][170/352]	Time  0.165 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.8233e-02 (1.3807e-01)	Acc@1  96.09 ( 95.20)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:47:22 - Epoch: [1][180/352]	Time  0.169 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.5022e-01 (1.3797e-01)	Acc@1  96.09 ( 95.19)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:24 - Epoch: [1][190/352]	Time  0.166 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1504e-01 (1.3723e-01)	Acc@1  96.09 ( 95.20)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:26 - Epoch: [1][200/352]	Time  0.168 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.8823e-01 (1.3572e-01)	Acc@1  93.75 ( 95.23)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:27 - Epoch: [1][210/352]	Time  0.170 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1502e-01 (1.3612e-01)	Acc@1  96.09 ( 95.21)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:29 - Epoch: [1][220/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.5820e-01 (1.3623e-01)	Acc@1  96.09 ( 95.22)	Acc@5  99.22 ( 99.96)
07-Mar-22 02:47:31 - Epoch: [1][230/352]	Time  0.167 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.0170e-01 (1.3502e-01)	Acc@1  98.44 ( 95.26)	Acc@5  99.22 ( 99.96)
07-Mar-22 02:47:32 - Epoch: [1][240/352]	Time  0.168 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.2557e-01 (1.3394e-01)	Acc@1  96.09 ( 95.30)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:34 - Epoch: [1][250/352]	Time  0.168 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.5474e-01 (1.3312e-01)	Acc@1  95.31 ( 95.33)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:36 - Epoch: [1][260/352]	Time  0.172 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.3487e-01 (1.3332e-01)	Acc@1  95.31 ( 95.32)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:37 - Epoch: [1][270/352]	Time  0.164 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.0459e-01 (1.3324e-01)	Acc@1  97.66 ( 95.32)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:39 - Epoch: [1][280/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.1504e-01 (1.3272e-01)	Acc@1  96.88 ( 95.34)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:41 - Epoch: [1][290/352]	Time  0.168 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.0638e-01 (1.3251e-01)	Acc@1  98.44 ( 95.37)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:42 - Epoch: [1][300/352]	Time  0.165 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.7803e-01 (1.3243e-01)	Acc@1  95.31 ( 95.37)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:44 - Epoch: [1][310/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.8575e-01 (1.3331e-01)	Acc@1  94.53 ( 95.35)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:46 - Epoch: [1][320/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.3620e-02 (1.3417e-01)	Acc@1  98.44 ( 95.32)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:47 - Epoch: [1][330/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.8247e-01 (1.3473e-01)	Acc@1  94.53 ( 95.28)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:49 - Epoch: [1][340/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.4683e-01 (1.3499e-01)	Acc@1  94.53 ( 95.28)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:51 - Epoch: [1][350/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.003)	Loss 6.5466e-02 (1.3409e-01)	Acc@1  97.66 ( 95.30)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:51 - Test: [ 0/20]	Time  0.401 ( 0.401)	Loss 4.2124e-01 (4.2124e-01)	Acc@1  87.11 ( 87.11)	Acc@5  98.83 ( 98.83)
07-Mar-22 02:47:52 - Test: [10/20]	Time  0.098 ( 0.127)	Loss 4.0017e-01 (4.2730e-01)	Acc@1  89.45 ( 87.39)	Acc@5  99.22 ( 99.15)
07-Mar-22 02:47:53 -  * Acc@1 87.600 Acc@5 99.220
07-Mar-22 02:47:53 - Best acc at epoch 1: 87.5999984741211
07-Mar-22 02:47:54 - Epoch: [2][  0/352]	Time  0.395 ( 0.395)	Data  0.232 ( 0.232)	Loss 3.2300e-01 (3.2300e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
07-Mar-22 02:47:56 - Epoch: [2][ 10/352]	Time  0.165 ( 0.187)	Data  0.002 ( 0.023)	Loss 1.9060e-01 (1.6503e-01)	Acc@1  92.97 ( 94.25)	Acc@5 100.00 (100.00)
07-Mar-22 02:47:57 - Epoch: [2][ 20/352]	Time  0.167 ( 0.180)	Data  0.002 ( 0.013)	Loss 1.1720e-01 (1.4785e-01)	Acc@1  97.66 ( 94.83)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:47:59 - Epoch: [2][ 30/352]	Time  0.168 ( 0.176)	Data  0.002 ( 0.009)	Loss 1.0647e-01 (1.3499e-01)	Acc@1  95.31 ( 95.26)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:01 - Epoch: [2][ 40/352]	Time  0.175 ( 0.174)	Data  0.003 ( 0.008)	Loss 7.0756e-02 (1.3165e-01)	Acc@1  97.66 ( 95.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:48:02 - Epoch: [2][ 50/352]	Time  0.151 ( 0.170)	Data  0.002 ( 0.007)	Loss 2.2828e-01 (1.3398e-01)	Acc@1  90.62 ( 95.33)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:04 - Epoch: [2][ 60/352]	Time  0.150 ( 0.167)	Data  0.002 ( 0.006)	Loss 9.4868e-02 (1.3063e-01)	Acc@1  98.44 ( 95.44)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:05 - Epoch: [2][ 70/352]	Time  0.157 ( 0.166)	Data  0.002 ( 0.005)	Loss 8.0674e-02 (1.2934e-01)	Acc@1  97.66 ( 95.53)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:07 - Epoch: [2][ 80/352]	Time  0.149 ( 0.164)	Data  0.002 ( 0.005)	Loss 1.2100e-01 (1.2851e-01)	Acc@1  95.31 ( 95.49)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:08 - Epoch: [2][ 90/352]	Time  0.146 ( 0.162)	Data  0.002 ( 0.004)	Loss 1.4664e-01 (1.2698e-01)	Acc@1  96.09 ( 95.54)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:10 - Epoch: [2][100/352]	Time  0.164 ( 0.162)	Data  0.002 ( 0.004)	Loss 8.7246e-02 (1.2454e-01)	Acc@1  96.88 ( 95.65)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:11 - Epoch: [2][110/352]	Time  0.141 ( 0.160)	Data  0.001 ( 0.004)	Loss 1.1280e-01 (1.2521e-01)	Acc@1  94.53 ( 95.61)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:13 - Epoch: [2][120/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.004)	Loss 5.6912e-02 (1.2425e-01)	Acc@1  99.22 ( 95.63)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:14 - Epoch: [2][130/352]	Time  0.156 ( 0.158)	Data  0.002 ( 0.004)	Loss 1.8033e-01 (1.2494e-01)	Acc@1  92.97 ( 95.59)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:16 - Epoch: [2][140/352]	Time  0.154 ( 0.158)	Data  0.001 ( 0.004)	Loss 1.2490e-01 (1.2573e-01)	Acc@1  93.75 ( 95.53)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:48:17 - Epoch: [2][150/352]	Time  0.143 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.3884e-02 (1.2680e-01)	Acc@1  97.66 ( 95.49)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:19 - Epoch: [2][160/352]	Time  0.154 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.2077e-01 (1.2800e-01)	Acc@1  95.31 ( 95.40)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:20 - Epoch: [2][170/352]	Time  0.151 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.1087e-01 (1.2784e-01)	Acc@1  97.66 ( 95.43)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:22 - Epoch: [2][180/352]	Time  0.157 ( 0.157)	Data  0.002 ( 0.003)	Loss 9.2817e-02 (1.2734e-01)	Acc@1  96.88 ( 95.47)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:23 - Epoch: [2][190/352]	Time  0.142 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.4302e-01 (1.2671e-01)	Acc@1  95.31 ( 95.50)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:25 - Epoch: [2][200/352]	Time  0.141 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.8687e-02 (1.2748e-01)	Acc@1  96.88 ( 95.48)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:26 - Epoch: [2][210/352]	Time  0.166 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.1863e-01 (1.2685e-01)	Acc@1  96.09 ( 95.50)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:28 - Epoch: [2][220/352]	Time  0.167 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.0256e-01 (1.2623e-01)	Acc@1  96.09 ( 95.52)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:30 - Epoch: [2][230/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.2761e-01 (1.2621e-01)	Acc@1  94.53 ( 95.50)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:31 - Epoch: [2][240/352]	Time  0.167 ( 0.158)	Data  0.002 ( 0.003)	Loss 6.9367e-02 (1.2613e-01)	Acc@1  98.44 ( 95.51)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:33 - Epoch: [2][250/352]	Time  0.160 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.6000e-01 (1.2641e-01)	Acc@1  93.75 ( 95.49)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:35 - Epoch: [2][260/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.003)	Loss 7.7305e-02 (1.2699e-01)	Acc@1  97.66 ( 95.47)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:36 - Epoch: [2][270/352]	Time  0.174 ( 0.159)	Data  0.002 ( 0.003)	Loss 2.1389e-01 (1.2706e-01)	Acc@1  90.62 ( 95.47)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:48:38 - Epoch: [2][280/352]	Time  0.144 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1561e-01 (1.2712e-01)	Acc@1  93.75 ( 95.47)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:40 - Epoch: [2][290/352]	Time  0.169 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.7515e-01 (1.2631e-01)	Acc@1  92.97 ( 95.50)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:41 - Epoch: [2][300/352]	Time  0.149 ( 0.159)	Data  0.002 ( 0.003)	Loss 2.1529e-01 (1.2642e-01)	Acc@1  92.19 ( 95.50)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:43 - Epoch: [2][310/352]	Time  0.164 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.8195e-01 (1.2626e-01)	Acc@1  93.75 ( 95.50)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:45 - Epoch: [2][320/352]	Time  0.165 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1046e-01 (1.2595e-01)	Acc@1  96.09 ( 95.50)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:46 - Epoch: [2][330/352]	Time  0.169 ( 0.160)	Data  0.001 ( 0.003)	Loss 1.5593e-01 (1.2611e-01)	Acc@1  94.53 ( 95.50)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:48 - Epoch: [2][340/352]	Time  0.165 ( 0.160)	Data  0.002 ( 0.003)	Loss 8.4949e-02 (1.2533e-01)	Acc@1  96.88 ( 95.52)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:50 - Epoch: [2][350/352]	Time  0.147 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.4725e-01 (1.2512e-01)	Acc@1  94.53 ( 95.53)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:48:50 - Test: [ 0/20]	Time  0.375 ( 0.375)	Loss 4.2507e-01 (4.2507e-01)	Acc@1  87.89 ( 87.89)	Acc@5  99.61 ( 99.61)
07-Mar-22 02:48:51 - Test: [10/20]	Time  0.100 ( 0.129)	Loss 4.6866e-01 (4.2135e-01)	Acc@1  89.06 ( 88.00)	Acc@5  98.83 ( 99.40)
07-Mar-22 02:48:52 -  * Acc@1 88.160 Acc@5 99.420
07-Mar-22 02:48:52 - Best acc at epoch 2: 88.15999603271484
07-Mar-22 02:48:53 - Epoch: [3][  0/352]	Time  0.393 ( 0.393)	Data  0.234 ( 0.234)	Loss 1.5994e-01 (1.5994e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
07-Mar-22 02:48:54 - Epoch: [3][ 10/352]	Time  0.166 ( 0.187)	Data  0.002 ( 0.023)	Loss 9.6748e-02 (1.3564e-01)	Acc@1  97.66 ( 95.24)	Acc@5 100.00 (100.00)
07-Mar-22 02:48:56 - Epoch: [3][ 20/352]	Time  0.167 ( 0.178)	Data  0.002 ( 0.013)	Loss 1.5928e-01 (1.3570e-01)	Acc@1  94.53 ( 95.24)	Acc@5 100.00 (100.00)
07-Mar-22 02:48:58 - Epoch: [3][ 30/352]	Time  0.173 ( 0.174)	Data  0.002 ( 0.010)	Loss 1.3196e-01 (1.3538e-01)	Acc@1  94.53 ( 95.24)	Acc@5  99.22 ( 99.97)
07-Mar-22 02:48:59 - Epoch: [3][ 40/352]	Time  0.166 ( 0.172)	Data  0.002 ( 0.008)	Loss 1.4632e-01 (1.3167e-01)	Acc@1  95.31 ( 95.45)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:01 - Epoch: [3][ 50/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.007)	Loss 5.0051e-02 (1.2825e-01)	Acc@1  98.44 ( 95.65)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:03 - Epoch: [3][ 60/352]	Time  0.165 ( 0.171)	Data  0.002 ( 0.006)	Loss 1.0913e-01 (1.2469e-01)	Acc@1  97.66 ( 95.72)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:49:04 - Epoch: [3][ 70/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.005)	Loss 9.0259e-02 (1.2355e-01)	Acc@1  97.66 ( 95.75)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:49:06 - Epoch: [3][ 80/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.5997e-01 (1.2403e-01)	Acc@1  92.19 ( 95.65)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:49:08 - Epoch: [3][ 90/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.005)	Loss 8.4894e-02 (1.2557e-01)	Acc@1  95.31 ( 95.69)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:09 - Epoch: [3][100/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.8765e-02 (1.2459e-01)	Acc@1  96.09 ( 95.72)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:11 - Epoch: [3][110/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.3848e-01 (1.2246e-01)	Acc@1  92.97 ( 95.82)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:13 - Epoch: [3][120/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.0810e-01 (1.2094e-01)	Acc@1  95.31 ( 95.87)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:14 - Epoch: [3][130/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.2857e-01 (1.2059e-01)	Acc@1  94.53 ( 95.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:16 - Epoch: [3][140/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.1966e-01 (1.2128e-01)	Acc@1  95.31 ( 95.86)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:18 - Epoch: [3][150/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.1828e-02 (1.2005e-01)	Acc@1  97.66 ( 95.89)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:19 - Epoch: [3][160/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.6793e-02 (1.1894e-01)	Acc@1  96.88 ( 95.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:21 - Epoch: [3][170/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0553e-01 (1.1822e-01)	Acc@1  95.31 ( 95.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:23 - Epoch: [3][180/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.9276e-02 (1.1865e-01)	Acc@1  96.09 ( 95.94)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:24 - Epoch: [3][190/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.2668e-02 (1.1933e-01)	Acc@1  98.44 ( 95.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:26 - Epoch: [3][200/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.3355e-01 (1.1907e-01)	Acc@1  96.09 ( 95.95)	Acc@5  99.22 ( 99.97)
07-Mar-22 02:49:28 - Epoch: [3][210/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.9084e-02 (1.1950e-01)	Acc@1  96.09 ( 95.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:29 - Epoch: [3][220/352]	Time  0.163 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0382e-01 (1.1881e-01)	Acc@1  96.88 ( 95.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:31 - Epoch: [3][230/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.9435e-01 (1.1988e-01)	Acc@1  92.19 ( 95.92)	Acc@5  99.22 ( 99.97)
07-Mar-22 02:49:33 - Epoch: [3][240/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.4277e-02 (1.1948e-01)	Acc@1  97.66 ( 95.95)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:34 - Epoch: [3][250/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1908e-01 (1.1908e-01)	Acc@1  94.53 ( 95.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:36 - Epoch: [3][260/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4072e-01 (1.1923e-01)	Acc@1  95.31 ( 95.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:38 - Epoch: [3][270/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2802e-01 (1.1994e-01)	Acc@1  93.75 ( 95.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:49:39 - Epoch: [3][280/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.6345e-02 (1.1965e-01)	Acc@1  96.09 ( 95.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:41 - Epoch: [3][290/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1281e-01 (1.1941e-01)	Acc@1  96.88 ( 95.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:43 - Epoch: [3][300/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4003e-01 (1.1883e-01)	Acc@1  92.97 ( 95.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:44 - Epoch: [3][310/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.7336e-02 (1.1855e-01)	Acc@1  99.22 ( 95.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:46 - Epoch: [3][320/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.5515e-01 (1.1927e-01)	Acc@1  96.09 ( 95.92)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:48 - Epoch: [3][330/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.4736e-02 (1.1919e-01)	Acc@1  97.66 ( 95.92)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:49 - Epoch: [3][340/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.7035e-01 (1.1968e-01)	Acc@1  93.75 ( 95.89)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:51 - Epoch: [3][350/352]	Time  0.168 ( 0.167)	Data  0.001 ( 0.003)	Loss 1.3807e-01 (1.1939e-01)	Acc@1  95.31 ( 95.90)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:49:52 - Test: [ 0/20]	Time  0.367 ( 0.367)	Loss 4.0775e-01 (4.0775e-01)	Acc@1  88.67 ( 88.67)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:49:53 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 3.8473e-01 (4.0269e-01)	Acc@1  88.67 ( 88.42)	Acc@5  99.61 ( 99.54)
07-Mar-22 02:49:54 -  * Acc@1 88.180 Acc@5 99.520
07-Mar-22 02:49:54 - Best acc at epoch 3: 88.18000030517578
07-Mar-22 02:49:54 - Epoch: [4][  0/352]	Time  0.428 ( 0.428)	Data  0.258 ( 0.258)	Loss 2.0419e-01 (2.0419e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
07-Mar-22 02:49:56 - Epoch: [4][ 10/352]	Time  0.167 ( 0.189)	Data  0.002 ( 0.025)	Loss 7.0146e-02 (1.1497e-01)	Acc@1  98.44 ( 96.16)	Acc@5 100.00 (100.00)
07-Mar-22 02:49:57 - Epoch: [4][ 20/352]	Time  0.170 ( 0.178)	Data  0.002 ( 0.014)	Loss 1.6990e-01 (1.2369e-01)	Acc@1  95.31 ( 95.83)	Acc@5 100.00 ( 99.89)
07-Mar-22 02:49:59 - Epoch: [4][ 30/352]	Time  0.167 ( 0.175)	Data  0.002 ( 0.010)	Loss 1.0979e-01 (1.2319e-01)	Acc@1  96.09 ( 95.72)	Acc@5 100.00 ( 99.90)
07-Mar-22 02:50:01 - Epoch: [4][ 40/352]	Time  0.171 ( 0.173)	Data  0.003 ( 0.008)	Loss 9.5551e-02 (1.2212e-01)	Acc@1  96.09 ( 95.79)	Acc@5 100.00 ( 99.92)
07-Mar-22 02:50:02 - Epoch: [4][ 50/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.007)	Loss 1.0782e-01 (1.2153e-01)	Acc@1  96.88 ( 95.89)	Acc@5 100.00 ( 99.94)
07-Mar-22 02:50:04 - Epoch: [4][ 60/352]	Time  0.170 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.4044e-01 (1.1739e-01)	Acc@1  94.53 ( 96.03)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:06 - Epoch: [4][ 70/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.006)	Loss 1.2002e-01 (1.1817e-01)	Acc@1  95.31 ( 95.98)	Acc@5 100.00 ( 99.93)
07-Mar-22 02:50:07 - Epoch: [4][ 80/352]	Time  0.161 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.4819e-01 (1.1713e-01)	Acc@1  95.31 ( 96.02)	Acc@5 100.00 ( 99.94)
07-Mar-22 02:50:09 - Epoch: [4][ 90/352]	Time  0.157 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.0273e-01 (1.1556e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:11 - Epoch: [4][100/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.2546e-01 (1.1361e-01)	Acc@1  96.88 ( 96.18)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:12 - Epoch: [4][110/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.4072e-02 (1.1339e-01)	Acc@1  97.66 ( 96.16)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:14 - Epoch: [4][120/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 9.5568e-02 (1.1332e-01)	Acc@1  95.31 ( 96.13)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:16 - Epoch: [4][130/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.9939e-02 (1.1411e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:17 - Epoch: [4][140/352]	Time  0.164 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.5096e-01 (1.1505e-01)	Acc@1  94.53 ( 95.99)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:19 - Epoch: [4][150/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.004)	Loss 2.0888e-01 (1.1639e-01)	Acc@1  94.53 ( 95.96)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:21 - Epoch: [4][160/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2104e-01 (1.1725e-01)	Acc@1  96.09 ( 95.89)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:22 - Epoch: [4][170/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.1903e-01 (1.1809e-01)	Acc@1  96.09 ( 95.88)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:24 - Epoch: [4][180/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2722e-01 (1.1741e-01)	Acc@1  92.97 ( 95.88)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:26 - Epoch: [4][190/352]	Time  0.157 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.8628e-02 (1.1745e-01)	Acc@1  97.66 ( 95.87)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:27 - Epoch: [4][200/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0051e-01 (1.1698e-01)	Acc@1  96.88 ( 95.91)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:29 - Epoch: [4][210/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.8544e-01 (1.1611e-01)	Acc@1  95.31 ( 95.97)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:31 - Epoch: [4][220/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.5709e-02 (1.1607e-01)	Acc@1  98.44 ( 95.99)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:50:32 - Epoch: [4][230/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.6037e-01 (1.1553e-01)	Acc@1  93.75 ( 95.99)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:34 - Epoch: [4][240/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0788e-01 (1.1475e-01)	Acc@1  95.31 ( 96.03)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:36 - Epoch: [4][250/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.0540e-02 (1.1398e-01)	Acc@1  96.88 ( 96.05)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:37 - Epoch: [4][260/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.9206e-02 (1.1371e-01)	Acc@1  96.88 ( 96.05)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:39 - Epoch: [4][270/352]	Time  0.142 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4243e-01 (1.1347e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:40 - Epoch: [4][280/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3534e-01 (1.1421e-01)	Acc@1  93.75 ( 96.00)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:42 - Epoch: [4][290/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4218e-01 (1.1471e-01)	Acc@1  96.09 ( 95.99)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:44 - Epoch: [4][300/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.1932e-02 (1.1489e-01)	Acc@1  98.44 ( 96.00)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:45 - Epoch: [4][310/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3555e-01 (1.1475e-01)	Acc@1  97.66 ( 96.01)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:47 - Epoch: [4][320/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3276e-01 (1.1363e-01)	Acc@1  94.53 ( 96.05)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:49 - Epoch: [4][330/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2236e-01 (1.1370e-01)	Acc@1  94.53 ( 96.04)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:50:51 - Epoch: [4][340/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.7064e-02 (1.1329e-01)	Acc@1  96.88 ( 96.04)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:50:52 - Epoch: [4][350/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4681e-01 (1.1311e-01)	Acc@1  94.53 ( 96.04)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:50:53 - Test: [ 0/20]	Time  0.400 ( 0.400)	Loss 4.3735e-01 (4.3735e-01)	Acc@1  87.11 ( 87.11)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:50:54 - Test: [10/20]	Time  0.098 ( 0.128)	Loss 3.7755e-01 (4.0494e-01)	Acc@1  89.06 ( 88.35)	Acc@5  99.61 ( 99.36)
07-Mar-22 02:50:55 -  * Acc@1 88.680 Acc@5 99.400
07-Mar-22 02:50:55 - Best acc at epoch 4: 88.68000030517578
07-Mar-22 02:50:55 - Epoch: [5][  0/352]	Time  0.415 ( 0.415)	Data  0.246 ( 0.246)	Loss 8.4952e-02 (8.4952e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 02:50:57 - Epoch: [5][ 10/352]	Time  0.165 ( 0.189)	Data  0.002 ( 0.024)	Loss 1.4818e-01 (1.2039e-01)	Acc@1  94.53 ( 95.81)	Acc@5 100.00 ( 99.93)
07-Mar-22 02:50:59 - Epoch: [5][ 20/352]	Time  0.165 ( 0.178)	Data  0.002 ( 0.014)	Loss 7.6700e-02 (1.1708e-01)	Acc@1  97.66 ( 95.91)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:51:00 - Epoch: [5][ 30/352]	Time  0.167 ( 0.174)	Data  0.002 ( 0.010)	Loss 1.4229e-01 (1.2111e-01)	Acc@1  95.31 ( 95.77)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:02 - Epoch: [5][ 40/352]	Time  0.165 ( 0.172)	Data  0.002 ( 0.008)	Loss 6.4886e-02 (1.2098e-01)	Acc@1  96.88 ( 95.75)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:04 - Epoch: [5][ 50/352]	Time  0.164 ( 0.171)	Data  0.002 ( 0.007)	Loss 8.0623e-02 (1.2001e-01)	Acc@1  96.88 ( 95.73)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:05 - Epoch: [5][ 60/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.006)	Loss 1.1802e-01 (1.1965e-01)	Acc@1  96.09 ( 95.76)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:51:07 - Epoch: [5][ 70/352]	Time  0.165 ( 0.170)	Data  0.002 ( 0.006)	Loss 6.1063e-02 (1.1582e-01)	Acc@1  97.66 ( 95.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:51:09 - Epoch: [5][ 80/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.1675e-01 (1.1587e-01)	Acc@1  96.88 ( 95.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:51:10 - Epoch: [5][ 90/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.1289e-01 (1.1654e-01)	Acc@1  96.09 ( 95.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:51:12 - Epoch: [5][100/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2118e-01 (1.1690e-01)	Acc@1  93.75 ( 95.82)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:14 - Epoch: [5][110/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.1503e-02 (1.1660e-01)	Acc@1  96.88 ( 95.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:15 - Epoch: [5][120/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.6912e-02 (1.1456e-01)	Acc@1  97.66 ( 95.95)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:17 - Epoch: [5][130/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.1486e-01 (1.1383e-01)	Acc@1  96.09 ( 96.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:19 - Epoch: [5][140/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2775e-01 (1.1267e-01)	Acc@1  94.53 ( 96.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:20 - Epoch: [5][150/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.2338e-02 (1.1315e-01)	Acc@1  96.88 ( 96.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:51:22 - Epoch: [5][160/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.6688e-01 (1.1408e-01)	Acc@1  96.88 ( 96.00)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:51:24 - Epoch: [5][170/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.1183e-02 (1.1393e-01)	Acc@1  97.66 ( 95.99)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:25 - Epoch: [5][180/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4594e-01 (1.1315e-01)	Acc@1  94.53 ( 96.05)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:27 - Epoch: [5][190/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.2235e-02 (1.1271e-01)	Acc@1  99.22 ( 96.08)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:29 - Epoch: [5][200/352]	Time  0.162 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0050e-01 (1.1339e-01)	Acc@1  96.88 ( 96.05)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:30 - Epoch: [5][210/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.3886e-02 (1.1425e-01)	Acc@1  96.88 ( 96.02)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:32 - Epoch: [5][220/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.6501e-01 (1.1320e-01)	Acc@1  93.75 ( 96.05)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:34 - Epoch: [5][230/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1262e-01 (1.1232e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:35 - Epoch: [5][240/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2859e-01 (1.1263e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:37 - Epoch: [5][250/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.2738e-02 (1.1230e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:39 - Epoch: [5][260/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1557e-01 (1.1174e-01)	Acc@1  97.66 ( 96.12)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:40 - Epoch: [5][270/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0445e-01 (1.1115e-01)	Acc@1  95.31 ( 96.14)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:42 - Epoch: [5][280/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1777e-01 (1.1104e-01)	Acc@1  96.09 ( 96.14)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:44 - Epoch: [5][290/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.5179e-01 (1.1095e-01)	Acc@1  96.09 ( 96.13)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:45 - Epoch: [5][300/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1407e-01 (1.1069e-01)	Acc@1  96.09 ( 96.12)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:47 - Epoch: [5][310/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2687e-01 (1.1061e-01)	Acc@1  93.75 ( 96.11)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:49 - Epoch: [5][320/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.1755e-02 (1.1108e-01)	Acc@1  97.66 ( 96.11)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:50 - Epoch: [5][330/352]	Time  0.171 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0923e-01 (1.1080e-01)	Acc@1  95.31 ( 96.12)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:52 - Epoch: [5][340/352]	Time  0.169 ( 0.167)	Data  0.001 ( 0.003)	Loss 2.0292e-01 (1.1096e-01)	Acc@1  93.75 ( 96.11)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:54 - Epoch: [5][350/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4626e-01 (1.1122e-01)	Acc@1  95.31 ( 96.08)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:51:54 - Test: [ 0/20]	Time  0.389 ( 0.389)	Loss 4.1934e-01 (4.1934e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:51:55 - Test: [10/20]	Time  0.097 ( 0.125)	Loss 3.2910e-01 (4.0358e-01)	Acc@1  90.62 ( 88.53)	Acc@5  99.22 ( 99.54)
07-Mar-22 02:51:56 -  * Acc@1 88.900 Acc@5 99.460
07-Mar-22 02:51:56 - Best acc at epoch 5: 88.9000015258789
07-Mar-22 02:51:57 - Epoch: [6][  0/352]	Time  0.375 ( 0.375)	Data  0.231 ( 0.231)	Loss 5.4320e-02 (5.4320e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 02:51:58 - Epoch: [6][ 10/352]	Time  0.161 ( 0.181)	Data  0.002 ( 0.023)	Loss 1.3950e-01 (9.9140e-02)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:00 - Epoch: [6][ 20/352]	Time  0.143 ( 0.164)	Data  0.002 ( 0.013)	Loss 1.1553e-01 (9.5985e-02)	Acc@1  95.31 ( 96.47)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:01 - Epoch: [6][ 30/352]	Time  0.157 ( 0.160)	Data  0.002 ( 0.009)	Loss 7.6308e-02 (9.7278e-02)	Acc@1  98.44 ( 96.65)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:03 - Epoch: [6][ 40/352]	Time  0.159 ( 0.159)	Data  0.003 ( 0.008)	Loss 1.2731e-01 (9.9956e-02)	Acc@1  92.97 ( 96.51)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:05 - Epoch: [6][ 50/352]	Time  0.157 ( 0.161)	Data  0.002 ( 0.007)	Loss 9.9399e-02 (1.0353e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:06 - Epoch: [6][ 60/352]	Time  0.142 ( 0.159)	Data  0.002 ( 0.006)	Loss 7.0821e-02 (1.0740e-01)	Acc@1  97.66 ( 96.32)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:08 - Epoch: [6][ 70/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.005)	Loss 1.7004e-01 (1.0669e-01)	Acc@1  93.75 ( 96.32)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:09 - Epoch: [6][ 80/352]	Time  0.142 ( 0.156)	Data  0.002 ( 0.005)	Loss 1.5059e-01 (1.0734e-01)	Acc@1  95.31 ( 96.34)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:11 - Epoch: [6][ 90/352]	Time  0.143 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.8997e-01 (1.0683e-01)	Acc@1  94.53 ( 96.36)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:12 - Epoch: [6][100/352]	Time  0.156 ( 0.156)	Data  0.001 ( 0.004)	Loss 1.3878e-01 (1.0749e-01)	Acc@1  95.31 ( 96.34)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:14 - Epoch: [6][110/352]	Time  0.156 ( 0.156)	Data  0.002 ( 0.004)	Loss 8.2528e-02 (1.0906e-01)	Acc@1  98.44 ( 96.30)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:52:15 - Epoch: [6][120/352]	Time  0.143 ( 0.155)	Data  0.002 ( 0.004)	Loss 1.0246e-01 (1.1147e-01)	Acc@1  95.31 ( 96.20)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:52:17 - Epoch: [6][130/352]	Time  0.168 ( 0.155)	Data  0.002 ( 0.004)	Loss 1.0513e-01 (1.1246e-01)	Acc@1  96.88 ( 96.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:52:18 - Epoch: [6][140/352]	Time  0.158 ( 0.155)	Data  0.002 ( 0.004)	Loss 1.1598e-01 (1.1132e-01)	Acc@1  96.88 ( 96.21)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:52:20 - Epoch: [6][150/352]	Time  0.157 ( 0.155)	Data  0.002 ( 0.003)	Loss 7.1115e-02 (1.1035e-01)	Acc@1  98.44 ( 96.22)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:52:22 - Epoch: [6][160/352]	Time  0.158 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.1972e-01 (1.1161e-01)	Acc@1  96.09 ( 96.17)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:23 - Epoch: [6][170/352]	Time  0.153 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.2362e-01 (1.1129e-01)	Acc@1  96.88 ( 96.20)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:25 - Epoch: [6][180/352]	Time  0.157 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.3027e-01 (1.1174e-01)	Acc@1  93.75 ( 96.18)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:26 - Epoch: [6][190/352]	Time  0.164 ( 0.156)	Data  0.002 ( 0.003)	Loss 4.4356e-02 (1.1118e-01)	Acc@1  98.44 ( 96.21)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:52:28 - Epoch: [6][200/352]	Time  0.149 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.4216e-02 (1.1029e-01)	Acc@1  96.88 ( 96.22)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:52:29 - Epoch: [6][210/352]	Time  0.158 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.7613e-02 (1.0981e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:52:31 - Epoch: [6][220/352]	Time  0.150 ( 0.156)	Data  0.002 ( 0.003)	Loss 4.7622e-02 (1.0928e-01)	Acc@1  99.22 ( 96.24)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:52:32 - Epoch: [6][230/352]	Time  0.143 ( 0.155)	Data  0.002 ( 0.003)	Loss 5.3372e-02 (1.0907e-01)	Acc@1  96.88 ( 96.24)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:34 - Epoch: [6][240/352]	Time  0.156 ( 0.155)	Data  0.002 ( 0.003)	Loss 7.9684e-02 (1.0842e-01)	Acc@1  97.66 ( 96.28)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:35 - Epoch: [6][250/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.6837e-01 (1.0857e-01)	Acc@1  94.53 ( 96.26)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:37 - Epoch: [6][260/352]	Time  0.154 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.1444e-01 (1.0857e-01)	Acc@1  94.53 ( 96.25)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:38 - Epoch: [6][270/352]	Time  0.143 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.2142e-01 (1.0903e-01)	Acc@1  96.09 ( 96.24)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:40 - Epoch: [6][280/352]	Time  0.166 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.0493e-01 (1.0894e-01)	Acc@1  97.66 ( 96.25)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:42 - Epoch: [6][290/352]	Time  0.168 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.2969e-01 (1.0935e-01)	Acc@1  94.53 ( 96.22)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:43 - Epoch: [6][300/352]	Time  0.166 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.5894e-02 (1.0919e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:45 - Epoch: [6][310/352]	Time  0.166 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.4270e-02 (1.0918e-01)	Acc@1  95.31 ( 96.20)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:47 - Epoch: [6][320/352]	Time  0.165 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.2074e-01 (1.0872e-01)	Acc@1  95.31 ( 96.22)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:48 - Epoch: [6][330/352]	Time  0.164 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.7344e-01 (1.0855e-01)	Acc@1  94.53 ( 96.22)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:50 - Epoch: [6][340/352]	Time  0.166 ( 0.158)	Data  0.002 ( 0.003)	Loss 2.1669e-01 (1.0890e-01)	Acc@1  95.31 ( 96.21)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:52 - Epoch: [6][350/352]	Time  0.166 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.0983e-01 (1.0878e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:52:52 - Test: [ 0/20]	Time  0.380 ( 0.380)	Loss 4.3238e-01 (4.3238e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.61 ( 99.61)
07-Mar-22 02:52:53 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 3.4207e-01 (4.1390e-01)	Acc@1  88.28 ( 87.89)	Acc@5  99.22 ( 99.36)
07-Mar-22 02:52:54 -  * Acc@1 88.180 Acc@5 99.360
07-Mar-22 02:52:54 - Best acc at epoch 6: 88.9000015258789
07-Mar-22 02:52:55 - Epoch: [7][  0/352]	Time  0.373 ( 0.373)	Data  0.224 ( 0.224)	Loss 7.3358e-02 (7.3358e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
07-Mar-22 02:52:56 - Epoch: [7][ 10/352]	Time  0.165 ( 0.183)	Data  0.002 ( 0.022)	Loss 1.5724e-01 (1.0043e-01)	Acc@1  94.53 ( 96.24)	Acc@5 100.00 ( 99.93)
07-Mar-22 02:52:58 - Epoch: [7][ 20/352]	Time  0.173 ( 0.176)	Data  0.003 ( 0.013)	Loss 1.1571e-01 (1.0021e-01)	Acc@1  96.09 ( 96.47)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:53:00 - Epoch: [7][ 30/352]	Time  0.171 ( 0.173)	Data  0.002 ( 0.009)	Loss 1.1291e-01 (1.0294e-01)	Acc@1  93.75 ( 96.37)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:01 - Epoch: [7][ 40/352]	Time  0.164 ( 0.172)	Data  0.002 ( 0.008)	Loss 3.1198e-01 (1.1180e-01)	Acc@1  88.28 ( 96.11)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:53:03 - Epoch: [7][ 50/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.006)	Loss 1.0085e-01 (1.1315e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:05 - Epoch: [7][ 60/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.006)	Loss 1.2628e-01 (1.1272e-01)	Acc@1  94.53 ( 95.99)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:06 - Epoch: [7][ 70/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.2149e-01 (1.1204e-01)	Acc@1  95.31 ( 95.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:08 - Epoch: [7][ 80/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.005)	Loss 8.0795e-02 (1.1339e-01)	Acc@1  96.88 ( 95.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:10 - Epoch: [7][ 90/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.0853e-01 (1.1269e-01)	Acc@1  96.09 ( 96.02)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:11 - Epoch: [7][100/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.3295e-01 (1.1396e-01)	Acc@1  94.53 ( 95.95)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:13 - Epoch: [7][110/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.2303e-02 (1.0990e-01)	Acc@1  98.44 ( 96.10)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:15 - Epoch: [7][120/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.8521e-02 (1.0993e-01)	Acc@1  98.44 ( 96.10)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:53:16 - Epoch: [7][130/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.8872e-01 (1.1102e-01)	Acc@1  94.53 ( 96.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:18 - Epoch: [7][140/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.7250e-02 (1.1106e-01)	Acc@1  97.66 ( 96.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:20 - Epoch: [7][150/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.8212e-01 (1.1066e-01)	Acc@1  93.75 ( 96.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:21 - Epoch: [7][160/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4782e-01 (1.1222e-01)	Acc@1  96.09 ( 96.05)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:23 - Epoch: [7][170/352]	Time  0.141 ( 0.168)	Data  0.001 ( 0.003)	Loss 9.9704e-02 (1.1087e-01)	Acc@1  95.31 ( 96.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:25 - Epoch: [7][180/352]	Time  0.141 ( 0.166)	Data  0.001 ( 0.003)	Loss 1.1502e-01 (1.1202e-01)	Acc@1  96.88 ( 96.09)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:26 - Epoch: [7][190/352]	Time  0.166 ( 0.166)	Data  0.001 ( 0.003)	Loss 6.5999e-02 (1.1042e-01)	Acc@1  96.88 ( 96.15)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:28 - Epoch: [7][200/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0571e-01 (1.1127e-01)	Acc@1  96.09 ( 96.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:30 - Epoch: [7][210/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.2386e-01 (1.1100e-01)	Acc@1  95.31 ( 96.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:31 - Epoch: [7][220/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.5613e-02 (1.1128e-01)	Acc@1  97.66 ( 96.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:33 - Epoch: [7][230/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.6952e-01 (1.1137e-01)	Acc@1  94.53 ( 96.13)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:35 - Epoch: [7][240/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.6700e-01 (1.1131e-01)	Acc@1  93.75 ( 96.11)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:53:36 - Epoch: [7][250/352]	Time  0.170 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.1506e-02 (1.1053e-01)	Acc@1  97.66 ( 96.13)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:38 - Epoch: [7][260/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.2395e-01 (1.1097e-01)	Acc@1  96.09 ( 96.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:39 - Epoch: [7][270/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.9438e-02 (1.1061e-01)	Acc@1  96.09 ( 96.15)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:41 - Epoch: [7][280/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.3471e-01 (1.1081e-01)	Acc@1  96.09 ( 96.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:43 - Epoch: [7][290/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0575e-01 (1.1078e-01)	Acc@1  96.09 ( 96.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:44 - Epoch: [7][300/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.6306e-01 (1.1085e-01)	Acc@1  94.53 ( 96.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:46 - Epoch: [7][310/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.1800e-02 (1.1074e-01)	Acc@1  98.44 ( 96.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:48 - Epoch: [7][320/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.4022e-01 (1.1115e-01)	Acc@1  93.75 ( 96.09)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:49 - Epoch: [7][330/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1870e-01 (1.1093e-01)	Acc@1  95.31 ( 96.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:51 - Epoch: [7][340/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 4.8221e-02 (1.1108e-01)	Acc@1  98.44 ( 96.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:53:53 - Epoch: [7][350/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1797e-01 (1.1114e-01)	Acc@1  97.66 ( 96.10)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:53:53 - Test: [ 0/20]	Time  0.498 ( 0.498)	Loss 3.8326e-01 (3.8326e-01)	Acc@1  89.06 ( 89.06)	Acc@5  98.83 ( 98.83)
07-Mar-22 02:53:54 - Test: [10/20]	Time  0.098 ( 0.136)	Loss 3.1705e-01 (4.0499e-01)	Acc@1  91.41 ( 88.46)	Acc@5  99.22 ( 99.43)
07-Mar-22 02:53:55 -  * Acc@1 88.840 Acc@5 99.400
07-Mar-22 02:53:55 - Best acc at epoch 7: 88.9000015258789
07-Mar-22 02:53:56 - Epoch: [8][  0/352]	Time  0.393 ( 0.393)	Data  0.234 ( 0.234)	Loss 7.5397e-02 (7.5397e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 02:53:58 - Epoch: [8][ 10/352]	Time  0.167 ( 0.188)	Data  0.002 ( 0.023)	Loss 1.3058e-01 (1.0984e-01)	Acc@1  92.97 ( 95.74)	Acc@5 100.00 (100.00)
07-Mar-22 02:53:59 - Epoch: [8][ 20/352]	Time  0.169 ( 0.178)	Data  0.002 ( 0.013)	Loss 8.5855e-02 (1.0854e-01)	Acc@1  97.66 ( 95.98)	Acc@5 100.00 (100.00)
07-Mar-22 02:54:01 - Epoch: [8][ 30/352]	Time  0.164 ( 0.175)	Data  0.002 ( 0.009)	Loss 7.7915e-02 (1.0506e-01)	Acc@1  97.66 ( 96.27)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:02 - Epoch: [8][ 40/352]	Time  0.146 ( 0.169)	Data  0.002 ( 0.008)	Loss 5.9924e-02 (1.0688e-01)	Acc@1  98.44 ( 96.27)	Acc@5 100.00 ( 99.94)
07-Mar-22 02:54:04 - Epoch: [8][ 50/352]	Time  0.141 ( 0.164)	Data  0.001 ( 0.006)	Loss 1.0735e-01 (1.0712e-01)	Acc@1  95.31 ( 96.29)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:54:05 - Epoch: [8][ 60/352]	Time  0.140 ( 0.160)	Data  0.001 ( 0.006)	Loss 8.1442e-02 (1.0458e-01)	Acc@1  95.31 ( 96.36)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:54:07 - Epoch: [8][ 70/352]	Time  0.140 ( 0.157)	Data  0.001 ( 0.005)	Loss 5.3759e-02 (1.0206e-01)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:08 - Epoch: [8][ 80/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.005)	Loss 1.3916e-01 (1.0299e-01)	Acc@1  93.75 ( 96.41)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:10 - Epoch: [8][ 90/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.004)	Loss 1.0246e-01 (1.0461e-01)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:12 - Epoch: [8][100/352]	Time  0.175 ( 0.159)	Data  0.002 ( 0.004)	Loss 1.2843e-01 (1.0374e-01)	Acc@1  95.31 ( 96.47)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:13 - Epoch: [8][110/352]	Time  0.165 ( 0.160)	Data  0.002 ( 0.004)	Loss 1.0292e-01 (1.0386e-01)	Acc@1  95.31 ( 96.43)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:54:15 - Epoch: [8][120/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.004)	Loss 1.2360e-01 (1.0573e-01)	Acc@1  94.53 ( 96.33)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:16 - Epoch: [8][130/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.004)	Loss 7.4471e-02 (1.0607e-01)	Acc@1  97.66 ( 96.30)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:18 - Epoch: [8][140/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.004)	Loss 9.8252e-02 (1.0553e-01)	Acc@1  96.09 ( 96.32)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:20 - Epoch: [8][150/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.003)	Loss 5.4466e-02 (1.0469e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:21 - Epoch: [8][160/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.7444e-02 (1.0413e-01)	Acc@1  98.44 ( 96.39)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:23 - Epoch: [8][170/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.5384e-01 (1.0425e-01)	Acc@1  93.75 ( 96.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:25 - Epoch: [8][180/352]	Time  0.168 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.3371e-02 (1.0426e-01)	Acc@1  98.44 ( 96.38)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:26 - Epoch: [8][190/352]	Time  0.141 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.9381e-02 (1.0435e-01)	Acc@1  98.44 ( 96.41)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:28 - Epoch: [8][200/352]	Time  0.140 ( 0.161)	Data  0.001 ( 0.003)	Loss 1.0037e-01 (1.0433e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:29 - Epoch: [8][210/352]	Time  0.141 ( 0.160)	Data  0.001 ( 0.003)	Loss 1.2895e-01 (1.0396e-01)	Acc@1  93.75 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:31 - Epoch: [8][220/352]	Time  0.141 ( 0.159)	Data  0.001 ( 0.003)	Loss 1.5891e-01 (1.0438e-01)	Acc@1  93.75 ( 96.37)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:32 - Epoch: [8][230/352]	Time  0.141 ( 0.158)	Data  0.001 ( 0.003)	Loss 1.3612e-01 (1.0540e-01)	Acc@1  96.88 ( 96.35)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:34 - Epoch: [8][240/352]	Time  0.167 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.0916e-02 (1.0477e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:35 - Epoch: [8][250/352]	Time  0.166 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.0406e-01 (1.0502e-01)	Acc@1  96.88 ( 96.34)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:37 - Epoch: [8][260/352]	Time  0.166 ( 0.159)	Data  0.002 ( 0.003)	Loss 6.9372e-02 (1.0458e-01)	Acc@1  98.44 ( 96.36)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:38 - Epoch: [8][270/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.003)	Loss 2.0958e-01 (1.0543e-01)	Acc@1  92.19 ( 96.34)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:40 - Epoch: [8][280/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.2355e-01 (1.0541e-01)	Acc@1  93.75 ( 96.33)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:54:41 - Epoch: [8][290/352]	Time  0.142 ( 0.157)	Data  0.002 ( 0.003)	Loss 9.4640e-02 (1.0541e-01)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:43 - Epoch: [8][300/352]	Time  0.166 ( 0.157)	Data  0.002 ( 0.003)	Loss 6.7193e-02 (1.0519e-01)	Acc@1  98.44 ( 96.34)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:44 - Epoch: [8][310/352]	Time  0.165 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.0103e-01 (1.0532e-01)	Acc@1  96.09 ( 96.34)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:46 - Epoch: [8][320/352]	Time  0.166 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.1527e-01 (1.0587e-01)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:48 - Epoch: [8][330/352]	Time  0.165 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.2113e-01 (1.0557e-01)	Acc@1  96.09 ( 96.33)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:49 - Epoch: [8][340/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.2760e-01 (1.0571e-01)	Acc@1  96.88 ( 96.34)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:51 - Epoch: [8][350/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.0827e-01 (1.0615e-01)	Acc@1  95.31 ( 96.32)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:54:52 - Test: [ 0/20]	Time  0.363 ( 0.363)	Loss 3.7319e-01 (3.7319e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.61 ( 99.61)
07-Mar-22 02:54:53 - Test: [10/20]	Time  0.098 ( 0.126)	Loss 3.9658e-01 (4.0719e-01)	Acc@1  87.89 ( 88.25)	Acc@5  98.83 ( 99.50)
07-Mar-22 02:54:54 -  * Acc@1 88.620 Acc@5 99.440
07-Mar-22 02:54:54 - Best acc at epoch 8: 88.9000015258789
07-Mar-22 02:54:54 - Epoch: [9][  0/352]	Time  0.396 ( 0.396)	Data  0.231 ( 0.231)	Loss 5.3302e-02 (5.3302e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
07-Mar-22 02:54:56 - Epoch: [9][ 10/352]	Time  0.171 ( 0.188)	Data  0.002 ( 0.023)	Loss 9.8116e-02 (1.1636e-01)	Acc@1  98.44 ( 96.45)	Acc@5  99.22 ( 99.93)
07-Mar-22 02:54:57 - Epoch: [9][ 20/352]	Time  0.142 ( 0.169)	Data  0.002 ( 0.013)	Loss 7.9407e-02 (1.0700e-01)	Acc@1  97.66 ( 96.32)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:54:59 - Epoch: [9][ 30/352]	Time  0.141 ( 0.160)	Data  0.002 ( 0.009)	Loss 7.3920e-02 (1.0108e-01)	Acc@1  96.88 ( 96.35)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:55:00 - Epoch: [9][ 40/352]	Time  0.151 ( 0.156)	Data  0.002 ( 0.007)	Loss 3.4497e-02 (9.8892e-02)	Acc@1 100.00 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:02 - Epoch: [9][ 50/352]	Time  0.143 ( 0.154)	Data  0.002 ( 0.006)	Loss 9.2972e-02 (9.6987e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:03 - Epoch: [9][ 60/352]	Time  0.142 ( 0.152)	Data  0.001 ( 0.006)	Loss 1.9594e-01 (9.8685e-02)	Acc@1  92.19 ( 96.44)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:04 - Epoch: [9][ 70/352]	Time  0.142 ( 0.150)	Data  0.002 ( 0.005)	Loss 1.0866e-01 (9.8423e-02)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:06 - Epoch: [9][ 80/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.005)	Loss 6.9205e-02 (9.8501e-02)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:07 - Epoch: [9][ 90/352]	Time  0.165 ( 0.149)	Data  0.002 ( 0.004)	Loss 2.0183e-01 (9.8657e-02)	Acc@1  90.62 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:09 - Epoch: [9][100/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.004)	Loss 1.1419e-01 (9.9817e-02)	Acc@1  96.09 ( 96.38)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:10 - Epoch: [9][110/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.004)	Loss 8.0921e-02 (9.8142e-02)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:12 - Epoch: [9][120/352]	Time  0.143 ( 0.148)	Data  0.002 ( 0.004)	Loss 1.3029e-01 (9.8457e-02)	Acc@1  92.97 ( 96.41)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:13 - Epoch: [9][130/352]	Time  0.143 ( 0.148)	Data  0.002 ( 0.004)	Loss 1.7670e-01 (9.9408e-02)	Acc@1  92.97 ( 96.36)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:15 - Epoch: [9][140/352]	Time  0.143 ( 0.148)	Data  0.002 ( 0.003)	Loss 5.8302e-02 (1.0010e-01)	Acc@1  98.44 ( 96.34)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:16 - Epoch: [9][150/352]	Time  0.142 ( 0.147)	Data  0.002 ( 0.003)	Loss 1.1948e-01 (9.9305e-02)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:17 - Epoch: [9][160/352]	Time  0.142 ( 0.147)	Data  0.002 ( 0.003)	Loss 1.7919e-01 (9.9827e-02)	Acc@1  91.41 ( 96.37)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:19 - Epoch: [9][170/352]	Time  0.141 ( 0.147)	Data  0.001 ( 0.003)	Loss 7.8314e-02 (9.9611e-02)	Acc@1  97.66 ( 96.37)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:20 - Epoch: [9][180/352]	Time  0.144 ( 0.147)	Data  0.002 ( 0.003)	Loss 1.6091e-01 (1.0098e-01)	Acc@1  94.53 ( 96.29)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:55:22 - Epoch: [9][190/352]	Time  0.142 ( 0.147)	Data  0.002 ( 0.003)	Loss 9.6006e-02 (1.0122e-01)	Acc@1  95.31 ( 96.27)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:23 - Epoch: [9][200/352]	Time  0.143 ( 0.146)	Data  0.002 ( 0.003)	Loss 8.5899e-02 (1.0167e-01)	Acc@1  96.09 ( 96.23)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:25 - Epoch: [9][210/352]	Time  0.151 ( 0.147)	Data  0.002 ( 0.003)	Loss 7.2681e-02 (1.0047e-01)	Acc@1  97.66 ( 96.28)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:26 - Epoch: [9][220/352]	Time  0.141 ( 0.147)	Data  0.002 ( 0.003)	Loss 9.9178e-02 (1.0078e-01)	Acc@1  97.66 ( 96.28)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:55:27 - Epoch: [9][230/352]	Time  0.141 ( 0.146)	Data  0.002 ( 0.003)	Loss 9.0871e-02 (1.0104e-01)	Acc@1  97.66 ( 96.27)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:29 - Epoch: [9][240/352]	Time  0.165 ( 0.147)	Data  0.002 ( 0.003)	Loss 4.8228e-02 (1.0072e-01)	Acc@1  98.44 ( 96.29)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:31 - Epoch: [9][250/352]	Time  0.167 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.1890e-02 (1.0077e-01)	Acc@1  96.09 ( 96.30)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:32 - Epoch: [9][260/352]	Time  0.163 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1990e-01 (1.0082e-01)	Acc@1  96.09 ( 96.32)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:34 - Epoch: [9][270/352]	Time  0.165 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.6451e-02 (1.0096e-01)	Acc@1  97.66 ( 96.31)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:36 - Epoch: [9][280/352]	Time  0.187 ( 0.150)	Data  0.003 ( 0.003)	Loss 2.0322e-01 (1.0133e-01)	Acc@1  91.41 ( 96.29)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:37 - Epoch: [9][290/352]	Time  0.152 ( 0.150)	Data  0.002 ( 0.003)	Loss 2.4205e-01 (1.0221e-01)	Acc@1  93.75 ( 96.28)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:39 - Epoch: [9][300/352]	Time  0.147 ( 0.150)	Data  0.002 ( 0.003)	Loss 3.6004e-02 (1.0165e-01)	Acc@1  99.22 ( 96.30)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:40 - Epoch: [9][310/352]	Time  0.167 ( 0.150)	Data  0.002 ( 0.003)	Loss 5.3362e-02 (1.0122e-01)	Acc@1  98.44 ( 96.32)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:42 - Epoch: [9][320/352]	Time  0.168 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.1083e-01 (1.0171e-01)	Acc@1  96.09 ( 96.32)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:44 - Epoch: [9][330/352]	Time  0.171 ( 0.151)	Data  0.002 ( 0.003)	Loss 6.5085e-02 (1.0201e-01)	Acc@1  98.44 ( 96.29)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:55:45 - Epoch: [9][340/352]	Time  0.168 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.2376e-01 (1.0201e-01)	Acc@1  94.53 ( 96.28)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:47 - Epoch: [9][350/352]	Time  0.166 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.5783e-01 (1.0230e-01)	Acc@1  94.53 ( 96.27)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:55:48 - Test: [ 0/20]	Time  0.353 ( 0.353)	Loss 3.6346e-01 (3.6346e-01)	Acc@1  88.67 ( 88.67)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:55:49 - Test: [10/20]	Time  0.113 ( 0.126)	Loss 3.2340e-01 (3.9327e-01)	Acc@1  89.06 ( 88.07)	Acc@5  99.61 ( 99.43)
07-Mar-22 02:55:50 -  * Acc@1 88.580 Acc@5 99.360
07-Mar-22 02:55:50 - Best acc at epoch 9: 88.9000015258789
07-Mar-22 02:55:50 - Epoch: [10][  0/352]	Time  0.409 ( 0.409)	Data  0.264 ( 0.264)	Loss 1.8460e-01 (1.8460e-01)	Acc@1  96.09 ( 96.09)	Acc@5  98.44 ( 98.44)
07-Mar-22 02:55:52 - Epoch: [10][ 10/352]	Time  0.154 ( 0.182)	Data  0.002 ( 0.026)	Loss 1.0346e-01 (1.1432e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.86)
07-Mar-22 02:55:53 - Epoch: [10][ 20/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.014)	Loss 7.4532e-02 (1.0777e-01)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.93)
07-Mar-22 02:55:55 - Epoch: [10][ 30/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.011)	Loss 1.1520e-01 (1.0606e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:55:57 - Epoch: [10][ 40/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.009)	Loss 1.2154e-01 (1.0374e-01)	Acc@1  94.53 ( 96.42)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:55:59 - Epoch: [10][ 50/352]	Time  0.173 ( 0.168)	Data  0.002 ( 0.007)	Loss 1.2862e-01 (1.0729e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:00 - Epoch: [10][ 60/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.007)	Loss 7.9608e-02 (1.0620e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:56:02 - Epoch: [10][ 70/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.006)	Loss 8.8770e-02 (1.0778e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:04 - Epoch: [10][ 80/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.005)	Loss 5.4177e-02 (1.0606e-01)	Acc@1  98.44 ( 96.33)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:05 - Epoch: [10][ 90/352]	Time  0.130 ( 0.166)	Data  0.001 ( 0.005)	Loss 5.9837e-02 (1.0282e-01)	Acc@1  98.44 ( 96.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:07 - Epoch: [10][100/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.005)	Loss 8.4011e-02 (1.0254e-01)	Acc@1  97.66 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:08 - Epoch: [10][110/352]	Time  0.159 ( 0.165)	Data  0.002 ( 0.005)	Loss 1.4524e-01 (1.0223e-01)	Acc@1  95.31 ( 96.40)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:10 - Epoch: [10][120/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.004)	Loss 5.2823e-02 (1.0103e-01)	Acc@1  97.66 ( 96.46)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:12 - Epoch: [10][130/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.004)	Loss 7.8741e-02 (1.0075e-01)	Acc@1  96.09 ( 96.49)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:13 - Epoch: [10][140/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.004)	Loss 1.5074e-01 (1.0044e-01)	Acc@1  92.97 ( 96.45)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:15 - Epoch: [10][150/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.004)	Loss 7.2376e-02 (9.8800e-02)	Acc@1  96.88 ( 96.53)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:17 - Epoch: [10][160/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.004)	Loss 1.1622e-01 (1.0053e-01)	Acc@1  94.53 ( 96.47)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:18 - Epoch: [10][170/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.1722e-01 (1.0063e-01)	Acc@1  94.53 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:20 - Epoch: [10][180/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.4432e-01 (1.0009e-01)	Acc@1  94.53 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:22 - Epoch: [10][190/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.3151e-02 (9.9254e-02)	Acc@1  95.31 ( 96.44)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:23 - Epoch: [10][200/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1785e-01 (9.9918e-02)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:25 - Epoch: [10][210/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.1461e-02 (9.9782e-02)	Acc@1  98.44 ( 96.45)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:27 - Epoch: [10][220/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.4261e-02 (9.9850e-02)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:28 - Epoch: [10][230/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 2.1873e-02 (9.9905e-02)	Acc@1 100.00 ( 96.45)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:30 - Epoch: [10][240/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.4609e-01 (1.0056e-01)	Acc@1  92.97 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:32 - Epoch: [10][250/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 4.9207e-02 (1.0041e-01)	Acc@1  99.22 ( 96.45)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:33 - Epoch: [10][260/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.1891e-02 (1.0057e-01)	Acc@1  98.44 ( 96.44)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:35 - Epoch: [10][270/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.0152e-02 (1.0075e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:37 - Epoch: [10][280/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.8896e-02 (1.0074e-01)	Acc@1  96.09 ( 96.44)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:38 - Epoch: [10][290/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0009e-01 (1.0103e-01)	Acc@1  95.31 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:40 - Epoch: [10][300/352]	Time  0.170 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.5433e-01 (1.0169e-01)	Acc@1  95.31 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:42 - Epoch: [10][310/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.5748e-01 (1.0227e-01)	Acc@1  93.75 ( 96.38)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:43 - Epoch: [10][320/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.3670e-02 (1.0221e-01)	Acc@1  98.44 ( 96.37)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:45 - Epoch: [10][330/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.0965e-02 (1.0197e-01)	Acc@1  96.88 ( 96.37)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:47 - Epoch: [10][340/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.6692e-01 (1.0164e-01)	Acc@1  92.97 ( 96.38)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:56:48 - Epoch: [10][350/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.3699e-01 (1.0156e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:56:49 - Test: [ 0/20]	Time  0.384 ( 0.384)	Loss 3.5009e-01 (3.5009e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:56:50 - Test: [10/20]	Time  0.120 ( 0.128)	Loss 3.6599e-01 (3.9475e-01)	Acc@1  89.06 ( 88.67)	Acc@5  98.83 ( 99.47)
07-Mar-22 02:56:51 -  * Acc@1 89.000 Acc@5 99.360
07-Mar-22 02:56:51 - Best acc at epoch 10: 89.0
07-Mar-22 02:56:52 - Epoch: [11][  0/352]	Time  0.394 ( 0.394)	Data  0.227 ( 0.227)	Loss 9.4217e-02 (9.4217e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 02:56:53 - Epoch: [11][ 10/352]	Time  0.168 ( 0.186)	Data  0.002 ( 0.023)	Loss 1.4346e-01 (9.5861e-02)	Acc@1  93.75 ( 96.52)	Acc@5 100.00 (100.00)
07-Mar-22 02:56:55 - Epoch: [11][ 20/352]	Time  0.142 ( 0.173)	Data  0.002 ( 0.013)	Loss 1.4159e-01 (1.0546e-01)	Acc@1  94.53 ( 95.98)	Acc@5 100.00 (100.00)
07-Mar-22 02:56:56 - Epoch: [11][ 30/352]	Time  0.142 ( 0.163)	Data  0.001 ( 0.009)	Loss 1.4257e-01 (1.0704e-01)	Acc@1  93.75 ( 95.94)	Acc@5 100.00 (100.00)
07-Mar-22 02:56:58 - Epoch: [11][ 40/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.007)	Loss 7.3356e-02 (1.0501e-01)	Acc@1  98.44 ( 96.17)	Acc@5 100.00 (100.00)
07-Mar-22 02:56:59 - Epoch: [11][ 50/352]	Time  0.142 ( 0.155)	Data  0.002 ( 0.006)	Loss 1.5269e-01 (1.0538e-01)	Acc@1  93.75 ( 96.20)	Acc@5 100.00 (100.00)
07-Mar-22 02:57:00 - Epoch: [11][ 60/352]	Time  0.168 ( 0.154)	Data  0.002 ( 0.005)	Loss 9.2490e-02 (1.0181e-01)	Acc@1  96.09 ( 96.34)	Acc@5 100.00 (100.00)
07-Mar-22 02:57:02 - Epoch: [11][ 70/352]	Time  0.168 ( 0.156)	Data  0.002 ( 0.005)	Loss 9.6058e-02 (1.0090e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:04 - Epoch: [11][ 80/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.005)	Loss 1.6795e-01 (1.0143e-01)	Acc@1  93.75 ( 96.33)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:06 - Epoch: [11][ 90/352]	Time  0.169 ( 0.158)	Data  0.002 ( 0.004)	Loss 2.4297e-01 (1.0296e-01)	Acc@1  93.75 ( 96.30)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:07 - Epoch: [11][100/352]	Time  0.168 ( 0.159)	Data  0.002 ( 0.004)	Loss 8.1657e-02 (1.0265e-01)	Acc@1  96.88 ( 96.27)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:09 - Epoch: [11][110/352]	Time  0.167 ( 0.160)	Data  0.002 ( 0.004)	Loss 5.8632e-02 (1.0173e-01)	Acc@1  97.66 ( 96.28)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:11 - Epoch: [11][120/352]	Time  0.167 ( 0.160)	Data  0.002 ( 0.004)	Loss 5.8809e-02 (1.0144e-01)	Acc@1  98.44 ( 96.33)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:12 - Epoch: [11][130/352]	Time  0.168 ( 0.161)	Data  0.002 ( 0.004)	Loss 7.8307e-02 (1.0036e-01)	Acc@1  97.66 ( 96.39)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:14 - Epoch: [11][140/352]	Time  0.165 ( 0.161)	Data  0.002 ( 0.004)	Loss 1.3226e-01 (1.0017e-01)	Acc@1  94.53 ( 96.41)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:16 - Epoch: [11][150/352]	Time  0.165 ( 0.162)	Data  0.002 ( 0.004)	Loss 7.9213e-02 (1.0106e-01)	Acc@1  96.88 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:17 - Epoch: [11][160/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 9.0444e-02 (9.9860e-02)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 ( 99.99)
07-Mar-22 02:57:19 - Epoch: [11][170/352]	Time  0.165 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.1578e-01 (9.9003e-02)	Acc@1  95.31 ( 96.45)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:57:21 - Epoch: [11][180/352]	Time  0.168 ( 0.163)	Data  0.002 ( 0.003)	Loss 5.8414e-02 (9.8164e-02)	Acc@1  98.44 ( 96.48)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:22 - Epoch: [11][190/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.003)	Loss 5.9125e-02 (9.7648e-02)	Acc@1  98.44 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:24 - Epoch: [11][200/352]	Time  0.168 ( 0.163)	Data  0.002 ( 0.003)	Loss 9.3210e-02 (9.8099e-02)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:26 - Epoch: [11][210/352]	Time  0.164 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.2753e-01 (9.8299e-02)	Acc@1  95.31 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:27 - Epoch: [11][220/352]	Time  0.167 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1653e-01 (9.7934e-02)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:29 - Epoch: [11][230/352]	Time  0.169 ( 0.164)	Data  0.002 ( 0.003)	Loss 4.7006e-02 (9.8180e-02)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:31 - Epoch: [11][240/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.0338e-01 (9.8645e-02)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:32 - Epoch: [11][250/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.2492e-01 (9.8715e-02)	Acc@1  95.31 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:34 - Epoch: [11][260/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.7705e-02 (9.8609e-02)	Acc@1  98.44 ( 96.52)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:36 - Epoch: [11][270/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.4007e-01 (9.9323e-02)	Acc@1  93.75 ( 96.48)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:37 - Epoch: [11][280/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.9260e-01 (9.9665e-02)	Acc@1  95.31 ( 96.49)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:39 - Epoch: [11][290/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.7223e-01 (9.9394e-02)	Acc@1  94.53 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:41 - Epoch: [11][300/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.4577e-01 (9.9915e-02)	Acc@1  96.88 ( 96.49)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:42 - Epoch: [11][310/352]	Time  0.165 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.1463e-01 (1.0030e-01)	Acc@1  96.09 ( 96.47)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:57:44 - Epoch: [11][320/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.5829e-01 (1.0057e-01)	Acc@1  93.75 ( 96.46)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:46 - Epoch: [11][330/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.1608e-01 (1.0130e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:47 - Epoch: [11][340/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.2565e-01 (1.0175e-01)	Acc@1  96.88 ( 96.43)	Acc@5  99.22 ( 99.98)
07-Mar-22 02:57:49 - Epoch: [11][350/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.3666e-01 (1.0177e-01)	Acc@1  93.75 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:57:50 - Test: [ 0/20]	Time  0.366 ( 0.366)	Loss 3.3386e-01 (3.3386e-01)	Acc@1  91.02 ( 91.02)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:57:51 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 3.7670e-01 (4.0056e-01)	Acc@1  89.84 ( 88.39)	Acc@5  99.22 ( 99.50)
07-Mar-22 02:57:51 -  * Acc@1 89.020 Acc@5 99.460
07-Mar-22 02:57:52 - Best acc at epoch 11: 89.0199966430664
07-Mar-22 02:57:52 - Epoch: [12][  0/352]	Time  0.386 ( 0.386)	Data  0.241 ( 0.241)	Loss 1.0227e-01 (1.0227e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 02:57:54 - Epoch: [12][ 10/352]	Time  0.151 ( 0.177)	Data  0.002 ( 0.024)	Loss 6.1623e-02 (8.9903e-02)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 (100.00)
07-Mar-22 02:57:55 - Epoch: [12][ 20/352]	Time  0.161 ( 0.168)	Data  0.003 ( 0.013)	Loss 7.9204e-02 (9.4798e-02)	Acc@1  97.66 ( 96.73)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:57:57 - Epoch: [12][ 30/352]	Time  0.157 ( 0.168)	Data  0.002 ( 0.010)	Loss 1.0111e-01 (9.9129e-02)	Acc@1  95.31 ( 96.67)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:57:58 - Epoch: [12][ 40/352]	Time  0.162 ( 0.165)	Data  0.002 ( 0.008)	Loss 9.0243e-02 (9.8836e-02)	Acc@1  97.66 ( 96.65)	Acc@5 100.00 ( 99.94)
07-Mar-22 02:58:00 - Epoch: [12][ 50/352]	Time  0.141 ( 0.162)	Data  0.001 ( 0.007)	Loss 4.8575e-02 (1.0261e-01)	Acc@1  98.44 ( 96.32)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:58:01 - Epoch: [12][ 60/352]	Time  0.161 ( 0.160)	Data  0.002 ( 0.006)	Loss 1.1605e-01 (1.0419e-01)	Acc@1  96.09 ( 96.25)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:58:03 - Epoch: [12][ 70/352]	Time  0.155 ( 0.160)	Data  0.002 ( 0.005)	Loss 1.1519e-01 (1.0164e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:58:04 - Epoch: [12][ 80/352]	Time  0.147 ( 0.158)	Data  0.002 ( 0.005)	Loss 7.2888e-02 (1.0162e-01)	Acc@1  98.44 ( 96.36)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:58:06 - Epoch: [12][ 90/352]	Time  0.142 ( 0.157)	Data  0.002 ( 0.005)	Loss 1.2448e-01 (1.0139e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:07 - Epoch: [12][100/352]	Time  0.157 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.1526e-01 (1.0008e-01)	Acc@1  97.66 ( 96.53)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:09 - Epoch: [12][110/352]	Time  0.142 ( 0.155)	Data  0.002 ( 0.004)	Loss 9.9576e-02 (9.9085e-02)	Acc@1  94.53 ( 96.57)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:10 - Epoch: [12][120/352]	Time  0.163 ( 0.155)	Data  0.002 ( 0.004)	Loss 8.3046e-02 (9.9329e-02)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:12 - Epoch: [12][130/352]	Time  0.143 ( 0.154)	Data  0.002 ( 0.004)	Loss 1.3748e-01 (9.8916e-02)	Acc@1  97.66 ( 96.61)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:58:13 - Epoch: [12][140/352]	Time  0.160 ( 0.154)	Data  0.002 ( 0.004)	Loss 5.7215e-02 (9.9210e-02)	Acc@1  99.22 ( 96.62)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:58:15 - Epoch: [12][150/352]	Time  0.142 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.2662e-01 (1.0033e-01)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:17 - Epoch: [12][160/352]	Time  0.166 ( 0.155)	Data  0.002 ( 0.003)	Loss 1.4971e-01 (1.0152e-01)	Acc@1  94.53 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:58:18 - Epoch: [12][170/352]	Time  0.167 ( 0.156)	Data  0.002 ( 0.003)	Loss 2.1514e-01 (1.0208e-01)	Acc@1  91.41 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:58:20 - Epoch: [12][180/352]	Time  0.172 ( 0.156)	Data  0.002 ( 0.003)	Loss 2.0605e-01 (1.0296e-01)	Acc@1  95.31 ( 96.48)	Acc@5  99.22 ( 99.97)
07-Mar-22 02:58:22 - Epoch: [12][190/352]	Time  0.166 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.1958e-01 (1.0341e-01)	Acc@1  96.09 ( 96.44)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:23 - Epoch: [12][200/352]	Time  0.167 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.0771e-01 (1.0370e-01)	Acc@1  98.44 ( 96.45)	Acc@5  99.22 ( 99.97)
07-Mar-22 02:58:25 - Epoch: [12][210/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.003)	Loss 7.5023e-02 (1.0318e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:27 - Epoch: [12][220/352]	Time  0.165 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.3379e-02 (1.0316e-01)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:28 - Epoch: [12][230/352]	Time  0.166 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.5184e-01 (1.0306e-01)	Acc@1  92.19 ( 96.46)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:30 - Epoch: [12][240/352]	Time  0.167 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.4391e-01 (1.0373e-01)	Acc@1  93.75 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:32 - Epoch: [12][250/352]	Time  0.163 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.6596e-01 (1.0346e-01)	Acc@1  92.97 ( 96.42)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:33 - Epoch: [12][260/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.003)	Loss 5.5015e-02 (1.0324e-01)	Acc@1  96.88 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:35 - Epoch: [12][270/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.003)	Loss 9.1226e-02 (1.0322e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:37 - Epoch: [12][280/352]	Time  0.146 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.1634e-01 (1.0354e-01)	Acc@1  94.53 ( 96.41)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:38 - Epoch: [12][290/352]	Time  0.154 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.3895e-01 (1.0290e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:40 - Epoch: [12][300/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.003)	Loss 8.5865e-02 (1.0264e-01)	Acc@1  95.31 ( 96.42)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:41 - Epoch: [12][310/352]	Time  0.161 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.0521e-01 (1.0274e-01)	Acc@1  95.31 ( 96.41)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:43 - Epoch: [12][320/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.1273e-01 (1.0289e-01)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:45 - Epoch: [12][330/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.4843e-01 (1.0285e-01)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:46 - Epoch: [12][340/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.4682e-01 (1.0292e-01)	Acc@1  96.88 ( 96.41)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:48 - Epoch: [12][350/352]	Time  0.166 ( 0.161)	Data  0.001 ( 0.003)	Loss 9.8580e-02 (1.0279e-01)	Acc@1  96.88 ( 96.41)	Acc@5  99.22 ( 99.97)
07-Mar-22 02:58:49 - Test: [ 0/20]	Time  0.370 ( 0.370)	Loss 3.3922e-01 (3.3922e-01)	Acc@1  89.45 ( 89.45)	Acc@5  99.22 ( 99.22)
07-Mar-22 02:58:50 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 3.8441e-01 (3.9572e-01)	Acc@1  87.89 ( 88.57)	Acc@5  99.22 ( 99.57)
07-Mar-22 02:58:51 -  * Acc@1 88.980 Acc@5 99.540
07-Mar-22 02:58:51 - Best acc at epoch 12: 89.0199966430664
07-Mar-22 02:58:51 - Epoch: [13][  0/352]	Time  0.392 ( 0.392)	Data  0.234 ( 0.234)	Loss 9.3384e-02 (9.3384e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 02:58:53 - Epoch: [13][ 10/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.023)	Loss 8.9570e-02 (1.0610e-01)	Acc@1  95.31 ( 96.24)	Acc@5 100.00 (100.00)
07-Mar-22 02:58:54 - Epoch: [13][ 20/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.013)	Loss 1.6067e-01 (1.0660e-01)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:58:56 - Epoch: [13][ 30/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.009)	Loss 1.7424e-01 (1.0685e-01)	Acc@1  93.75 ( 96.32)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:58:58 - Epoch: [13][ 40/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.008)	Loss 9.2040e-02 (1.0055e-01)	Acc@1  96.09 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:58:59 - Epoch: [13][ 50/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.007)	Loss 6.3230e-02 (1.0051e-01)	Acc@1  98.44 ( 96.46)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:01 - Epoch: [13][ 60/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.006)	Loss 7.8282e-02 (1.0266e-01)	Acc@1  96.88 ( 96.34)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:59:03 - Epoch: [13][ 70/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.1156e-01 (1.0196e-01)	Acc@1  95.31 ( 96.35)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:04 - Epoch: [13][ 80/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.005)	Loss 8.0387e-02 (1.0141e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:59:06 - Epoch: [13][ 90/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.0775e-01 (1.0025e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:59:08 - Epoch: [13][100/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.3825e-02 (1.0278e-01)	Acc@1  96.09 ( 96.31)	Acc@5 100.00 ( 99.95)
07-Mar-22 02:59:09 - Epoch: [13][110/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.5845e-02 (1.0262e-01)	Acc@1  98.44 ( 96.35)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:59:11 - Epoch: [13][120/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.1033e-01 (1.0237e-01)	Acc@1  97.66 ( 96.38)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:59:13 - Epoch: [13][130/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.4007e-01 (1.0206e-01)	Acc@1  95.31 ( 96.42)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:59:14 - Epoch: [13][140/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.0638e-01 (1.0163e-01)	Acc@1  95.31 ( 96.43)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:16 - Epoch: [13][150/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.2936e-01 (1.0179e-01)	Acc@1  94.53 ( 96.44)	Acc@5 100.00 ( 99.96)
07-Mar-22 02:59:18 - Epoch: [13][160/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2141e-01 (1.0206e-01)	Acc@1  95.31 ( 96.39)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:19 - Epoch: [13][170/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.6428e-02 (1.0182e-01)	Acc@1  97.66 ( 96.39)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:21 - Epoch: [13][180/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.7166e-01 (1.0224e-01)	Acc@1  92.19 ( 96.37)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:23 - Epoch: [13][190/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.5857e-01 (1.0131e-01)	Acc@1  93.75 ( 96.39)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:24 - Epoch: [13][200/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 2.4820e-01 (1.0245e-01)	Acc@1  94.53 ( 96.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:26 - Epoch: [13][210/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.5900e-01 (1.0287e-01)	Acc@1  94.53 ( 96.38)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:28 - Epoch: [13][220/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.8709e-01 (1.0436e-01)	Acc@1  96.09 ( 96.33)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:29 - Epoch: [13][230/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.7332e-02 (1.0425e-01)	Acc@1  97.66 ( 96.33)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:31 - Epoch: [13][240/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0391e-01 (1.0321e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.97)
07-Mar-22 02:59:33 - Epoch: [13][250/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.4513e-02 (1.0236e-01)	Acc@1  96.88 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:34 - Epoch: [13][260/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1005e-01 (1.0255e-01)	Acc@1  96.09 ( 96.41)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:36 - Epoch: [13][270/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.7203e-02 (1.0159e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:38 - Epoch: [13][280/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4220e-01 (1.0165e-01)	Acc@1  94.53 ( 96.43)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:39 - Epoch: [13][290/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0332e-01 (1.0205e-01)	Acc@1  95.31 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:41 - Epoch: [13][300/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.2379e-02 (1.0200e-01)	Acc@1  97.66 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:43 - Epoch: [13][310/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0879e-01 (1.0223e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:44 - Epoch: [13][320/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.3685e-02 (1.0241e-01)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:46 - Epoch: [13][330/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.1558e-02 (1.0154e-01)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:48 - Epoch: [13][340/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.6483e-02 (1.0145e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:49 - Epoch: [13][350/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1505e-01 (1.0156e-01)	Acc@1  92.19 ( 96.43)	Acc@5 100.00 ( 99.98)
07-Mar-22 02:59:50 - Test: [ 0/20]	Time  0.381 ( 0.381)	Loss 3.3911e-01 (3.3911e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.61 ( 99.61)
07-Mar-22 02:59:51 - Test: [10/20]	Time  0.099 ( 0.125)	Loss 3.9044e-01 (4.0154e-01)	Acc@1  87.50 ( 88.35)	Acc@5  99.61 ( 99.40)
07-Mar-22 02:59:52 -  * Acc@1 88.960 Acc@5 99.420
07-Mar-22 02:59:52 - Best acc at epoch 13: 89.0199966430664
07-Mar-22 02:59:53 - Epoch: [14][  0/352]	Time  0.398 ( 0.398)	Data  0.242 ( 0.242)	Loss 6.5588e-02 (6.5588e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 02:59:54 - Epoch: [14][ 10/352]	Time  0.169 ( 0.188)	Data  0.002 ( 0.024)	Loss 5.9356e-02 (8.4721e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 02:59:56 - Epoch: [14][ 20/352]	Time  0.168 ( 0.180)	Data  0.002 ( 0.014)	Loss 9.7797e-02 (9.7085e-02)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 (100.00)
07-Mar-22 02:59:58 - Epoch: [14][ 30/352]	Time  0.172 ( 0.176)	Data  0.002 ( 0.010)	Loss 4.4180e-02 (9.0479e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 02:59:59 - Epoch: [14][ 40/352]	Time  0.170 ( 0.175)	Data  0.003 ( 0.008)	Loss 1.0970e-01 (9.3686e-02)	Acc@1  96.09 ( 96.76)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:01 - Epoch: [14][ 50/352]	Time  0.168 ( 0.174)	Data  0.003 ( 0.007)	Loss 1.2543e-01 (9.6039e-02)	Acc@1  95.31 ( 96.61)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:03 - Epoch: [14][ 60/352]	Time  0.168 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.5243e-01 (9.6811e-02)	Acc@1  94.53 ( 96.55)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:05 - Epoch: [14][ 70/352]	Time  0.170 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.0563e-01 (9.5907e-02)	Acc@1  96.88 ( 96.57)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:06 - Epoch: [14][ 80/352]	Time  0.169 ( 0.172)	Data  0.003 ( 0.005)	Loss 1.4755e-01 (9.8075e-02)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:08 - Epoch: [14][ 90/352]	Time  0.165 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.2391e-01 (9.6709e-02)	Acc@1  95.31 ( 96.59)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:10 - Epoch: [14][100/352]	Time  0.171 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.5696e-01 (9.5450e-02)	Acc@1  96.09 ( 96.64)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:11 - Epoch: [14][110/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.1157e-01 (9.6588e-02)	Acc@1  94.53 ( 96.55)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:13 - Epoch: [14][120/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.0614e-01 (9.8664e-02)	Acc@1  94.53 ( 96.49)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:14 - Epoch: [14][130/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.0704e-02 (9.7699e-02)	Acc@1  98.44 ( 96.58)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:16 - Epoch: [14][140/352]	Time  0.131 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.1574e-02 (9.7029e-02)	Acc@1  98.44 ( 96.58)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:18 - Epoch: [14][150/352]	Time  0.151 ( 0.168)	Data  0.002 ( 0.004)	Loss 4.9180e-02 (9.7394e-02)	Acc@1  98.44 ( 96.57)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:19 - Epoch: [14][160/352]	Time  0.132 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.7000e-01 (9.7532e-02)	Acc@1  92.97 ( 96.59)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:21 - Epoch: [14][170/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.004)	Loss 1.5964e-01 (9.7030e-02)	Acc@1  93.75 ( 96.61)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:22 - Epoch: [14][180/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.004)	Loss 1.1833e-01 (9.6659e-02)	Acc@1  96.09 ( 96.61)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:24 - Epoch: [14][190/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.004)	Loss 4.8772e-02 (9.6383e-02)	Acc@1 100.00 ( 96.61)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:26 - Epoch: [14][200/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.004)	Loss 9.9074e-02 (9.6747e-02)	Acc@1  96.88 ( 96.59)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:27 - Epoch: [14][210/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.4434e-01 (9.7656e-02)	Acc@1  94.53 ( 96.57)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:29 - Epoch: [14][220/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.5089e-01 (9.7695e-02)	Acc@1  93.75 ( 96.57)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:31 - Epoch: [14][230/352]	Time  0.171 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.9481e-02 (9.8165e-02)	Acc@1  98.44 ( 96.56)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:32 - Epoch: [14][240/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0580e-01 (9.9071e-02)	Acc@1  95.31 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:34 - Epoch: [14][250/352]	Time  0.167 ( 0.166)	Data  0.003 ( 0.003)	Loss 6.9321e-02 (9.8006e-02)	Acc@1  97.66 ( 96.56)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:36 - Epoch: [14][260/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.8458e-02 (9.8352e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:00:37 - Epoch: [14][270/352]	Time  0.137 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0177e-01 (9.8388e-02)	Acc@1  97.66 ( 96.57)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:39 - Epoch: [14][280/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.8738e-02 (9.8890e-02)	Acc@1  98.44 ( 96.54)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:41 - Epoch: [14][290/352]	Time  0.170 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.1833e-02 (9.8017e-02)	Acc@1  97.66 ( 96.56)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:42 - Epoch: [14][300/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1464e-01 (9.8016e-02)	Acc@1  94.53 ( 96.56)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:44 - Epoch: [14][310/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1500e-01 (9.8029e-02)	Acc@1  95.31 ( 96.56)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:46 - Epoch: [14][320/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 3.8361e-02 (9.7784e-02)	Acc@1  99.22 ( 96.57)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:47 - Epoch: [14][330/352]	Time  0.169 ( 0.166)	Data  0.003 ( 0.003)	Loss 9.2520e-02 (9.7553e-02)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:49 - Epoch: [14][340/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1854e-01 (9.7867e-02)	Acc@1  96.09 ( 96.58)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:50 - Epoch: [14][350/352]	Time  0.141 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.2666e-01 (9.8338e-02)	Acc@1  94.53 ( 96.57)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:00:51 - Test: [ 0/20]	Time  0.384 ( 0.384)	Loss 3.2057e-01 (3.2057e-01)	Acc@1  88.67 ( 88.67)	Acc@5  98.83 ( 98.83)
07-Mar-22 03:00:52 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.5032e-01 (3.9542e-01)	Acc@1  89.06 ( 88.32)	Acc@5  99.22 ( 99.18)
07-Mar-22 03:00:53 -  * Acc@1 89.200 Acc@5 99.260
07-Mar-22 03:00:53 - Best acc at epoch 14: 89.19999694824219
07-Mar-22 03:00:53 - Epoch: [15][  0/352]	Time  0.404 ( 0.404)	Data  0.238 ( 0.238)	Loss 1.6242e-01 (1.6242e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:55 - Epoch: [15][ 10/352]	Time  0.167 ( 0.188)	Data  0.002 ( 0.024)	Loss 1.0489e-01 (9.1043e-02)	Acc@1  95.31 ( 97.02)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:57 - Epoch: [15][ 20/352]	Time  0.170 ( 0.176)	Data  0.002 ( 0.013)	Loss 1.3617e-01 (9.5276e-02)	Acc@1  95.31 ( 96.84)	Acc@5 100.00 (100.00)
07-Mar-22 03:00:58 - Epoch: [15][ 30/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.010)	Loss 1.0884e-01 (9.5495e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 (100.00)
07-Mar-22 03:01:00 - Epoch: [15][ 40/352]	Time  0.165 ( 0.171)	Data  0.001 ( 0.008)	Loss 1.5274e-01 (9.8080e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 (100.00)
07-Mar-22 03:01:02 - Epoch: [15][ 50/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.007)	Loss 1.1980e-01 (9.8889e-02)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:03 - Epoch: [15][ 60/352]	Time  0.162 ( 0.169)	Data  0.002 ( 0.006)	Loss 6.5328e-02 (9.8663e-02)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:05 - Epoch: [15][ 70/352]	Time  0.162 ( 0.168)	Data  0.002 ( 0.005)	Loss 7.6503e-02 (9.9226e-02)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:07 - Epoch: [15][ 80/352]	Time  0.143 ( 0.167)	Data  0.002 ( 0.005)	Loss 1.5218e-01 (9.9430e-02)	Acc@1  94.53 ( 96.60)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:08 - Epoch: [15][ 90/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.005)	Loss 1.4407e-01 (1.0070e-01)	Acc@1  93.75 ( 96.53)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:10 - Epoch: [15][100/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.004)	Loss 7.7855e-02 (9.9140e-02)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:11 - Epoch: [15][110/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.6170e-01 (1.0061e-01)	Acc@1  95.31 ( 96.49)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:13 - Epoch: [15][120/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.1812e-01 (1.0072e-01)	Acc@1  93.75 ( 96.46)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:15 - Epoch: [15][130/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.004)	Loss 5.5572e-02 (9.9678e-02)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:01:16 - Epoch: [15][140/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.004)	Loss 6.3158e-02 (9.9406e-02)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:18 - Epoch: [15][150/352]	Time  0.165 ( 0.167)	Data  0.003 ( 0.004)	Loss 9.5624e-02 (9.9712e-02)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:20 - Epoch: [15][160/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.004)	Loss 9.8501e-02 (1.0054e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:21 - Epoch: [15][170/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.2821e-02 (1.0104e-01)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:01:23 - Epoch: [15][180/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2252e-01 (1.0199e-01)	Acc@1  93.75 ( 96.44)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:01:25 - Epoch: [15][190/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.6717e-02 (1.0200e-01)	Acc@1  98.44 ( 96.40)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:27 - Epoch: [15][200/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.5007e-02 (1.0138e-01)	Acc@1  96.88 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:28 - Epoch: [15][210/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.9840e-02 (1.0111e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:30 - Epoch: [15][220/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1278e-01 (1.0056e-01)	Acc@1  95.31 ( 96.39)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:32 - Epoch: [15][230/352]	Time  0.170 ( 0.167)	Data  0.001 ( 0.003)	Loss 1.1173e-01 (1.0067e-01)	Acc@1  97.66 ( 96.38)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:33 - Epoch: [15][240/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.3742e-02 (1.0047e-01)	Acc@1  98.44 ( 96.40)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:35 - Epoch: [15][250/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.9351e-02 (1.0052e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:37 - Epoch: [15][260/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4748e-01 (1.0110e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:38 - Epoch: [15][270/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4039e-01 (1.0180e-01)	Acc@1  97.66 ( 96.36)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:40 - Epoch: [15][280/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.9288e-02 (1.0120e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:41 - Epoch: [15][290/352]	Time  0.168 ( 0.167)	Data  0.001 ( 0.003)	Loss 1.2315e-01 (1.0104e-01)	Acc@1  94.53 ( 96.40)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:43 - Epoch: [15][300/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1605e-01 (1.0079e-01)	Acc@1  96.88 ( 96.41)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:45 - Epoch: [15][310/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3722e-01 (1.0041e-01)	Acc@1  94.53 ( 96.43)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:46 - Epoch: [15][320/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.4980e-02 (1.0024e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:48 - Epoch: [15][330/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.4136e-02 (1.0015e-01)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:50 - Epoch: [15][340/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1158e-01 (9.9937e-02)	Acc@1  94.53 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:52 - Epoch: [15][350/352]	Time  0.163 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.9580e-02 (9.9824e-02)	Acc@1  98.44 ( 96.42)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:01:52 - Test: [ 0/20]	Time  0.360 ( 0.360)	Loss 3.3637e-01 (3.3637e-01)	Acc@1  88.28 ( 88.28)	Acc@5  98.83 ( 98.83)
07-Mar-22 03:01:53 - Test: [10/20]	Time  0.098 ( 0.122)	Loss 4.1733e-01 (4.1805e-01)	Acc@1  87.89 ( 87.68)	Acc@5  99.22 ( 99.40)
07-Mar-22 03:01:54 -  * Acc@1 88.680 Acc@5 99.440
07-Mar-22 03:01:54 - Best acc at epoch 15: 89.19999694824219
07-Mar-22 03:01:55 - Epoch: [16][  0/352]	Time  0.418 ( 0.418)	Data  0.256 ( 0.256)	Loss 8.0725e-02 (8.0725e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:01:56 - Epoch: [16][ 10/352]	Time  0.171 ( 0.188)	Data  0.002 ( 0.025)	Loss 1.0043e-01 (1.0868e-01)	Acc@1  95.31 ( 95.74)	Acc@5 100.00 (100.00)
07-Mar-22 03:01:58 - Epoch: [16][ 20/352]	Time  0.163 ( 0.183)	Data  0.002 ( 0.014)	Loss 9.7398e-02 (9.7881e-02)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 (100.00)
07-Mar-22 03:02:00 - Epoch: [16][ 30/352]	Time  0.169 ( 0.178)	Data  0.002 ( 0.010)	Loss 9.7505e-02 (1.0542e-01)	Acc@1  96.88 ( 96.14)	Acc@5 100.00 (100.00)
07-Mar-22 03:02:01 - Epoch: [16][ 40/352]	Time  0.165 ( 0.175)	Data  0.002 ( 0.008)	Loss 1.0867e-01 (1.0079e-01)	Acc@1  95.31 ( 96.32)	Acc@5 100.00 (100.00)
07-Mar-22 03:02:03 - Epoch: [16][ 50/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.007)	Loss 9.9412e-02 (1.0533e-01)	Acc@1  96.88 ( 96.31)	Acc@5 100.00 (100.00)
07-Mar-22 03:02:05 - Epoch: [16][ 60/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.006)	Loss 8.1806e-02 (1.0278e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 (100.00)
07-Mar-22 03:02:06 - Epoch: [16][ 70/352]	Time  0.166 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.1603e-01 (1.0319e-01)	Acc@1  96.09 ( 96.38)	Acc@5 100.00 (100.00)
07-Mar-22 03:02:08 - Epoch: [16][ 80/352]	Time  0.165 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.7935e-01 (1.0291e-01)	Acc@1  93.75 ( 96.43)	Acc@5 100.00 (100.00)
07-Mar-22 03:02:10 - Epoch: [16][ 90/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.005)	Loss 7.5971e-02 (1.0176e-01)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 (100.00)
07-Mar-22 03:02:11 - Epoch: [16][100/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.005)	Loss 8.5298e-02 (1.0107e-01)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:02:13 - Epoch: [16][110/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.7930e-01 (1.0222e-01)	Acc@1  93.75 ( 96.39)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:02:15 - Epoch: [16][120/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 9.7281e-02 (1.0028e-01)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:16 - Epoch: [16][130/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.3157e-01 (1.0073e-01)	Acc@1  94.53 ( 96.46)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:18 - Epoch: [16][140/352]	Time  0.188 ( 0.169)	Data  0.003 ( 0.004)	Loss 7.3332e-02 (9.9673e-02)	Acc@1  97.66 ( 96.54)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:20 - Epoch: [16][150/352]	Time  0.141 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.7204e-01 (9.8706e-02)	Acc@1  94.53 ( 96.57)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:21 - Epoch: [16][160/352]	Time  0.163 ( 0.167)	Data  0.002 ( 0.004)	Loss 8.5102e-02 (9.8931e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:23 - Epoch: [16][170/352]	Time  0.142 ( 0.167)	Data  0.002 ( 0.004)	Loss 6.9154e-02 (9.8868e-02)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:24 - Epoch: [16][180/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1497e-01 (9.8523e-02)	Acc@1  94.53 ( 96.55)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:26 - Epoch: [16][190/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.5907e-01 (9.9256e-02)	Acc@1  94.53 ( 96.52)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:28 - Epoch: [16][200/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.8570e-02 (9.9494e-02)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:29 - Epoch: [16][210/352]	Time  0.176 ( 0.167)	Data  0.003 ( 0.003)	Loss 8.2021e-02 (9.9946e-02)	Acc@1  94.53 ( 96.52)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:31 - Epoch: [16][220/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0602e-01 (1.0116e-01)	Acc@1  96.09 ( 96.48)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:33 - Epoch: [16][230/352]	Time  0.163 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0919e-01 (1.0078e-01)	Acc@1  96.09 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:34 - Epoch: [16][240/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.2458e-02 (1.0049e-01)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:36 - Epoch: [16][250/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.9271e-02 (1.0043e-01)	Acc@1  98.44 ( 96.53)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:38 - Epoch: [16][260/352]	Time  0.163 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0335e-01 (1.0065e-01)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:39 - Epoch: [16][270/352]	Time  0.144 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.7244e-02 (1.0080e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:41 - Epoch: [16][280/352]	Time  0.143 ( 0.166)	Data  0.001 ( 0.003)	Loss 8.0017e-02 (1.0011e-01)	Acc@1  97.66 ( 96.53)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:42 - Epoch: [16][290/352]	Time  0.149 ( 0.165)	Data  0.001 ( 0.003)	Loss 8.6506e-02 (9.9364e-02)	Acc@1  93.75 ( 96.54)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:44 - Epoch: [16][300/352]	Time  0.143 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.1226e-01 (9.9153e-02)	Acc@1  94.53 ( 96.54)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:45 - Epoch: [16][310/352]	Time  0.149 ( 0.164)	Data  0.002 ( 0.003)	Loss 4.2968e-02 (9.8451e-02)	Acc@1  98.44 ( 96.57)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:47 - Epoch: [16][320/352]	Time  0.142 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.0946e-02 (9.8475e-02)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:48 - Epoch: [16][330/352]	Time  0.141 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.1358e-01 (9.8806e-02)	Acc@1  94.53 ( 96.55)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:49 - Epoch: [16][340/352]	Time  0.144 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.2335e-01 (9.8957e-02)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:51 - Epoch: [16][350/352]	Time  0.169 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.5756e-02 (9.8802e-02)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:02:51 - Test: [ 0/20]	Time  0.342 ( 0.342)	Loss 3.6191e-01 (3.6191e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:02:53 - Test: [10/20]	Time  0.116 ( 0.127)	Loss 3.6577e-01 (3.9740e-01)	Acc@1  90.23 ( 88.57)	Acc@5  99.22 ( 99.43)
07-Mar-22 03:02:53 -  * Acc@1 89.180 Acc@5 99.380
07-Mar-22 03:02:54 - Best acc at epoch 16: 89.19999694824219
07-Mar-22 03:02:54 - Epoch: [17][  0/352]	Time  0.423 ( 0.423)	Data  0.259 ( 0.259)	Loss 1.3842e-01 (1.3842e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:02:56 - Epoch: [17][ 10/352]	Time  0.192 ( 0.193)	Data  0.002 ( 0.026)	Loss 8.6989e-02 (1.1304e-01)	Acc@1  96.88 ( 95.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:02:57 - Epoch: [17][ 20/352]	Time  0.168 ( 0.182)	Data  0.002 ( 0.015)	Loss 9.4712e-02 (1.0528e-01)	Acc@1  96.09 ( 96.24)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:02:59 - Epoch: [17][ 30/352]	Time  0.167 ( 0.178)	Data  0.002 ( 0.011)	Loss 9.6885e-02 (1.0213e-01)	Acc@1  94.53 ( 96.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:03:01 - Epoch: [17][ 40/352]	Time  0.164 ( 0.175)	Data  0.002 ( 0.009)	Loss 7.9017e-02 (9.7704e-02)	Acc@1  98.44 ( 96.61)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:03:02 - Epoch: [17][ 50/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.007)	Loss 6.7382e-02 (9.7471e-02)	Acc@1  96.88 ( 96.60)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:03:04 - Epoch: [17][ 60/352]	Time  0.174 ( 0.173)	Data  0.002 ( 0.007)	Loss 8.1240e-02 (9.4134e-02)	Acc@1  96.88 ( 96.71)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:03:06 - Epoch: [17][ 70/352]	Time  0.164 ( 0.172)	Data  0.002 ( 0.006)	Loss 6.5341e-02 (9.2369e-02)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:03:07 - Epoch: [17][ 80/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.006)	Loss 4.9796e-02 (9.0548e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:03:09 - Epoch: [17][ 90/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.2280e-01 (9.1542e-02)	Acc@1  96.09 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:03:11 - Epoch: [17][100/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.005)	Loss 8.6618e-02 (9.3412e-02)	Acc@1  98.44 ( 96.80)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:03:13 - Epoch: [17][110/352]	Time  0.170 ( 0.171)	Data  0.002 ( 0.005)	Loss 6.4990e-02 (9.4666e-02)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:14 - Epoch: [17][120/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.005)	Loss 3.9841e-02 (9.3165e-02)	Acc@1 100.00 ( 96.82)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:16 - Epoch: [17][130/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.0283e-01 (9.3627e-02)	Acc@1  96.09 ( 96.78)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:18 - Epoch: [17][140/352]	Time  0.170 ( 0.170)	Data  0.002 ( 0.004)	Loss 7.9311e-02 (9.3201e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:19 - Epoch: [17][150/352]	Time  0.171 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.1594e-01 (9.2381e-02)	Acc@1  93.75 ( 96.80)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:21 - Epoch: [17][160/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.4940e-01 (9.2553e-02)	Acc@1  95.31 ( 96.79)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:23 - Epoch: [17][170/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.8402e-01 (9.3189e-02)	Acc@1  94.53 ( 96.76)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:24 - Epoch: [17][180/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.004)	Loss 9.7044e-02 (9.3256e-02)	Acc@1  95.31 ( 96.76)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:26 - Epoch: [17][190/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.004)	Loss 8.4834e-02 (9.3956e-02)	Acc@1  96.09 ( 96.70)	Acc@5  99.22 ( 99.99)
07-Mar-22 03:03:28 - Epoch: [17][200/352]	Time  0.156 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.0332e-01 (9.4920e-02)	Acc@1  97.66 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:29 - Epoch: [17][210/352]	Time  0.141 ( 0.169)	Data  0.002 ( 0.004)	Loss 8.3861e-02 (9.4956e-02)	Acc@1  97.66 ( 96.69)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:31 - Epoch: [17][220/352]	Time  0.143 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.6802e-02 (9.5289e-02)	Acc@1  96.88 ( 96.68)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:32 - Epoch: [17][230/352]	Time  0.152 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.5445e-02 (9.5039e-02)	Acc@1  97.66 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:34 - Epoch: [17][240/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.4776e-02 (9.5348e-02)	Acc@1  96.09 ( 96.65)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:36 - Epoch: [17][250/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.5746e-02 (9.5479e-02)	Acc@1  96.09 ( 96.63)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:03:37 - Epoch: [17][260/352]	Time  0.142 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.3084e-02 (9.5299e-02)	Acc@1  97.66 ( 96.64)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:39 - Epoch: [17][270/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0955e-01 (9.5565e-02)	Acc@1  97.66 ( 96.64)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:40 - Epoch: [17][280/352]	Time  0.144 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.3417e-02 (9.5251e-02)	Acc@1  96.09 ( 96.65)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:42 - Epoch: [17][290/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.7189e-02 (9.5176e-02)	Acc@1  96.88 ( 96.66)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:43 - Epoch: [17][300/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.2991e-01 (9.5470e-02)	Acc@1  94.53 ( 96.64)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:45 - Epoch: [17][310/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.5522e-02 (9.5159e-02)	Acc@1  97.66 ( 96.66)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:47 - Epoch: [17][320/352]	Time  0.172 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.0635e-02 (9.4807e-02)	Acc@1  96.09 ( 96.66)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:48 - Epoch: [17][330/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.9186e-02 (9.4430e-02)	Acc@1  96.09 ( 96.68)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:50 - Epoch: [17][340/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.5783e-02 (9.4621e-02)	Acc@1  98.44 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:52 - Epoch: [17][350/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0264e-01 (9.4913e-02)	Acc@1  97.66 ( 96.68)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:03:52 - Test: [ 0/20]	Time  0.365 ( 0.365)	Loss 3.5472e-01 (3.5472e-01)	Acc@1  89.45 ( 89.45)	Acc@5 100.00 (100.00)
07-Mar-22 03:03:53 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 4.0212e-01 (3.9721e-01)	Acc@1  89.06 ( 88.85)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:03:54 -  * Acc@1 89.100 Acc@5 99.460
07-Mar-22 03:03:54 - Best acc at epoch 17: 89.19999694824219
07-Mar-22 03:03:55 - Epoch: [18][  0/352]	Time  0.400 ( 0.400)	Data  0.244 ( 0.244)	Loss 1.2563e-01 (1.2563e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 03:03:57 - Epoch: [18][ 10/352]	Time  0.172 ( 0.189)	Data  0.002 ( 0.024)	Loss 4.5603e-02 (8.5132e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 (100.00)
07-Mar-22 03:03:58 - Epoch: [18][ 20/352]	Time  0.166 ( 0.179)	Data  0.003 ( 0.014)	Loss 7.7397e-02 (8.6703e-02)	Acc@1  96.09 ( 96.65)	Acc@5 100.00 (100.00)
07-Mar-22 03:04:00 - Epoch: [18][ 30/352]	Time  0.163 ( 0.177)	Data  0.002 ( 0.010)	Loss 1.1979e-01 (8.6715e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 (100.00)
07-Mar-22 03:04:02 - Epoch: [18][ 40/352]	Time  0.170 ( 0.175)	Data  0.002 ( 0.008)	Loss 1.0458e-01 (8.9518e-02)	Acc@1  95.31 ( 96.63)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:03 - Epoch: [18][ 50/352]	Time  0.168 ( 0.173)	Data  0.002 ( 0.007)	Loss 1.0717e-01 (9.2407e-02)	Acc@1  96.09 ( 96.57)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:05 - Epoch: [18][ 60/352]	Time  0.167 ( 0.172)	Data  0.002 ( 0.006)	Loss 6.0005e-02 (9.2424e-02)	Acc@1  97.66 ( 96.62)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:04:07 - Epoch: [18][ 70/352]	Time  0.169 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.8394e-01 (9.5076e-02)	Acc@1  94.53 ( 96.53)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:08 - Epoch: [18][ 80/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.005)	Loss 6.7581e-02 (9.3138e-02)	Acc@1  96.88 ( 96.60)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:10 - Epoch: [18][ 90/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.005)	Loss 6.1241e-02 (9.4246e-02)	Acc@1  98.44 ( 96.58)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:12 - Epoch: [18][100/352]	Time  0.191 ( 0.170)	Data  0.003 ( 0.005)	Loss 1.0190e-01 (9.3379e-02)	Acc@1  96.09 ( 96.64)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:13 - Epoch: [18][110/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.004)	Loss 9.8588e-02 (9.3307e-02)	Acc@1  95.31 ( 96.60)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:04:15 - Epoch: [18][120/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.004)	Loss 9.8541e-02 (9.2759e-02)	Acc@1  96.88 ( 96.62)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:17 - Epoch: [18][130/352]	Time  0.171 ( 0.170)	Data  0.002 ( 0.004)	Loss 9.8682e-02 (9.2369e-02)	Acc@1  96.88 ( 96.68)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:18 - Epoch: [18][140/352]	Time  0.170 ( 0.170)	Data  0.002 ( 0.004)	Loss 7.5702e-02 (9.2691e-02)	Acc@1  97.66 ( 96.68)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:20 - Epoch: [18][150/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.6630e-01 (9.3841e-02)	Acc@1  95.31 ( 96.65)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:22 - Epoch: [18][160/352]	Time  0.149 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.7106e-02 (9.3238e-02)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:04:23 - Epoch: [18][170/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.0801e-01 (9.3552e-02)	Acc@1  96.09 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:04:25 - Epoch: [18][180/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.6111e-02 (9.2972e-02)	Acc@1  96.88 ( 96.71)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:04:27 - Epoch: [18][190/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.7469e-02 (9.3534e-02)	Acc@1  98.44 ( 96.72)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:04:28 - Epoch: [18][200/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1341e-01 (9.4486e-02)	Acc@1  95.31 ( 96.70)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:30 - Epoch: [18][210/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.4714e-02 (9.3511e-02)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:32 - Epoch: [18][220/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.5082e-01 (9.4485e-02)	Acc@1  94.53 ( 96.71)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:33 - Epoch: [18][230/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.8326e-02 (9.3722e-02)	Acc@1  99.22 ( 96.72)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:35 - Epoch: [18][240/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.9019e-02 (9.3258e-02)	Acc@1  98.44 ( 96.74)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:37 - Epoch: [18][250/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.1581e-02 (9.4246e-02)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:38 - Epoch: [18][260/352]	Time  0.172 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.0605e-02 (9.4763e-02)	Acc@1  99.22 ( 96.69)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:40 - Epoch: [18][270/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.1405e-02 (9.4897e-02)	Acc@1  96.09 ( 96.67)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:42 - Epoch: [18][280/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4195e-01 (9.5128e-02)	Acc@1  96.09 ( 96.66)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:43 - Epoch: [18][290/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.7192e-01 (9.4926e-02)	Acc@1  94.53 ( 96.67)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:45 - Epoch: [18][300/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.0899e-02 (9.4422e-02)	Acc@1  99.22 ( 96.70)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:47 - Epoch: [18][310/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1049e-01 (9.4703e-02)	Acc@1  96.09 ( 96.69)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:48 - Epoch: [18][320/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1533e-01 (9.5011e-02)	Acc@1  96.88 ( 96.67)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:50 - Epoch: [18][330/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.9982e-02 (9.4867e-02)	Acc@1  98.44 ( 96.68)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:52 - Epoch: [18][340/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.3488e-02 (9.4907e-02)	Acc@1  96.88 ( 96.68)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:54 - Epoch: [18][350/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0285e-01 (9.4753e-02)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:04:54 - Test: [ 0/20]	Time  0.371 ( 0.371)	Loss 3.3681e-01 (3.3681e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:04:55 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 3.8531e-01 (3.8792e-01)	Acc@1  86.33 ( 88.71)	Acc@5  99.22 ( 99.47)
07-Mar-22 03:04:56 -  * Acc@1 89.520 Acc@5 99.440
07-Mar-22 03:04:56 - Best acc at epoch 18: 89.5199966430664
07-Mar-22 03:04:57 - Epoch: [19][  0/352]	Time  0.387 ( 0.387)	Data  0.242 ( 0.242)	Loss 1.1257e-01 (1.1257e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 03:04:58 - Epoch: [19][ 10/352]	Time  0.167 ( 0.184)	Data  0.003 ( 0.024)	Loss 7.6107e-02 (9.6148e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:05:00 - Epoch: [19][ 20/352]	Time  0.166 ( 0.178)	Data  0.002 ( 0.014)	Loss 7.3338e-02 (9.4099e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:05:02 - Epoch: [19][ 30/352]	Time  0.167 ( 0.174)	Data  0.002 ( 0.010)	Loss 9.8712e-02 (9.0545e-02)	Acc@1  95.31 ( 96.82)	Acc@5 100.00 ( 99.95)
07-Mar-22 03:05:03 - Epoch: [19][ 40/352]	Time  0.165 ( 0.172)	Data  0.002 ( 0.008)	Loss 1.0228e-01 (9.1522e-02)	Acc@1  96.09 ( 96.65)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:05:05 - Epoch: [19][ 50/352]	Time  0.170 ( 0.171)	Data  0.002 ( 0.007)	Loss 1.1646e-01 (9.2557e-02)	Acc@1  96.09 ( 96.68)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:05:07 - Epoch: [19][ 60/352]	Time  0.177 ( 0.171)	Data  0.002 ( 0.006)	Loss 4.8969e-02 (9.1991e-02)	Acc@1  99.22 ( 96.71)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:05:08 - Epoch: [19][ 70/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.1965e-01 (9.1887e-02)	Acc@1  95.31 ( 96.73)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:05:10 - Epoch: [19][ 80/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.1878e-01 (9.2809e-02)	Acc@1  93.75 ( 96.72)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:05:12 - Epoch: [19][ 90/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.005)	Loss 6.1023e-02 (9.1733e-02)	Acc@1  97.66 ( 96.75)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:05:13 - Epoch: [19][100/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.3907e-02 (9.0052e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:05:15 - Epoch: [19][110/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.004)	Loss 7.6429e-02 (9.0175e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:17 - Epoch: [19][120/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.2316e-01 (9.1641e-02)	Acc@1  95.31 ( 96.77)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:19 - Epoch: [19][130/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.004)	Loss 6.8015e-02 (9.0878e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:20 - Epoch: [19][140/352]	Time  0.164 ( 0.170)	Data  0.002 ( 0.004)	Loss 6.2321e-02 (9.0466e-02)	Acc@1  97.66 ( 96.79)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:22 - Epoch: [19][150/352]	Time  0.165 ( 0.170)	Data  0.002 ( 0.004)	Loss 8.4402e-02 (9.0476e-02)	Acc@1  99.22 ( 96.80)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:24 - Epoch: [19][160/352]	Time  0.167 ( 0.169)	Data  0.001 ( 0.004)	Loss 7.2875e-02 (8.9079e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:25 - Epoch: [19][170/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.2036e-02 (8.9154e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:27 - Epoch: [19][180/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 6.6197e-02 (9.0350e-02)	Acc@1  97.66 ( 96.79)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:29 - Epoch: [19][190/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.003)	Loss 4.6124e-02 (8.9871e-02)	Acc@1  98.44 ( 96.83)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:30 - Epoch: [19][200/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 6.2057e-02 (8.9897e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:32 - Epoch: [19][210/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 5.6404e-02 (9.0211e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:34 - Epoch: [19][220/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.003)	Loss 8.2960e-02 (9.0877e-02)	Acc@1  98.44 ( 96.81)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:35 - Epoch: [19][230/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0401e-01 (9.1642e-02)	Acc@1  96.88 ( 96.78)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:37 - Epoch: [19][240/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.6252e-01 (9.2005e-02)	Acc@1  93.75 ( 96.77)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:39 - Epoch: [19][250/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.003)	Loss 7.2137e-02 (9.2057e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:40 - Epoch: [19][260/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4491e-01 (9.3515e-02)	Acc@1  94.53 ( 96.73)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:42 - Epoch: [19][270/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1746e-01 (9.3891e-02)	Acc@1  95.31 ( 96.71)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:44 - Epoch: [19][280/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.1719e-02 (9.4737e-02)	Acc@1  96.88 ( 96.68)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:45 - Epoch: [19][290/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0062e-01 (9.6186e-02)	Acc@1  96.09 ( 96.62)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:05:47 - Epoch: [19][300/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.8421e-01 (9.6263e-02)	Acc@1  93.75 ( 96.63)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:05:49 - Epoch: [19][310/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.0136e-02 (9.6378e-02)	Acc@1  97.66 ( 96.62)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:05:50 - Epoch: [19][320/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.5124e-02 (9.6139e-02)	Acc@1  98.44 ( 96.63)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:52 - Epoch: [19][330/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0849e-01 (9.6695e-02)	Acc@1  96.09 ( 96.62)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:54 - Epoch: [19][340/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0193e-01 (9.6757e-02)	Acc@1  94.53 ( 96.60)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:55 - Epoch: [19][350/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.8117e-02 (9.6313e-02)	Acc@1  96.88 ( 96.61)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:05:56 - Test: [ 0/20]	Time  0.385 ( 0.385)	Loss 3.3434e-01 (3.3434e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:05:57 - Test: [10/20]	Time  0.098 ( 0.126)	Loss 4.1605e-01 (3.9343e-01)	Acc@1  88.67 ( 88.60)	Acc@5  99.61 ( 99.47)
07-Mar-22 03:05:58 -  * Acc@1 89.380 Acc@5 99.380
07-Mar-22 03:05:58 - Best acc at epoch 19: 89.5199966430664
07-Mar-22 03:05:59 - Epoch: [20][  0/352]	Time  0.420 ( 0.420)	Data  0.262 ( 0.262)	Loss 1.5619e-01 (1.5619e-01)	Acc@1  95.31 ( 95.31)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:06:00 - Epoch: [20][ 10/352]	Time  0.165 ( 0.193)	Data  0.002 ( 0.025)	Loss 8.1747e-02 (8.9850e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:06:02 - Epoch: [20][ 20/352]	Time  0.168 ( 0.181)	Data  0.002 ( 0.014)	Loss 2.5734e-02 (9.1074e-02)	Acc@1 100.00 ( 96.95)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:06:04 - Epoch: [20][ 30/352]	Time  0.167 ( 0.176)	Data  0.002 ( 0.010)	Loss 9.2386e-02 (9.0200e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:06:05 - Epoch: [20][ 40/352]	Time  0.168 ( 0.173)	Data  0.001 ( 0.008)	Loss 7.2359e-02 (9.0915e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:07 - Epoch: [20][ 50/352]	Time  0.144 ( 0.170)	Data  0.002 ( 0.007)	Loss 1.0519e-01 (8.9542e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:08 - Epoch: [20][ 60/352]	Time  0.155 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.4216e-01 (8.9637e-02)	Acc@1  92.97 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:06:10 - Epoch: [20][ 70/352]	Time  0.152 ( 0.163)	Data  0.002 ( 0.006)	Loss 6.8260e-02 (9.0579e-02)	Acc@1  98.44 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:11 - Epoch: [20][ 80/352]	Time  0.160 ( 0.162)	Data  0.002 ( 0.005)	Loss 1.0684e-01 (9.4719e-02)	Acc@1  96.09 ( 96.66)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:13 - Epoch: [20][ 90/352]	Time  0.143 ( 0.163)	Data  0.002 ( 0.005)	Loss 9.3034e-02 (9.5165e-02)	Acc@1  97.66 ( 96.65)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:14 - Epoch: [20][100/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.004)	Loss 9.9114e-02 (9.4868e-02)	Acc@1  96.88 ( 96.67)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:16 - Epoch: [20][110/352]	Time  0.141 ( 0.160)	Data  0.002 ( 0.004)	Loss 6.2423e-02 (9.4295e-02)	Acc@1  98.44 ( 96.70)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:06:17 - Epoch: [20][120/352]	Time  0.155 ( 0.159)	Data  0.002 ( 0.004)	Loss 5.8476e-02 (9.4558e-02)	Acc@1  97.66 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:06:19 - Epoch: [20][130/352]	Time  0.157 ( 0.159)	Data  0.002 ( 0.004)	Loss 9.1127e-02 (9.4379e-02)	Acc@1  96.88 ( 96.71)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:06:21 - Epoch: [20][140/352]	Time  0.160 ( 0.159)	Data  0.002 ( 0.004)	Loss 1.0041e-01 (9.4756e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:22 - Epoch: [20][150/352]	Time  0.154 ( 0.159)	Data  0.002 ( 0.004)	Loss 1.4252e-01 (9.4983e-02)	Acc@1  96.09 ( 96.71)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:24 - Epoch: [20][160/352]	Time  0.141 ( 0.159)	Data  0.001 ( 0.004)	Loss 7.1519e-02 (9.4076e-02)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:25 - Epoch: [20][170/352]	Time  0.148 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.3429e-02 (9.3952e-02)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:27 - Epoch: [20][180/352]	Time  0.165 ( 0.159)	Data  0.002 ( 0.003)	Loss 7.9731e-02 (9.4094e-02)	Acc@1  97.66 ( 96.75)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:28 - Epoch: [20][190/352]	Time  0.164 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1693e-01 (9.5012e-02)	Acc@1  95.31 ( 96.71)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:06:30 - Epoch: [20][200/352]	Time  0.144 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.4410e-01 (9.4808e-02)	Acc@1  96.09 ( 96.71)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:06:32 - Epoch: [20][210/352]	Time  0.170 ( 0.159)	Data  0.002 ( 0.003)	Loss 8.3413e-02 (9.4847e-02)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:06:33 - Epoch: [20][220/352]	Time  0.164 ( 0.159)	Data  0.001 ( 0.003)	Loss 6.2150e-02 (9.4741e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:35 - Epoch: [20][230/352]	Time  0.164 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1151e-01 (9.5385e-02)	Acc@1  96.09 ( 96.69)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:36 - Epoch: [20][240/352]	Time  0.149 ( 0.159)	Data  0.001 ( 0.003)	Loss 7.1536e-02 (9.4765e-02)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:38 - Epoch: [20][250/352]	Time  0.168 ( 0.159)	Data  0.002 ( 0.003)	Loss 6.2777e-02 (9.4574e-02)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:40 - Epoch: [20][260/352]	Time  0.156 ( 0.159)	Data  0.002 ( 0.003)	Loss 8.2777e-02 (9.5288e-02)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:06:41 - Epoch: [20][270/352]	Time  0.144 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.8129e-02 (9.5052e-02)	Acc@1  96.09 ( 96.70)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:06:42 - Epoch: [20][280/352]	Time  0.143 ( 0.158)	Data  0.001 ( 0.003)	Loss 8.9023e-02 (9.5462e-02)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:06:44 - Epoch: [20][290/352]	Time  0.141 ( 0.157)	Data  0.001 ( 0.003)	Loss 9.9925e-02 (9.5572e-02)	Acc@1  94.53 ( 96.66)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:45 - Epoch: [20][300/352]	Time  0.146 ( 0.157)	Data  0.002 ( 0.003)	Loss 7.7187e-02 (9.4984e-02)	Acc@1  98.44 ( 96.69)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:47 - Epoch: [20][310/352]	Time  0.143 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.2964e-01 (9.5127e-02)	Acc@1  93.75 ( 96.68)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:06:48 - Epoch: [20][320/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.0613e-01 (9.4957e-02)	Acc@1  94.53 ( 96.69)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:50 - Epoch: [20][330/352]	Time  0.169 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.1346e-02 (9.5343e-02)	Acc@1  99.22 ( 96.68)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:52 - Epoch: [20][340/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.4714e-01 (9.5885e-02)	Acc@1  94.53 ( 96.67)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:06:53 - Epoch: [20][350/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.4905e-01 (9.5375e-02)	Acc@1  95.31 ( 96.69)	Acc@5  99.22 ( 99.98)
07-Mar-22 03:06:54 - Test: [ 0/20]	Time  0.387 ( 0.387)	Loss 3.3020e-01 (3.3020e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:06:55 - Test: [10/20]	Time  0.099 ( 0.126)	Loss 3.8686e-01 (3.9209e-01)	Acc@1  88.67 ( 88.57)	Acc@5  98.83 ( 99.64)
07-Mar-22 03:06:56 -  * Acc@1 89.080 Acc@5 99.580
07-Mar-22 03:06:56 - Best acc at epoch 20: 89.5199966430664
07-Mar-22 03:06:57 - Epoch: [21][  0/352]	Time  0.386 ( 0.386)	Data  0.238 ( 0.238)	Loss 1.6516e-01 (1.6516e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:06:58 - Epoch: [21][ 10/352]	Time  0.189 ( 0.182)	Data  0.002 ( 0.024)	Loss 4.3269e-02 (1.0597e-01)	Acc@1  99.22 ( 96.16)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:00 - Epoch: [21][ 20/352]	Time  0.143 ( 0.168)	Data  0.002 ( 0.013)	Loss 8.1107e-02 (9.8118e-02)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:01 - Epoch: [21][ 30/352]	Time  0.174 ( 0.168)	Data  0.002 ( 0.010)	Loss 6.2318e-02 (9.7720e-02)	Acc@1  96.09 ( 96.19)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:03 - Epoch: [21][ 40/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.008)	Loss 1.2838e-01 (9.6131e-02)	Acc@1  97.66 ( 96.46)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:05 - Epoch: [21][ 50/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.007)	Loss 3.5757e-02 (9.5279e-02)	Acc@1  99.22 ( 96.60)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:06 - Epoch: [21][ 60/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.006)	Loss 1.2851e-01 (9.4373e-02)	Acc@1  96.09 ( 96.67)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:08 - Epoch: [21][ 70/352]	Time  0.168 ( 0.168)	Data  0.003 ( 0.006)	Loss 1.2226e-01 (9.2929e-02)	Acc@1  95.31 ( 96.72)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:10 - Epoch: [21][ 80/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.3307e-01 (9.2194e-02)	Acc@1  96.09 ( 96.74)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:11 - Epoch: [21][ 90/352]	Time  0.167 ( 0.168)	Data  0.003 ( 0.005)	Loss 6.2159e-02 (9.4216e-02)	Acc@1  98.44 ( 96.71)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:13 - Epoch: [21][100/352]	Time  0.171 ( 0.168)	Data  0.003 ( 0.005)	Loss 8.4364e-02 (9.3885e-02)	Acc@1  96.88 ( 96.68)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:15 - Epoch: [21][110/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.1057e-02 (9.4006e-02)	Acc@1  96.88 ( 96.68)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:17 - Epoch: [21][120/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 4.2630e-02 (9.3757e-02)	Acc@1  99.22 ( 96.69)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:18 - Epoch: [21][130/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.6207e-02 (9.3532e-02)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:20 - Epoch: [21][140/352]	Time  0.176 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.6123e-02 (9.3209e-02)	Acc@1  96.09 ( 96.73)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:22 - Epoch: [21][150/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.9191e-02 (9.2744e-02)	Acc@1  98.44 ( 96.72)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:23 - Epoch: [21][160/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.5611e-02 (9.3008e-02)	Acc@1  97.66 ( 96.75)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:25 - Epoch: [21][170/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.2591e-02 (9.2745e-02)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:27 - Epoch: [21][180/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.8164e-01 (9.3185e-02)	Acc@1  93.75 ( 96.72)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:28 - Epoch: [21][190/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.5350e-02 (9.2730e-02)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:30 - Epoch: [21][200/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.9308e-02 (9.2638e-02)	Acc@1  96.88 ( 96.71)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:32 - Epoch: [21][210/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0961e-01 (9.2949e-02)	Acc@1  96.09 ( 96.69)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:33 - Epoch: [21][220/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.0914e-02 (9.3295e-02)	Acc@1  96.09 ( 96.70)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:35 - Epoch: [21][230/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2260e-01 (9.3502e-02)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:36 - Epoch: [21][240/352]	Time  0.141 ( 0.167)	Data  0.002 ( 0.003)	Loss 3.8660e-02 (9.2747e-02)	Acc@1 100.00 ( 96.72)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:38 - Epoch: [21][250/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.6188e-01 (9.2860e-02)	Acc@1  93.75 ( 96.73)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:40 - Epoch: [21][260/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.0537e-02 (9.2929e-02)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:41 - Epoch: [21][270/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.2287e-02 (9.3347e-02)	Acc@1  98.44 ( 96.69)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:43 - Epoch: [21][280/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.5473e-02 (9.2992e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:45 - Epoch: [21][290/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1151e-01 (9.2451e-02)	Acc@1  97.66 ( 96.75)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:46 - Epoch: [21][300/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2079e-01 (9.2191e-02)	Acc@1  95.31 ( 96.76)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:48 - Epoch: [21][310/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.7499e-02 (9.2351e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:50 - Epoch: [21][320/352]	Time  0.169 ( 0.167)	Data  0.003 ( 0.003)	Loss 9.3934e-02 (9.2725e-02)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:51 - Epoch: [21][330/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1465e-01 (9.3627e-02)	Acc@1  95.31 ( 96.67)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:53 - Epoch: [21][340/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.2839e-02 (9.3659e-02)	Acc@1  96.88 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:55 - Epoch: [21][350/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.9493e-02 (9.3821e-02)	Acc@1  96.88 ( 96.66)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:07:55 - Test: [ 0/20]	Time  0.373 ( 0.373)	Loss 3.7488e-01 (3.7488e-01)	Acc@1  87.89 ( 87.89)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:07:56 - Test: [10/20]	Time  0.098 ( 0.127)	Loss 3.8719e-01 (4.0715e-01)	Acc@1  87.89 ( 87.93)	Acc@5  98.44 ( 99.22)
07-Mar-22 03:07:57 -  * Acc@1 88.560 Acc@5 99.160
07-Mar-22 03:07:57 - Best acc at epoch 21: 89.5199966430664
07-Mar-22 03:07:58 - Epoch: [22][  0/352]	Time  0.379 ( 0.379)	Data  0.244 ( 0.244)	Loss 9.9911e-02 (9.9911e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:07:59 - Epoch: [22][ 10/352]	Time  0.167 ( 0.184)	Data  0.002 ( 0.024)	Loss 8.3781e-02 (1.0078e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:01 - Epoch: [22][ 20/352]	Time  0.181 ( 0.178)	Data  0.002 ( 0.014)	Loss 7.8028e-02 (9.4639e-02)	Acc@1  95.31 ( 96.50)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:03 - Epoch: [22][ 30/352]	Time  0.169 ( 0.176)	Data  0.002 ( 0.010)	Loss 6.3650e-02 (9.4956e-02)	Acc@1  98.44 ( 96.52)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:05 - Epoch: [22][ 40/352]	Time  0.164 ( 0.174)	Data  0.002 ( 0.008)	Loss 5.5097e-02 (9.4760e-02)	Acc@1  98.44 ( 96.61)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:06 - Epoch: [22][ 50/352]	Time  0.167 ( 0.172)	Data  0.002 ( 0.007)	Loss 6.1726e-02 (9.3010e-02)	Acc@1  97.66 ( 96.69)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:08 - Epoch: [22][ 60/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.006)	Loss 8.7110e-02 (9.3093e-02)	Acc@1  97.66 ( 96.68)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:10 - Epoch: [22][ 70/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.006)	Loss 8.5174e-02 (9.2949e-02)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:11 - Epoch: [22][ 80/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.5735e-01 (9.5328e-02)	Acc@1  94.53 ( 96.58)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:13 - Epoch: [22][ 90/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.005)	Loss 8.4778e-02 (9.4604e-02)	Acc@1  96.88 ( 96.61)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:15 - Epoch: [22][100/352]	Time  0.165 ( 0.170)	Data  0.002 ( 0.005)	Loss 8.7939e-02 (9.5073e-02)	Acc@1  96.88 ( 96.59)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:16 - Epoch: [22][110/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.004)	Loss 7.0142e-02 (9.3363e-02)	Acc@1  97.66 ( 96.66)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:18 - Epoch: [22][120/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 3.0941e-02 (9.1179e-02)	Acc@1 100.00 ( 96.78)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:20 - Epoch: [22][130/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.2117e-01 (9.0867e-02)	Acc@1  96.09 ( 96.79)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:21 - Epoch: [22][140/352]	Time  0.167 ( 0.169)	Data  0.003 ( 0.004)	Loss 8.2204e-02 (8.9878e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:23 - Epoch: [22][150/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.0535e-01 (9.1561e-02)	Acc@1  94.53 ( 96.78)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:25 - Epoch: [22][160/352]	Time  0.160 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.0906e-01 (9.2722e-02)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:26 - Epoch: [22][170/352]	Time  0.157 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.4584e-01 (9.3144e-02)	Acc@1  94.53 ( 96.71)	Acc@5  99.22 ( 99.99)
07-Mar-22 03:08:28 - Epoch: [22][180/352]	Time  0.161 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.8769e-02 (9.3548e-02)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:08:29 - Epoch: [22][190/352]	Time  0.143 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.4696e-02 (9.3398e-02)	Acc@1  99.22 ( 96.71)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:08:31 - Epoch: [22][200/352]	Time  0.143 ( 0.165)	Data  0.002 ( 0.003)	Loss 9.1067e-02 (9.3788e-02)	Acc@1  97.66 ( 96.69)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:08:32 - Epoch: [22][210/352]	Time  0.145 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.2491e-02 (9.4415e-02)	Acc@1  97.66 ( 96.66)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:08:34 - Epoch: [22][220/352]	Time  0.142 ( 0.163)	Data  0.002 ( 0.003)	Loss 3.7419e-02 (9.4019e-02)	Acc@1  97.66 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:08:35 - Epoch: [22][230/352]	Time  0.142 ( 0.162)	Data  0.002 ( 0.003)	Loss 4.4145e-02 (9.3872e-02)	Acc@1  99.22 ( 96.68)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:08:36 - Epoch: [22][240/352]	Time  0.141 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.3198e-01 (9.3950e-02)	Acc@1  96.09 ( 96.71)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:08:38 - Epoch: [22][250/352]	Time  0.142 ( 0.161)	Data  0.002 ( 0.003)	Loss 5.6836e-02 (9.4056e-02)	Acc@1  99.22 ( 96.72)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:08:39 - Epoch: [22][260/352]	Time  0.143 ( 0.160)	Data  0.002 ( 0.003)	Loss 7.3968e-02 (9.4662e-02)	Acc@1  97.66 ( 96.69)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:08:41 - Epoch: [22][270/352]	Time  0.142 ( 0.159)	Data  0.002 ( 0.003)	Loss 4.1150e-02 (9.5117e-02)	Acc@1  99.22 ( 96.65)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:08:42 - Epoch: [22][280/352]	Time  0.140 ( 0.159)	Data  0.001 ( 0.003)	Loss 9.6713e-02 (9.4871e-02)	Acc@1  96.09 ( 96.65)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:08:44 - Epoch: [22][290/352]	Time  0.167 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1092e-01 (9.5277e-02)	Acc@1  96.09 ( 96.64)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:08:45 - Epoch: [22][300/352]	Time  0.167 ( 0.159)	Data  0.002 ( 0.003)	Loss 8.4562e-02 (9.5575e-02)	Acc@1  97.66 ( 96.64)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:08:47 - Epoch: [22][310/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.2042e-01 (9.5644e-02)	Acc@1  96.09 ( 96.63)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:08:49 - Epoch: [22][320/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.003)	Loss 7.6055e-02 (9.4715e-02)	Acc@1  97.66 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:08:50 - Epoch: [22][330/352]	Time  0.167 ( 0.160)	Data  0.002 ( 0.003)	Loss 6.3020e-02 (9.4336e-02)	Acc@1  99.22 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:08:52 - Epoch: [22][340/352]	Time  0.160 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.1592e-01 (9.4033e-02)	Acc@1  92.97 ( 96.68)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:08:54 - Epoch: [22][350/352]	Time  0.168 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.0463e-01 (9.4228e-02)	Acc@1  96.09 ( 96.65)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:08:54 - Test: [ 0/20]	Time  0.373 ( 0.373)	Loss 3.7135e-01 (3.7135e-01)	Acc@1  88.67 ( 88.67)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:55 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 3.6137e-01 (3.9064e-01)	Acc@1  88.67 ( 88.88)	Acc@5  99.61 ( 99.57)
07-Mar-22 03:08:56 -  * Acc@1 89.360 Acc@5 99.380
07-Mar-22 03:08:56 - Best acc at epoch 22: 89.5199966430664
07-Mar-22 03:08:57 - Epoch: [23][  0/352]	Time  0.371 ( 0.371)	Data  0.226 ( 0.226)	Loss 5.4945e-02 (5.4945e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
07-Mar-22 03:08:58 - Epoch: [23][ 10/352]	Time  0.165 ( 0.173)	Data  0.002 ( 0.022)	Loss 7.9401e-02 (1.0176e-01)	Acc@1  97.66 ( 96.02)	Acc@5 100.00 (100.00)
07-Mar-22 03:09:00 - Epoch: [23][ 20/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.013)	Loss 6.6482e-02 (8.3924e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:09:02 - Epoch: [23][ 30/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.009)	Loss 9.9490e-02 (8.1832e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 03:09:03 - Epoch: [23][ 40/352]	Time  0.166 ( 0.169)	Data  0.001 ( 0.007)	Loss 8.0323e-02 (8.3922e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 (100.00)
07-Mar-22 03:09:05 - Epoch: [23][ 50/352]	Time  0.141 ( 0.165)	Data  0.002 ( 0.006)	Loss 1.1208e-01 (8.3824e-02)	Acc@1  95.31 ( 97.01)	Acc@5 100.00 (100.00)
07-Mar-22 03:09:06 - Epoch: [23][ 60/352]	Time  0.168 ( 0.163)	Data  0.002 ( 0.006)	Loss 1.3546e-01 (8.4631e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 (100.00)
07-Mar-22 03:09:08 - Epoch: [23][ 70/352]	Time  0.165 ( 0.163)	Data  0.002 ( 0.005)	Loss 1.0701e-01 (8.6041e-02)	Acc@1  95.31 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:10 - Epoch: [23][ 80/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.005)	Loss 1.1255e-01 (8.5623e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:11 - Epoch: [23][ 90/352]	Time  0.165 ( 0.164)	Data  0.002 ( 0.004)	Loss 4.7075e-02 (8.7168e-02)	Acc@1 100.00 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:13 - Epoch: [23][100/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.004)	Loss 1.1247e-01 (8.8082e-02)	Acc@1  95.31 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:15 - Epoch: [23][110/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.004)	Loss 5.6619e-02 (8.7736e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:16 - Epoch: [23][120/352]	Time  0.143 ( 0.164)	Data  0.002 ( 0.004)	Loss 5.4719e-02 (8.7754e-02)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:18 - Epoch: [23][130/352]	Time  0.143 ( 0.163)	Data  0.002 ( 0.004)	Loss 6.7736e-02 (8.7874e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:19 - Epoch: [23][140/352]	Time  0.141 ( 0.161)	Data  0.002 ( 0.003)	Loss 6.4437e-02 (8.8741e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:21 - Epoch: [23][150/352]	Time  0.165 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.1782e-01 (8.8991e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:22 - Epoch: [23][160/352]	Time  0.164 ( 0.161)	Data  0.002 ( 0.003)	Loss 4.6334e-02 (9.0306e-02)	Acc@1  98.44 ( 96.82)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:24 - Epoch: [23][170/352]	Time  0.168 ( 0.161)	Data  0.002 ( 0.003)	Loss 8.2522e-02 (8.9589e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:25 - Epoch: [23][180/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.003)	Loss 7.3977e-02 (8.9213e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:27 - Epoch: [23][190/352]	Time  0.170 ( 0.161)	Data  0.003 ( 0.003)	Loss 1.1239e-01 (9.0952e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:29 - Epoch: [23][200/352]	Time  0.143 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.0439e-01 (9.1436e-02)	Acc@1  96.09 ( 96.77)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:30 - Epoch: [23][210/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.7908e-01 (9.2003e-02)	Acc@1  95.31 ( 96.73)	Acc@5  99.22 ( 99.99)
07-Mar-22 03:09:32 - Epoch: [23][220/352]	Time  0.169 ( 0.161)	Data  0.002 ( 0.003)	Loss 8.4092e-02 (9.2307e-02)	Acc@1  96.88 ( 96.71)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:34 - Epoch: [23][230/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.5465e-02 (9.2866e-02)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:35 - Epoch: [23][240/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.0707e-01 (9.2968e-02)	Acc@1  94.53 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:37 - Epoch: [23][250/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.9393e-02 (9.3244e-02)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:38 - Epoch: [23][260/352]	Time  0.143 ( 0.161)	Data  0.001 ( 0.003)	Loss 6.8010e-02 (9.3430e-02)	Acc@1  97.66 ( 96.64)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:40 - Epoch: [23][270/352]	Time  0.154 ( 0.161)	Data  0.002 ( 0.003)	Loss 2.0142e-01 (9.3347e-02)	Acc@1  95.31 ( 96.66)	Acc@5  99.22 ( 99.99)
07-Mar-22 03:09:41 - Epoch: [23][280/352]	Time  0.169 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.2582e-01 (9.3309e-02)	Acc@1  96.09 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:43 - Epoch: [23][290/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.003)	Loss 6.5701e-02 (9.3302e-02)	Acc@1  96.88 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:45 - Epoch: [23][300/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.003)	Loss 8.7111e-02 (9.3356e-02)	Acc@1  96.88 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:47 - Epoch: [23][310/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.003)	Loss 9.9915e-02 (9.3630e-02)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:48 - Epoch: [23][320/352]	Time  0.164 ( 0.162)	Data  0.002 ( 0.003)	Loss 6.6563e-02 (9.3981e-02)	Acc@1  97.66 ( 96.64)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:50 - Epoch: [23][330/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.003)	Loss 9.9913e-02 (9.4378e-02)	Acc@1  94.53 ( 96.61)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:52 - Epoch: [23][340/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 6.4262e-02 (9.4373e-02)	Acc@1  97.66 ( 96.62)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:53 - Epoch: [23][350/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.6268e-01 (9.4629e-02)	Acc@1  94.53 ( 96.62)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:09:54 - Test: [ 0/20]	Time  0.355 ( 0.355)	Loss 3.3319e-01 (3.3319e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:09:55 - Test: [10/20]	Time  0.100 ( 0.126)	Loss 3.6735e-01 (3.8717e-01)	Acc@1  89.45 ( 88.57)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:09:56 -  * Acc@1 89.240 Acc@5 99.440
07-Mar-22 03:09:56 - Best acc at epoch 23: 89.5199966430664
07-Mar-22 03:09:56 - Epoch: [24][  0/352]	Time  0.383 ( 0.383)	Data  0.236 ( 0.236)	Loss 6.5039e-02 (6.5039e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
07-Mar-22 03:09:58 - Epoch: [24][ 10/352]	Time  0.146 ( 0.183)	Data  0.002 ( 0.023)	Loss 1.0263e-01 (8.3105e-02)	Acc@1  95.31 ( 97.37)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:09:59 - Epoch: [24][ 20/352]	Time  0.143 ( 0.164)	Data  0.001 ( 0.013)	Loss 1.1188e-01 (9.5399e-02)	Acc@1  94.53 ( 96.84)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:10:01 - Epoch: [24][ 30/352]	Time  0.165 ( 0.160)	Data  0.002 ( 0.009)	Loss 1.2683e-01 (9.7674e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:10:02 - Epoch: [24][ 40/352]	Time  0.146 ( 0.158)	Data  0.002 ( 0.007)	Loss 6.5914e-02 (9.2297e-02)	Acc@1  98.44 ( 96.80)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:04 - Epoch: [24][ 50/352]	Time  0.156 ( 0.158)	Data  0.002 ( 0.006)	Loss 8.0147e-02 (8.9499e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:05 - Epoch: [24][ 60/352]	Time  0.149 ( 0.157)	Data  0.002 ( 0.006)	Loss 1.2072e-01 (9.3663e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:10:07 - Epoch: [24][ 70/352]	Time  0.143 ( 0.157)	Data  0.001 ( 0.005)	Loss 1.2459e-01 (9.4940e-02)	Acc@1  95.31 ( 96.70)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:10:08 - Epoch: [24][ 80/352]	Time  0.143 ( 0.155)	Data  0.002 ( 0.005)	Loss 4.9997e-02 (9.6725e-02)	Acc@1  99.22 ( 96.65)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:10:10 - Epoch: [24][ 90/352]	Time  0.152 ( 0.154)	Data  0.002 ( 0.004)	Loss 8.9394e-02 (9.6229e-02)	Acc@1  96.09 ( 96.70)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:10:12 - Epoch: [24][100/352]	Time  0.165 ( 0.155)	Data  0.002 ( 0.004)	Loss 6.1306e-02 (9.5207e-02)	Acc@1  97.66 ( 96.68)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:13 - Epoch: [24][110/352]	Time  0.165 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.1348e-01 (9.4639e-02)	Acc@1  96.09 ( 96.70)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:15 - Epoch: [24][120/352]	Time  0.167 ( 0.157)	Data  0.002 ( 0.004)	Loss 7.1608e-02 (9.3988e-02)	Acc@1  96.09 ( 96.74)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:10:16 - Epoch: [24][130/352]	Time  0.146 ( 0.157)	Data  0.002 ( 0.004)	Loss 1.5547e-01 (9.5008e-02)	Acc@1  93.75 ( 96.68)	Acc@5  99.22 ( 99.97)
07-Mar-22 03:10:18 - Epoch: [24][140/352]	Time  0.144 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.9807e-02 (9.3756e-02)	Acc@1  96.88 ( 96.71)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:10:20 - Epoch: [24][150/352]	Time  0.169 ( 0.157)	Data  0.002 ( 0.003)	Loss 9.5753e-02 (9.3163e-02)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:10:21 - Epoch: [24][160/352]	Time  0.167 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.1265e-02 (9.2239e-02)	Acc@1  98.44 ( 96.76)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:23 - Epoch: [24][170/352]	Time  0.167 ( 0.158)	Data  0.002 ( 0.003)	Loss 5.5530e-02 (9.1267e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:25 - Epoch: [24][180/352]	Time  0.167 ( 0.159)	Data  0.002 ( 0.003)	Loss 3.9419e-02 (9.0975e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:26 - Epoch: [24][190/352]	Time  0.167 ( 0.159)	Data  0.002 ( 0.003)	Loss 4.3272e-02 (9.0783e-02)	Acc@1  99.22 ( 96.82)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:28 - Epoch: [24][200/352]	Time  0.168 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.5853e-01 (9.1093e-02)	Acc@1  92.19 ( 96.82)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:29 - Epoch: [24][210/352]	Time  0.142 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.0801e-01 (9.0712e-02)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:31 - Epoch: [24][220/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.003)	Loss 7.7501e-02 (9.1452e-02)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:32 - Epoch: [24][230/352]	Time  0.141 ( 0.158)	Data  0.002 ( 0.003)	Loss 6.0399e-02 (9.1189e-02)	Acc@1  98.44 ( 96.80)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:34 - Epoch: [24][240/352]	Time  0.141 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.6345e-01 (9.1865e-02)	Acc@1  92.97 ( 96.76)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:35 - Epoch: [24][250/352]	Time  0.174 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.1852e-01 (9.1887e-02)	Acc@1  95.31 ( 96.77)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:37 - Epoch: [24][260/352]	Time  0.164 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.3042e-01 (9.2093e-02)	Acc@1  93.75 ( 96.73)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:39 - Epoch: [24][270/352]	Time  0.166 ( 0.158)	Data  0.002 ( 0.003)	Loss 6.8340e-02 (9.2648e-02)	Acc@1  99.22 ( 96.69)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:40 - Epoch: [24][280/352]	Time  0.144 ( 0.158)	Data  0.001 ( 0.003)	Loss 7.9019e-02 (9.2367e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:42 - Epoch: [24][290/352]	Time  0.141 ( 0.157)	Data  0.001 ( 0.003)	Loss 9.5725e-02 (9.2749e-02)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:43 - Epoch: [24][300/352]	Time  0.141 ( 0.157)	Data  0.001 ( 0.003)	Loss 6.7641e-02 (9.2683e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:45 - Epoch: [24][310/352]	Time  0.141 ( 0.156)	Data  0.001 ( 0.003)	Loss 9.7580e-02 (9.2291e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:46 - Epoch: [24][320/352]	Time  0.141 ( 0.156)	Data  0.001 ( 0.003)	Loss 1.7647e-01 (9.2792e-02)	Acc@1  92.97 ( 96.71)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:47 - Epoch: [24][330/352]	Time  0.141 ( 0.155)	Data  0.001 ( 0.003)	Loss 5.7394e-02 (9.2363e-02)	Acc@1  98.44 ( 96.72)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:49 - Epoch: [24][340/352]	Time  0.141 ( 0.155)	Data  0.001 ( 0.002)	Loss 5.5579e-02 (9.2016e-02)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:50 - Epoch: [24][350/352]	Time  0.141 ( 0.155)	Data  0.001 ( 0.002)	Loss 7.3811e-02 (9.2227e-02)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:10:51 - Test: [ 0/20]	Time  0.378 ( 0.378)	Loss 3.3002e-01 (3.3002e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:10:52 - Test: [10/20]	Time  0.099 ( 0.125)	Loss 3.8425e-01 (3.7879e-01)	Acc@1  88.28 ( 88.71)	Acc@5  99.22 ( 99.43)
07-Mar-22 03:10:53 -  * Acc@1 89.480 Acc@5 99.440
07-Mar-22 03:10:53 - Best acc at epoch 24: 89.5199966430664
07-Mar-22 03:10:53 - Epoch: [25][  0/352]	Time  0.408 ( 0.408)	Data  0.245 ( 0.245)	Loss 7.9221e-02 (7.9221e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:10:55 - Epoch: [25][ 10/352]	Time  0.166 ( 0.189)	Data  0.002 ( 0.024)	Loss 1.5873e-01 (7.7460e-02)	Acc@1  95.31 ( 97.37)	Acc@5 100.00 (100.00)
07-Mar-22 03:10:57 - Epoch: [25][ 20/352]	Time  0.175 ( 0.179)	Data  0.002 ( 0.014)	Loss 1.0459e-01 (8.2215e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 03:10:58 - Epoch: [25][ 30/352]	Time  0.142 ( 0.175)	Data  0.001 ( 0.010)	Loss 8.8700e-02 (8.5035e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:11:00 - Epoch: [25][ 40/352]	Time  0.167 ( 0.172)	Data  0.002 ( 0.008)	Loss 5.5741e-02 (8.3582e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 (100.00)
07-Mar-22 03:11:02 - Epoch: [25][ 50/352]	Time  0.164 ( 0.171)	Data  0.002 ( 0.007)	Loss 9.8923e-02 (8.3519e-02)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:11:03 - Epoch: [25][ 60/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.006)	Loss 9.8257e-02 (8.4048e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 (100.00)
07-Mar-22 03:11:05 - Epoch: [25][ 70/352]	Time  0.143 ( 0.169)	Data  0.002 ( 0.005)	Loss 8.6131e-02 (8.4872e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 (100.00)
07-Mar-22 03:11:06 - Epoch: [25][ 80/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.005)	Loss 4.0915e-02 (8.3489e-02)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 (100.00)
07-Mar-22 03:11:08 - Epoch: [25][ 90/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.005)	Loss 1.1982e-01 (8.5317e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 (100.00)
07-Mar-22 03:11:09 - Epoch: [25][100/352]	Time  0.142 ( 0.165)	Data  0.002 ( 0.004)	Loss 6.8859e-02 (8.4726e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 (100.00)
07-Mar-22 03:11:11 - Epoch: [25][110/352]	Time  0.164 ( 0.163)	Data  0.002 ( 0.004)	Loss 1.1725e-01 (8.6114e-02)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:12 - Epoch: [25][120/352]	Time  0.148 ( 0.162)	Data  0.002 ( 0.004)	Loss 1.2087e-01 (8.7772e-02)	Acc@1  96.09 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:14 - Epoch: [25][130/352]	Time  0.177 ( 0.162)	Data  0.003 ( 0.004)	Loss 9.5025e-02 (8.8838e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:14 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 03:11:14 - => creating PyTorchCV model 'resnet20_unfold'
07-Mar-22 03:11:14 - match all modules defined in bit_config: True
07-Mar-22 03:11:14 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_conv_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
07-Mar-22 03:11:16 - Epoch: [25][140/352]	Time  0.209 ( 0.163)	Data  0.003 ( 0.004)	Loss 6.6844e-02 (8.9048e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:18 - Epoch: [25][150/352]	Time  0.182 ( 0.164)	Data  0.002 ( 0.004)	Loss 5.9958e-02 (8.9271e-02)	Acc@1 100.00 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:19 - Epoch: [0][  0/352]	Time  0.460 ( 0.460)	Data  0.215 ( 0.215)	Loss 1.1630e+00 (1.1630e+00)	Acc@1  65.62 ( 65.62)	Acc@5  92.97 ( 92.97)
07-Mar-22 03:11:19 - Epoch: [25][160/352]	Time  0.142 ( 0.164)	Data  0.002 ( 0.004)	Loss 8.5337e-02 (8.8627e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:21 - Epoch: [25][170/352]	Time  0.165 ( 0.163)	Data  0.002 ( 0.004)	Loss 3.9813e-02 (8.8248e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:21 - Epoch: [0][ 10/352]	Time  0.173 ( 0.194)	Data  0.002 ( 0.022)	Loss 3.3839e-01 (7.2983e-01)	Acc@1  89.84 ( 78.91)	Acc@5 100.00 ( 97.23)
07-Mar-22 03:11:22 - Epoch: [25][180/352]	Time  0.207 ( 0.164)	Data  0.002 ( 0.003)	Loss 9.9043e-02 (8.8553e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:23 - Epoch: [0][ 20/352]	Time  0.180 ( 0.187)	Data  0.002 ( 0.012)	Loss 3.5554e-01 (5.4696e-01)	Acc@1  92.19 ( 83.82)	Acc@5 100.00 ( 98.33)
07-Mar-22 03:11:24 - Epoch: [25][190/352]	Time  0.141 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.4805e-01 (8.9332e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:24 - Epoch: [0][ 30/352]	Time  0.187 ( 0.183)	Data  0.002 ( 0.009)	Loss 3.4848e-01 (4.6233e-01)	Acc@1  89.06 ( 85.99)	Acc@5  99.22 ( 98.82)
07-Mar-22 03:11:26 - Epoch: [25][200/352]	Time  0.141 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.5350e-02 (9.0261e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:26 - Epoch: [0][ 40/352]	Time  0.170 ( 0.178)	Data  0.002 ( 0.007)	Loss 3.1338e-01 (4.1219e-01)	Acc@1  89.06 ( 87.31)	Acc@5 100.00 ( 99.05)
07-Mar-22 03:11:27 - Epoch: [25][210/352]	Time  0.141 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.2624e-01 (9.0286e-02)	Acc@1  94.53 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:28 - Epoch: [0][ 50/352]	Time  0.171 ( 0.177)	Data  0.002 ( 0.006)	Loss 2.8701e-01 (3.7706e-01)	Acc@1  89.84 ( 88.33)	Acc@5 100.00 ( 99.20)
07-Mar-22 03:11:29 - Epoch: [25][220/352]	Time  0.164 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.4172e-01 (9.0826e-02)	Acc@1  93.75 ( 96.83)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:29 - Epoch: [0][ 60/352]	Time  0.170 ( 0.175)	Data  0.002 ( 0.006)	Loss 1.9397e-01 (3.5235e-01)	Acc@1  92.97 ( 89.09)	Acc@5 100.00 ( 99.24)
07-Mar-22 03:11:30 - Epoch: [25][230/352]	Time  0.158 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.0697e-01 (9.0554e-02)	Acc@1  92.19 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:31 - Epoch: [0][ 70/352]	Time  0.192 ( 0.175)	Data  0.002 ( 0.005)	Loss 2.8677e-01 (3.3687e-01)	Acc@1  92.19 ( 89.60)	Acc@5  98.44 ( 99.27)
07-Mar-22 03:11:32 - Epoch: [25][240/352]	Time  0.165 ( 0.162)	Data  0.002 ( 0.003)	Loss 4.4279e-02 (9.1250e-02)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:33 - Epoch: [0][ 80/352]	Time  0.168 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.1798e-01 (3.1880e-01)	Acc@1  96.88 ( 90.03)	Acc@5 100.00 ( 99.34)
07-Mar-22 03:11:33 - Epoch: [25][250/352]	Time  0.159 ( 0.161)	Data  0.002 ( 0.003)	Loss 7.5362e-02 (9.2154e-02)	Acc@1  97.66 ( 96.79)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:35 - Epoch: [0][ 90/352]	Time  0.181 ( 0.174)	Data  0.002 ( 0.005)	Loss 2.9079e-01 (3.0888e-01)	Acc@1  91.41 ( 90.26)	Acc@5  98.44 ( 99.36)
07-Mar-22 03:11:35 - Epoch: [25][260/352]	Time  0.163 ( 0.161)	Data  0.002 ( 0.003)	Loss 5.9184e-02 (9.1721e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:36 - Epoch: [0][100/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.004)	Loss 1.5424e-01 (2.9874e-01)	Acc@1  93.75 ( 90.62)	Acc@5 100.00 ( 99.41)
07-Mar-22 03:11:36 - Epoch: [25][270/352]	Time  0.167 ( 0.161)	Data  0.003 ( 0.003)	Loss 1.5124e-01 (9.2353e-02)	Acc@1  95.31 ( 96.77)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:38 - Epoch: [0][110/352]	Time  0.172 ( 0.173)	Data  0.002 ( 0.004)	Loss 2.3008e-01 (2.9094e-01)	Acc@1  92.19 ( 90.82)	Acc@5 100.00 ( 99.47)
07-Mar-22 03:11:38 - Epoch: [25][280/352]	Time  0.164 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.1828e-01 (9.2289e-02)	Acc@1  94.53 ( 96.77)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:40 - Epoch: [0][120/352]	Time  0.169 ( 0.172)	Data  0.002 ( 0.004)	Loss 1.4109e-01 (2.8213e-01)	Acc@1  94.53 ( 91.05)	Acc@5 100.00 ( 99.50)
07-Mar-22 03:11:40 - Epoch: [25][290/352]	Time  0.161 ( 0.161)	Data  0.002 ( 0.003)	Loss 6.1835e-02 (9.1864e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:41 - Epoch: [0][130/352]	Time  0.174 ( 0.172)	Data  0.002 ( 0.004)	Loss 1.8839e-01 (2.7637e-01)	Acc@1  92.19 ( 91.19)	Acc@5 100.00 ( 99.53)
07-Mar-22 03:11:41 - Epoch: [25][300/352]	Time  0.163 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.0722e-01 (9.1591e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:43 - Epoch: [0][140/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.004)	Loss 1.8858e-01 (2.7016e-01)	Acc@1  94.53 ( 91.37)	Acc@5 100.00 ( 99.56)
07-Mar-22 03:11:43 - Epoch: [25][310/352]	Time  0.142 ( 0.161)	Data  0.002 ( 0.003)	Loss 6.0224e-02 (9.1517e-02)	Acc@1  98.44 ( 96.79)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:44 - Epoch: [25][320/352]	Time  0.165 ( 0.161)	Data  0.002 ( 0.003)	Loss 5.9190e-02 (9.1483e-02)	Acc@1  97.66 ( 96.79)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:45 - Epoch: [0][150/352]	Time  0.179 ( 0.171)	Data  0.001 ( 0.004)	Loss 1.4291e-01 (2.6794e-01)	Acc@1  95.31 ( 91.44)	Acc@5 100.00 ( 99.57)
07-Mar-22 03:11:46 - Epoch: [0][160/352]	Time  0.153 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.6315e-01 (2.6340e-01)	Acc@1  93.75 ( 91.59)	Acc@5 100.00 ( 99.58)
07-Mar-22 03:11:46 - Epoch: [25][330/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.003)	Loss 7.9457e-02 (9.1409e-02)	Acc@1  96.09 ( 96.79)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:48 - Epoch: [25][340/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.003)	Loss 4.0577e-02 (9.1599e-02)	Acc@1  99.22 ( 96.77)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:48 - Epoch: [0][170/352]	Time  0.178 ( 0.171)	Data  0.002 ( 0.003)	Loss 2.2018e-01 (2.6072e-01)	Acc@1  92.97 ( 91.65)	Acc@5 100.00 ( 99.60)
07-Mar-22 03:11:50 - Epoch: [25][350/352]	Time  0.165 ( 0.161)	Data  0.002 ( 0.003)	Loss 6.3755e-02 (9.1770e-02)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:11:50 - Epoch: [0][180/352]	Time  0.171 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.8710e-01 (2.5743e-01)	Acc@1  95.31 ( 91.81)	Acc@5 100.00 ( 99.62)
07-Mar-22 03:11:50 - Test: [ 0/20]	Time  0.374 ( 0.374)	Loss 3.7540e-01 (3.7540e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:11:51 - Test: [10/20]	Time  0.114 ( 0.128)	Loss 4.1800e-01 (4.0626e-01)	Acc@1  87.89 ( 88.74)	Acc@5  99.22 ( 99.36)
07-Mar-22 03:11:51 - Epoch: [0][190/352]	Time  0.168 ( 0.170)	Data  0.001 ( 0.003)	Loss 1.5036e-01 (2.5427e-01)	Acc@1  95.31 ( 91.86)	Acc@5 100.00 ( 99.63)
07-Mar-22 03:11:52 -  * Acc@1 89.000 Acc@5 99.320
07-Mar-22 03:11:52 - Best acc at epoch 25: 89.5199966430664
07-Mar-22 03:11:53 - Epoch: [26][  0/352]	Time  0.382 ( 0.382)	Data  0.237 ( 0.237)	Loss 7.2307e-02 (7.2307e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
07-Mar-22 03:11:53 - Epoch: [0][200/352]	Time  0.141 ( 0.169)	Data  0.002 ( 0.003)	Loss 2.1712e-01 (2.5074e-01)	Acc@1  92.19 ( 91.97)	Acc@5 100.00 ( 99.64)
07-Mar-22 03:11:54 - Epoch: [26][ 10/352]	Time  0.142 ( 0.176)	Data  0.002 ( 0.023)	Loss 5.0978e-02 (1.0015e-01)	Acc@1  99.22 ( 96.59)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:11:54 - Epoch: [0][210/352]	Time  0.174 ( 0.169)	Data  0.002 ( 0.003)	Loss 2.0080e-01 (2.4751e-01)	Acc@1  92.97 ( 92.04)	Acc@5 100.00 ( 99.64)
07-Mar-22 03:11:56 - Epoch: [26][ 20/352]	Time  0.163 ( 0.172)	Data  0.002 ( 0.013)	Loss 1.2300e-01 (1.0498e-01)	Acc@1  95.31 ( 96.43)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:11:56 - Epoch: [0][220/352]	Time  0.172 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.3329e-01 (2.4462e-01)	Acc@1  97.66 ( 92.12)	Acc@5 100.00 ( 99.66)
07-Mar-22 03:11:57 - Epoch: [26][ 30/352]	Time  0.144 ( 0.170)	Data  0.002 ( 0.010)	Loss 1.3697e-01 (1.0083e-01)	Acc@1  94.53 ( 96.52)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:11:58 - Epoch: [0][230/352]	Time  0.192 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.5127e-01 (2.4245e-01)	Acc@1  94.53 ( 92.18)	Acc@5 100.00 ( 99.67)
07-Mar-22 03:11:59 - Epoch: [26][ 40/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.008)	Loss 8.0353e-02 (9.6695e-02)	Acc@1  98.44 ( 96.76)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:11:59 - Epoch: [0][240/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 2.8448e-01 (2.4240e-01)	Acc@1  89.06 ( 92.16)	Acc@5 100.00 ( 99.68)
07-Mar-22 03:12:01 - Epoch: [26][ 50/352]	Time  0.143 ( 0.164)	Data  0.002 ( 0.007)	Loss 7.5660e-02 (1.0266e-01)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:01 - Epoch: [0][250/352]	Time  0.176 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.8348e-01 (2.4017e-01)	Acc@1  92.97 ( 92.20)	Acc@5 100.00 ( 99.68)
07-Mar-22 03:12:02 - Epoch: [26][ 60/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.006)	Loss 1.2275e-01 (1.0248e-01)	Acc@1  96.09 ( 96.47)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:12:03 - Epoch: [0][260/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.5363e-01 (2.3890e-01)	Acc@1  94.53 ( 92.24)	Acc@5  99.22 ( 99.68)
07-Mar-22 03:12:04 - Epoch: [26][ 70/352]	Time  0.163 ( 0.165)	Data  0.002 ( 0.005)	Loss 9.1339e-02 (9.8786e-02)	Acc@1  96.09 ( 96.51)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:12:04 - Epoch: [0][270/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.8432e-01 (2.3624e-01)	Acc@1  93.75 ( 92.33)	Acc@5 100.00 ( 99.69)
07-Mar-22 03:12:06 - Epoch: [26][ 80/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.005)	Loss 9.2227e-02 (9.8455e-02)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:06 - Epoch: [0][280/352]	Time  0.173 ( 0.169)	Data  0.002 ( 0.003)	Loss 2.1798e-01 (2.3478e-01)	Acc@1  92.97 ( 92.37)	Acc@5 100.00 ( 99.70)
07-Mar-22 03:12:07 - Epoch: [26][ 90/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.005)	Loss 1.6063e-01 (9.8001e-02)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:12:08 - Epoch: [0][290/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.003)	Loss 8.1185e-02 (2.3327e-01)	Acc@1  97.66 ( 92.42)	Acc@5 100.00 ( 99.71)
07-Mar-22 03:12:09 - Epoch: [26][100/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.004)	Loss 2.1984e-02 (9.7568e-02)	Acc@1 100.00 ( 96.50)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:12:10 - Epoch: [0][300/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.3213e-01 (2.3137e-01)	Acc@1  96.88 ( 92.47)	Acc@5 100.00 ( 99.72)
07-Mar-22 03:12:11 - Epoch: [26][110/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.004)	Loss 9.3505e-02 (9.6396e-02)	Acc@1  94.53 ( 96.52)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:12:11 - Epoch: [0][310/352]	Time  0.171 ( 0.170)	Data  0.002 ( 0.003)	Loss 2.2848e-01 (2.3043e-01)	Acc@1  92.97 ( 92.49)	Acc@5 100.00 ( 99.72)
07-Mar-22 03:12:12 - Epoch: [26][120/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.004)	Loss 9.1180e-02 (9.5686e-02)	Acc@1  97.66 ( 96.51)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:12:13 - Epoch: [0][320/352]	Time  0.172 ( 0.170)	Data  0.002 ( 0.003)	Loss 1.7685e-01 (2.2939e-01)	Acc@1  93.75 ( 92.52)	Acc@5 100.00 ( 99.72)
07-Mar-22 03:12:14 - Epoch: [26][130/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.004)	Loss 7.7471e-02 (9.6110e-02)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:15 - Epoch: [0][330/352]	Time  0.171 ( 0.170)	Data  0.002 ( 0.003)	Loss 1.8497e-01 (2.2787e-01)	Acc@1  93.75 ( 92.59)	Acc@5 100.00 ( 99.72)
07-Mar-22 03:12:15 - Epoch: [26][140/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.004)	Loss 1.6771e-01 (9.6361e-02)	Acc@1  94.53 ( 96.53)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:17 - Epoch: [0][340/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.003)	Loss 2.1492e-01 (2.2673e-01)	Acc@1  94.53 ( 92.64)	Acc@5  99.22 ( 99.73)
07-Mar-22 03:12:17 - Epoch: [26][150/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.004)	Loss 7.5442e-02 (9.6348e-02)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:18 - Epoch: [0][350/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.003)	Loss 2.2560e-01 (2.2713e-01)	Acc@1  92.97 ( 92.62)	Acc@5 100.00 ( 99.73)
07-Mar-22 03:12:19 - Epoch: [26][160/352]	Time  0.155 ( 0.165)	Data  0.003 ( 0.004)	Loss 1.1420e-01 (9.5856e-02)	Acc@1  95.31 ( 96.54)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:19 - Test: [ 0/20]	Time  0.375 ( 0.375)	Loss 1.6272e-01 (1.6272e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 03:12:20 - Test: [10/20]	Time  0.101 ( 0.131)	Loss 1.8180e-01 (1.8743e-01)	Acc@1  94.53 ( 94.03)	Acc@5 100.00 ( 99.89)
07-Mar-22 03:12:20 - Epoch: [26][170/352]	Time  0.151 ( 0.164)	Data  0.002 ( 0.004)	Loss 6.1482e-02 (9.6045e-02)	Acc@1  96.09 ( 96.53)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:12:21 -  * Acc@1 94.180 Acc@5 99.840
07-Mar-22 03:12:21 - Best acc at epoch 0: 94.18000030517578
07-Mar-22 03:12:22 - Epoch: [1][  0/352]	Time  0.401 ( 0.401)	Data  0.224 ( 0.224)	Loss 2.2305e-01 (2.2305e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:12:22 - Epoch: [26][180/352]	Time  0.133 ( 0.164)	Data  0.002 ( 0.003)	Loss 4.4473e-02 (9.5278e-02)	Acc@1  97.66 ( 96.57)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:12:23 - Epoch: [1][ 10/352]	Time  0.171 ( 0.190)	Data  0.002 ( 0.022)	Loss 1.2961e-01 (1.8074e-01)	Acc@1  96.88 ( 94.60)	Acc@5  99.22 ( 99.79)
07-Mar-22 03:12:24 - Epoch: [26][190/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 2.1334e-01 (9.5179e-02)	Acc@1  92.19 ( 96.58)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:25 - Epoch: [1][ 20/352]	Time  0.172 ( 0.181)	Data  0.003 ( 0.013)	Loss 1.7707e-01 (1.8707e-01)	Acc@1  92.19 ( 94.01)	Acc@5 100.00 ( 99.78)
07-Mar-22 03:12:25 - Epoch: [26][200/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.4517e-02 (9.4580e-02)	Acc@1  97.66 ( 96.63)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:27 - Epoch: [1][ 30/352]	Time  0.173 ( 0.178)	Data  0.002 ( 0.009)	Loss 1.4677e-01 (1.8111e-01)	Acc@1  96.09 ( 94.23)	Acc@5 100.00 ( 99.82)
07-Mar-22 03:12:27 - Epoch: [26][210/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 8.6901e-02 (9.4605e-02)	Acc@1  96.09 ( 96.62)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:12:28 - Epoch: [1][ 40/352]	Time  0.172 ( 0.176)	Data  0.003 ( 0.008)	Loss 1.8175e-01 (1.7927e-01)	Acc@1  96.88 ( 94.38)	Acc@5  99.22 ( 99.85)
07-Mar-22 03:12:29 - Epoch: [26][220/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.2511e-01 (9.4646e-02)	Acc@1  95.31 ( 96.62)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:12:30 - Epoch: [1][ 50/352]	Time  0.173 ( 0.175)	Data  0.002 ( 0.007)	Loss 1.5056e-01 (1.8247e-01)	Acc@1  95.31 ( 94.27)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:12:30 - Epoch: [26][230/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 8.0053e-02 (9.3432e-02)	Acc@1  96.88 ( 96.67)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:12:32 - Epoch: [1][ 60/352]	Time  0.172 ( 0.175)	Data  0.002 ( 0.006)	Loss 2.0739e-01 (1.8374e-01)	Acc@1  92.19 ( 94.21)	Acc@5 100.00 ( 99.82)
07-Mar-22 03:12:32 - Epoch: [26][240/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.2787e-01 (9.3469e-02)	Acc@1  95.31 ( 96.68)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:12:34 - Epoch: [1][ 70/352]	Time  0.176 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.8061e-01 (1.8456e-01)	Acc@1  92.97 ( 94.15)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:12:34 - Epoch: [26][250/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.6584e-01 (9.3586e-02)	Acc@1  96.09 ( 96.68)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:35 - Epoch: [26][260/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 8.1331e-02 (9.2939e-02)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:35 - Epoch: [1][ 80/352]	Time  0.168 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.3905e-01 (1.8494e-01)	Acc@1  97.66 ( 94.10)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:12:37 - Epoch: [26][270/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.3991e-02 (9.2477e-02)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:37 - Epoch: [1][ 90/352]	Time  0.176 ( 0.174)	Data  0.003 ( 0.005)	Loss 1.6063e-01 (1.8474e-01)	Acc@1  96.88 ( 94.12)	Acc@5  99.22 ( 99.85)
07-Mar-22 03:12:39 - Epoch: [26][280/352]	Time  0.183 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.2874e-02 (9.2450e-02)	Acc@1  98.44 ( 96.71)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:39 - Epoch: [1][100/352]	Time  0.150 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.4924e-01 (1.8458e-01)	Acc@1  96.09 ( 94.04)	Acc@5 100.00 ( 99.84)
07-Mar-22 03:12:40 - Epoch: [26][290/352]	Time  0.146 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.3507e-01 (9.2525e-02)	Acc@1  95.31 ( 96.70)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:40 - Epoch: [1][110/352]	Time  0.173 ( 0.173)	Data  0.002 ( 0.004)	Loss 1.8182e-01 (1.8440e-01)	Acc@1  92.97 ( 94.03)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:12:42 - Epoch: [26][300/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 8.2521e-02 (9.2705e-02)	Acc@1  97.66 ( 96.69)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:42 - Epoch: [1][120/352]	Time  0.174 ( 0.173)	Data  0.002 ( 0.004)	Loss 1.9903e-01 (1.8424e-01)	Acc@1  93.75 ( 93.99)	Acc@5  99.22 ( 99.85)
07-Mar-22 03:12:43 - Epoch: [26][310/352]	Time  0.140 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.8427e-02 (9.2734e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:12:44 - Epoch: [1][130/352]	Time  0.170 ( 0.173)	Data  0.002 ( 0.004)	Loss 1.5811e-01 (1.8134e-01)	Acc@1  96.88 ( 94.13)	Acc@5  99.22 ( 99.84)
07-Mar-22 03:12:45 - Epoch: [26][320/352]	Time  0.144 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.1156e-01 (9.3349e-02)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:46 - Epoch: [1][140/352]	Time  0.168 ( 0.173)	Data  0.002 ( 0.004)	Loss 2.0925e-01 (1.8106e-01)	Acc@1  92.97 ( 94.13)	Acc@5  99.22 ( 99.84)
07-Mar-22 03:12:46 - Epoch: [26][330/352]	Time  0.157 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.3166e-01 (9.3983e-02)	Acc@1  94.53 ( 96.67)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:47 - Epoch: [1][150/352]	Time  0.171 ( 0.172)	Data  0.002 ( 0.004)	Loss 1.7175e-01 (1.8149e-01)	Acc@1  94.53 ( 94.13)	Acc@5  99.22 ( 99.83)
07-Mar-22 03:12:48 - Epoch: [26][340/352]	Time  0.141 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.7443e-01 (9.4002e-02)	Acc@1  92.97 ( 96.68)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:49 - Epoch: [1][160/352]	Time  0.167 ( 0.172)	Data  0.002 ( 0.004)	Loss 1.5611e-01 (1.8183e-01)	Acc@1  92.97 ( 94.05)	Acc@5 100.00 ( 99.84)
07-Mar-22 03:12:49 - Epoch: [26][350/352]	Time  0.141 ( 0.162)	Data  0.001 ( 0.003)	Loss 1.5482e-01 (9.4202e-02)	Acc@1  92.97 ( 96.67)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:12:50 - Test: [ 0/20]	Time  0.337 ( 0.337)	Loss 3.7344e-01 (3.7344e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
07-Mar-22 03:12:51 - Epoch: [1][170/352]	Time  0.209 ( 0.172)	Data  0.002 ( 0.004)	Loss 2.2646e-01 (1.8255e-01)	Acc@1  92.19 ( 94.03)	Acc@5  99.22 ( 99.83)
07-Mar-22 03:12:51 - Test: [10/20]	Time  0.099 ( 0.125)	Loss 3.2786e-01 (3.9772e-01)	Acc@1  89.06 ( 88.42)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:12:52 -  * Acc@1 88.960 Acc@5 99.460
07-Mar-22 03:12:52 - Best acc at epoch 26: 89.5199966430664
07-Mar-22 03:12:52 - Epoch: [27][  0/352]	Time  0.382 ( 0.382)	Data  0.238 ( 0.238)	Loss 1.0704e-01 (1.0704e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 03:12:52 - Epoch: [1][180/352]	Time  0.174 ( 0.172)	Data  0.002 ( 0.004)	Loss 1.9532e-01 (1.8242e-01)	Acc@1  91.41 ( 94.01)	Acc@5  99.22 ( 99.84)
07-Mar-22 03:12:54 - Epoch: [27][ 10/352]	Time  0.166 ( 0.185)	Data  0.002 ( 0.024)	Loss 9.9909e-02 (8.4658e-02)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 (100.00)
07-Mar-22 03:12:54 - Epoch: [1][190/352]	Time  0.173 ( 0.172)	Data  0.002 ( 0.003)	Loss 2.3215e-01 (1.8344e-01)	Acc@1  92.97 ( 93.98)	Acc@5 100.00 ( 99.84)
07-Mar-22 03:12:55 - Epoch: [27][ 20/352]	Time  0.167 ( 0.176)	Data  0.002 ( 0.013)	Loss 5.2613e-02 (9.3495e-02)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 (100.00)
07-Mar-22 03:12:56 - Epoch: [1][200/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.003)	Loss 2.2725e-01 (1.8325e-01)	Acc@1  92.97 ( 93.98)	Acc@5 100.00 ( 99.83)
07-Mar-22 03:12:57 - Epoch: [27][ 30/352]	Time  0.165 ( 0.172)	Data  0.002 ( 0.010)	Loss 6.5869e-02 (8.7683e-02)	Acc@1  98.44 ( 96.75)	Acc@5 100.00 (100.00)
07-Mar-22 03:12:58 - Epoch: [1][210/352]	Time  0.173 ( 0.172)	Data  0.002 ( 0.003)	Loss 1.9430e-01 (1.8320e-01)	Acc@1  94.53 ( 93.99)	Acc@5 100.00 ( 99.84)
07-Mar-22 03:12:59 - Epoch: [27][ 40/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.008)	Loss 1.3683e-01 (9.4674e-02)	Acc@1  94.53 ( 96.57)	Acc@5 100.00 (100.00)
07-Mar-22 03:12:59 - Epoch: [1][220/352]	Time  0.172 ( 0.172)	Data  0.002 ( 0.003)	Loss 1.4532e-01 (1.8292e-01)	Acc@1  96.09 ( 93.99)	Acc@5 100.00 ( 99.83)
07-Mar-22 03:13:00 - Epoch: [27][ 50/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.007)	Loss 7.9912e-02 (9.1951e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 (100.00)
07-Mar-22 03:13:01 - Epoch: [1][230/352]	Time  0.175 ( 0.172)	Data  0.003 ( 0.003)	Loss 2.0194e-01 (1.8185e-01)	Acc@1  92.19 ( 94.01)	Acc@5 100.00 ( 99.83)
07-Mar-22 03:13:02 - Epoch: [27][ 60/352]	Time  0.162 ( 0.169)	Data  0.002 ( 0.006)	Loss 6.3473e-02 (9.2016e-02)	Acc@1  98.44 ( 96.70)	Acc@5 100.00 (100.00)
07-Mar-22 03:13:03 - Epoch: [1][240/352]	Time  0.169 ( 0.172)	Data  0.002 ( 0.003)	Loss 1.7178e-01 (1.8230e-01)	Acc@1  93.75 ( 93.97)	Acc@5 100.00 ( 99.83)
07-Mar-22 03:13:04 - Epoch: [27][ 70/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.006)	Loss 1.1129e-01 (9.3638e-02)	Acc@1  96.09 ( 96.63)	Acc@5 100.00 (100.00)
07-Mar-22 03:13:04 - Epoch: [1][250/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.003)	Loss 1.3054e-01 (1.8214e-01)	Acc@1  96.88 ( 94.01)	Acc@5 100.00 ( 99.84)
07-Mar-22 03:13:05 - Epoch: [27][ 80/352]	Time  0.163 ( 0.166)	Data  0.002 ( 0.005)	Loss 4.4932e-02 (9.3372e-02)	Acc@1  98.44 ( 96.61)	Acc@5 100.00 (100.00)
07-Mar-22 03:13:06 - Epoch: [1][260/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.003)	Loss 9.3464e-02 (1.8203e-01)	Acc@1  98.44 ( 94.03)	Acc@5 100.00 ( 99.84)
07-Mar-22 03:13:07 - Epoch: [27][ 90/352]	Time  0.187 ( 0.166)	Data  0.003 ( 0.005)	Loss 5.3222e-02 (9.3990e-02)	Acc@1  99.22 ( 96.61)	Acc@5 100.00 (100.00)
07-Mar-22 03:13:08 - Epoch: [1][270/352]	Time  0.174 ( 0.172)	Data  0.002 ( 0.003)	Loss 1.5755e-01 (1.8235e-01)	Acc@1  95.31 ( 94.03)	Acc@5 100.00 ( 99.84)
07-Mar-22 03:13:09 - Epoch: [27][100/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.005)	Loss 6.3249e-02 (9.3528e-02)	Acc@1  97.66 ( 96.56)	Acc@5 100.00 (100.00)
07-Mar-22 03:13:10 - Epoch: [1][280/352]	Time  0.170 ( 0.172)	Data  0.002 ( 0.003)	Loss 9.3105e-02 (1.8128e-01)	Acc@1  98.44 ( 94.06)	Acc@5 100.00 ( 99.84)
07-Mar-22 03:13:10 - Epoch: [27][110/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.004)	Loss 8.0325e-02 (9.3377e-02)	Acc@1  97.66 ( 96.54)	Acc@5 100.00 (100.00)
07-Mar-22 03:13:11 - Epoch: [1][290/352]	Time  0.173 ( 0.172)	Data  0.001 ( 0.003)	Loss 1.6404e-01 (1.8059e-01)	Acc@1  94.53 ( 94.07)	Acc@5 100.00 ( 99.84)
07-Mar-22 03:13:12 - Epoch: [27][120/352]	Time  0.146 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.3150e-01 (9.4757e-02)	Acc@1  95.31 ( 96.51)	Acc@5 100.00 (100.00)
07-Mar-22 03:13:13 - Epoch: [1][300/352]	Time  0.172 ( 0.172)	Data  0.002 ( 0.003)	Loss 1.5923e-01 (1.8012e-01)	Acc@1  94.53 ( 94.08)	Acc@5 100.00 ( 99.84)
07-Mar-22 03:13:13 - Epoch: [27][130/352]	Time  0.141 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.0638e-01 (9.4653e-02)	Acc@1  96.09 ( 96.48)	Acc@5 100.00 (100.00)
07-Mar-22 03:13:15 - Epoch: [1][310/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.003)	Loss 2.0524e-01 (1.7957e-01)	Acc@1  95.31 ( 94.11)	Acc@5 100.00 ( 99.84)
07-Mar-22 03:13:15 - Epoch: [27][140/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.004)	Loss 8.7462e-02 (9.4871e-02)	Acc@1  96.09 ( 96.46)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:16 - Epoch: [1][320/352]	Time  0.171 ( 0.172)	Data  0.002 ( 0.003)	Loss 1.4164e-01 (1.7915e-01)	Acc@1  96.09 ( 94.12)	Acc@5  99.22 ( 99.84)
07-Mar-22 03:13:17 - Epoch: [27][150/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.004)	Loss 8.3035e-02 (9.4456e-02)	Acc@1  97.66 ( 96.46)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:18 - Epoch: [1][330/352]	Time  0.172 ( 0.172)	Data  0.002 ( 0.003)	Loss 2.5049e-01 (1.7897e-01)	Acc@1  89.84 ( 94.13)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:13:18 - Epoch: [27][160/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.004)	Loss 1.0118e-01 (9.4053e-02)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:20 - Epoch: [1][340/352]	Time  0.172 ( 0.172)	Data  0.002 ( 0.003)	Loss 1.4677e-01 (1.7934e-01)	Acc@1  95.31 ( 94.10)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:13:20 - Epoch: [27][170/352]	Time  0.165 ( 0.164)	Data  0.002 ( 0.004)	Loss 3.7058e-02 (9.4322e-02)	Acc@1  99.22 ( 96.54)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:22 - Epoch: [27][180/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.4118e-01 (9.5718e-02)	Acc@1  95.31 ( 96.47)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:22 - Epoch: [1][350/352]	Time  0.169 ( 0.172)	Data  0.002 ( 0.003)	Loss 1.3297e-01 (1.7868e-01)	Acc@1  93.75 ( 94.12)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:13:22 - Test: [ 0/20]	Time  0.354 ( 0.354)	Loss 1.4359e-01 (1.4359e-01)	Acc@1  94.92 ( 94.92)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:13:23 - Epoch: [27][190/352]	Time  0.144 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.6942e-01 (9.5644e-02)	Acc@1  95.31 ( 96.50)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:23 - Test: [10/20]	Time  0.122 ( 0.135)	Loss 1.4639e-01 (1.5654e-01)	Acc@1  95.70 ( 94.74)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:13:24 -  * Acc@1 94.840 Acc@5 99.940
07-Mar-22 03:13:24 - Best acc at epoch 1: 94.83999633789062
07-Mar-22 03:13:25 - Epoch: [27][200/352]	Time  0.131 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.4797e-01 (9.5706e-02)	Acc@1  94.53 ( 96.50)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:25 - Epoch: [2][  0/352]	Time  0.405 ( 0.405)	Data  0.231 ( 0.231)	Loss 1.5090e-01 (1.5090e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:13:26 - Epoch: [27][210/352]	Time  0.140 ( 0.163)	Data  0.001 ( 0.003)	Loss 7.0272e-02 (9.5027e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:26 - Epoch: [2][ 10/352]	Time  0.170 ( 0.185)	Data  0.002 ( 0.023)	Loss 1.5898e-01 (1.6337e-01)	Acc@1  95.31 ( 95.03)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:13:28 - Epoch: [27][220/352]	Time  0.158 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.1111e-01 (9.5166e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:28 - Epoch: [2][ 20/352]	Time  0.168 ( 0.177)	Data  0.002 ( 0.013)	Loss 1.5094e-01 (1.6239e-01)	Acc@1  95.31 ( 94.94)	Acc@5 100.00 ( 99.89)
07-Mar-22 03:13:29 - Epoch: [27][230/352]	Time  0.168 ( 0.163)	Data  0.002 ( 0.003)	Loss 3.4428e-02 (9.4440e-02)	Acc@1 100.00 ( 96.57)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:30 - Epoch: [2][ 30/352]	Time  0.173 ( 0.176)	Data  0.003 ( 0.010)	Loss 1.9783e-01 (1.7152e-01)	Acc@1  92.97 ( 94.53)	Acc@5 100.00 ( 99.87)
07-Mar-22 03:13:31 - Epoch: [27][240/352]	Time  0.160 ( 0.163)	Data  0.002 ( 0.003)	Loss 9.4313e-02 (9.4261e-02)	Acc@1  95.31 ( 96.56)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:32 - Epoch: [2][ 40/352]	Time  0.169 ( 0.175)	Data  0.002 ( 0.008)	Loss 1.5056e-01 (1.6960e-01)	Acc@1  96.88 ( 94.61)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:13:33 - Epoch: [27][250/352]	Time  0.166 ( 0.163)	Data  0.003 ( 0.003)	Loss 8.6270e-02 (9.3957e-02)	Acc@1  96.88 ( 96.56)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:33 - Epoch: [2][ 50/352]	Time  0.172 ( 0.174)	Data  0.002 ( 0.007)	Loss 1.8465e-01 (1.7123e-01)	Acc@1  95.31 ( 94.58)	Acc@5 100.00 ( 99.83)
07-Mar-22 03:13:34 - Epoch: [27][260/352]	Time  0.164 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.0487e-01 (9.4167e-02)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:35 - Epoch: [2][ 60/352]	Time  0.147 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.8705e-01 (1.7374e-01)	Acc@1  96.09 ( 94.39)	Acc@5 100.00 ( 99.83)
07-Mar-22 03:13:36 - Epoch: [27][270/352]	Time  0.168 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.4042e-02 (9.3643e-02)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:37 - Epoch: [2][ 70/352]	Time  0.171 ( 0.172)	Data  0.002 ( 0.005)	Loss 9.2069e-02 (1.7620e-01)	Acc@1  98.44 ( 94.26)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:13:37 - Epoch: [27][280/352]	Time  0.142 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.8988e-02 (9.3270e-02)	Acc@1  97.66 ( 96.60)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:38 - Epoch: [2][ 80/352]	Time  0.149 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.3472e-01 (1.7437e-01)	Acc@1  96.88 ( 94.28)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:13:39 - Epoch: [27][290/352]	Time  0.162 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.3682e-01 (9.3767e-02)	Acc@1  96.88 ( 96.61)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:40 - Epoch: [2][ 90/352]	Time  0.150 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.6610e-01 (1.7559e-01)	Acc@1  94.53 ( 94.24)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:13:41 - Epoch: [27][300/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.9354e-02 (9.3939e-02)	Acc@1  97.66 ( 96.60)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:41 - Epoch: [2][100/352]	Time  0.172 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.3929e-01 (1.7686e-01)	Acc@1  96.09 ( 94.24)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:13:42 - Epoch: [27][310/352]	Time  0.161 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.9970e-02 (9.4500e-02)	Acc@1  96.09 ( 96.56)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:43 - Epoch: [2][110/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.9390e-01 (1.7868e-01)	Acc@1  95.31 ( 94.15)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:13:44 - Epoch: [27][320/352]	Time  0.142 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.1486e-02 (9.4349e-02)	Acc@1  96.09 ( 96.57)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:45 - Epoch: [2][120/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.004)	Loss 2.0036e-01 (1.7958e-01)	Acc@1  92.19 ( 94.12)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:13:45 - Epoch: [27][330/352]	Time  0.142 ( 0.162)	Data  0.001 ( 0.003)	Loss 6.6340e-02 (9.4240e-02)	Acc@1  97.66 ( 96.58)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:13:47 - Epoch: [2][130/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.004)	Loss 2.6173e-01 (1.8099e-01)	Acc@1  92.19 ( 94.11)	Acc@5 100.00 ( 99.84)
07-Mar-22 03:13:47 - Epoch: [27][340/352]	Time  0.144 ( 0.162)	Data  0.002 ( 0.003)	Loss 5.4439e-02 (9.4253e-02)	Acc@1  97.66 ( 96.58)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:13:48 - Epoch: [2][140/352]	Time  0.173 ( 0.169)	Data  0.003 ( 0.004)	Loss 1.8720e-01 (1.8066e-01)	Acc@1  92.97 ( 94.09)	Acc@5 100.00 ( 99.85)
07-Mar-22 03:13:48 - Epoch: [27][350/352]	Time  0.164 ( 0.162)	Data  0.002 ( 0.003)	Loss 9.2806e-02 (9.4044e-02)	Acc@1  96.09 ( 96.59)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:13:49 - Test: [ 0/20]	Time  0.401 ( 0.401)	Loss 3.5725e-01 (3.5725e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:13:50 - Test: [10/20]	Time  0.098 ( 0.128)	Loss 3.9960e-01 (3.9174e-01)	Acc@1  90.23 ( 89.10)	Acc@5  99.22 ( 99.43)
07-Mar-22 03:13:51 -  * Acc@1 89.380 Acc@5 99.500
07-Mar-22 03:13:51 - Best acc at epoch 27: 89.5199966430664
07-Mar-22 03:13:52 - Epoch: [28][  0/352]	Time  0.381 ( 0.381)	Data  0.234 ( 0.234)	Loss 1.0475e-01 (1.0475e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 03:13:53 - Epoch: [28][ 10/352]	Time  0.170 ( 0.187)	Data  0.002 ( 0.023)	Loss 9.0934e-02 (9.8778e-02)	Acc@1  96.88 ( 96.31)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:13:55 - Epoch: [28][ 20/352]	Time  0.166 ( 0.177)	Data  0.002 ( 0.013)	Loss 8.3397e-02 (1.0063e-01)	Acc@1  97.66 ( 96.39)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:13:57 - Epoch: [28][ 30/352]	Time  0.168 ( 0.174)	Data  0.002 ( 0.010)	Loss 1.2708e-01 (1.0138e-01)	Acc@1  94.53 ( 96.27)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:13:58 - Epoch: [28][ 40/352]	Time  0.172 ( 0.172)	Data  0.004 ( 0.008)	Loss 9.9465e-02 (9.9219e-02)	Acc@1  95.31 ( 96.40)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:14:00 - Epoch: [28][ 50/352]	Time  0.166 ( 0.172)	Data  0.002 ( 0.007)	Loss 1.1964e-01 (9.6601e-02)	Acc@1  95.31 ( 96.57)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:14:02 - Epoch: [28][ 60/352]	Time  0.174 ( 0.171)	Data  0.002 ( 0.006)	Loss 9.8804e-02 (9.5599e-02)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:03 - Epoch: [28][ 70/352]	Time  0.143 ( 0.170)	Data  0.001 ( 0.005)	Loss 9.3311e-02 (9.4112e-02)	Acc@1  96.09 ( 96.60)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:05 - Epoch: [28][ 80/352]	Time  0.162 ( 0.169)	Data  0.002 ( 0.005)	Loss 5.1617e-02 (9.5540e-02)	Acc@1  99.22 ( 96.56)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:06 - Epoch: [28][ 90/352]	Time  0.156 ( 0.168)	Data  0.002 ( 0.005)	Loss 6.6294e-02 (9.4187e-02)	Acc@1  97.66 ( 96.61)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:08 - Epoch: [28][100/352]	Time  0.159 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.6005e-02 (9.3825e-02)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:10 - Epoch: [28][110/352]	Time  0.160 ( 0.167)	Data  0.002 ( 0.004)	Loss 9.9539e-02 (9.4160e-02)	Acc@1  96.09 ( 96.61)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:11 - Epoch: [28][120/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.7750e-01 (9.4132e-02)	Acc@1  94.53 ( 96.62)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:13 - Epoch: [28][130/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.004)	Loss 6.5408e-02 (9.3042e-02)	Acc@1  98.44 ( 96.68)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:15 - Epoch: [28][140/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.0933e-01 (9.3261e-02)	Acc@1  96.09 ( 96.62)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:16 - Epoch: [28][150/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.004)	Loss 4.5631e-02 (9.2895e-02)	Acc@1  99.22 ( 96.65)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:18 - Epoch: [28][160/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.5771e-02 (9.3051e-02)	Acc@1  97.66 ( 96.66)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:20 - Epoch: [28][170/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.3712e-02 (9.2661e-02)	Acc@1  97.66 ( 96.69)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:21 - Epoch: [28][180/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.2305e-02 (9.2461e-02)	Acc@1  98.44 ( 96.71)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:23 - Epoch: [28][190/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.7701e-02 (9.1987e-02)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:25 - Epoch: [28][200/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1810e-01 (9.2349e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:26 - Epoch: [28][210/352]	Time  0.171 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.3209e-02 (9.2363e-02)	Acc@1  97.66 ( 96.73)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:28 - Epoch: [28][220/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.0933e-02 (9.1952e-02)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:30 - Epoch: [28][230/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1467e-01 (9.2658e-02)	Acc@1  95.31 ( 96.71)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:31 - Epoch: [28][240/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.2047e-02 (9.1968e-02)	Acc@1  99.22 ( 96.74)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:33 - Epoch: [28][250/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3510e-01 (9.2128e-02)	Acc@1  95.31 ( 96.73)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:35 - Epoch: [28][260/352]	Time  0.167 ( 0.167)	Data  0.001 ( 0.003)	Loss 6.2296e-02 (9.1234e-02)	Acc@1  96.09 ( 96.77)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:36 - Epoch: [28][270/352]	Time  0.145 ( 0.167)	Data  0.001 ( 0.003)	Loss 1.2860e-01 (9.0846e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:38 - Epoch: [28][280/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2689e-01 (9.1134e-02)	Acc@1  93.75 ( 96.80)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:40 - Epoch: [28][290/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1219e-01 (9.1130e-02)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:41 - Epoch: [28][300/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3580e-01 (9.1301e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:43 - Epoch: [28][310/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.6210e-02 (9.0745e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:45 - Epoch: [28][320/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.0088e-02 (9.0716e-02)	Acc@1  96.09 ( 96.82)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:46 - Epoch: [28][330/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.5117e-01 (9.0626e-02)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:48 - Epoch: [28][340/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.2092e-02 (9.0901e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:50 - Epoch: [28][350/352]	Time  0.166 ( 0.167)	Data  0.003 ( 0.003)	Loss 1.4081e-01 (9.1041e-02)	Acc@1  96.09 ( 96.79)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:14:50 - Test: [ 0/20]	Time  0.373 ( 0.373)	Loss 3.0882e-01 (3.0882e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:14:51 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 3.3542e-01 (3.8088e-01)	Acc@1  90.23 ( 88.64)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:14:52 -  * Acc@1 89.020 Acc@5 99.500
07-Mar-22 03:14:52 - Best acc at epoch 28: 89.5199966430664
07-Mar-22 03:14:53 - Epoch: [29][  0/352]	Time  0.368 ( 0.368)	Data  0.223 ( 0.223)	Loss 7.8149e-02 (7.8149e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:14:55 - Epoch: [29][ 10/352]	Time  0.180 ( 0.188)	Data  0.002 ( 0.022)	Loss 5.2363e-02 (7.8190e-02)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 (100.00)
07-Mar-22 03:14:56 - Epoch: [29][ 20/352]	Time  0.166 ( 0.180)	Data  0.002 ( 0.013)	Loss 1.3071e-01 (8.0136e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 (100.00)
07-Mar-22 03:14:58 - Epoch: [29][ 30/352]	Time  0.168 ( 0.176)	Data  0.002 ( 0.009)	Loss 6.9803e-02 (8.1293e-02)	Acc@1  96.88 ( 97.20)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:15:00 - Epoch: [29][ 40/352]	Time  0.168 ( 0.173)	Data  0.002 ( 0.008)	Loss 8.5241e-02 (8.3555e-02)	Acc@1  95.31 ( 97.20)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:15:01 - Epoch: [29][ 50/352]	Time  0.179 ( 0.174)	Data  0.002 ( 0.007)	Loss 1.1031e-01 (8.5059e-02)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:15:03 - Epoch: [29][ 60/352]	Time  0.166 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.0070e-01 (8.6956e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:15:05 - Epoch: [29][ 70/352]	Time  0.166 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.0105e-01 (8.8546e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:15:06 - Epoch: [29][ 80/352]	Time  0.164 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.0375e-01 (8.8889e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:15:08 - Epoch: [29][ 90/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.005)	Loss 7.0791e-02 (8.9055e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:15:10 - Epoch: [29][100/352]	Time  0.143 ( 0.170)	Data  0.002 ( 0.004)	Loss 7.1163e-02 (8.8274e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:15:11 - Epoch: [29][110/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.004)	Loss 3.2279e-02 (8.8251e-02)	Acc@1 100.00 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:15:13 - Epoch: [29][120/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 8.1784e-02 (8.6608e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:15:15 - Epoch: [29][130/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.8228e-01 (8.7822e-02)	Acc@1  94.53 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:15:16 - Epoch: [29][140/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.0197e-01 (8.7277e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:15:18 - Epoch: [29][150/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.5206e-02 (8.7156e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:15:20 - Epoch: [29][160/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2398e-01 (8.7435e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:15:21 - Epoch: [29][170/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.2614e-01 (8.7509e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:15:23 - Epoch: [29][180/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.2122e-02 (8.7481e-02)	Acc@1 100.00 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:15:25 - Epoch: [29][190/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0040e-01 (8.8064e-02)	Acc@1  96.09 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:15:26 - Epoch: [29][200/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2164e-01 (8.8353e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:15:28 - Epoch: [29][210/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0784e-01 (8.8157e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:15:30 - Epoch: [29][220/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2467e-01 (8.8550e-02)	Acc@1  94.53 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:15:31 - Epoch: [29][230/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0051e-01 (8.8478e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:15:33 - Epoch: [29][240/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.7965e-02 (8.8091e-02)	Acc@1  99.22 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:15:35 - Epoch: [29][250/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.3670e-01 (8.8851e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:15:36 - Epoch: [29][260/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.9591e-02 (8.8724e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:15:38 - Epoch: [29][270/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.6953e-01 (8.8760e-02)	Acc@1  94.53 ( 96.93)	Acc@5  99.22 ( 99.97)
07-Mar-22 03:15:40 - Epoch: [29][280/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.4478e-02 (8.9216e-02)	Acc@1  95.31 ( 96.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:15:41 - Epoch: [29][290/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.3628e-01 (8.9075e-02)	Acc@1  93.75 ( 96.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:15:43 - Epoch: [29][300/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.4216e-02 (9.0222e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:15:45 - Epoch: [29][310/352]	Time  0.190 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0494e-01 (9.0552e-02)	Acc@1  96.09 ( 96.86)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:15:47 - Epoch: [29][320/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.0801e-02 (9.0341e-02)	Acc@1  99.22 ( 96.87)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:15:48 - Epoch: [29][330/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.0734e-02 (9.0204e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:15:50 - Epoch: [29][340/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2503e-01 (8.9918e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:15:52 - Epoch: [29][350/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.0920e-02 (9.0264e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:15:52 - Test: [ 0/20]	Time  0.381 ( 0.381)	Loss 3.2873e-01 (3.2873e-01)	Acc@1  87.89 ( 87.89)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:15:53 - Test: [10/20]	Time  0.099 ( 0.125)	Loss 3.9148e-01 (3.7552e-01)	Acc@1  87.50 ( 88.78)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:15:54 -  * Acc@1 89.120 Acc@5 99.460
07-Mar-22 03:15:54 - Best acc at epoch 29: 89.5199966430664
07-Mar-22 03:15:55 - Epoch: [30][  0/352]	Time  0.389 ( 0.389)	Data  0.238 ( 0.238)	Loss 4.9936e-02 (4.9936e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
07-Mar-22 03:15:56 - Epoch: [30][ 10/352]	Time  0.167 ( 0.182)	Data  0.002 ( 0.023)	Loss 1.1520e-01 (9.2534e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 (100.00)
07-Mar-22 03:15:58 - Epoch: [30][ 20/352]	Time  0.168 ( 0.175)	Data  0.002 ( 0.013)	Loss 1.3336e-01 (9.3296e-02)	Acc@1  94.53 ( 96.61)	Acc@5 100.00 (100.00)
07-Mar-22 03:16:00 - Epoch: [30][ 30/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.010)	Loss 1.9554e-01 (9.7656e-02)	Acc@1  92.97 ( 96.55)	Acc@5 100.00 (100.00)
07-Mar-22 03:16:01 - Epoch: [30][ 40/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.008)	Loss 6.5910e-02 (9.4652e-02)	Acc@1  97.66 ( 96.63)	Acc@5 100.00 (100.00)
07-Mar-22 03:16:03 - Epoch: [30][ 50/352]	Time  0.165 ( 0.170)	Data  0.002 ( 0.007)	Loss 7.8032e-02 (9.4025e-02)	Acc@1  98.44 ( 96.61)	Acc@5 100.00 (100.00)
07-Mar-22 03:16:05 - Epoch: [30][ 60/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.006)	Loss 1.2900e-01 (9.3762e-02)	Acc@1  94.53 ( 96.62)	Acc@5 100.00 (100.00)
07-Mar-22 03:16:06 - Epoch: [30][ 70/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.0727e-01 (9.2784e-02)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 (100.00)
07-Mar-22 03:16:08 - Epoch: [30][ 80/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.005)	Loss 7.6932e-02 (9.3279e-02)	Acc@1  96.09 ( 96.59)	Acc@5 100.00 (100.00)
07-Mar-22 03:16:10 - Epoch: [30][ 90/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.005)	Loss 6.4826e-02 (9.2624e-02)	Acc@1  98.44 ( 96.65)	Acc@5 100.00 (100.00)
07-Mar-22 03:16:11 - Epoch: [30][100/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 9.7230e-02 (9.1937e-02)	Acc@1  95.31 ( 96.70)	Acc@5 100.00 (100.00)
07-Mar-22 03:16:13 - Epoch: [30][110/352]	Time  0.173 ( 0.167)	Data  0.002 ( 0.004)	Loss 6.2587e-02 (8.9590e-02)	Acc@1  99.22 ( 96.80)	Acc@5 100.00 (100.00)
07-Mar-22 03:16:14 - Epoch: [30][120/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 6.1622e-02 (8.9514e-02)	Acc@1  96.88 ( 96.81)	Acc@5 100.00 (100.00)
07-Mar-22 03:16:16 - Epoch: [30][130/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.004)	Loss 9.3907e-02 (8.9111e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:16:18 - Epoch: [30][140/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 7.6066e-02 (8.9854e-02)	Acc@1  97.66 ( 96.85)	Acc@5  99.22 ( 99.98)
07-Mar-22 03:16:19 - Epoch: [30][150/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 6.8630e-02 (8.9211e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:21 - Epoch: [30][160/352]	Time  0.171 ( 0.167)	Data  0.002 ( 0.004)	Loss 6.6269e-02 (8.8994e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:23 - Epoch: [30][170/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2302e-01 (8.9497e-02)	Acc@1  95.31 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:24 - Epoch: [30][180/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.2389e-02 (8.9265e-02)	Acc@1  99.22 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:26 - Epoch: [30][190/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.8898e-02 (8.8491e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:28 - Epoch: [30][200/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0382e-01 (8.8145e-02)	Acc@1  94.53 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:30 - Epoch: [30][210/352]	Time  0.208 ( 0.167)	Data  0.003 ( 0.003)	Loss 1.1650e-01 (8.9121e-02)	Acc@1  96.88 ( 96.88)	Acc@5  99.22 ( 99.98)
07-Mar-22 03:16:31 - Epoch: [30][220/352]	Time  0.145 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.2204e-01 (8.9090e-02)	Acc@1  95.31 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:32 - Epoch: [30][230/352]	Time  0.144 ( 0.165)	Data  0.002 ( 0.003)	Loss 8.1254e-02 (8.8659e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:34 - Epoch: [30][240/352]	Time  0.146 ( 0.165)	Data  0.002 ( 0.003)	Loss 3.1214e-02 (8.8050e-02)	Acc@1  99.22 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:35 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 03:16:35 - => creating PyTorchCV model 'resnet20_unfold'
07-Mar-22 03:16:35 - match all modules defined in bit_config: True
07-Mar-22 03:16:35 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_conv_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
07-Mar-22 03:16:35 - Epoch: [30][250/352]	Time  0.150 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.0235e-01 (8.7362e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:37 - Epoch: [30][260/352]	Time  0.167 ( 0.164)	Data  0.003 ( 0.003)	Loss 6.6400e-02 (8.7172e-02)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:16:39 - Epoch: [30][270/352]	Time  0.177 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.3896e-02 (8.6663e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:16:40 - Epoch: [0][  0/352]	Time  0.443 ( 0.443)	Data  0.217 ( 0.217)	Loss 1.2932e+00 (1.2932e+00)	Acc@1  57.03 ( 57.03)	Acc@5  93.75 ( 93.75)
07-Mar-22 03:16:41 - Epoch: [30][280/352]	Time  0.144 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.1732e-01 (8.6549e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:42 - Epoch: [0][ 10/352]	Time  0.172 ( 0.194)	Data  0.002 ( 0.021)	Loss 5.0137e-01 (7.9028e-01)	Acc@1  85.94 ( 76.21)	Acc@5  98.44 ( 96.88)
07-Mar-22 03:16:42 - Epoch: [30][290/352]	Time  0.162 ( 0.165)	Data  0.002 ( 0.003)	Loss 4.6044e-02 (8.6458e-02)	Acc@1  99.22 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:43 - Epoch: [0][ 20/352]	Time  0.147 ( 0.178)	Data  0.001 ( 0.012)	Loss 3.4052e-01 (6.1005e-01)	Acc@1  89.84 ( 81.10)	Acc@5 100.00 ( 97.99)
07-Mar-22 03:16:44 - Epoch: [30][300/352]	Time  0.190 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.5108e-01 (8.6343e-02)	Acc@1  96.09 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:45 - Epoch: [0][ 30/352]	Time  0.169 ( 0.176)	Data  0.002 ( 0.009)	Loss 2.2166e-01 (5.1189e-01)	Acc@1  91.41 ( 83.95)	Acc@5 100.00 ( 98.56)
07-Mar-22 03:16:46 - Epoch: [30][310/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.3724e-01 (8.7080e-02)	Acc@1  94.53 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:47 - Epoch: [0][ 40/352]	Time  0.170 ( 0.174)	Data  0.002 ( 0.007)	Loss 3.0890e-01 (4.5394e-01)	Acc@1  89.84 ( 85.71)	Acc@5 100.00 ( 98.84)
07-Mar-22 03:16:47 - Epoch: [30][320/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.003)	Loss 8.6486e-02 (8.7084e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:48 - Epoch: [0][ 50/352]	Time  0.173 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.9247e-01 (4.0642e-01)	Acc@1  92.97 ( 87.24)	Acc@5 100.00 ( 99.00)
07-Mar-22 03:16:49 - Epoch: [30][330/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.9328e-02 (8.6985e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:50 - Epoch: [0][ 60/352]	Time  0.146 ( 0.170)	Data  0.002 ( 0.005)	Loss 2.0793e-01 (3.8474e-01)	Acc@1  93.75 ( 87.85)	Acc@5 100.00 ( 99.12)
07-Mar-22 03:16:50 - Epoch: [30][340/352]	Time  0.162 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.1107e-02 (8.6862e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:51 - Epoch: [0][ 70/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.005)	Loss 2.5535e-01 (3.6140e-01)	Acc@1  89.84 ( 88.55)	Acc@5 100.00 ( 99.21)
07-Mar-22 03:16:52 - Epoch: [30][350/352]	Time  0.155 ( 0.165)	Data  0.002 ( 0.003)	Loss 3.7807e-02 (8.6960e-02)	Acc@1  99.22 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:16:53 - Test: [ 0/20]	Time  0.333 ( 0.333)	Loss 3.5756e-01 (3.5756e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:16:53 - Epoch: [0][ 80/352]	Time  0.147 ( 0.167)	Data  0.001 ( 0.005)	Loss 1.6312e-01 (3.3911e-01)	Acc@1  93.75 ( 89.17)	Acc@5 100.00 ( 99.30)
07-Mar-22 03:16:54 - Test: [10/20]	Time  0.098 ( 0.120)	Loss 3.3829e-01 (3.9037e-01)	Acc@1  88.28 ( 88.64)	Acc@5  99.22 ( 99.54)
07-Mar-22 03:16:55 -  * Acc@1 89.040 Acc@5 99.420
07-Mar-22 03:16:55 - Best acc at epoch 30: 89.5199966430664
07-Mar-22 03:16:55 - Epoch: [0][ 90/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.4194e-01 (3.2773e-01)	Acc@1  96.09 ( 89.50)	Acc@5 100.00 ( 99.35)
07-Mar-22 03:16:55 - Epoch: [31][  0/352]	Time  0.389 ( 0.389)	Data  0.242 ( 0.242)	Loss 4.7182e-02 (4.7182e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
07-Mar-22 03:16:56 - Epoch: [0][100/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.9564e-01 (3.1599e-01)	Acc@1  93.75 ( 89.85)	Acc@5 100.00 ( 99.40)
07-Mar-22 03:16:57 - Epoch: [31][ 10/352]	Time  0.159 ( 0.186)	Data  0.002 ( 0.024)	Loss 9.4114e-02 (9.8195e-02)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 (100.00)
07-Mar-22 03:16:58 - Epoch: [0][110/352]	Time  0.173 ( 0.168)	Data  0.003 ( 0.004)	Loss 2.2375e-01 (3.0804e-01)	Acc@1  92.19 ( 90.01)	Acc@5 100.00 ( 99.43)
07-Mar-22 03:16:58 - Epoch: [31][ 20/352]	Time  0.166 ( 0.183)	Data  0.002 ( 0.014)	Loss 6.9080e-02 (9.5668e-02)	Acc@1  96.09 ( 96.24)	Acc@5 100.00 (100.00)
07-Mar-22 03:17:00 - Epoch: [0][120/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.004)	Loss 2.1344e-01 (3.0334e-01)	Acc@1  91.41 ( 90.14)	Acc@5 100.00 ( 99.46)
07-Mar-22 03:17:00 - Epoch: [31][ 30/352]	Time  0.167 ( 0.180)	Data  0.002 ( 0.010)	Loss 7.4563e-02 (8.8241e-02)	Acc@1  98.44 ( 96.75)	Acc@5 100.00 (100.00)
07-Mar-22 03:17:02 - Epoch: [0][130/352]	Time  0.172 ( 0.168)	Data  0.002 ( 0.004)	Loss 2.6114e-01 (2.9654e-01)	Acc@1  93.75 ( 90.40)	Acc@5  99.22 ( 99.48)
07-Mar-22 03:17:02 - Epoch: [31][ 40/352]	Time  0.191 ( 0.181)	Data  0.003 ( 0.008)	Loss 1.2951e-01 (9.4614e-02)	Acc@1  96.88 ( 96.63)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:17:03 - Epoch: [0][140/352]	Time  0.174 ( 0.169)	Data  0.002 ( 0.004)	Loss 2.6260e-01 (2.8918e-01)	Acc@1  91.41 ( 90.66)	Acc@5 100.00 ( 99.50)
07-Mar-22 03:17:04 - Epoch: [31][ 50/352]	Time  0.166 ( 0.180)	Data  0.002 ( 0.007)	Loss 8.1750e-02 (9.5101e-02)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:17:05 - Epoch: [0][150/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.004)	Loss 2.4867e-01 (2.8446e-01)	Acc@1  91.41 ( 90.82)	Acc@5 100.00 ( 99.52)
07-Mar-22 03:17:05 - Epoch: [31][ 60/352]	Time  0.147 ( 0.177)	Data  0.002 ( 0.006)	Loss 1.0753e-01 (9.1741e-02)	Acc@1  96.09 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:07 - Epoch: [0][160/352]	Time  0.173 ( 0.169)	Data  0.002 ( 0.003)	Loss 2.0051e-01 (2.7852e-01)	Acc@1  92.19 ( 91.03)	Acc@5 100.00 ( 99.54)
07-Mar-22 03:17:07 - Epoch: [31][ 70/352]	Time  0.153 ( 0.174)	Data  0.002 ( 0.006)	Loss 5.7281e-02 (9.0186e-02)	Acc@1  98.44 ( 96.72)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:08 - Epoch: [0][170/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.6117e-01 (2.7401e-01)	Acc@1  95.31 ( 91.17)	Acc@5 100.00 ( 99.56)
07-Mar-22 03:17:09 - Epoch: [31][ 80/352]	Time  0.163 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.1208e-01 (8.8449e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:10 - Epoch: [0][180/352]	Time  0.174 ( 0.169)	Data  0.002 ( 0.003)	Loss 2.8815e-01 (2.7014e-01)	Acc@1  89.06 ( 91.28)	Acc@5 100.00 ( 99.58)
07-Mar-22 03:17:10 - Epoch: [31][ 90/352]	Time  0.165 ( 0.172)	Data  0.002 ( 0.005)	Loss 7.1401e-02 (8.8044e-02)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:12 - Epoch: [0][190/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.8251e-01 (2.6603e-01)	Acc@1  93.75 ( 91.42)	Acc@5 100.00 ( 99.60)
07-Mar-22 03:17:12 - Epoch: [31][100/352]	Time  0.164 ( 0.171)	Data  0.002 ( 0.005)	Loss 7.1047e-02 (8.8301e-02)	Acc@1  98.44 ( 96.82)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:17:13 - Epoch: [0][200/352]	Time  0.172 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.5091e-01 (2.6337e-01)	Acc@1  95.31 ( 91.50)	Acc@5 100.00 ( 99.61)
07-Mar-22 03:17:14 - Epoch: [31][110/352]	Time  0.165 ( 0.171)	Data  0.002 ( 0.005)	Loss 7.4903e-02 (8.8301e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:15 - Epoch: [31][120/352]	Time  0.156 ( 0.170)	Data  0.002 ( 0.004)	Loss 2.9340e-02 (8.8873e-02)	Acc@1  99.22 ( 96.80)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:15 - Epoch: [0][210/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.6992e-01 (2.6010e-01)	Acc@1  93.75 ( 91.59)	Acc@5 100.00 ( 99.63)
07-Mar-22 03:17:17 - Epoch: [31][130/352]	Time  0.170 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.2032e-01 (8.9209e-02)	Acc@1  96.09 ( 96.76)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:17 - Epoch: [0][220/352]	Time  0.172 ( 0.169)	Data  0.002 ( 0.003)	Loss 2.1109e-01 (2.5745e-01)	Acc@1  92.19 ( 91.68)	Acc@5 100.00 ( 99.63)
07-Mar-22 03:17:18 - Epoch: [31][140/352]	Time  0.164 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.9925e-02 (8.8607e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:17:19 - Epoch: [0][230/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 2.8583e-01 (2.5459e-01)	Acc@1  89.84 ( 91.77)	Acc@5  99.22 ( 99.64)
07-Mar-22 03:17:20 - Epoch: [31][150/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.2283e-02 (8.7893e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:17:20 - Epoch: [0][240/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.5377e-01 (2.5179e-01)	Acc@1  94.53 ( 91.87)	Acc@5 100.00 ( 99.65)
07-Mar-22 03:17:22 - Epoch: [31][160/352]	Time  0.163 ( 0.169)	Data  0.003 ( 0.004)	Loss 5.1651e-02 (8.7982e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:22 - Epoch: [0][250/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.5113e-01 (2.4904e-01)	Acc@1  96.88 ( 91.97)	Acc@5 100.00 ( 99.66)
07-Mar-22 03:17:23 - Epoch: [31][170/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.5712e-02 (8.8006e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:24 - Epoch: [0][260/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.003)	Loss 2.4095e-01 (2.4644e-01)	Acc@1  93.75 ( 92.06)	Acc@5 100.00 ( 99.67)
07-Mar-22 03:17:25 - Epoch: [31][180/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.4913e-01 (8.7626e-02)	Acc@1  95.31 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:25 - Epoch: [0][270/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.6933e-01 (2.4441e-01)	Acc@1  93.75 ( 92.16)	Acc@5 100.00 ( 99.67)
07-Mar-22 03:17:27 - Epoch: [31][190/352]	Time  0.166 ( 0.168)	Data  0.003 ( 0.004)	Loss 7.6979e-02 (8.7451e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:27 - Epoch: [0][280/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.9758e-01 (2.4281e-01)	Acc@1  92.97 ( 92.21)	Acc@5  99.22 ( 99.67)
07-Mar-22 03:17:28 - Epoch: [31][200/352]	Time  0.162 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.2136e-02 (8.6752e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:29 - Epoch: [0][290/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.9456e-01 (2.4104e-01)	Acc@1  94.53 ( 92.26)	Acc@5 100.00 ( 99.68)
07-Mar-22 03:17:30 - Epoch: [31][210/352]	Time  0.162 ( 0.168)	Data  0.002 ( 0.004)	Loss 4.4832e-02 (8.6888e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:30 - Epoch: [0][300/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 2.5072e-01 (2.3965e-01)	Acc@1  91.41 ( 92.28)	Acc@5 100.00 ( 99.68)
07-Mar-22 03:17:32 - Epoch: [31][220/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.9631e-02 (8.5855e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:32 - Epoch: [0][310/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.5375e-01 (2.3808e-01)	Acc@1  95.31 ( 92.32)	Acc@5 100.00 ( 99.69)
07-Mar-22 03:17:33 - Epoch: [31][230/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.5361e-02 (8.5528e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:34 - Epoch: [0][320/352]	Time  0.214 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.6457e-01 (2.3679e-01)	Acc@1  96.09 ( 92.36)	Acc@5 100.00 ( 99.70)
07-Mar-22 03:17:35 - Epoch: [31][240/352]	Time  0.141 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.6645e-02 (8.5406e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:36 - Epoch: [0][330/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.9856e-01 (2.3491e-01)	Acc@1  92.97 ( 92.42)	Acc@5 100.00 ( 99.70)
07-Mar-22 03:17:36 - Epoch: [31][250/352]	Time  0.141 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.9277e-02 (8.5437e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:37 - Epoch: [0][340/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.6217e-01 (2.3306e-01)	Acc@1  97.66 ( 92.48)	Acc@5 100.00 ( 99.70)
07-Mar-22 03:17:38 - Epoch: [31][260/352]	Time  0.142 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.4018e-02 (8.5786e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:39 - Epoch: [0][350/352]	Time  0.167 ( 0.169)	Data  0.004 ( 0.003)	Loss 1.2904e-01 (2.3177e-01)	Acc@1  95.31 ( 92.50)	Acc@5 100.00 ( 99.70)
07-Mar-22 03:17:39 - Epoch: [31][270/352]	Time  0.174 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.3197e-02 (8.6283e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:40 - Test: [ 0/20]	Time  0.409 ( 0.409)	Loss 1.7986e-01 (1.7986e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:17:41 - Epoch: [31][280/352]	Time  0.147 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.2249e-01 (8.5950e-02)	Acc@1  95.31 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:41 - Test: [10/20]	Time  0.122 ( 0.136)	Loss 1.3050e-01 (1.7276e-01)	Acc@1  95.70 ( 94.03)	Acc@5 100.00 ( 99.79)
07-Mar-22 03:17:42 -  * Acc@1 94.160 Acc@5 99.800
07-Mar-22 03:17:42 - Best acc at epoch 0: 94.15999603271484
07-Mar-22 03:17:42 - Epoch: [31][290/352]	Time  0.156 ( 0.164)	Data  0.003 ( 0.003)	Loss 1.0995e-01 (8.6212e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:42 - Epoch: [1][  0/352]	Time  0.395 ( 0.395)	Data  0.233 ( 0.233)	Loss 2.6126e-01 (2.6126e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
07-Mar-22 03:17:44 - Epoch: [31][300/352]	Time  0.163 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.0938e-01 (8.6388e-02)	Acc@1  95.31 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:44 - Epoch: [1][ 10/352]	Time  0.146 ( 0.180)	Data  0.001 ( 0.023)	Loss 1.8527e-01 (1.8011e-01)	Acc@1  94.53 ( 94.32)	Acc@5 100.00 ( 99.86)
07-Mar-22 03:17:45 - Epoch: [1][ 20/352]	Time  0.159 ( 0.166)	Data  0.002 ( 0.013)	Loss 1.6827e-01 (1.8159e-01)	Acc@1  96.09 ( 94.20)	Acc@5 100.00 ( 99.89)
07-Mar-22 03:17:46 - Epoch: [31][310/352]	Time  0.165 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.7702e-02 (8.5984e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:47 - Epoch: [1][ 30/352]	Time  0.173 ( 0.168)	Data  0.002 ( 0.009)	Loss 1.9447e-01 (1.8205e-01)	Acc@1  93.75 ( 94.33)	Acc@5 100.00 ( 99.92)
07-Mar-22 03:17:47 - Epoch: [31][320/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.2820e-02 (8.6257e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:49 - Epoch: [31][330/352]	Time  0.188 ( 0.164)	Data  0.003 ( 0.003)	Loss 1.7876e-01 (8.6335e-02)	Acc@1  93.75 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:49 - Epoch: [1][ 40/352]	Time  0.203 ( 0.168)	Data  0.002 ( 0.008)	Loss 1.3186e-01 (1.8262e-01)	Acc@1  96.88 ( 94.38)	Acc@5 100.00 ( 99.94)
07-Mar-22 03:17:50 - Epoch: [31][340/352]	Time  0.149 ( 0.164)	Data  0.003 ( 0.003)	Loss 7.2070e-02 (8.6142e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:51 - Epoch: [1][ 50/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.007)	Loss 2.0402e-01 (1.7964e-01)	Acc@1  92.19 ( 94.35)	Acc@5 100.00 ( 99.91)
07-Mar-22 03:17:52 - Epoch: [31][350/352]	Time  0.143 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.0533e-02 (8.6077e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:17:52 - Epoch: [1][ 60/352]	Time  0.180 ( 0.169)	Data  0.002 ( 0.006)	Loss 2.6075e-01 (1.8009e-01)	Acc@1  92.19 ( 94.33)	Acc@5  99.22 ( 99.91)
07-Mar-22 03:17:53 - Test: [ 0/20]	Time  0.363 ( 0.363)	Loss 3.4990e-01 (3.4990e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:17:54 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.7118e-01 (3.8221e-01)	Acc@1  89.45 ( 88.96)	Acc@5  99.61 ( 99.57)
07-Mar-22 03:17:54 - Epoch: [1][ 70/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.3370e-01 (1.7780e-01)	Acc@1  94.53 ( 94.36)	Acc@5 100.00 ( 99.92)
07-Mar-22 03:17:55 -  * Acc@1 89.400 Acc@5 99.420
07-Mar-22 03:17:55 - Best acc at epoch 31: 89.5199966430664
07-Mar-22 03:17:55 - Epoch: [32][  0/352]	Time  0.421 ( 0.421)	Data  0.270 ( 0.270)	Loss 1.2714e-01 (1.2714e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:17:55 - Epoch: [1][ 80/352]	Time  0.158 ( 0.167)	Data  0.002 ( 0.005)	Loss 1.8727e-01 (1.7948e-01)	Acc@1  92.97 ( 94.29)	Acc@5 100.00 ( 99.92)
07-Mar-22 03:17:57 - Epoch: [32][ 10/352]	Time  0.165 ( 0.186)	Data  0.002 ( 0.026)	Loss 1.0943e-01 (1.0404e-01)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 (100.00)
07-Mar-22 03:17:57 - Epoch: [1][ 90/352]	Time  0.172 ( 0.167)	Data  0.002 ( 0.005)	Loss 1.5223e-01 (1.8068e-01)	Acc@1  94.53 ( 94.20)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:17:58 - Epoch: [32][ 20/352]	Time  0.163 ( 0.175)	Data  0.002 ( 0.015)	Loss 7.0445e-02 (9.5195e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:17:59 - Epoch: [1][100/352]	Time  0.174 ( 0.167)	Data  0.002 ( 0.005)	Loss 1.6103e-01 (1.7999e-01)	Acc@1  93.75 ( 94.19)	Acc@5 100.00 ( 99.94)
07-Mar-22 03:18:00 - Epoch: [32][ 30/352]	Time  0.167 ( 0.172)	Data  0.002 ( 0.011)	Loss 1.1350e-01 (9.3355e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:18:01 - Epoch: [1][110/352]	Time  0.171 ( 0.167)	Data  0.002 ( 0.004)	Loss 2.1799e-01 (1.8064e-01)	Acc@1  93.75 ( 94.12)	Acc@5 100.00 ( 99.91)
07-Mar-22 03:18:02 - Epoch: [32][ 40/352]	Time  0.165 ( 0.170)	Data  0.002 ( 0.009)	Loss 1.3387e-01 (9.4264e-02)	Acc@1  95.31 ( 96.76)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:18:02 - Epoch: [1][120/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.0414e-01 (1.7773e-01)	Acc@1  96.88 ( 94.23)	Acc@5 100.00 ( 99.91)
07-Mar-22 03:18:03 - Epoch: [32][ 50/352]	Time  0.162 ( 0.169)	Data  0.002 ( 0.007)	Loss 1.0642e-01 (9.2991e-02)	Acc@1  96.09 ( 96.78)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:18:04 - Epoch: [1][130/352]	Time  0.175 ( 0.169)	Data  0.003 ( 0.004)	Loss 1.5675e-01 (1.7672e-01)	Acc@1  96.88 ( 94.27)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:05 - Epoch: [32][ 60/352]	Time  0.164 ( 0.169)	Data  0.002 ( 0.007)	Loss 1.6658e-01 (9.2240e-02)	Acc@1  93.75 ( 96.77)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:18:06 - Epoch: [1][140/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.1348e-01 (1.7639e-01)	Acc@1  97.66 ( 94.34)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:07 - Epoch: [32][ 70/352]	Time  0.165 ( 0.168)	Data  0.001 ( 0.006)	Loss 6.8589e-02 (8.9832e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:07 - Epoch: [1][150/352]	Time  0.145 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.0528e-01 (1.7732e-01)	Acc@1  96.09 ( 94.31)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:08 - Epoch: [32][ 80/352]	Time  0.163 ( 0.168)	Data  0.002 ( 0.005)	Loss 4.4753e-02 (8.7428e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:09 - Epoch: [1][160/352]	Time  0.145 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.4834e-01 (1.7559e-01)	Acc@1  96.09 ( 94.39)	Acc@5 100.00 ( 99.91)
07-Mar-22 03:18:10 - Epoch: [32][ 90/352]	Time  0.161 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.4308e-01 (8.9979e-02)	Acc@1  95.31 ( 96.81)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:10 - Epoch: [1][170/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.004)	Loss 2.1267e-01 (1.7537e-01)	Acc@1  93.75 ( 94.38)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:12 - Epoch: [32][100/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.1233e-01 (9.1342e-02)	Acc@1  96.88 ( 96.75)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:12 - Epoch: [1][180/352]	Time  0.170 ( 0.167)	Data  0.003 ( 0.004)	Loss 1.2219e-01 (1.7502e-01)	Acc@1  96.09 ( 94.37)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:13 - Epoch: [32][110/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.005)	Loss 1.3146e-01 (9.0402e-02)	Acc@1  92.19 ( 96.76)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:14 - Epoch: [1][190/352]	Time  0.161 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.7804e-01 (1.7404e-01)	Acc@1  95.31 ( 94.43)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:15 - Epoch: [32][120/352]	Time  0.185 ( 0.167)	Data  0.002 ( 0.004)	Loss 8.7065e-02 (8.9305e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:16 - Epoch: [1][200/352]	Time  0.177 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.7568e-02 (1.7276e-01)	Acc@1  98.44 ( 94.49)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:17 - Epoch: [32][130/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 9.5211e-02 (8.8768e-02)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:17 - Epoch: [1][210/352]	Time  0.186 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.1795e-01 (1.7182e-01)	Acc@1  92.97 ( 94.52)	Acc@5  99.22 ( 99.90)
07-Mar-22 03:18:18 - Epoch: [32][140/352]	Time  0.160 ( 0.166)	Data  0.002 ( 0.004)	Loss 7.2492e-02 (8.8468e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:19 - Epoch: [1][220/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.7712e-01 (1.7218e-01)	Acc@1  93.75 ( 94.52)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:20 - Epoch: [32][150/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.004)	Loss 3.3752e-02 (8.8274e-02)	Acc@1 100.00 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:21 - Epoch: [1][230/352]	Time  0.171 ( 0.168)	Data  0.003 ( 0.003)	Loss 2.3285e-01 (1.7395e-01)	Acc@1  91.41 ( 94.43)	Acc@5  98.44 ( 99.89)
07-Mar-22 03:18:21 - Epoch: [32][160/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.004)	Loss 1.1681e-01 (8.8625e-02)	Acc@1  92.97 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:18:22 - Epoch: [1][240/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.3049e-01 (1.7388e-01)	Acc@1  94.53 ( 94.42)	Acc@5 100.00 ( 99.89)
07-Mar-22 03:18:23 - Epoch: [32][170/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.004)	Loss 8.6603e-02 (8.8560e-02)	Acc@1  95.31 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:18:24 - Epoch: [1][250/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.2463e-01 (1.7427e-01)	Acc@1  91.41 ( 94.40)	Acc@5 100.00 ( 99.89)
07-Mar-22 03:18:25 - Epoch: [32][180/352]	Time  0.175 ( 0.165)	Data  0.002 ( 0.004)	Loss 9.6695e-02 (8.8947e-02)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:26 - Epoch: [1][260/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.5068e-01 (1.7324e-01)	Acc@1  96.09 ( 94.44)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:26 - Epoch: [32][190/352]	Time  0.163 ( 0.165)	Data  0.002 ( 0.004)	Loss 4.8331e-02 (8.7880e-02)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:28 - Epoch: [1][270/352]	Time  0.173 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.0256e-01 (1.7314e-01)	Acc@1  92.97 ( 94.47)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:28 - Epoch: [32][200/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.4356e-01 (8.9086e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:29 - Epoch: [1][280/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.5163e-01 (1.7372e-01)	Acc@1  96.09 ( 94.45)	Acc@5 100.00 ( 99.89)
07-Mar-22 03:18:30 - Epoch: [32][210/352]	Time  0.163 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.3762e-01 (9.0411e-02)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:31 - Epoch: [1][290/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.4615e-01 (1.7421e-01)	Acc@1  90.62 ( 94.43)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:31 - Epoch: [32][220/352]	Time  0.163 ( 0.165)	Data  0.003 ( 0.003)	Loss 7.0461e-02 (9.0199e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:33 - Epoch: [1][300/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.5117e-01 (1.7437e-01)	Acc@1  96.88 ( 94.43)	Acc@5 100.00 ( 99.89)
07-Mar-22 03:18:33 - Epoch: [32][230/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.0090e-01 (9.0308e-02)	Acc@1  95.31 ( 96.82)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:34 - Epoch: [1][310/352]	Time  0.175 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.7200e-01 (1.7401e-01)	Acc@1  94.53 ( 94.41)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:35 - Epoch: [32][240/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.8977e-02 (8.9687e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:36 - Epoch: [1][320/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.7597e-01 (1.7403e-01)	Acc@1  94.53 ( 94.40)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:36 - Epoch: [32][250/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.003)	Loss 3.3899e-02 (8.9463e-02)	Acc@1  99.22 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:38 - Epoch: [1][330/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.003)	Loss 2.0269e-01 (1.7368e-01)	Acc@1  92.97 ( 94.43)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:38 - Epoch: [32][260/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.1120e-02 (8.9284e-02)	Acc@1  99.22 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:39 - Epoch: [32][270/352]	Time  0.147 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.2069e-01 (8.9144e-02)	Acc@1  95.31 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:39 - Epoch: [1][340/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.7826e-01 (1.7385e-01)	Acc@1  93.75 ( 94.41)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:41 - Epoch: [32][280/352]	Time  0.142 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.4596e-01 (8.9033e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:41 - Epoch: [1][350/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 2.8697e-01 (1.7400e-01)	Acc@1  91.41 ( 94.40)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:18:42 - Test: [ 0/20]	Time  0.354 ( 0.354)	Loss 1.9349e-01 (1.9349e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
07-Mar-22 03:18:42 - Epoch: [32][290/352]	Time  0.146 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.6095e-02 (8.9130e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:43 - Test: [10/20]	Time  0.101 ( 0.125)	Loss 1.6216e-01 (1.6892e-01)	Acc@1  96.09 ( 94.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:18:44 -  * Acc@1 94.840 Acc@5 99.940
07-Mar-22 03:18:44 - Best acc at epoch 1: 94.83999633789062
07-Mar-22 03:18:44 - Epoch: [32][300/352]	Time  0.137 ( 0.164)	Data  0.002 ( 0.003)	Loss 5.2710e-02 (8.8988e-02)	Acc@1  99.22 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:44 - Epoch: [2][  0/352]	Time  0.408 ( 0.408)	Data  0.228 ( 0.228)	Loss 2.0537e-01 (2.0537e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
07-Mar-22 03:18:46 - Epoch: [32][310/352]	Time  0.152 ( 0.164)	Data  0.002 ( 0.003)	Loss 3.1146e-02 (8.8821e-02)	Acc@1  99.22 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:47 - Epoch: [32][320/352]	Time  0.182 ( 0.163)	Data  0.003 ( 0.003)	Loss 1.0854e-01 (8.8715e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:49 - Epoch: [32][330/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.6441e-02 (8.9075e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:51 - Epoch: [32][340/352]	Time  0.168 ( 0.164)	Data  0.003 ( 0.003)	Loss 1.3533e-01 (8.9309e-02)	Acc@1  94.53 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:52 - Epoch: [32][350/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.5147e-02 (8.9134e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:18:53 - Test: [ 0/20]	Time  0.392 ( 0.392)	Loss 3.1864e-01 (3.1864e-01)	Acc@1  91.80 ( 91.80)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:18:54 - Test: [10/20]	Time  0.098 ( 0.129)	Loss 3.4813e-01 (3.8778e-01)	Acc@1  89.84 ( 89.38)	Acc@5  99.22 ( 99.47)
07-Mar-22 03:18:55 -  * Acc@1 89.520 Acc@5 99.360
07-Mar-22 03:18:55 - Best acc at epoch 32: 89.5199966430664
07-Mar-22 03:18:55 - Epoch: [33][  0/352]	Time  0.408 ( 0.408)	Data  0.246 ( 0.246)	Loss 8.1898e-02 (8.1898e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
07-Mar-22 03:18:57 - Epoch: [33][ 10/352]	Time  0.175 ( 0.190)	Data  0.002 ( 0.024)	Loss 5.2811e-02 (9.5849e-02)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 (100.00)
07-Mar-22 03:18:59 - Epoch: [33][ 20/352]	Time  0.154 ( 0.175)	Data  0.002 ( 0.014)	Loss 1.4278e-01 (9.5889e-02)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:19:00 - Epoch: [33][ 30/352]	Time  0.206 ( 0.170)	Data  0.002 ( 0.010)	Loss 1.2416e-01 (9.6642e-02)	Acc@1  93.75 ( 96.65)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:19:02 - Epoch: [33][ 40/352]	Time  0.143 ( 0.168)	Data  0.002 ( 0.008)	Loss 9.3101e-02 (9.8460e-02)	Acc@1  97.66 ( 96.59)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:03 - Epoch: [33][ 50/352]	Time  0.153 ( 0.163)	Data  0.002 ( 0.007)	Loss 6.1785e-02 (9.3999e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:05 - Epoch: [33][ 60/352]	Time  0.155 ( 0.161)	Data  0.003 ( 0.006)	Loss 6.4897e-02 (9.1852e-02)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:19:06 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 03:19:06 - => creating PyTorchCV model 'resnet20_unfold'
07-Mar-22 03:19:06 - match all modules defined in bit_config: True
07-Mar-22 03:19:06 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_conv_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
07-Mar-22 03:19:06 - Epoch: [33][ 70/352]	Time  0.172 ( 0.161)	Data  0.002 ( 0.006)	Loss 1.3532e-01 (9.2603e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:19:08 - Epoch: [33][ 80/352]	Time  0.166 ( 0.163)	Data  0.003 ( 0.005)	Loss 1.3244e-01 (9.2323e-02)	Acc@1  94.53 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:10 - Epoch: [33][ 90/352]	Time  0.185 ( 0.165)	Data  0.003 ( 0.005)	Loss 3.8457e-02 (9.2802e-02)	Acc@1  98.44 ( 96.83)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:19:11 - Epoch: [0][  0/352]	Time  0.448 ( 0.448)	Data  0.228 ( 0.228)	Loss 1.2411e+00 (1.2411e+00)	Acc@1  58.59 ( 58.59)	Acc@5  94.53 ( 94.53)
07-Mar-22 03:19:12 - Epoch: [33][100/352]	Time  0.156 ( 0.164)	Data  0.002 ( 0.005)	Loss 6.5375e-02 (9.1773e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:19:13 - Epoch: [0][ 10/352]	Time  0.173 ( 0.200)	Data  0.003 ( 0.022)	Loss 4.5018e-01 (7.9915e-01)	Acc@1  88.28 ( 75.64)	Acc@5  99.22 ( 97.09)
07-Mar-22 03:19:13 - Epoch: [33][110/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.005)	Loss 7.5632e-02 (9.2118e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:19:14 - Epoch: [0][ 20/352]	Time  0.150 ( 0.186)	Data  0.002 ( 0.013)	Loss 3.4986e-01 (5.8100e-01)	Acc@1  90.62 ( 82.11)	Acc@5 100.00 ( 98.36)
07-Mar-22 03:19:15 - Epoch: [33][120/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.004)	Loss 8.0901e-02 (9.3255e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:19:16 - Epoch: [0][ 30/352]	Time  0.167 ( 0.180)	Data  0.002 ( 0.009)	Loss 3.2575e-01 (5.0025e-01)	Acc@1  90.62 ( 84.45)	Acc@5 100.00 ( 98.71)
07-Mar-22 03:19:16 - Epoch: [33][130/352]	Time  0.142 ( 0.163)	Data  0.002 ( 0.004)	Loss 1.3976e-01 (9.3890e-02)	Acc@1  96.88 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:18 - Epoch: [0][ 40/352]	Time  0.190 ( 0.179)	Data  0.002 ( 0.008)	Loss 3.1701e-01 (4.4169e-01)	Acc@1  88.28 ( 86.07)	Acc@5 100.00 ( 98.95)
07-Mar-22 03:19:18 - Epoch: [33][140/352]	Time  0.163 ( 0.164)	Data  0.002 ( 0.004)	Loss 1.0215e-01 (9.3716e-02)	Acc@1  96.88 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:19 - Epoch: [0][ 50/352]	Time  0.146 ( 0.175)	Data  0.001 ( 0.007)	Loss 2.4107e-01 (4.0340e-01)	Acc@1  94.53 ( 87.13)	Acc@5 100.00 ( 99.10)
07-Mar-22 03:19:20 - Epoch: [33][150/352]	Time  0.148 ( 0.163)	Data  0.002 ( 0.004)	Loss 1.2301e-01 (9.2760e-02)	Acc@1  95.31 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:21 - Epoch: [0][ 60/352]	Time  0.150 ( 0.174)	Data  0.002 ( 0.006)	Loss 2.0318e-01 (3.7029e-01)	Acc@1  92.19 ( 88.10)	Acc@5 100.00 ( 99.23)
07-Mar-22 03:19:21 - Epoch: [33][160/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.004)	Loss 1.5748e-01 (9.3272e-02)	Acc@1  92.97 ( 96.80)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:23 - Epoch: [0][ 70/352]	Time  0.194 ( 0.174)	Data  0.002 ( 0.005)	Loss 2.1417e-01 (3.5314e-01)	Acc@1  92.19 ( 88.56)	Acc@5  99.22 ( 99.27)
07-Mar-22 03:19:23 - Epoch: [33][170/352]	Time  0.165 ( 0.163)	Data  0.002 ( 0.004)	Loss 5.7530e-02 (9.2005e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:24 - Epoch: [33][180/352]	Time  0.163 ( 0.163)	Data  0.002 ( 0.004)	Loss 1.1781e-01 (9.1870e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:24 - Epoch: [0][ 80/352]	Time  0.161 ( 0.174)	Data  0.002 ( 0.005)	Loss 3.0187e-01 (3.3444e-01)	Acc@1  89.84 ( 89.18)	Acc@5 100.00 ( 99.35)
07-Mar-22 03:19:26 - Epoch: [33][190/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.004)	Loss 7.1551e-02 (9.1213e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:26 - Epoch: [0][ 90/352]	Time  0.167 ( 0.174)	Data  0.003 ( 0.005)	Loss 3.0163e-01 (3.2404e-01)	Acc@1  91.41 ( 89.45)	Acc@5  99.22 ( 99.41)
07-Mar-22 03:19:28 - Epoch: [33][200/352]	Time  0.162 ( 0.163)	Data  0.002 ( 0.003)	Loss 3.0066e-02 (9.1267e-02)	Acc@1  99.22 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:28 - Epoch: [0][100/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.004)	Loss 1.8164e-01 (3.1124e-01)	Acc@1  92.97 ( 89.85)	Acc@5  99.22 ( 99.45)
07-Mar-22 03:19:29 - Epoch: [33][210/352]	Time  0.162 ( 0.163)	Data  0.002 ( 0.003)	Loss 9.6579e-02 (9.2143e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:19:30 - Epoch: [0][110/352]	Time  0.173 ( 0.173)	Data  0.003 ( 0.004)	Loss 2.2601e-01 (3.0140e-01)	Acc@1  93.75 ( 90.18)	Acc@5  99.22 ( 99.49)
07-Mar-22 03:19:31 - Epoch: [33][220/352]	Time  0.163 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.1893e-02 (9.2309e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:19:31 - Epoch: [0][120/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.004)	Loss 1.2729e-01 (2.9312e-01)	Acc@1  96.88 ( 90.46)	Acc@5 100.00 ( 99.53)
07-Mar-22 03:19:33 - Epoch: [33][230/352]	Time  0.152 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.1082e-02 (9.1864e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:19:33 - Epoch: [0][130/352]	Time  0.151 ( 0.172)	Data  0.002 ( 0.004)	Loss 2.1872e-01 (2.8664e-01)	Acc@1  92.97 ( 90.64)	Acc@5 100.00 ( 99.56)
07-Mar-22 03:19:34 - Epoch: [33][240/352]	Time  0.168 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.8100e-02 (9.1459e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:19:35 - Epoch: [0][140/352]	Time  0.170 ( 0.172)	Data  0.003 ( 0.004)	Loss 2.9170e-01 (2.8152e-01)	Acc@1  90.62 ( 90.80)	Acc@5  99.22 ( 99.58)
07-Mar-22 03:19:36 - Epoch: [33][250/352]	Time  0.177 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.3436e-02 (9.1391e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:36 - Epoch: [0][150/352]	Time  0.165 ( 0.172)	Data  0.002 ( 0.004)	Loss 1.8383e-01 (2.7894e-01)	Acc@1  92.97 ( 90.89)	Acc@5 100.00 ( 99.60)
07-Mar-22 03:19:38 - Epoch: [33][260/352]	Time  0.165 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.4525e-02 (9.1577e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:38 - Epoch: [0][160/352]	Time  0.172 ( 0.172)	Data  0.002 ( 0.004)	Loss 1.3256e-01 (2.7409e-01)	Acc@1  97.66 ( 91.06)	Acc@5 100.00 ( 99.60)
07-Mar-22 03:19:39 - Epoch: [33][270/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.2041e-02 (9.0856e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:40 - Epoch: [0][170/352]	Time  0.175 ( 0.172)	Data  0.002 ( 0.004)	Loss 2.0551e-01 (2.6979e-01)	Acc@1  93.75 ( 91.16)	Acc@5 100.00 ( 99.62)
07-Mar-22 03:19:41 - Epoch: [33][280/352]	Time  0.164 ( 0.163)	Data  0.002 ( 0.003)	Loss 5.3559e-02 (9.0094e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:41 - Epoch: [0][180/352]	Time  0.171 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.4663e-01 (2.6713e-01)	Acc@1  95.31 ( 91.23)	Acc@5 100.00 ( 99.63)
07-Mar-22 03:19:42 - Epoch: [33][290/352]	Time  0.164 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.1674e-01 (9.0034e-02)	Acc@1  93.75 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:43 - Epoch: [0][190/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.2347e-01 (2.6273e-01)	Acc@1  95.31 ( 91.41)	Acc@5 100.00 ( 99.64)
07-Mar-22 03:19:44 - Epoch: [33][300/352]	Time  0.168 ( 0.163)	Data  0.002 ( 0.003)	Loss 5.2315e-02 (8.9483e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:45 - Epoch: [0][200/352]	Time  0.163 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.6562e-01 (2.5940e-01)	Acc@1  92.97 ( 91.52)	Acc@5 100.00 ( 99.65)
07-Mar-22 03:19:46 - Epoch: [33][310/352]	Time  0.153 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.1071e-01 (8.9496e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:46 - Epoch: [0][210/352]	Time  0.164 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.4382e-01 (2.5520e-01)	Acc@1  96.09 ( 91.67)	Acc@5 100.00 ( 99.67)
07-Mar-22 03:19:47 - Epoch: [33][320/352]	Time  0.165 ( 0.163)	Data  0.002 ( 0.003)	Loss 4.8829e-02 (8.9322e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:48 - Epoch: [0][220/352]	Time  0.170 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.8421e-01 (2.5281e-01)	Acc@1  94.53 ( 91.71)	Acc@5 100.00 ( 99.68)
07-Mar-22 03:19:49 - Epoch: [33][330/352]	Time  0.170 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.6406e-02 (8.8600e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:50 - Epoch: [0][230/352]	Time  0.172 ( 0.171)	Data  0.003 ( 0.003)	Loss 1.9875e-01 (2.4993e-01)	Acc@1  93.75 ( 91.80)	Acc@5 100.00 ( 99.69)
07-Mar-22 03:19:51 - Epoch: [33][340/352]	Time  0.171 ( 0.164)	Data  0.002 ( 0.003)	Loss 6.3157e-02 (8.8674e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:52 - Epoch: [0][240/352]	Time  0.190 ( 0.171)	Data  0.002 ( 0.003)	Loss 2.0846e-01 (2.4725e-01)	Acc@1  93.75 ( 91.89)	Acc@5 100.00 ( 99.69)
07-Mar-22 03:19:52 - Epoch: [33][350/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 5.0607e-02 (8.8790e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:19:53 - Test: [ 0/20]	Time  0.414 ( 0.414)	Loss 3.4224e-01 (3.4224e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:19:53 - Epoch: [0][250/352]	Time  0.165 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.6552e-01 (2.4491e-01)	Acc@1  94.53 ( 91.96)	Acc@5 100.00 ( 99.70)
07-Mar-22 03:19:54 - Test: [10/20]	Time  0.100 ( 0.131)	Loss 3.5930e-01 (3.8021e-01)	Acc@1  89.45 ( 88.85)	Acc@5  99.61 ( 99.50)
07-Mar-22 03:19:55 -  * Acc@1 89.280 Acc@5 99.500
07-Mar-22 03:19:55 - Best acc at epoch 33: 89.5199966430664
07-Mar-22 03:19:55 - Epoch: [0][260/352]	Time  0.199 ( 0.171)	Data  0.002 ( 0.003)	Loss 2.4928e-01 (2.4382e-01)	Acc@1  92.19 ( 92.00)	Acc@5 100.00 ( 99.71)
07-Mar-22 03:19:55 - Epoch: [34][  0/352]	Time  0.373 ( 0.373)	Data  0.227 ( 0.227)	Loss 5.8421e-02 (5.8421e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
07-Mar-22 03:19:57 - Epoch: [0][270/352]	Time  0.159 ( 0.171)	Data  0.003 ( 0.003)	Loss 1.6226e-01 (2.4201e-01)	Acc@1  93.75 ( 92.06)	Acc@5 100.00 ( 99.71)
07-Mar-22 03:19:57 - Epoch: [34][ 10/352]	Time  0.163 ( 0.190)	Data  0.002 ( 0.023)	Loss 7.7488e-02 (9.2089e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:19:58 - Epoch: [0][280/352]	Time  0.170 ( 0.171)	Data  0.002 ( 0.003)	Loss 3.6080e-01 (2.4035e-01)	Acc@1  85.16 ( 92.11)	Acc@5  99.22 ( 99.71)
07-Mar-22 03:19:59 - Epoch: [34][ 20/352]	Time  0.156 ( 0.176)	Data  0.001 ( 0.013)	Loss 1.1062e-01 (8.7529e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:20:00 - Epoch: [0][290/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.4728e-01 (2.3892e-01)	Acc@1  95.31 ( 92.16)	Acc@5 100.00 ( 99.72)
07-Mar-22 03:20:01 - Epoch: [34][ 30/352]	Time  0.168 ( 0.176)	Data  0.002 ( 0.010)	Loss 4.3418e-02 (8.4052e-02)	Acc@1  99.22 ( 97.03)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:20:02 - Epoch: [0][300/352]	Time  0.172 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.3272e-01 (2.3760e-01)	Acc@1  95.31 ( 92.18)	Acc@5 100.00 ( 99.72)
07-Mar-22 03:20:02 - Epoch: [34][ 40/352]	Time  0.167 ( 0.174)	Data  0.002 ( 0.008)	Loss 8.1219e-02 (8.4898e-02)	Acc@1  95.31 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:03 - Epoch: [0][310/352]	Time  0.165 ( 0.171)	Data  0.002 ( 0.003)	Loss 2.3187e-01 (2.3598e-01)	Acc@1  93.75 ( 92.26)	Acc@5 100.00 ( 99.73)
07-Mar-22 03:20:04 - Epoch: [34][ 50/352]	Time  0.168 ( 0.173)	Data  0.002 ( 0.007)	Loss 9.3440e-02 (8.4299e-02)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:05 - Epoch: [0][320/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.003)	Loss 2.3716e-01 (2.3448e-01)	Acc@1  92.97 ( 92.32)	Acc@5  99.22 ( 99.73)
07-Mar-22 03:20:06 - Epoch: [34][ 60/352]	Time  0.178 ( 0.172)	Data  0.002 ( 0.006)	Loss 8.0630e-02 (8.8251e-02)	Acc@1  96.09 ( 96.86)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:20:07 - Epoch: [0][330/352]	Time  0.171 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.6323e-01 (2.3289e-01)	Acc@1  93.75 ( 92.39)	Acc@5 100.00 ( 99.73)
07-Mar-22 03:20:07 - Epoch: [34][ 70/352]	Time  0.163 ( 0.171)	Data  0.002 ( 0.006)	Loss 7.0664e-02 (8.6951e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:08 - Epoch: [0][340/352]	Time  0.165 ( 0.170)	Data  0.002 ( 0.003)	Loss 1.7239e-01 (2.3041e-01)	Acc@1  93.75 ( 92.48)	Acc@5 100.00 ( 99.74)
07-Mar-22 03:20:09 - Epoch: [34][ 80/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.9446e-01 (8.8956e-02)	Acc@1  92.19 ( 96.82)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:20:10 - Epoch: [0][350/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.4671e-01 (2.2926e-01)	Acc@1  95.31 ( 92.53)	Acc@5 100.00 ( 99.74)
07-Mar-22 03:20:10 - Epoch: [34][ 90/352]	Time  0.178 ( 0.169)	Data  0.003 ( 0.005)	Loss 1.0705e-01 (8.8959e-02)	Acc@1  92.97 ( 96.81)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:20:11 - Test: [ 0/20]	Time  0.397 ( 0.397)	Loss 1.6672e-01 (1.6672e-01)	Acc@1  94.14 ( 94.14)	Acc@5 100.00 (100.00)
07-Mar-22 03:20:12 - Test: [10/20]	Time  0.103 ( 0.137)	Loss 1.5903e-01 (1.7756e-01)	Acc@1  95.31 ( 93.82)	Acc@5  99.61 ( 99.89)
07-Mar-22 03:20:12 - Epoch: [34][100/352]	Time  0.155 ( 0.169)	Data  0.002 ( 0.005)	Loss 7.3283e-02 (8.8946e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:20:13 -  * Acc@1 93.820 Acc@5 99.880
07-Mar-22 03:20:13 - Best acc at epoch 0: 93.81999969482422
07-Mar-22 03:20:14 - Epoch: [1][  0/352]	Time  0.378 ( 0.378)	Data  0.235 ( 0.235)	Loss 1.2809e-01 (1.2809e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 03:20:14 - Epoch: [34][110/352]	Time  0.163 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.6070e-02 (8.9845e-02)	Acc@1  98.44 ( 96.79)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:20:15 - Epoch: [34][120/352]	Time  0.141 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.0626e-01 (8.9981e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:20:15 - Epoch: [1][ 10/352]	Time  0.154 ( 0.180)	Data  0.002 ( 0.023)	Loss 1.7423e-01 (1.7406e-01)	Acc@1  95.31 ( 94.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:20:17 - Epoch: [34][130/352]	Time  0.174 ( 0.165)	Data  0.002 ( 0.004)	Loss 5.7827e-02 (8.9465e-02)	Acc@1  98.44 ( 96.80)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:17 - Epoch: [1][ 20/352]	Time  0.169 ( 0.176)	Data  0.002 ( 0.013)	Loss 2.1987e-01 (1.7818e-01)	Acc@1  93.75 ( 94.08)	Acc@5 100.00 (100.00)
07-Mar-22 03:20:18 - Epoch: [34][140/352]	Time  0.153 ( 0.165)	Data  0.002 ( 0.004)	Loss 9.7048e-02 (8.8708e-02)	Acc@1  94.53 ( 96.81)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:20 - Epoch: [34][150/352]	Time  0.180 ( 0.165)	Data  0.002 ( 0.004)	Loss 3.8200e-02 (8.8323e-02)	Acc@1  99.22 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:22 - Epoch: [34][160/352]	Time  0.177 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.0539e-01 (8.7740e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:24 - Epoch: [34][170/352]	Time  0.178 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2053e-01 (8.8172e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:25 - Epoch: [34][180/352]	Time  0.178 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.5102e-02 (8.9298e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:27 - Epoch: [34][190/352]	Time  0.180 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4117e-01 (8.8553e-02)	Acc@1  96.09 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:29 - Epoch: [34][200/352]	Time  0.176 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.5205e-02 (8.8813e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:31 - Epoch: [34][210/352]	Time  0.183 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.1098e-01 (8.8198e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:33 - Epoch: [34][220/352]	Time  0.178 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.1632e-01 (8.8480e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:34 - Epoch: [34][230/352]	Time  0.179 ( 0.170)	Data  0.002 ( 0.003)	Loss 8.0794e-02 (8.7734e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:36 - Epoch: [34][240/352]	Time  0.176 ( 0.170)	Data  0.002 ( 0.003)	Loss 5.7551e-02 (8.7720e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:38 - Epoch: [34][250/352]	Time  0.176 ( 0.170)	Data  0.002 ( 0.003)	Loss 1.0728e-01 (8.7139e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:40 - Epoch: [34][260/352]	Time  0.178 ( 0.171)	Data  0.002 ( 0.003)	Loss 7.1836e-02 (8.7395e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:20:41 - Epoch: [34][270/352]	Time  0.177 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.7014e-01 (8.7579e-02)	Acc@1  89.84 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:20:43 - Epoch: [34][280/352]	Time  0.176 ( 0.171)	Data  0.002 ( 0.003)	Loss 1.0946e-01 (8.7539e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:20:45 - Epoch: [34][290/352]	Time  0.178 ( 0.171)	Data  0.002 ( 0.003)	Loss 8.8961e-02 (8.7313e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:20:47 - Epoch: [34][300/352]	Time  0.177 ( 0.172)	Data  0.002 ( 0.003)	Loss 2.7825e-02 (8.7079e-02)	Acc@1 100.00 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:49 - Epoch: [34][310/352]	Time  0.183 ( 0.172)	Data  0.002 ( 0.003)	Loss 5.6985e-02 (8.6237e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:50 - Epoch: [34][320/352]	Time  0.179 ( 0.172)	Data  0.002 ( 0.003)	Loss 8.6963e-02 (8.6330e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:52 - Epoch: [34][330/352]	Time  0.175 ( 0.172)	Data  0.002 ( 0.003)	Loss 1.2110e-01 (8.6245e-02)	Acc@1  94.53 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:54 - Epoch: [34][340/352]	Time  0.179 ( 0.172)	Data  0.002 ( 0.003)	Loss 9.5346e-02 (8.6752e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:56 - Epoch: [34][350/352]	Time  0.176 ( 0.173)	Data  0.002 ( 0.003)	Loss 1.1297e-01 (8.6792e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:20:56 - Test: [ 0/20]	Time  0.373 ( 0.373)	Loss 3.4875e-01 (3.4875e-01)	Acc@1  88.67 ( 88.67)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:20:57 - Test: [10/20]	Time  0.101 ( 0.134)	Loss 4.1772e-01 (3.8943e-01)	Acc@1  87.11 ( 88.60)	Acc@5  98.83 ( 99.57)
07-Mar-22 03:20:58 -  * Acc@1 88.900 Acc@5 99.560
07-Mar-22 03:20:58 - Best acc at epoch 34: 89.5199966430664
07-Mar-22 03:20:59 - Epoch: [35][  0/352]	Time  0.390 ( 0.390)	Data  0.234 ( 0.234)	Loss 1.2331e-01 (1.2331e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:21:01 - Epoch: [35][ 10/352]	Time  0.163 ( 0.192)	Data  0.002 ( 0.023)	Loss 6.3699e-02 (1.0630e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 (100.00)
07-Mar-22 03:21:02 - Epoch: [35][ 20/352]	Time  0.153 ( 0.176)	Data  0.002 ( 0.013)	Loss 8.7388e-02 (9.3753e-02)	Acc@1  96.09 ( 96.65)	Acc@5 100.00 (100.00)
07-Mar-22 03:21:04 - Epoch: [35][ 30/352]	Time  0.179 ( 0.173)	Data  0.002 ( 0.009)	Loss 8.5787e-02 (1.0031e-01)	Acc@1  99.22 ( 96.50)	Acc@5 100.00 (100.00)
07-Mar-22 03:21:06 - Epoch: [35][ 40/352]	Time  0.155 ( 0.173)	Data  0.002 ( 0.008)	Loss 6.0204e-02 (9.4481e-02)	Acc@1  98.44 ( 96.78)	Acc@5 100.00 (100.00)
07-Mar-22 03:21:07 - Epoch: [35][ 50/352]	Time  0.180 ( 0.173)	Data  0.002 ( 0.007)	Loss 9.0245e-02 (9.5205e-02)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 (100.00)
07-Mar-22 03:21:09 - Epoch: [35][ 60/352]	Time  0.165 ( 0.173)	Data  0.002 ( 0.006)	Loss 8.6584e-02 (9.0401e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 (100.00)
07-Mar-22 03:21:11 - Epoch: [35][ 70/352]	Time  0.181 ( 0.173)	Data  0.002 ( 0.005)	Loss 4.4299e-02 (8.9929e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 (100.00)
07-Mar-22 03:21:12 - Epoch: [35][ 80/352]	Time  0.172 ( 0.173)	Data  0.002 ( 0.005)	Loss 6.8193e-02 (8.9065e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 (100.00)
07-Mar-22 03:21:14 - Epoch: [35][ 90/352]	Time  0.165 ( 0.172)	Data  0.002 ( 0.005)	Loss 2.9804e-02 (8.8243e-02)	Acc@1 100.00 ( 96.92)	Acc@5 100.00 (100.00)
07-Mar-22 03:21:16 - Epoch: [35][100/352]	Time  0.156 ( 0.171)	Data  0.002 ( 0.004)	Loss 1.6662e-01 (8.8577e-02)	Acc@1  95.31 ( 96.87)	Acc@5  99.22 ( 99.99)
07-Mar-22 03:21:17 - Epoch: [35][110/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.004)	Loss 1.0273e-01 (8.8834e-02)	Acc@1  96.09 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:19 - Epoch: [35][120/352]	Time  0.175 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.4685e-01 (8.7158e-02)	Acc@1  94.53 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:21 - Epoch: [35][130/352]	Time  0.155 ( 0.170)	Data  0.002 ( 0.004)	Loss 9.1119e-02 (8.6575e-02)	Acc@1  95.31 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:22 - Epoch: [35][140/352]	Time  0.153 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.5945e-02 (8.6007e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:24 - Epoch: [35][150/352]	Time  0.154 ( 0.167)	Data  0.003 ( 0.004)	Loss 1.1483e-01 (8.6553e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:25 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=1, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=False, fix_BN_threshold=None, fixed_point_quantization=False, gpu=None, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=False, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=False, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 03:21:25 - => creating PyTorchCV model 'resnet20_unfold'
07-Mar-22 03:21:25 - match all modules defined in bit_config: True
07-Mar-22 03:21:25 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_conv_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=False)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
07-Mar-22 03:21:25 - Epoch: [35][160/352]	Time  0.179 ( 0.167)	Data  0.003 ( 0.004)	Loss 4.8802e-02 (8.7557e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:27 - Epoch: [35][170/352]	Time  0.204 ( 0.168)	Data  0.003 ( 0.004)	Loss 9.8396e-02 (8.7278e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:29 - Epoch: [35][180/352]	Time  0.140 ( 0.169)	Data  0.003 ( 0.003)	Loss 8.7217e-02 (8.7414e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:29 - Epoch: [0][  0/352]	Time  0.451 ( 0.451)	Data  0.217 ( 0.217)	Loss 1.4110e+00 (1.4110e+00)	Acc@1  52.34 ( 52.34)	Acc@5  90.62 ( 90.62)
07-Mar-22 03:21:31 - Epoch: [35][190/352]	Time  0.150 ( 0.168)	Data  0.003 ( 0.003)	Loss 4.7662e-02 (8.6301e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:31 - Epoch: [0][ 10/352]	Time  0.168 ( 0.191)	Data  0.002 ( 0.022)	Loss 3.8579e-01 (7.6699e-01)	Acc@1  87.50 ( 77.41)	Acc@5  98.44 ( 97.30)
07-Mar-22 03:21:32 - Epoch: [35][200/352]	Time  0.167 ( 0.168)	Data  0.003 ( 0.003)	Loss 1.0493e-01 (8.6136e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:33 - Epoch: [0][ 20/352]	Time  0.165 ( 0.182)	Data  0.002 ( 0.012)	Loss 1.9024e-01 (5.6732e-01)	Acc@1  96.09 ( 83.41)	Acc@5 100.00 ( 98.25)
07-Mar-22 03:21:34 - Epoch: [35][210/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.7626e-02 (8.5794e-02)	Acc@1  95.31 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:34 - Epoch: [0][ 30/352]	Time  0.177 ( 0.177)	Data  0.002 ( 0.009)	Loss 2.2634e-01 (4.6485e-01)	Acc@1  92.19 ( 86.16)	Acc@5  99.22 ( 98.77)
07-Mar-22 03:21:35 - Epoch: [35][220/352]	Time  0.167 ( 0.167)	Data  0.003 ( 0.003)	Loss 7.6327e-02 (8.5936e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:36 - Epoch: [0][ 40/352]	Time  0.177 ( 0.176)	Data  0.002 ( 0.008)	Loss 2.6928e-01 (4.1736e-01)	Acc@1  91.41 ( 87.25)	Acc@5  99.22 ( 98.99)
07-Mar-22 03:21:37 - Epoch: [35][230/352]	Time  0.170 ( 0.167)	Data  0.003 ( 0.003)	Loss 1.0461e-01 (8.5435e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:38 - Epoch: [0][ 50/352]	Time  0.173 ( 0.175)	Data  0.002 ( 0.007)	Loss 3.1847e-01 (3.8158e-01)	Acc@1  90.62 ( 88.34)	Acc@5 100.00 ( 99.17)
07-Mar-22 03:21:39 - Epoch: [35][240/352]	Time  0.141 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.8007e-02 (8.5747e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:39 - Epoch: [0][ 60/352]	Time  0.168 ( 0.175)	Data  0.002 ( 0.006)	Loss 1.1585e-01 (3.5301e-01)	Acc@1  96.88 ( 89.09)	Acc@5 100.00 ( 99.27)
07-Mar-22 03:21:40 - Epoch: [35][250/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4579e-01 (8.5902e-02)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:21:41 - Epoch: [0][ 70/352]	Time  0.176 ( 0.174)	Data  0.002 ( 0.005)	Loss 2.5276e-01 (3.3755e-01)	Acc@1  91.41 ( 89.50)	Acc@5 100.00 ( 99.34)
07-Mar-22 03:21:42 - Epoch: [35][260/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.0226e-02 (8.6113e-02)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:43 - Epoch: [0][ 80/352]	Time  0.169 ( 0.174)	Data  0.002 ( 0.005)	Loss 3.6136e-01 (3.1919e-01)	Acc@1  87.50 ( 90.06)	Acc@5  99.22 ( 99.40)
07-Mar-22 03:21:44 - Epoch: [35][270/352]	Time  0.164 ( 0.167)	Data  0.003 ( 0.003)	Loss 1.1196e-01 (8.6225e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:44 - Epoch: [0][ 90/352]	Time  0.167 ( 0.173)	Data  0.003 ( 0.005)	Loss 1.5927e-01 (3.0851e-01)	Acc@1  92.19 ( 90.26)	Acc@5 100.00 ( 99.45)
07-Mar-22 03:21:45 - Epoch: [35][280/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.7983e-02 (8.6480e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:46 - Epoch: [0][100/352]	Time  0.168 ( 0.173)	Data  0.002 ( 0.004)	Loss 2.6143e-01 (2.9965e-01)	Acc@1  88.28 ( 90.48)	Acc@5  99.22 ( 99.48)
07-Mar-22 03:21:47 - Epoch: [35][290/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.6686e-01 (8.6905e-02)	Acc@1  94.53 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:21:48 - Epoch: [0][110/352]	Time  0.171 ( 0.172)	Data  0.002 ( 0.004)	Loss 1.9955e-01 (2.9083e-01)	Acc@1  92.19 ( 90.70)	Acc@5  99.22 ( 99.51)
07-Mar-22 03:21:49 - Epoch: [35][300/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.4120e-02 (8.6294e-02)	Acc@1  99.22 ( 97.05)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:21:49 - Epoch: [0][120/352]	Time  0.161 ( 0.172)	Data  0.002 ( 0.004)	Loss 1.4692e-01 (2.8648e-01)	Acc@1  96.88 ( 90.86)	Acc@5 100.00 ( 99.54)
07-Mar-22 03:21:50 - Epoch: [35][310/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.1895e-02 (8.6850e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:21:51 - Epoch: [0][130/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.004)	Loss 9.7715e-02 (2.7967e-01)	Acc@1  96.88 ( 91.07)	Acc@5 100.00 ( 99.55)
07-Mar-22 03:21:52 - Epoch: [35][320/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.5536e-02 (8.7086e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:53 - Epoch: [0][140/352]	Time  0.165 ( 0.171)	Data  0.002 ( 0.004)	Loss 2.2596e-01 (2.7384e-01)	Acc@1  93.75 ( 91.23)	Acc@5  99.22 ( 99.57)
07-Mar-22 03:21:54 - Epoch: [35][330/352]	Time  0.166 ( 0.167)	Data  0.003 ( 0.003)	Loss 8.6303e-02 (8.7134e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:55 - Epoch: [0][150/352]	Time  0.173 ( 0.171)	Data  0.002 ( 0.004)	Loss 2.6965e-01 (2.6844e-01)	Acc@1  92.19 ( 91.42)	Acc@5  99.22 ( 99.58)
07-Mar-22 03:21:55 - Epoch: [35][340/352]	Time  0.163 ( 0.167)	Data  0.003 ( 0.003)	Loss 6.5441e-02 (8.6854e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:56 - Epoch: [0][160/352]	Time  0.172 ( 0.171)	Data  0.002 ( 0.004)	Loss 1.6332e-01 (2.6414e-01)	Acc@1  94.53 ( 91.52)	Acc@5 100.00 ( 99.58)
07-Mar-22 03:21:57 - Epoch: [35][350/352]	Time  0.166 ( 0.166)	Data  0.003 ( 0.003)	Loss 6.2604e-02 (8.6267e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:21:57 - Test: [ 0/20]	Time  0.354 ( 0.354)	Loss 3.4025e-01 (3.4025e-01)	Acc@1  89.45 ( 89.45)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:21:58 - Epoch: [0][170/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.004)	Loss 2.0848e-01 (2.5960e-01)	Acc@1  92.19 ( 91.67)	Acc@5  99.22 ( 99.59)
07-Mar-22 03:21:59 - Test: [10/20]	Time  0.098 ( 0.127)	Loss 3.5743e-01 (3.8275e-01)	Acc@1  88.28 ( 88.67)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:21:59 -  * Acc@1 89.240 Acc@5 99.480
07-Mar-22 03:21:59 - Best acc at epoch 35: 89.5199966430664
07-Mar-22 03:22:00 - Epoch: [0][180/352]	Time  0.186 ( 0.171)	Data  0.002 ( 0.004)	Loss 2.3381e-01 (2.5679e-01)	Acc@1  92.19 ( 91.73)	Acc@5 100.00 ( 99.61)
07-Mar-22 03:22:00 - Epoch: [36][  0/352]	Time  0.379 ( 0.379)	Data  0.231 ( 0.231)	Loss 1.2160e-01 (1.2160e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:22:01 - Epoch: [0][190/352]	Time  0.147 ( 0.170)	Data  0.001 ( 0.003)	Loss 1.8007e-01 (2.5289e-01)	Acc@1  93.75 ( 91.83)	Acc@5 100.00 ( 99.63)
07-Mar-22 03:22:02 - Epoch: [36][ 10/352]	Time  0.211 ( 0.187)	Data  0.003 ( 0.023)	Loss 6.2801e-02 (8.8727e-02)	Acc@1  96.09 ( 96.38)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:22:03 - Epoch: [0][200/352]	Time  0.145 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.5368e-01 (2.5070e-01)	Acc@1  94.53 ( 91.89)	Acc@5 100.00 ( 99.64)
07-Mar-22 03:22:03 - Epoch: [36][ 20/352]	Time  0.199 ( 0.185)	Data  0.002 ( 0.013)	Loss 6.1855e-02 (8.8285e-02)	Acc@1  97.66 ( 96.69)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:22:04 - Epoch: [0][210/352]	Time  0.144 ( 0.168)	Data  0.001 ( 0.003)	Loss 4.0806e-01 (2.4978e-01)	Acc@1  87.50 ( 91.91)	Acc@5  97.66 ( 99.64)
07-Mar-22 03:22:05 - Epoch: [36][ 30/352]	Time  0.161 ( 0.185)	Data  0.002 ( 0.010)	Loss 2.9797e-02 (8.0213e-02)	Acc@1 100.00 ( 97.20)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:22:06 - Epoch: [0][220/352]	Time  0.144 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.9546e-01 (2.4638e-01)	Acc@1  93.75 ( 92.03)	Acc@5 100.00 ( 99.64)
07-Mar-22 03:22:07 - Epoch: [0][230/352]	Time  0.144 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.6820e-01 (2.4396e-01)	Acc@1  95.31 ( 92.14)	Acc@5 100.00 ( 99.65)
07-Mar-22 03:22:07 - Epoch: [36][ 40/352]	Time  0.194 ( 0.187)	Data  0.002 ( 0.008)	Loss 1.0735e-01 (8.3955e-02)	Acc@1  95.31 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:22:09 - Epoch: [0][240/352]	Time  0.143 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.2338e-01 (2.4205e-01)	Acc@1  96.09 ( 92.18)	Acc@5 100.00 ( 99.66)
07-Mar-22 03:22:09 - Epoch: [36][ 50/352]	Time  0.196 ( 0.188)	Data  0.002 ( 0.007)	Loss 1.1594e-01 (8.3760e-02)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:22:10 - Epoch: [0][250/352]	Time  0.132 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.4641e-01 (2.4015e-01)	Acc@1  90.62 ( 92.21)	Acc@5 100.00 ( 99.67)
07-Mar-22 03:22:11 - Epoch: [36][ 60/352]	Time  0.161 ( 0.186)	Data  0.002 ( 0.006)	Loss 7.1367e-02 (8.3528e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:11 - Epoch: [0][260/352]	Time  0.143 ( 0.164)	Data  0.002 ( 0.003)	Loss 2.2444e-01 (2.3894e-01)	Acc@1  92.19 ( 92.26)	Acc@5  99.22 ( 99.67)
07-Mar-22 03:22:13 - Epoch: [36][ 70/352]	Time  0.160 ( 0.183)	Data  0.003 ( 0.006)	Loss 1.0720e-01 (8.3527e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:13 - Epoch: [0][270/352]	Time  0.172 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.5048e-01 (2.3710e-01)	Acc@1  94.53 ( 92.31)	Acc@5 100.00 ( 99.68)
07-Mar-22 03:22:14 - Epoch: [36][ 80/352]	Time  0.163 ( 0.181)	Data  0.002 ( 0.005)	Loss 6.2441e-02 (8.3434e-02)	Acc@1  95.31 ( 97.25)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:15 - Epoch: [0][280/352]	Time  0.167 ( 0.164)	Data  0.003 ( 0.003)	Loss 2.1966e-01 (2.3528e-01)	Acc@1  92.97 ( 92.37)	Acc@5 100.00 ( 99.68)
07-Mar-22 03:22:16 - Epoch: [36][ 90/352]	Time  0.206 ( 0.179)	Data  0.002 ( 0.005)	Loss 1.4486e-01 (8.4845e-02)	Acc@1  95.31 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:17 - Epoch: [0][290/352]	Time  0.205 ( 0.164)	Data  0.002 ( 0.003)	Loss 3.1141e-01 (2.3363e-01)	Acc@1  89.06 ( 92.42)	Acc@5 100.00 ( 99.69)
07-Mar-22 03:22:18 - Epoch: [36][100/352]	Time  0.156 ( 0.179)	Data  0.002 ( 0.005)	Loss 8.0080e-02 (8.3390e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:18 - Epoch: [0][300/352]	Time  0.176 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.5040e-01 (2.3274e-01)	Acc@1  94.53 ( 92.45)	Acc@5 100.00 ( 99.69)
07-Mar-22 03:22:19 - Epoch: [36][110/352]	Time  0.186 ( 0.178)	Data  0.003 ( 0.005)	Loss 1.1477e-01 (8.3220e-02)	Acc@1  95.31 ( 97.19)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:20 - Epoch: [0][310/352]	Time  0.170 ( 0.165)	Data  0.002 ( 0.003)	Loss 9.6549e-02 (2.3092e-01)	Acc@1  96.88 ( 92.50)	Acc@5 100.00 ( 99.70)
07-Mar-22 03:22:21 - Epoch: [36][120/352]	Time  0.169 ( 0.177)	Data  0.003 ( 0.004)	Loss 1.2314e-01 (8.4121e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:22 - Epoch: [0][320/352]	Time  0.176 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.6075e-01 (2.2875e-01)	Acc@1  92.19 ( 92.56)	Acc@5 100.00 ( 99.71)
07-Mar-22 03:22:23 - Epoch: [36][130/352]	Time  0.170 ( 0.177)	Data  0.003 ( 0.004)	Loss 9.2407e-02 (8.4381e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:24 - Epoch: [0][330/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.9825e-01 (2.2692e-01)	Acc@1  94.53 ( 92.62)	Acc@5 100.00 ( 99.71)
07-Mar-22 03:22:24 - Epoch: [36][140/352]	Time  0.168 ( 0.176)	Data  0.003 ( 0.004)	Loss 6.4934e-02 (8.4802e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:25 - Epoch: [0][340/352]	Time  0.171 ( 0.166)	Data  0.002 ( 0.003)	Loss 2.0376e-01 (2.2563e-01)	Acc@1  95.31 ( 92.67)	Acc@5 100.00 ( 99.72)
07-Mar-22 03:22:26 - Epoch: [36][150/352]	Time  0.165 ( 0.176)	Data  0.002 ( 0.004)	Loss 1.0729e-01 (8.5807e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:27 - Epoch: [0][350/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.7533e-01 (2.2470e-01)	Acc@1  92.19 ( 92.69)	Acc@5 100.00 ( 99.72)
07-Mar-22 03:22:28 - Test: [ 0/20]	Time  0.366 ( 0.366)	Loss 1.5371e-01 (1.5371e-01)	Acc@1  93.36 ( 93.36)	Acc@5 100.00 (100.00)
07-Mar-22 03:22:28 - Epoch: [36][160/352]	Time  0.165 ( 0.175)	Data  0.002 ( 0.004)	Loss 1.4355e-01 (8.6182e-02)	Acc@1  94.53 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:29 - Test: [10/20]	Time  0.100 ( 0.129)	Loss 1.4100e-01 (1.5592e-01)	Acc@1  96.09 ( 94.82)	Acc@5  99.61 ( 99.93)
07-Mar-22 03:22:29 - Epoch: [36][170/352]	Time  0.193 ( 0.175)	Data  0.003 ( 0.004)	Loss 4.1586e-02 (8.6140e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:30 -  * Acc@1 94.660 Acc@5 99.860
07-Mar-22 03:22:30 - Best acc at epoch 0: 94.65999603271484
07-Mar-22 03:22:30 - Epoch: [1][  0/352]	Time  0.404 ( 0.404)	Data  0.243 ( 0.243)	Loss 2.2285e-01 (2.2285e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:22:31 - Epoch: [36][180/352]	Time  0.167 ( 0.175)	Data  0.003 ( 0.004)	Loss 6.2951e-02 (8.6272e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:32 - Epoch: [1][ 10/352]	Time  0.149 ( 0.181)	Data  0.003 ( 0.024)	Loss 2.6381e-01 (1.8299e-01)	Acc@1  92.19 ( 94.32)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:22:33 - Epoch: [36][190/352]	Time  0.179 ( 0.175)	Data  0.003 ( 0.004)	Loss 9.5544e-02 (8.5920e-02)	Acc@1  95.31 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:34 - Epoch: [1][ 20/352]	Time  0.186 ( 0.178)	Data  0.002 ( 0.014)	Loss 1.7855e-01 (1.8031e-01)	Acc@1  92.19 ( 94.27)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:22:35 - Epoch: [36][200/352]	Time  0.179 ( 0.175)	Data  0.002 ( 0.004)	Loss 7.8049e-02 (8.5702e-02)	Acc@1  99.22 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:35 - Epoch: [1][ 30/352]	Time  0.169 ( 0.177)	Data  0.002 ( 0.010)	Loss 1.7231e-01 (1.7601e-01)	Acc@1  92.97 ( 94.30)	Acc@5 100.00 ( 99.90)
07-Mar-22 03:22:37 - Epoch: [36][210/352]	Time  0.180 ( 0.175)	Data  0.002 ( 0.004)	Loss 9.4896e-02 (8.6345e-02)	Acc@1  96.09 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:38 - Epoch: [36][220/352]	Time  0.175 ( 0.175)	Data  0.002 ( 0.004)	Loss 4.2318e-02 (8.5677e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:40 - Epoch: [36][230/352]	Time  0.183 ( 0.175)	Data  0.003 ( 0.003)	Loss 7.9404e-02 (8.5578e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:42 - Epoch: [36][240/352]	Time  0.179 ( 0.175)	Data  0.002 ( 0.003)	Loss 1.1051e-01 (8.5556e-02)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:44 - Epoch: [36][250/352]	Time  0.179 ( 0.176)	Data  0.002 ( 0.003)	Loss 8.4313e-02 (8.5632e-02)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:45 - Epoch: [36][260/352]	Time  0.179 ( 0.176)	Data  0.002 ( 0.003)	Loss 8.4396e-02 (8.6225e-02)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:47 - Epoch: [36][270/352]	Time  0.183 ( 0.176)	Data  0.002 ( 0.003)	Loss 9.6998e-02 (8.6289e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:49 - Epoch: [36][280/352]	Time  0.178 ( 0.176)	Data  0.002 ( 0.003)	Loss 7.5048e-02 (8.6459e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:51 - Epoch: [36][290/352]	Time  0.180 ( 0.176)	Data  0.002 ( 0.003)	Loss 1.1808e-01 (8.7013e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:53 - Epoch: [36][300/352]	Time  0.180 ( 0.176)	Data  0.002 ( 0.003)	Loss 7.3879e-02 (8.6496e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:54 - Epoch: [36][310/352]	Time  0.181 ( 0.176)	Data  0.003 ( 0.003)	Loss 6.1117e-02 (8.6512e-02)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:56 - Epoch: [36][320/352]	Time  0.182 ( 0.176)	Data  0.002 ( 0.003)	Loss 6.7103e-02 (8.6286e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:22:58 - Epoch: [36][330/352]	Time  0.181 ( 0.177)	Data  0.003 ( 0.003)	Loss 9.8411e-02 (8.6962e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:23:00 - Epoch: [36][340/352]	Time  0.185 ( 0.177)	Data  0.003 ( 0.003)	Loss 6.9071e-02 (8.6715e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:23:02 - Epoch: [36][350/352]	Time  0.177 ( 0.177)	Data  0.002 ( 0.003)	Loss 6.4285e-02 (8.6923e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:23:02 - Test: [ 0/20]	Time  0.370 ( 0.370)	Loss 2.7316e-01 (2.7316e-01)	Acc@1  91.02 ( 91.02)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:03 - Test: [10/20]	Time  0.114 ( 0.125)	Loss 3.9848e-01 (3.8237e-01)	Acc@1  88.67 ( 88.78)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:23:04 -  * Acc@1 89.000 Acc@5 99.480
07-Mar-22 03:23:04 - Best acc at epoch 36: 89.5199966430664
07-Mar-22 03:23:05 - Epoch: [37][  0/352]	Time  0.382 ( 0.382)	Data  0.236 ( 0.236)	Loss 6.1992e-02 (6.1992e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:06 - Epoch: [37][ 10/352]	Time  0.168 ( 0.186)	Data  0.002 ( 0.023)	Loss 6.2917e-02 (8.1323e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:08 - Epoch: [37][ 20/352]	Time  0.168 ( 0.179)	Data  0.002 ( 0.013)	Loss 7.5379e-02 (8.4868e-02)	Acc@1  96.88 ( 97.32)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:10 - Epoch: [37][ 30/352]	Time  0.152 ( 0.171)	Data  0.002 ( 0.010)	Loss 8.9997e-02 (8.2508e-02)	Acc@1  96.88 ( 97.33)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:11 - Epoch: [37][ 40/352]	Time  0.141 ( 0.165)	Data  0.002 ( 0.008)	Loss 1.2838e-01 (8.1014e-02)	Acc@1  95.31 ( 97.31)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:13 - Epoch: [37][ 50/352]	Time  0.161 ( 0.163)	Data  0.002 ( 0.007)	Loss 6.4005e-02 (8.3382e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:14 - Epoch: [37][ 60/352]	Time  0.155 ( 0.163)	Data  0.002 ( 0.006)	Loss 1.2064e-01 (8.5887e-02)	Acc@1  94.53 ( 97.16)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:16 - Epoch: [37][ 70/352]	Time  0.156 ( 0.163)	Data  0.002 ( 0.005)	Loss 5.6208e-02 (8.5531e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:17 - Epoch: [37][ 80/352]	Time  0.151 ( 0.162)	Data  0.002 ( 0.005)	Loss 6.0254e-02 (8.4912e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:19 - Epoch: [37][ 90/352]	Time  0.145 ( 0.161)	Data  0.002 ( 0.005)	Loss 8.9146e-02 (8.3772e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:20 - Epoch: [37][100/352]	Time  0.157 ( 0.160)	Data  0.002 ( 0.004)	Loss 8.3752e-02 (8.3330e-02)	Acc@1  98.44 ( 97.15)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:22 - Epoch: [37][110/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.004)	Loss 1.1210e-01 (8.5034e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:24 - Epoch: [37][120/352]	Time  0.145 ( 0.159)	Data  0.002 ( 0.004)	Loss 7.5611e-02 (8.3607e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:25 - Epoch: [37][130/352]	Time  0.155 ( 0.158)	Data  0.002 ( 0.004)	Loss 5.4296e-02 (8.2403e-02)	Acc@1  99.22 ( 97.14)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:27 - Epoch: [37][140/352]	Time  0.148 ( 0.158)	Data  0.002 ( 0.004)	Loss 1.1468e-01 (8.1797e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:28 - Epoch: [37][150/352]	Time  0.168 ( 0.159)	Data  0.002 ( 0.003)	Loss 6.6936e-02 (8.1892e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:30 - Epoch: [37][160/352]	Time  0.166 ( 0.159)	Data  0.002 ( 0.003)	Loss 3.6311e-02 (8.1254e-02)	Acc@1  99.22 ( 97.13)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:32 - Epoch: [37][170/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.003)	Loss 4.3179e-02 (8.0916e-02)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:33 - Epoch: [37][180/352]	Time  0.167 ( 0.160)	Data  0.002 ( 0.003)	Loss 3.3503e-02 (8.1639e-02)	Acc@1  99.22 ( 97.14)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:35 - Epoch: [37][190/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.003)	Loss 4.1958e-02 (8.1027e-02)	Acc@1  99.22 ( 97.16)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:37 - Epoch: [37][200/352]	Time  0.164 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.1520e-01 (8.0948e-02)	Acc@1  95.31 ( 97.15)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:38 - Epoch: [37][210/352]	Time  0.164 ( 0.161)	Data  0.002 ( 0.003)	Loss 9.1871e-02 (8.0434e-02)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:40 - Epoch: [37][220/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.0106e-02 (8.1520e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:42 - Epoch: [37][230/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.4097e-02 (8.0997e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:43 - Epoch: [37][240/352]	Time  0.155 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.0610e-02 (8.0847e-02)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 (100.00)
07-Mar-22 03:23:45 - Epoch: [37][250/352]	Time  0.161 ( 0.162)	Data  0.002 ( 0.003)	Loss 3.1346e-02 (8.1231e-02)	Acc@1  98.44 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:23:47 - Epoch: [37][260/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.2054e-01 (8.1069e-02)	Acc@1  95.31 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:23:48 - Epoch: [37][270/352]	Time  0.164 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.8125e-02 (8.0966e-02)	Acc@1  96.09 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:23:50 - Epoch: [37][280/352]	Time  0.176 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.3997e-02 (8.1066e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:23:52 - Epoch: [37][290/352]	Time  0.182 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.2377e-01 (8.2245e-02)	Acc@1  95.31 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:23:54 - Epoch: [37][300/352]	Time  0.175 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.1888e-02 (8.2547e-02)	Acc@1  95.31 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:23:55 - Epoch: [37][310/352]	Time  0.180 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.0880e-01 (8.3159e-02)	Acc@1  95.31 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:23:57 - Epoch: [37][320/352]	Time  0.177 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.0374e-02 (8.4043e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:23:59 - Epoch: [37][330/352]	Time  0.177 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.3115e-02 (8.4510e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:01 - Epoch: [37][340/352]	Time  0.181 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.5263e-02 (8.4681e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:02 - Epoch: [37][350/352]	Time  0.178 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.4152e-01 (8.4458e-02)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:03 - Test: [ 0/20]	Time  0.347 ( 0.347)	Loss 3.4657e-01 (3.4657e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:24:04 - Test: [10/20]	Time  0.099 ( 0.123)	Loss 3.7031e-01 (3.8668e-01)	Acc@1  89.06 ( 89.42)	Acc@5  99.22 ( 99.54)
07-Mar-22 03:24:05 -  * Acc@1 89.500 Acc@5 99.500
07-Mar-22 03:24:05 - Best acc at epoch 37: 89.5199966430664
07-Mar-22 03:24:06 - Epoch: [38][  0/352]	Time  0.422 ( 0.422)	Data  0.240 ( 0.240)	Loss 7.6522e-02 (7.6522e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:24:07 - Epoch: [38][ 10/352]	Time  0.183 ( 0.194)	Data  0.002 ( 0.024)	Loss 4.3210e-02 (8.9129e-02)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 (100.00)
07-Mar-22 03:24:09 - Epoch: [38][ 20/352]	Time  0.182 ( 0.187)	Data  0.002 ( 0.014)	Loss 1.4175e-01 (9.5971e-02)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 (100.00)
07-Mar-22 03:24:11 - Epoch: [38][ 30/352]	Time  0.178 ( 0.184)	Data  0.002 ( 0.010)	Loss 9.7198e-02 (9.7418e-02)	Acc@1  96.09 ( 96.47)	Acc@5 100.00 (100.00)
07-Mar-22 03:24:13 - Epoch: [38][ 40/352]	Time  0.177 ( 0.183)	Data  0.002 ( 0.008)	Loss 8.1386e-02 (9.3662e-02)	Acc@1  96.09 ( 96.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:24:14 - Epoch: [38][ 50/352]	Time  0.177 ( 0.182)	Data  0.002 ( 0.007)	Loss 1.0326e-01 (9.1419e-02)	Acc@1  93.75 ( 96.54)	Acc@5 100.00 (100.00)
07-Mar-22 03:24:16 - Epoch: [38][ 60/352]	Time  0.176 ( 0.181)	Data  0.002 ( 0.006)	Loss 8.6043e-02 (9.1382e-02)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 (100.00)
07-Mar-22 03:24:18 - Epoch: [38][ 70/352]	Time  0.180 ( 0.181)	Data  0.002 ( 0.006)	Loss 7.5114e-02 (9.0552e-02)	Acc@1  97.66 ( 96.61)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:20 - Epoch: [38][ 80/352]	Time  0.178 ( 0.180)	Data  0.002 ( 0.005)	Loss 1.2167e-01 (9.3109e-02)	Acc@1  94.53 ( 96.54)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:22 - Epoch: [38][ 90/352]	Time  0.170 ( 0.180)	Data  0.002 ( 0.005)	Loss 1.1453e-01 (9.0318e-02)	Acc@1  95.31 ( 96.67)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:23 - Epoch: [38][100/352]	Time  0.170 ( 0.178)	Data  0.002 ( 0.004)	Loss 3.3764e-02 (8.8706e-02)	Acc@1  98.44 ( 96.74)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:25 - Epoch: [38][110/352]	Time  0.178 ( 0.177)	Data  0.002 ( 0.004)	Loss 6.4251e-02 (8.6301e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:27 - Epoch: [38][120/352]	Time  0.169 ( 0.177)	Data  0.002 ( 0.004)	Loss 9.8275e-02 (8.5808e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:28 - Epoch: [38][130/352]	Time  0.166 ( 0.176)	Data  0.002 ( 0.004)	Loss 8.0270e-02 (8.6945e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:30 - Epoch: [38][140/352]	Time  0.176 ( 0.175)	Data  0.002 ( 0.004)	Loss 9.9608e-02 (8.7961e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:32 - Epoch: [38][150/352]	Time  0.156 ( 0.175)	Data  0.002 ( 0.004)	Loss 5.2598e-02 (8.7114e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:33 - Epoch: [38][160/352]	Time  0.160 ( 0.174)	Data  0.002 ( 0.004)	Loss 8.6295e-02 (8.7048e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:24:35 - Epoch: [38][170/352]	Time  0.180 ( 0.174)	Data  0.002 ( 0.003)	Loss 6.4965e-02 (8.7164e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 (100.00)
07-Mar-22 03:24:37 - Epoch: [38][180/352]	Time  0.177 ( 0.174)	Data  0.002 ( 0.003)	Loss 6.4142e-02 (8.7396e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:24:38 - Epoch: [38][190/352]	Time  0.177 ( 0.174)	Data  0.002 ( 0.003)	Loss 8.4480e-02 (8.7351e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:40 - Epoch: [38][200/352]	Time  0.184 ( 0.175)	Data  0.002 ( 0.003)	Loss 7.4200e-02 (8.7423e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:42 - Epoch: [38][210/352]	Time  0.182 ( 0.175)	Data  0.002 ( 0.003)	Loss 5.2457e-02 (8.7065e-02)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:44 - Epoch: [38][220/352]	Time  0.181 ( 0.175)	Data  0.002 ( 0.003)	Loss 1.0119e-01 (8.6346e-02)	Acc@1  94.53 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:46 - Epoch: [38][230/352]	Time  0.176 ( 0.175)	Data  0.002 ( 0.003)	Loss 4.6309e-02 (8.5861e-02)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:47 - Epoch: [38][240/352]	Time  0.175 ( 0.175)	Data  0.002 ( 0.003)	Loss 9.3269e-02 (8.6606e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:49 - Epoch: [38][250/352]	Time  0.179 ( 0.175)	Data  0.002 ( 0.003)	Loss 1.1795e-01 (8.6915e-02)	Acc@1  95.31 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:51 - Epoch: [38][260/352]	Time  0.178 ( 0.175)	Data  0.002 ( 0.003)	Loss 8.5723e-02 (8.7032e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:53 - Epoch: [38][270/352]	Time  0.182 ( 0.175)	Data  0.002 ( 0.003)	Loss 9.6208e-02 (8.6851e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:54 - Epoch: [38][280/352]	Time  0.178 ( 0.176)	Data  0.002 ( 0.003)	Loss 6.7145e-02 (8.6748e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:56 - Epoch: [38][290/352]	Time  0.179 ( 0.176)	Data  0.002 ( 0.003)	Loss 1.2390e-01 (8.6856e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:24:58 - Epoch: [38][300/352]	Time  0.180 ( 0.176)	Data  0.002 ( 0.003)	Loss 1.4297e-01 (8.7161e-02)	Acc@1  95.31 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:25:00 - Epoch: [38][310/352]	Time  0.180 ( 0.176)	Data  0.002 ( 0.003)	Loss 5.7314e-02 (8.6985e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:25:02 - Epoch: [38][320/352]	Time  0.175 ( 0.176)	Data  0.002 ( 0.003)	Loss 5.4383e-02 (8.6553e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:25:03 - Epoch: [38][330/352]	Time  0.178 ( 0.176)	Data  0.002 ( 0.003)	Loss 9.5000e-02 (8.6807e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:25:05 - Epoch: [38][340/352]	Time  0.175 ( 0.176)	Data  0.002 ( 0.003)	Loss 1.4829e-01 (8.7124e-02)	Acc@1  92.97 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:25:07 - Epoch: [38][350/352]	Time  0.176 ( 0.176)	Data  0.001 ( 0.003)	Loss 5.9105e-02 (8.7009e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:25:08 - Test: [ 0/20]	Time  0.411 ( 0.411)	Loss 3.4758e-01 (3.4758e-01)	Acc@1  89.45 ( 89.45)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:25:09 - Test: [10/20]	Time  0.098 ( 0.127)	Loss 4.1071e-01 (3.7412e-01)	Acc@1  88.28 ( 89.31)	Acc@5  99.61 ( 99.68)
07-Mar-22 03:25:10 -  * Acc@1 89.640 Acc@5 99.540
07-Mar-22 03:25:10 - Best acc at epoch 38: 89.63999938964844
07-Mar-22 03:25:10 - Epoch: [39][  0/352]	Time  0.429 ( 0.429)	Data  0.264 ( 0.264)	Loss 1.5486e-01 (1.5486e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:25:12 - Epoch: [39][ 10/352]	Time  0.173 ( 0.195)	Data  0.002 ( 0.025)	Loss 5.8395e-02 (9.2108e-02)	Acc@1  97.66 ( 96.66)	Acc@5 100.00 (100.00)
07-Mar-22 03:25:14 - Epoch: [39][ 20/352]	Time  0.177 ( 0.186)	Data  0.002 ( 0.014)	Loss 1.0279e-01 (8.5254e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:25:15 - Epoch: [39][ 30/352]	Time  0.179 ( 0.183)	Data  0.002 ( 0.010)	Loss 6.7512e-02 (8.6879e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 (100.00)
07-Mar-22 03:25:17 - Epoch: [39][ 40/352]	Time  0.178 ( 0.182)	Data  0.002 ( 0.008)	Loss 8.3643e-02 (8.8407e-02)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 (100.00)
07-Mar-22 03:25:19 - Epoch: [39][ 50/352]	Time  0.175 ( 0.181)	Data  0.002 ( 0.007)	Loss 8.6102e-02 (8.7738e-02)	Acc@1  96.09 ( 96.78)	Acc@5 100.00 (100.00)
07-Mar-22 03:25:21 - Epoch: [39][ 60/352]	Time  0.183 ( 0.181)	Data  0.003 ( 0.006)	Loss 1.6142e-01 (8.6914e-02)	Acc@1  93.75 ( 96.79)	Acc@5 100.00 (100.00)
07-Mar-22 03:25:23 - Epoch: [39][ 70/352]	Time  0.177 ( 0.181)	Data  0.002 ( 0.006)	Loss 1.1073e-01 (8.6279e-02)	Acc@1  95.31 ( 96.83)	Acc@5 100.00 (100.00)
07-Mar-22 03:25:24 - Epoch: [39][ 80/352]	Time  0.180 ( 0.180)	Data  0.002 ( 0.005)	Loss 4.9373e-02 (8.6566e-02)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:25:26 - Epoch: [39][ 90/352]	Time  0.183 ( 0.180)	Data  0.002 ( 0.005)	Loss 9.5171e-02 (8.6761e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:25:28 - Epoch: [39][100/352]	Time  0.177 ( 0.180)	Data  0.002 ( 0.005)	Loss 3.7167e-02 (8.6446e-02)	Acc@1 100.00 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:25:30 - Epoch: [39][110/352]	Time  0.176 ( 0.180)	Data  0.002 ( 0.004)	Loss 1.1302e-01 (8.5570e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:25:32 - Epoch: [39][120/352]	Time  0.180 ( 0.180)	Data  0.002 ( 0.004)	Loss 7.2393e-02 (8.5271e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:25:33 - Epoch: [39][130/352]	Time  0.181 ( 0.179)	Data  0.002 ( 0.004)	Loss 1.0289e-01 (8.6691e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:25:35 - Epoch: [39][140/352]	Time  0.176 ( 0.179)	Data  0.002 ( 0.004)	Loss 5.5759e-02 (8.7111e-02)	Acc@1  99.22 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:25:37 - Epoch: [39][150/352]	Time  0.175 ( 0.179)	Data  0.002 ( 0.004)	Loss 8.9430e-02 (8.6826e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:25:39 - Epoch: [39][160/352]	Time  0.153 ( 0.179)	Data  0.001 ( 0.004)	Loss 8.6112e-02 (8.6502e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:25:40 - Epoch: [39][170/352]	Time  0.177 ( 0.178)	Data  0.002 ( 0.004)	Loss 1.2326e-01 (8.6440e-02)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:25:42 - Epoch: [39][180/352]	Time  0.153 ( 0.177)	Data  0.002 ( 0.003)	Loss 4.4400e-02 (8.6163e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:25:44 - Epoch: [39][190/352]	Time  0.178 ( 0.177)	Data  0.001 ( 0.003)	Loss 7.5638e-02 (8.6837e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:25:45 - Epoch: [39][200/352]	Time  0.177 ( 0.177)	Data  0.002 ( 0.003)	Loss 4.8623e-02 (8.6190e-02)	Acc@1  99.22 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:25:47 - Epoch: [39][210/352]	Time  0.180 ( 0.177)	Data  0.002 ( 0.003)	Loss 1.2151e-01 (8.6790e-02)	Acc@1  95.31 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:25:49 - Epoch: [39][220/352]	Time  0.180 ( 0.178)	Data  0.002 ( 0.003)	Loss 4.5643e-02 (8.6651e-02)	Acc@1 100.00 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:25:51 - Epoch: [39][230/352]	Time  0.176 ( 0.178)	Data  0.002 ( 0.003)	Loss 6.9117e-02 (8.6538e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:25:53 - Epoch: [39][240/352]	Time  0.181 ( 0.178)	Data  0.002 ( 0.003)	Loss 1.7063e-01 (8.6782e-02)	Acc@1  93.75 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:25:54 - Epoch: [39][250/352]	Time  0.175 ( 0.178)	Data  0.002 ( 0.003)	Loss 1.0695e-01 (8.6665e-02)	Acc@1  97.66 ( 96.92)	Acc@5  99.22 ( 99.98)
07-Mar-22 03:25:56 - Epoch: [39][260/352]	Time  0.176 ( 0.178)	Data  0.002 ( 0.003)	Loss 9.2909e-02 (8.6534e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:25:58 - Epoch: [39][270/352]	Time  0.180 ( 0.178)	Data  0.002 ( 0.003)	Loss 1.2631e-01 (8.6495e-02)	Acc@1  95.31 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:26:00 - Epoch: [39][280/352]	Time  0.179 ( 0.178)	Data  0.002 ( 0.003)	Loss 7.3517e-02 (8.5896e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:26:02 - Epoch: [39][290/352]	Time  0.179 ( 0.178)	Data  0.002 ( 0.003)	Loss 8.9978e-02 (8.6047e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:26:03 - Epoch: [39][300/352]	Time  0.183 ( 0.178)	Data  0.002 ( 0.003)	Loss 1.3046e-01 (8.5850e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:26:05 - Epoch: [39][310/352]	Time  0.177 ( 0.178)	Data  0.001 ( 0.003)	Loss 8.2166e-02 (8.6342e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:26:07 - Epoch: [39][320/352]	Time  0.181 ( 0.178)	Data  0.002 ( 0.003)	Loss 1.0799e-01 (8.6860e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:09 - Epoch: [39][330/352]	Time  0.180 ( 0.178)	Data  0.002 ( 0.003)	Loss 5.5996e-02 (8.7043e-02)	Acc@1  99.22 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:26:10 - Epoch: [39][340/352]	Time  0.177 ( 0.178)	Data  0.002 ( 0.003)	Loss 9.5967e-02 (8.6651e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:26:12 - Epoch: [39][350/352]	Time  0.180 ( 0.178)	Data  0.002 ( 0.003)	Loss 1.3159e-01 (8.6511e-02)	Acc@1  95.31 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:26:13 - Test: [ 0/20]	Time  0.365 ( 0.365)	Loss 3.5337e-01 (3.5337e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:26:14 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 3.9209e-01 (3.8958e-01)	Acc@1  88.67 ( 89.17)	Acc@5  99.61 ( 99.57)
07-Mar-22 03:26:15 -  * Acc@1 89.560 Acc@5 99.520
07-Mar-22 03:26:15 - Best acc at epoch 39: 89.63999938964844
07-Mar-22 03:26:15 - Epoch: [40][  0/352]	Time  0.392 ( 0.392)	Data  0.239 ( 0.239)	Loss 1.0038e-01 (1.0038e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:26:17 - Epoch: [40][ 10/352]	Time  0.205 ( 0.189)	Data  0.002 ( 0.023)	Loss 5.1637e-02 (9.3263e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:26:19 - Epoch: [40][ 20/352]	Time  0.170 ( 0.185)	Data  0.002 ( 0.013)	Loss 8.3182e-02 (8.9057e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:26:21 - Epoch: [40][ 30/352]	Time  0.186 ( 0.185)	Data  0.002 ( 0.010)	Loss 1.4761e-01 (8.9643e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:26:22 - Epoch: [40][ 40/352]	Time  0.164 ( 0.181)	Data  0.002 ( 0.008)	Loss 6.7461e-02 (8.6777e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 03:26:24 - Epoch: [40][ 50/352]	Time  0.166 ( 0.179)	Data  0.002 ( 0.007)	Loss 7.1769e-02 (8.6135e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 (100.00)
07-Mar-22 03:26:26 - Epoch: [40][ 60/352]	Time  0.166 ( 0.177)	Data  0.002 ( 0.006)	Loss 7.0689e-02 (8.4994e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 (100.00)
07-Mar-22 03:26:27 - Epoch: [40][ 70/352]	Time  0.168 ( 0.175)	Data  0.002 ( 0.006)	Loss 7.1419e-02 (8.2160e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 (100.00)
07-Mar-22 03:26:29 - Epoch: [40][ 80/352]	Time  0.166 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.0428e-01 (8.2141e-02)	Acc@1  93.75 ( 97.24)	Acc@5 100.00 (100.00)
07-Mar-22 03:26:31 - Epoch: [40][ 90/352]	Time  0.164 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.0345e-01 (8.2111e-02)	Acc@1  95.31 ( 97.26)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:32 - Epoch: [40][100/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.0015e-01 (8.3131e-02)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:34 - Epoch: [40][110/352]	Time  0.165 ( 0.172)	Data  0.002 ( 0.004)	Loss 9.2006e-02 (8.3375e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:36 - Epoch: [40][120/352]	Time  0.167 ( 0.172)	Data  0.002 ( 0.004)	Loss 1.0292e-01 (8.2425e-02)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:37 - Epoch: [40][130/352]	Time  0.165 ( 0.172)	Data  0.002 ( 0.004)	Loss 1.8362e-01 (8.4411e-02)	Acc@1  92.19 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:39 - Epoch: [40][140/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.004)	Loss 6.5457e-02 (8.6232e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:41 - Epoch: [40][150/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.004)	Loss 1.1101e-01 (8.5506e-02)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:42 - Epoch: [40][160/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.004)	Loss 1.1413e-01 (8.5340e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:44 - Epoch: [40][170/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.004)	Loss 3.9579e-02 (8.5029e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:46 - Epoch: [40][180/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.003)	Loss 6.1452e-02 (8.5030e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:47 - Epoch: [40][190/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.003)	Loss 4.8131e-02 (8.4726e-02)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:49 - Epoch: [40][200/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.003)	Loss 5.4839e-02 (8.3793e-02)	Acc@1  98.44 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:51 - Epoch: [40][210/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.003)	Loss 9.7793e-02 (8.3878e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:52 - Epoch: [40][220/352]	Time  0.170 ( 0.170)	Data  0.002 ( 0.003)	Loss 5.8871e-02 (8.3860e-02)	Acc@1  98.44 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:26:54 - Epoch: [40][230/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.003)	Loss 6.0771e-02 (8.3492e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:26:56 - Epoch: [40][240/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.003)	Loss 8.1739e-02 (8.3159e-02)	Acc@1  95.31 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:26:57 - Epoch: [40][250/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 9.3297e-02 (8.3252e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:26:59 - Epoch: [40][260/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.003)	Loss 9.8995e-02 (8.3727e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:27:01 - Epoch: [40][270/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.7490e-01 (8.4256e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:27:02 - Epoch: [40][280/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.1301e-01 (8.4618e-02)	Acc@1  95.31 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:27:04 - Epoch: [40][290/352]	Time  0.173 ( 0.169)	Data  0.001 ( 0.003)	Loss 5.0040e-02 (8.5124e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:27:06 - Epoch: [40][300/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 6.9029e-02 (8.5208e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:27:07 - Epoch: [40][310/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.003)	Loss 7.2458e-02 (8.4788e-02)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:27:09 - Epoch: [40][320/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 5.9214e-02 (8.5136e-02)	Acc@1  99.22 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:27:11 - Epoch: [40][330/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.3535e-01 (8.5476e-02)	Acc@1  95.31 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:27:12 - Epoch: [40][340/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.1168e-01 (8.5285e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:27:14 - Epoch: [40][350/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.4319e-01 (8.5803e-02)	Acc@1  94.53 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:27:15 - Test: [ 0/20]	Time  0.403 ( 0.403)	Loss 3.2573e-01 (3.2573e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:27:16 - Test: [10/20]	Time  0.098 ( 0.127)	Loss 3.9336e-01 (3.7606e-01)	Acc@1  89.45 ( 89.24)	Acc@5  99.22 ( 99.36)
07-Mar-22 03:27:17 -  * Acc@1 89.500 Acc@5 99.420
07-Mar-22 03:27:17 - Best acc at epoch 40: 89.63999938964844
07-Mar-22 03:27:17 - Epoch: [41][  0/352]	Time  0.395 ( 0.395)	Data  0.250 ( 0.250)	Loss 1.1556e-01 (1.1556e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:27:19 - Epoch: [41][ 10/352]	Time  0.169 ( 0.189)	Data  0.003 ( 0.025)	Loss 5.5424e-02 (7.8621e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 (100.00)
07-Mar-22 03:27:20 - Epoch: [41][ 20/352]	Time  0.168 ( 0.179)	Data  0.002 ( 0.014)	Loss 5.0365e-02 (7.6324e-02)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 (100.00)
07-Mar-22 03:27:22 - Epoch: [41][ 30/352]	Time  0.168 ( 0.176)	Data  0.002 ( 0.010)	Loss 8.0228e-02 (7.6846e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 (100.00)
07-Mar-22 03:27:24 - Epoch: [41][ 40/352]	Time  0.167 ( 0.174)	Data  0.002 ( 0.008)	Loss 1.2291e-01 (8.3049e-02)	Acc@1  93.75 ( 97.22)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:27:26 - Epoch: [41][ 50/352]	Time  0.167 ( 0.173)	Data  0.002 ( 0.007)	Loss 3.5512e-02 (8.2090e-02)	Acc@1  99.22 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:27:27 - Epoch: [41][ 60/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.006)	Loss 8.1696e-02 (8.2837e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:27:29 - Epoch: [41][ 70/352]	Time  0.167 ( 0.172)	Data  0.002 ( 0.006)	Loss 7.7389e-02 (8.3565e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:27:31 - Epoch: [41][ 80/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.005)	Loss 8.4541e-02 (8.4365e-02)	Acc@1  95.31 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:27:32 - Epoch: [41][ 90/352]	Time  0.164 ( 0.171)	Data  0.002 ( 0.005)	Loss 7.2838e-02 (8.4115e-02)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:27:34 - Epoch: [41][100/352]	Time  0.170 ( 0.170)	Data  0.002 ( 0.005)	Loss 5.4511e-02 (8.4326e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:27:36 - Epoch: [41][110/352]	Time  0.170 ( 0.170)	Data  0.002 ( 0.005)	Loss 7.2390e-02 (8.5290e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:27:37 - Epoch: [41][120/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.5792e-02 (8.5530e-02)	Acc@1 100.00 ( 97.05)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:27:39 - Epoch: [41][130/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.7311e-01 (8.7378e-02)	Acc@1  95.31 ( 96.99)	Acc@5  99.22 ( 99.97)
07-Mar-22 03:27:41 - Epoch: [41][140/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.004)	Loss 8.7728e-02 (8.6104e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:27:42 - Epoch: [41][150/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.004)	Loss 5.0919e-02 (8.6441e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:27:44 - Epoch: [41][160/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.0344e-01 (8.6289e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:27:46 - Epoch: [41][170/352]	Time  0.169 ( 0.170)	Data  0.003 ( 0.004)	Loss 1.0899e-01 (8.6566e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:27:47 - Epoch: [41][180/352]	Time  0.162 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.4997e-02 (8.7024e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:27:49 - Epoch: [41][190/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 4.0471e-02 (8.6383e-02)	Acc@1 100.00 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:27:51 - Epoch: [41][200/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.4233e-02 (8.6464e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:27:52 - Epoch: [41][210/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.4113e-02 (8.5184e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:27:54 - Epoch: [41][220/352]	Time  0.175 ( 0.169)	Data  0.003 ( 0.004)	Loss 5.6755e-02 (8.5243e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:27:56 - Epoch: [41][230/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 9.7858e-02 (8.4951e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:27:58 - Epoch: [41][240/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 7.5229e-02 (8.5017e-02)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:27:59 - Epoch: [41][250/352]	Time  0.171 ( 0.170)	Data  0.002 ( 0.003)	Loss 6.1986e-02 (8.4444e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:28:01 - Epoch: [41][260/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 7.4675e-02 (8.4011e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:28:03 - Epoch: [41][270/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.5650e-01 (8.3952e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:28:04 - Epoch: [41][280/352]	Time  0.164 ( 0.169)	Data  0.002 ( 0.003)	Loss 8.2352e-02 (8.3376e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:28:06 - Epoch: [41][290/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0001e-01 (8.3498e-02)	Acc@1  95.31 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:28:08 - Epoch: [41][300/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.003)	Loss 7.4303e-02 (8.3131e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:28:09 - Epoch: [41][310/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 6.0269e-02 (8.2938e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:28:11 - Epoch: [41][320/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 7.7052e-02 (8.2645e-02)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:28:13 - Epoch: [41][330/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.5742e-01 (8.3155e-02)	Acc@1  95.31 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:28:14 - Epoch: [41][340/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0044e-01 (8.2940e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:28:16 - Epoch: [41][350/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 3.9994e-02 (8.2561e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:28:17 - Test: [ 0/20]	Time  0.401 ( 0.401)	Loss 3.1273e-01 (3.1273e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:28:18 - Test: [10/20]	Time  0.100 ( 0.126)	Loss 3.9713e-01 (3.8228e-01)	Acc@1  87.89 ( 88.81)	Acc@5  98.83 ( 99.47)
07-Mar-22 03:28:19 -  * Acc@1 89.120 Acc@5 99.420
07-Mar-22 03:28:19 - Best acc at epoch 41: 89.63999938964844
07-Mar-22 03:28:19 - Epoch: [42][  0/352]	Time  0.421 ( 0.421)	Data  0.258 ( 0.258)	Loss 7.3295e-02 (7.3295e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:21 - Epoch: [42][ 10/352]	Time  0.165 ( 0.190)	Data  0.002 ( 0.025)	Loss 1.1241e-01 (8.5434e-02)	Acc@1  96.88 ( 97.37)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:23 - Epoch: [42][ 20/352]	Time  0.166 ( 0.179)	Data  0.002 ( 0.014)	Loss 9.0093e-02 (8.7969e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:24 - Epoch: [42][ 30/352]	Time  0.171 ( 0.176)	Data  0.002 ( 0.010)	Loss 8.7790e-02 (8.9592e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:26 - Epoch: [42][ 40/352]	Time  0.169 ( 0.173)	Data  0.002 ( 0.008)	Loss 1.0701e-01 (9.1953e-02)	Acc@1  95.31 ( 96.76)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:28 - Epoch: [42][ 50/352]	Time  0.142 ( 0.171)	Data  0.002 ( 0.007)	Loss 5.1822e-02 (8.8607e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:29 - Epoch: [42][ 60/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.006)	Loss 1.1846e-01 (8.7827e-02)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:31 - Epoch: [42][ 70/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.006)	Loss 5.6973e-02 (8.8224e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:32 - Epoch: [42][ 80/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.005)	Loss 7.1690e-02 (8.7043e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:34 - Epoch: [42][ 90/352]	Time  0.167 ( 0.168)	Data  0.003 ( 0.005)	Loss 1.2076e-01 (8.7090e-02)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:36 - Epoch: [42][100/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.6616e-01 (9.0057e-02)	Acc@1  93.75 ( 96.92)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:37 - Epoch: [42][110/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.8204e-02 (9.0120e-02)	Acc@1  94.53 ( 96.90)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:39 - Epoch: [42][120/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.1319e-02 (8.8925e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:41 - Epoch: [42][130/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.004)	Loss 8.1090e-02 (8.9173e-02)	Acc@1  95.31 ( 96.83)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:42 - Epoch: [42][140/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 7.3366e-02 (8.8429e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:44 - Epoch: [42][150/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.0806e-01 (8.7966e-02)	Acc@1  95.31 ( 96.91)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:46 - Epoch: [42][160/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.004)	Loss 3.7562e-02 (8.8012e-02)	Acc@1  99.22 ( 96.92)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:47 - Epoch: [42][170/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 7.0608e-02 (8.8806e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:49 - Epoch: [42][180/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.6327e-02 (8.8599e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:51 - Epoch: [42][190/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.1705e-02 (8.8902e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:52 - Epoch: [42][200/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.1389e-02 (8.8956e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:54 - Epoch: [42][210/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.9154e-02 (8.9213e-02)	Acc@1  95.31 ( 96.85)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:56 - Epoch: [42][220/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.3343e-02 (8.8790e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:57 - Epoch: [42][230/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.8636e-02 (8.8324e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:28:59 - Epoch: [42][240/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 3.2059e-02 (8.8074e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:01 - Epoch: [42][250/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.7834e-02 (8.7655e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:02 - Epoch: [42][260/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.4790e-02 (8.7942e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:04 - Epoch: [42][270/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 3.1710e-02 (8.7712e-02)	Acc@1  99.22 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:06 - Epoch: [42][280/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.5313e-02 (8.7487e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:07 - Epoch: [42][290/352]	Time  0.153 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.4770e-02 (8.7654e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:09 - Epoch: [42][300/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.8343e-02 (8.7954e-02)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:11 - Epoch: [42][310/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.4311e-02 (8.8417e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:12 - Epoch: [42][320/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.3323e-02 (8.7898e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:14 - Epoch: [42][330/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1582e-01 (8.7822e-02)	Acc@1  96.09 ( 96.92)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:16 - Epoch: [42][340/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.0873e-02 (8.7535e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:17 - Epoch: [42][350/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0337e-01 (8.7613e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:18 - Test: [ 0/20]	Time  0.369 ( 0.369)	Loss 3.5830e-01 (3.5830e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:29:19 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 3.6028e-01 (3.9766e-01)	Acc@1  90.23 ( 89.13)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:29:20 -  * Acc@1 89.360 Acc@5 99.500
07-Mar-22 03:29:20 - Best acc at epoch 42: 89.63999938964844
07-Mar-22 03:29:21 - Epoch: [43][  0/352]	Time  0.416 ( 0.416)	Data  0.249 ( 0.249)	Loss 5.1836e-02 (5.1836e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:22 - Epoch: [43][ 10/352]	Time  0.162 ( 0.186)	Data  0.002 ( 0.024)	Loss 1.1164e-01 (9.1923e-02)	Acc@1  95.31 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:24 - Epoch: [43][ 20/352]	Time  0.168 ( 0.176)	Data  0.002 ( 0.014)	Loss 5.9847e-02 (9.2001e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:26 - Epoch: [43][ 30/352]	Time  0.167 ( 0.173)	Data  0.002 ( 0.010)	Loss 4.8695e-02 (9.1516e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:27 - Epoch: [43][ 40/352]	Time  0.169 ( 0.172)	Data  0.002 ( 0.008)	Loss 1.3396e-01 (8.9748e-02)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:29 - Epoch: [43][ 50/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.007)	Loss 1.0343e-01 (8.9744e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:31 - Epoch: [43][ 60/352]	Time  0.165 ( 0.170)	Data  0.002 ( 0.006)	Loss 7.0879e-02 (8.5284e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:32 - Epoch: [43][ 70/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.006)	Loss 1.4522e-01 (8.7165e-02)	Acc@1  94.53 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:34 - Epoch: [43][ 80/352]	Time  0.164 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.2157e-01 (8.7968e-02)	Acc@1  95.31 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:36 - Epoch: [43][ 90/352]	Time  0.164 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.4102e-01 (8.8186e-02)	Acc@1  93.75 ( 96.87)	Acc@5 100.00 (100.00)
07-Mar-22 03:29:37 - Epoch: [43][100/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.1824e-01 (8.7992e-02)	Acc@1  96.09 ( 96.87)	Acc@5  99.22 ( 99.99)
07-Mar-22 03:29:39 - Epoch: [43][110/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 8.7505e-02 (8.9221e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:29:41 - Epoch: [43][120/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.0087e-01 (8.9362e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:29:42 - Epoch: [43][130/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 8.4840e-02 (8.9155e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:29:44 - Epoch: [43][140/352]	Time  0.172 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.3292e-02 (8.7967e-02)	Acc@1  95.31 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:29:46 - Epoch: [43][150/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.6895e-02 (8.7173e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:29:47 - Epoch: [43][160/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.1745e-02 (8.6434e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:29:49 - Epoch: [43][170/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.9797e-02 (8.6217e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:29:51 - Epoch: [43][180/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.4936e-02 (8.5723e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:29:52 - Epoch: [43][190/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.2929e-02 (8.5630e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:29:54 - Epoch: [43][200/352]	Time  0.168 ( 0.168)	Data  0.003 ( 0.003)	Loss 1.2657e-01 (8.5315e-02)	Acc@1  94.53 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:29:56 - Epoch: [43][210/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.0626e-02 (8.5507e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:29:57 - Epoch: [43][220/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.1610e-02 (8.5890e-02)	Acc@1  99.22 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:29:59 - Epoch: [43][230/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.1206e-02 (8.6286e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:01 - Epoch: [43][240/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.7937e-02 (8.5988e-02)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:02 - Epoch: [43][250/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.4904e-02 (8.5837e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:04 - Epoch: [43][260/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.8828e-02 (8.6023e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:06 - Epoch: [43][270/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0868e-01 (8.6018e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:07 - Epoch: [43][280/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.1061e-02 (8.6302e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:09 - Epoch: [43][290/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.2608e-02 (8.5586e-02)	Acc@1 100.00 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:11 - Epoch: [43][300/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.8367e-02 (8.6010e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:12 - Epoch: [43][310/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.2615e-02 (8.5997e-02)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:14 - Epoch: [43][320/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0768e-01 (8.6395e-02)	Acc@1  95.31 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:16 - Epoch: [43][330/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.2497e-02 (8.6939e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:17 - Epoch: [43][340/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0284e-01 (8.6872e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:19 - Epoch: [43][350/352]	Time  0.172 ( 0.168)	Data  0.001 ( 0.003)	Loss 5.8398e-02 (8.6582e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:20 - Test: [ 0/20]	Time  0.405 ( 0.405)	Loss 3.2192e-01 (3.2192e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:30:21 - Test: [10/20]	Time  0.098 ( 0.126)	Loss 3.7294e-01 (3.7881e-01)	Acc@1  88.67 ( 88.96)	Acc@5  99.22 ( 99.43)
07-Mar-22 03:30:22 -  * Acc@1 89.080 Acc@5 99.440
07-Mar-22 03:30:22 - Best acc at epoch 43: 89.63999938964844
07-Mar-22 03:30:22 - Epoch: [44][  0/352]	Time  0.403 ( 0.403)	Data  0.259 ( 0.259)	Loss 5.6738e-02 (5.6738e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 03:30:24 - Epoch: [44][ 10/352]	Time  0.168 ( 0.190)	Data  0.002 ( 0.025)	Loss 1.7363e-01 (9.1731e-02)	Acc@1  95.31 ( 97.09)	Acc@5  99.22 ( 99.93)
07-Mar-22 03:30:25 - Epoch: [44][ 20/352]	Time  0.185 ( 0.180)	Data  0.003 ( 0.014)	Loss 5.3643e-02 (8.8288e-02)	Acc@1  99.22 ( 97.10)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:30:27 - Epoch: [44][ 30/352]	Time  0.149 ( 0.176)	Data  0.002 ( 0.010)	Loss 7.9819e-02 (8.6638e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:30:29 - Epoch: [44][ 40/352]	Time  0.140 ( 0.168)	Data  0.001 ( 0.008)	Loss 6.8419e-02 (8.1677e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:30:30 - Epoch: [44][ 50/352]	Time  0.140 ( 0.162)	Data  0.001 ( 0.007)	Loss 3.3955e-02 (8.2098e-02)	Acc@1  99.22 ( 97.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:30:31 - Epoch: [44][ 60/352]	Time  0.140 ( 0.160)	Data  0.001 ( 0.006)	Loss 5.2003e-02 (8.3196e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:33 - Epoch: [44][ 70/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.005)	Loss 3.5921e-02 (8.3434e-02)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:35 - Epoch: [44][ 80/352]	Time  0.169 ( 0.162)	Data  0.002 ( 0.005)	Loss 6.9969e-02 (8.3590e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:36 - Epoch: [44][ 90/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.005)	Loss 8.8781e-02 (8.3672e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:30:38 - Epoch: [44][100/352]	Time  0.168 ( 0.163)	Data  0.002 ( 0.004)	Loss 5.0275e-02 (8.4004e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:30:40 - Epoch: [44][110/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.004)	Loss 6.1669e-02 (8.2443e-02)	Acc@1  96.09 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:41 - Epoch: [44][120/352]	Time  0.164 ( 0.163)	Data  0.002 ( 0.004)	Loss 5.3873e-02 (8.2899e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:43 - Epoch: [44][130/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.004)	Loss 3.8073e-02 (8.2120e-02)	Acc@1  99.22 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:45 - Epoch: [44][140/352]	Time  0.169 ( 0.164)	Data  0.002 ( 0.004)	Loss 7.5077e-02 (8.2362e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:46 - Epoch: [44][150/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.004)	Loss 4.6953e-02 (8.2099e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:48 - Epoch: [44][160/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.004)	Loss 1.2457e-01 (8.2903e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:50 - Epoch: [44][170/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.004)	Loss 1.1771e-01 (8.3307e-02)	Acc@1  94.53 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:51 - Epoch: [44][180/352]	Time  0.149 ( 0.164)	Data  0.002 ( 0.003)	Loss 6.7153e-02 (8.3429e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:53 - Epoch: [44][190/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.0514e-01 (8.3421e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:55 - Epoch: [44][200/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.1218e-01 (8.2812e-02)	Acc@1  96.88 ( 97.00)	Acc@5  99.22 ( 99.99)
07-Mar-22 03:30:56 - Epoch: [44][210/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.0338e-01 (8.3177e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:30:58 - Epoch: [44][220/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.3573e-01 (8.3222e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:00 - Epoch: [44][230/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.2429e-02 (8.2388e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:01 - Epoch: [44][240/352]	Time  0.165 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.8773e-02 (8.2310e-02)	Acc@1  95.31 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:03 - Epoch: [44][250/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 4.7584e-02 (8.2150e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:05 - Epoch: [44][260/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 6.4561e-02 (8.1566e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:06 - Epoch: [44][270/352]	Time  0.170 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.7680e-01 (8.1879e-02)	Acc@1  92.97 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:08 - Epoch: [44][280/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 9.5686e-02 (8.1955e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:10 - Epoch: [44][290/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.1035e-02 (8.2028e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:11 - Epoch: [44][300/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.3815e-02 (8.2078e-02)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:13 - Epoch: [44][310/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 4.8366e-02 (8.1998e-02)	Acc@1  99.22 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:15 - Epoch: [44][320/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 3.0104e-02 (8.1694e-02)	Acc@1  99.22 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:16 - Epoch: [44][330/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.1457e-02 (8.1469e-02)	Acc@1  98.44 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:18 - Epoch: [44][340/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.1382e-01 (8.1948e-02)	Acc@1  96.09 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:20 - Epoch: [44][350/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.4392e-02 (8.2319e-02)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:20 - Test: [ 0/20]	Time  0.371 ( 0.371)	Loss 3.6200e-01 (3.6200e-01)	Acc@1  89.45 ( 89.45)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:31:21 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 3.8668e-01 (3.7889e-01)	Acc@1  89.84 ( 89.20)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:31:22 -  * Acc@1 89.180 Acc@5 99.500
07-Mar-22 03:31:22 - Best acc at epoch 44: 89.63999938964844
07-Mar-22 03:31:23 - Epoch: [45][  0/352]	Time  0.411 ( 0.411)	Data  0.264 ( 0.264)	Loss 4.3702e-02 (4.3702e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
07-Mar-22 03:31:24 - Epoch: [45][ 10/352]	Time  0.170 ( 0.194)	Data  0.002 ( 0.026)	Loss 4.1451e-02 (8.4551e-02)	Acc@1  98.44 ( 96.52)	Acc@5 100.00 (100.00)
07-Mar-22 03:31:26 - Epoch: [45][ 20/352]	Time  0.167 ( 0.182)	Data  0.002 ( 0.015)	Loss 7.7338e-02 (8.3374e-02)	Acc@1  96.09 ( 96.58)	Acc@5 100.00 (100.00)
07-Mar-22 03:31:28 - Epoch: [45][ 30/352]	Time  0.164 ( 0.177)	Data  0.002 ( 0.011)	Loss 3.7182e-02 (8.1840e-02)	Acc@1  99.22 ( 96.57)	Acc@5 100.00 (100.00)
07-Mar-22 03:31:29 - Epoch: [45][ 40/352]	Time  0.166 ( 0.174)	Data  0.002 ( 0.009)	Loss 9.8632e-02 (8.2537e-02)	Acc@1  97.66 ( 96.80)	Acc@5  99.22 ( 99.98)
07-Mar-22 03:31:31 - Epoch: [45][ 50/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.007)	Loss 4.7856e-02 (8.2103e-02)	Acc@1  99.22 ( 96.94)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:31:33 - Epoch: [45][ 60/352]	Time  0.145 ( 0.172)	Data  0.002 ( 0.006)	Loss 5.1774e-02 (8.0939e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:31:34 - Epoch: [45][ 70/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.006)	Loss 5.1455e-02 (8.0880e-02)	Acc@1  98.44 ( 97.08)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:31:36 - Epoch: [45][ 80/352]	Time  0.161 ( 0.170)	Data  0.002 ( 0.005)	Loss 6.6653e-02 (8.1234e-02)	Acc@1  99.22 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:31:38 - Epoch: [45][ 90/352]	Time  0.158 ( 0.169)	Data  0.002 ( 0.005)	Loss 5.7723e-02 (8.2078e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:31:39 - Epoch: [45][100/352]	Time  0.159 ( 0.168)	Data  0.002 ( 0.005)	Loss 7.6219e-02 (8.2411e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:31:41 - Epoch: [45][110/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.3994e-02 (8.3398e-02)	Acc@1  96.09 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:43 - Epoch: [45][120/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.004)	Loss 3.6718e-02 (8.3245e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:44 - Epoch: [45][130/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.7563e-02 (8.4214e-02)	Acc@1  96.09 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:46 - Epoch: [45][140/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.7136e-02 (8.4694e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:48 - Epoch: [45][150/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.5112e-02 (8.4975e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:31:49 - Epoch: [45][160/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.1813e-01 (8.5153e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:51 - Epoch: [45][170/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.0725e-02 (8.4607e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:53 - Epoch: [45][180/352]	Time  0.161 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.1090e-01 (8.4627e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:54 - Epoch: [45][190/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.1056e-01 (8.5105e-02)	Acc@1  95.31 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:56 - Epoch: [45][200/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.3404e-02 (8.4480e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:57 - Epoch: [45][210/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.4727e-02 (8.4298e-02)	Acc@1  99.22 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:31:59 - Epoch: [45][220/352]	Time  0.167 ( 0.167)	Data  0.001 ( 0.003)	Loss 5.9800e-02 (8.4054e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:01 - Epoch: [45][230/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.5198e-02 (8.3622e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:03 - Epoch: [45][240/352]	Time  0.168 ( 0.167)	Data  0.001 ( 0.003)	Loss 1.2690e-01 (8.3273e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:04 - Epoch: [45][250/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.9169e-02 (8.3989e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:06 - Epoch: [45][260/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.8622e-02 (8.3729e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:07 - Epoch: [45][270/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.1289e-02 (8.4267e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:09 - Epoch: [45][280/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 3.1591e-02 (8.4758e-02)	Acc@1 100.00 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:11 - Epoch: [45][290/352]	Time  0.171 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4054e-01 (8.4912e-02)	Acc@1  93.75 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:12 - Epoch: [45][300/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.9690e-02 (8.4778e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:14 - Epoch: [45][310/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0886e-01 (8.4756e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:16 - Epoch: [45][320/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.5074e-02 (8.4149e-02)	Acc@1 100.00 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:18 - Epoch: [45][330/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.0597e-02 (8.4691e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:19 - Epoch: [45][340/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.8518e-02 (8.5011e-02)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:21 - Epoch: [45][350/352]	Time  0.168 ( 0.167)	Data  0.001 ( 0.003)	Loss 1.3224e-01 (8.5924e-02)	Acc@1  94.53 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:21 - Test: [ 0/20]	Time  0.377 ( 0.377)	Loss 2.9571e-01 (2.9571e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:32:22 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.7498e-01 (3.8352e-01)	Acc@1  89.45 ( 88.67)	Acc@5  99.22 ( 99.43)
07-Mar-22 03:32:23 -  * Acc@1 88.940 Acc@5 99.400
07-Mar-22 03:32:23 - Best acc at epoch 45: 89.63999938964844
07-Mar-22 03:32:24 - Epoch: [46][  0/352]	Time  0.389 ( 0.389)	Data  0.244 ( 0.244)	Loss 4.5975e-02 (4.5975e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
07-Mar-22 03:32:25 - Epoch: [46][ 10/352]	Time  0.141 ( 0.179)	Data  0.001 ( 0.024)	Loss 5.9276e-02 (9.0735e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:32:27 - Epoch: [46][ 20/352]	Time  0.166 ( 0.172)	Data  0.002 ( 0.013)	Loss 3.8559e-02 (8.8027e-02)	Acc@1 100.00 ( 96.88)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:32:29 - Epoch: [46][ 30/352]	Time  0.166 ( 0.170)	Data  0.003 ( 0.010)	Loss 1.1827e-01 (8.6705e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:32:30 - Epoch: [46][ 40/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.008)	Loss 1.0227e-01 (8.8979e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:32 - Epoch: [46][ 50/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.007)	Loss 8.1938e-02 (8.8416e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:34 - Epoch: [46][ 60/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.006)	Loss 6.9673e-02 (8.9513e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:32:35 - Epoch: [46][ 70/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.006)	Loss 6.5095e-02 (8.8189e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:37 - Epoch: [46][ 80/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.005)	Loss 5.5759e-02 (8.9272e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:39 - Epoch: [46][ 90/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.005)	Loss 7.0559e-02 (8.9230e-02)	Acc@1  96.88 ( 96.81)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:41 - Epoch: [46][100/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.005)	Loss 5.1150e-02 (9.0169e-02)	Acc@1  98.44 ( 96.77)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:42 - Epoch: [46][110/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.8538e-02 (9.0113e-02)	Acc@1  97.66 ( 96.76)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:44 - Epoch: [46][120/352]	Time  0.171 ( 0.168)	Data  0.003 ( 0.004)	Loss 6.7128e-02 (9.0250e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:32:46 - Epoch: [46][130/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.8938e-02 (8.9404e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:47 - Epoch: [46][140/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.3700e-02 (8.8623e-02)	Acc@1  99.22 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:49 - Epoch: [46][150/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.9132e-02 (8.8656e-02)	Acc@1  99.22 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:51 - Epoch: [46][160/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.3913e-02 (8.8279e-02)	Acc@1  96.88 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:32:52 - Epoch: [46][170/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.5138e-02 (8.9226e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:32:54 - Epoch: [46][180/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.5515e-02 (8.8985e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:32:56 - Epoch: [46][190/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.0990e-02 (8.9005e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:32:57 - Epoch: [46][200/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2302e-01 (8.9157e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:32:59 - Epoch: [46][210/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.0742e-02 (8.9105e-02)	Acc@1  99.22 ( 96.79)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:33:01 - Epoch: [46][220/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.8585e-02 (8.8873e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:02 - Epoch: [46][230/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.7765e-02 (8.8388e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:33:04 - Epoch: [46][240/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.6154e-02 (8.8682e-02)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:33:06 - Epoch: [46][250/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.3746e-02 (8.8900e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:07 - Epoch: [46][260/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.8338e-02 (8.8505e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:09 - Epoch: [46][270/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.3279e-01 (8.8106e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:11 - Epoch: [46][280/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.3984e-02 (8.7655e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:12 - Epoch: [46][290/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.9394e-01 (8.8355e-02)	Acc@1  93.75 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:14 - Epoch: [46][300/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.4035e-02 (8.8604e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:16 - Epoch: [46][310/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.7473e-02 (8.8868e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:33:17 - Epoch: [46][320/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.1927e-02 (8.8738e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:19 - Epoch: [46][330/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.7852e-02 (8.8817e-02)	Acc@1  99.22 ( 96.87)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:33:21 - Epoch: [46][340/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 2.1532e-01 (8.9051e-02)	Acc@1  93.75 ( 96.85)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:33:22 - Epoch: [46][350/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.3081e-02 (8.9036e-02)	Acc@1  99.22 ( 96.86)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:33:23 - Test: [ 0/20]	Time  0.367 ( 0.367)	Loss 3.5393e-01 (3.5393e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:33:24 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 4.1990e-01 (3.8893e-01)	Acc@1  87.89 ( 89.03)	Acc@5  99.22 ( 99.43)
07-Mar-22 03:33:25 -  * Acc@1 89.340 Acc@5 99.480
07-Mar-22 03:33:25 - Best acc at epoch 46: 89.63999938964844
07-Mar-22 03:33:25 - Epoch: [47][  0/352]	Time  0.398 ( 0.398)	Data  0.246 ( 0.246)	Loss 3.8551e-02 (3.8551e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
07-Mar-22 03:33:27 - Epoch: [47][ 10/352]	Time  0.157 ( 0.184)	Data  0.002 ( 0.024)	Loss 9.7716e-02 (9.5045e-02)	Acc@1  95.31 ( 96.31)	Acc@5 100.00 (100.00)
07-Mar-22 03:33:29 - Epoch: [47][ 20/352]	Time  0.176 ( 0.181)	Data  0.002 ( 0.014)	Loss 6.6074e-02 (9.7546e-02)	Acc@1  97.66 ( 96.54)	Acc@5 100.00 (100.00)
07-Mar-22 03:33:30 - Epoch: [47][ 30/352]	Time  0.181 ( 0.181)	Data  0.002 ( 0.010)	Loss 9.3052e-02 (9.5673e-02)	Acc@1  94.53 ( 96.37)	Acc@5 100.00 (100.00)
07-Mar-22 03:33:32 - Epoch: [47][ 40/352]	Time  0.184 ( 0.180)	Data  0.002 ( 0.008)	Loss 1.0818e-01 (9.1151e-02)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 (100.00)
07-Mar-22 03:33:34 - Epoch: [47][ 50/352]	Time  0.177 ( 0.180)	Data  0.002 ( 0.007)	Loss 1.2416e-01 (9.0253e-02)	Acc@1  96.09 ( 96.78)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:36 - Epoch: [47][ 60/352]	Time  0.173 ( 0.180)	Data  0.002 ( 0.006)	Loss 5.5083e-02 (8.9813e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:33:38 - Epoch: [47][ 70/352]	Time  0.153 ( 0.179)	Data  0.002 ( 0.006)	Loss 5.1964e-02 (8.8396e-02)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:39 - Epoch: [47][ 80/352]	Time  0.153 ( 0.175)	Data  0.002 ( 0.005)	Loss 1.2134e-01 (8.8066e-02)	Acc@1  95.31 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:41 - Epoch: [47][ 90/352]	Time  0.154 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.2307e-01 (8.7576e-02)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:42 - Epoch: [47][100/352]	Time  0.153 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.4919e-01 (8.7489e-02)	Acc@1  93.75 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:44 - Epoch: [47][110/352]	Time  0.152 ( 0.169)	Data  0.001 ( 0.004)	Loss 1.0862e-01 (8.8054e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:45 - Epoch: [47][120/352]	Time  0.151 ( 0.168)	Data  0.001 ( 0.004)	Loss 4.1091e-02 (8.8184e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:47 - Epoch: [47][130/352]	Time  0.177 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.6290e-02 (8.6871e-02)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:49 - Epoch: [47][140/352]	Time  0.177 ( 0.169)	Data  0.002 ( 0.004)	Loss 4.5902e-02 (8.7019e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:50 - Epoch: [47][150/352]	Time  0.179 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.4389e-01 (8.7065e-02)	Acc@1  94.53 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:52 - Epoch: [47][160/352]	Time  0.180 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.4115e-01 (8.7977e-02)	Acc@1  94.53 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:54 - Epoch: [47][170/352]	Time  0.179 ( 0.171)	Data  0.003 ( 0.003)	Loss 9.0830e-02 (8.7288e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:56 - Epoch: [47][180/352]	Time  0.179 ( 0.171)	Data  0.002 ( 0.003)	Loss 8.1257e-02 (8.7153e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:58 - Epoch: [47][190/352]	Time  0.175 ( 0.171)	Data  0.002 ( 0.003)	Loss 6.6967e-02 (8.7422e-02)	Acc@1  99.22 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:33:59 - Epoch: [47][200/352]	Time  0.177 ( 0.172)	Data  0.002 ( 0.003)	Loss 7.8197e-02 (8.6866e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:34:01 - Epoch: [47][210/352]	Time  0.177 ( 0.172)	Data  0.002 ( 0.003)	Loss 1.1678e-01 (8.6320e-02)	Acc@1  94.53 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:03 - Epoch: [47][220/352]	Time  0.183 ( 0.172)	Data  0.002 ( 0.003)	Loss 1.2069e-01 (8.7338e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:34:05 - Epoch: [47][230/352]	Time  0.180 ( 0.173)	Data  0.002 ( 0.003)	Loss 7.4196e-02 (8.7530e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:34:07 - Epoch: [47][240/352]	Time  0.182 ( 0.173)	Data  0.002 ( 0.003)	Loss 1.4640e-01 (8.7282e-02)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:34:08 - Epoch: [47][250/352]	Time  0.179 ( 0.173)	Data  0.002 ( 0.003)	Loss 1.2301e-01 (8.7729e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:34:10 - Epoch: [47][260/352]	Time  0.182 ( 0.173)	Data  0.002 ( 0.003)	Loss 9.5199e-02 (8.7933e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:12 - Epoch: [47][270/352]	Time  0.180 ( 0.173)	Data  0.002 ( 0.003)	Loss 9.2844e-02 (8.8129e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:14 - Epoch: [47][280/352]	Time  0.154 ( 0.173)	Data  0.002 ( 0.003)	Loss 1.3392e-01 (8.7930e-02)	Acc@1  94.53 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:15 - Epoch: [47][290/352]	Time  0.177 ( 0.174)	Data  0.002 ( 0.003)	Loss 1.9567e-02 (8.7615e-02)	Acc@1 100.00 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:17 - Epoch: [47][300/352]	Time  0.180 ( 0.174)	Data  0.002 ( 0.003)	Loss 8.0281e-02 (8.7453e-02)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:19 - Epoch: [47][310/352]	Time  0.177 ( 0.174)	Data  0.001 ( 0.003)	Loss 5.9656e-02 (8.7719e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:21 - Epoch: [47][320/352]	Time  0.178 ( 0.174)	Data  0.002 ( 0.003)	Loss 1.0827e-01 (8.7537e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:23 - Epoch: [47][330/352]	Time  0.177 ( 0.174)	Data  0.002 ( 0.003)	Loss 7.2416e-02 (8.7660e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:24 - Epoch: [47][340/352]	Time  0.178 ( 0.174)	Data  0.002 ( 0.003)	Loss 2.5173e-02 (8.7269e-02)	Acc@1 100.00 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:26 - Epoch: [47][350/352]	Time  0.177 ( 0.174)	Data  0.002 ( 0.003)	Loss 7.0725e-02 (8.6989e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:27 - Test: [ 0/20]	Time  0.385 ( 0.385)	Loss 3.6298e-01 (3.6298e-01)	Acc@1  87.89 ( 87.89)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:34:28 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.7297e-01 (4.0053e-01)	Acc@1  88.67 ( 88.60)	Acc@5  98.83 ( 99.47)
07-Mar-22 03:34:29 -  * Acc@1 88.880 Acc@5 99.480
07-Mar-22 03:34:29 - Best acc at epoch 47: 89.63999938964844
07-Mar-22 03:34:29 - Epoch: [48][  0/352]	Time  0.409 ( 0.409)	Data  0.240 ( 0.240)	Loss 7.6231e-02 (7.6231e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:34:31 - Epoch: [48][ 10/352]	Time  0.168 ( 0.190)	Data  0.002 ( 0.024)	Loss 6.7911e-02 (8.9323e-02)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 (100.00)
07-Mar-22 03:34:33 - Epoch: [48][ 20/352]	Time  0.172 ( 0.179)	Data  0.002 ( 0.014)	Loss 9.9887e-02 (8.9026e-02)	Acc@1  95.31 ( 96.54)	Acc@5 100.00 (100.00)
07-Mar-22 03:34:34 - Epoch: [48][ 30/352]	Time  0.171 ( 0.176)	Data  0.003 ( 0.010)	Loss 6.8140e-02 (9.0878e-02)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 (100.00)
07-Mar-22 03:34:36 - Epoch: [48][ 40/352]	Time  0.169 ( 0.174)	Data  0.002 ( 0.008)	Loss 9.4101e-02 (9.2906e-02)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 (100.00)
07-Mar-22 03:34:38 - Epoch: [48][ 50/352]	Time  0.170 ( 0.173)	Data  0.002 ( 0.007)	Loss 7.4940e-02 (9.5850e-02)	Acc@1  96.09 ( 96.37)	Acc@5 100.00 (100.00)
07-Mar-22 03:34:39 - Epoch: [48][ 60/352]	Time  0.172 ( 0.172)	Data  0.002 ( 0.006)	Loss 5.6823e-02 (9.4653e-02)	Acc@1  98.44 ( 96.47)	Acc@5 100.00 (100.00)
07-Mar-22 03:34:41 - Epoch: [48][ 70/352]	Time  0.170 ( 0.172)	Data  0.002 ( 0.006)	Loss 6.5240e-02 (9.4001e-02)	Acc@1  99.22 ( 96.61)	Acc@5 100.00 (100.00)
07-Mar-22 03:34:43 - Epoch: [48][ 80/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.005)	Loss 5.2708e-02 (9.2696e-02)	Acc@1  98.44 ( 96.66)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:44 - Epoch: [48][ 90/352]	Time  0.165 ( 0.171)	Data  0.002 ( 0.005)	Loss 9.4506e-02 (9.3585e-02)	Acc@1  96.09 ( 96.68)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:46 - Epoch: [48][100/352]	Time  0.172 ( 0.171)	Data  0.002 ( 0.005)	Loss 6.4486e-02 (9.2598e-02)	Acc@1  99.22 ( 96.74)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:48 - Epoch: [48][110/352]	Time  0.172 ( 0.171)	Data  0.002 ( 0.004)	Loss 1.1327e-01 (9.0334e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:49 - Epoch: [48][120/352]	Time  0.170 ( 0.170)	Data  0.002 ( 0.004)	Loss 6.4471e-02 (9.0850e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:51 - Epoch: [48][130/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.004)	Loss 8.9322e-02 (8.9756e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:53 - Epoch: [48][140/352]	Time  0.165 ( 0.170)	Data  0.002 ( 0.004)	Loss 4.8871e-02 (8.9514e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:54 - Epoch: [48][150/352]	Time  0.171 ( 0.170)	Data  0.003 ( 0.004)	Loss 3.9643e-02 (8.8396e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:34:56 - Epoch: [48][160/352]	Time  0.191 ( 0.170)	Data  0.003 ( 0.004)	Loss 8.2881e-02 (8.7252e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:34:58 - Epoch: [48][170/352]	Time  0.164 ( 0.170)	Data  0.002 ( 0.004)	Loss 8.2636e-02 (8.7576e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:34:59 - Epoch: [48][180/352]	Time  0.170 ( 0.170)	Data  0.002 ( 0.004)	Loss 6.7636e-02 (8.8401e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:01 - Epoch: [48][190/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.1921e-02 (8.8193e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:03 - Epoch: [48][200/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.1214e-02 (8.7985e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:04 - Epoch: [48][210/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.4449e-01 (8.7975e-02)	Acc@1  96.09 ( 96.86)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:06 - Epoch: [48][220/352]	Time  0.171 ( 0.169)	Data  0.003 ( 0.003)	Loss 1.2021e-01 (8.7625e-02)	Acc@1  94.53 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:08 - Epoch: [48][230/352]	Time  0.162 ( 0.169)	Data  0.002 ( 0.003)	Loss 5.6081e-02 (8.7358e-02)	Acc@1  99.22 ( 96.90)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:09 - Epoch: [48][240/352]	Time  0.164 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.3453e-01 (8.7536e-02)	Acc@1  93.75 ( 96.90)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:11 - Epoch: [48][250/352]	Time  0.163 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0175e-01 (8.7707e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:13 - Epoch: [48][260/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.5028e-02 (8.7726e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:14 - Epoch: [48][270/352]	Time  0.162 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2285e-01 (8.7962e-02)	Acc@1  95.31 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:16 - Epoch: [48][280/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.0086e-02 (8.7745e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:18 - Epoch: [48][290/352]	Time  0.195 ( 0.168)	Data  0.003 ( 0.003)	Loss 7.4613e-02 (8.7363e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:19 - Epoch: [48][300/352]	Time  0.158 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.2617e-02 (8.7690e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:21 - Epoch: [48][310/352]	Time  0.159 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.9535e-02 (8.7362e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:23 - Epoch: [48][320/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.3292e-02 (8.7099e-02)	Acc@1 100.00 ( 96.90)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:24 - Epoch: [48][330/352]	Time  0.161 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.3856e-02 (8.6832e-02)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:26 - Epoch: [48][340/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.5418e-02 (8.7146e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:28 - Epoch: [48][350/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0334e-01 (8.6768e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:28 - Test: [ 0/20]	Time  0.403 ( 0.403)	Loss 3.3615e-01 (3.3615e-01)	Acc@1  89.45 ( 89.45)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:29 - Test: [10/20]	Time  0.100 ( 0.127)	Loss 3.8189e-01 (3.9040e-01)	Acc@1  90.23 ( 89.13)	Acc@5  99.61 ( 99.47)
07-Mar-22 03:35:30 -  * Acc@1 89.360 Acc@5 99.500
07-Mar-22 03:35:30 - Best acc at epoch 48: 89.63999938964844
07-Mar-22 03:35:31 - Epoch: [49][  0/352]	Time  0.438 ( 0.438)	Data  0.252 ( 0.252)	Loss 6.3366e-02 (6.3366e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:32 - Epoch: [49][ 10/352]	Time  0.171 ( 0.191)	Data  0.002 ( 0.025)	Loss 6.2906e-02 (8.4871e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:34 - Epoch: [49][ 20/352]	Time  0.168 ( 0.180)	Data  0.002 ( 0.014)	Loss 9.4403e-02 (8.6024e-02)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:36 - Epoch: [49][ 30/352]	Time  0.169 ( 0.176)	Data  0.003 ( 0.010)	Loss 8.6953e-02 (8.9724e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:38 - Epoch: [49][ 40/352]	Time  0.167 ( 0.174)	Data  0.002 ( 0.008)	Loss 1.0129e-01 (8.8556e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:39 - Epoch: [49][ 50/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.007)	Loss 1.2514e-01 (8.9351e-02)	Acc@1  94.53 ( 96.92)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:41 - Epoch: [49][ 60/352]	Time  0.169 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.0797e-01 (8.7939e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:43 - Epoch: [49][ 70/352]	Time  0.170 ( 0.171)	Data  0.002 ( 0.006)	Loss 5.6690e-02 (8.5500e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:44 - Epoch: [49][ 80/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.005)	Loss 8.8116e-02 (8.5558e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:46 - Epoch: [49][ 90/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.005)	Loss 6.3651e-02 (8.4918e-02)	Acc@1  99.22 ( 97.06)	Acc@5 100.00 (100.00)
07-Mar-22 03:35:48 - Epoch: [49][100/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.005)	Loss 6.2272e-02 (8.5456e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:35:49 - Epoch: [49][110/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.5770e-01 (8.4999e-02)	Acc@1  95.31 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:35:51 - Epoch: [49][120/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.3392e-02 (8.5887e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:35:53 - Epoch: [49][130/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 8.9949e-02 (8.5659e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:35:54 - Epoch: [49][140/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 2.7453e-02 (8.5850e-02)	Acc@1  99.22 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:35:56 - Epoch: [49][150/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.5802e-02 (8.5436e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:35:58 - Epoch: [49][160/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.3715e-02 (8.5555e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:35:59 - Epoch: [49][170/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.1283e-02 (8.4823e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:01 - Epoch: [49][180/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.9549e-02 (8.4713e-02)	Acc@1  95.31 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:03 - Epoch: [49][190/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.6343e-02 (8.4956e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:04 - Epoch: [49][200/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.6659e-02 (8.4803e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:06 - Epoch: [49][210/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.5639e-02 (8.5239e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:36:08 - Epoch: [49][220/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1214e-01 (8.5436e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:09 - Epoch: [49][230/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.5114e-02 (8.5622e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:11 - Epoch: [49][240/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.3833e-02 (8.6048e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:13 - Epoch: [49][250/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.6154e-02 (8.6434e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:14 - Epoch: [49][260/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4273e-01 (8.6636e-02)	Acc@1  93.75 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:16 - Epoch: [49][270/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.8274e-02 (8.6756e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:18 - Epoch: [49][280/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.9650e-02 (8.6994e-02)	Acc@1  96.09 ( 96.84)	Acc@5  99.22 ( 99.98)
07-Mar-22 03:36:19 - Epoch: [49][290/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.6235e-02 (8.7419e-02)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:21 - Epoch: [49][300/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.3470e-02 (8.7196e-02)	Acc@1  99.22 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:23 - Epoch: [49][310/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.2282e-02 (8.6816e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:24 - Epoch: [49][320/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.3759e-02 (8.6788e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:26 - Epoch: [49][330/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.1002e-02 (8.7002e-02)	Acc@1 100.00 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:28 - Epoch: [49][340/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.9421e-02 (8.7289e-02)	Acc@1  98.44 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:29 - Epoch: [49][350/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.3820e-02 (8.7215e-02)	Acc@1  95.31 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:30 - Test: [ 0/20]	Time  0.367 ( 0.367)	Loss 2.8927e-01 (2.8927e-01)	Acc@1  90.23 ( 90.23)	Acc@5 100.00 (100.00)
07-Mar-22 03:36:31 - Test: [10/20]	Time  0.098 ( 0.127)	Loss 4.1644e-01 (3.9018e-01)	Acc@1  87.11 ( 88.67)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:36:32 -  * Acc@1 89.200 Acc@5 99.460
07-Mar-22 03:36:32 - Best acc at epoch 49: 89.63999938964844
07-Mar-22 03:36:32 - Epoch: [50][  0/352]	Time  0.394 ( 0.394)	Data  0.243 ( 0.243)	Loss 8.5898e-02 (8.5898e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:36:34 - Epoch: [50][ 10/352]	Time  0.168 ( 0.188)	Data  0.002 ( 0.024)	Loss 9.7301e-02 (8.3741e-02)	Acc@1  97.66 ( 97.16)	Acc@5 100.00 (100.00)
07-Mar-22 03:36:36 - Epoch: [50][ 20/352]	Time  0.141 ( 0.173)	Data  0.001 ( 0.013)	Loss 1.5571e-01 (7.9779e-02)	Acc@1  93.75 ( 97.25)	Acc@5 100.00 (100.00)
07-Mar-22 03:36:37 - Epoch: [50][ 30/352]	Time  0.170 ( 0.172)	Data  0.003 ( 0.010)	Loss 6.1636e-02 (8.1137e-02)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 (100.00)
07-Mar-22 03:36:39 - Epoch: [50][ 40/352]	Time  0.166 ( 0.172)	Data  0.002 ( 0.008)	Loss 6.1743e-02 (8.1747e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 (100.00)
07-Mar-22 03:36:40 - Epoch: [50][ 50/352]	Time  0.143 ( 0.167)	Data  0.002 ( 0.007)	Loss 1.1360e-01 (8.1957e-02)	Acc@1  96.09 ( 97.21)	Acc@5 100.00 (100.00)
07-Mar-22 03:36:42 - Epoch: [50][ 60/352]	Time  0.165 ( 0.164)	Data  0.002 ( 0.006)	Loss 8.5748e-02 (8.2059e-02)	Acc@1  96.09 ( 97.17)	Acc@5 100.00 (100.00)
07-Mar-22 03:36:44 - Epoch: [50][ 70/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.005)	Loss 5.5073e-02 (8.3065e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 (100.00)
07-Mar-22 03:36:45 - Epoch: [50][ 80/352]	Time  0.170 ( 0.165)	Data  0.002 ( 0.005)	Loss 1.2483e-01 (8.4499e-02)	Acc@1  95.31 ( 97.08)	Acc@5  99.22 ( 99.99)
07-Mar-22 03:36:47 - Epoch: [50][ 90/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.005)	Loss 9.0338e-02 (8.6294e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:36:49 - Epoch: [50][100/352]	Time  0.143 ( 0.164)	Data  0.002 ( 0.004)	Loss 1.0292e-01 (8.5848e-02)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:50 - Epoch: [50][110/352]	Time  0.143 ( 0.163)	Data  0.001 ( 0.004)	Loss 6.0842e-02 (8.6797e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:52 - Epoch: [50][120/352]	Time  0.143 ( 0.162)	Data  0.002 ( 0.004)	Loss 4.2668e-02 (8.7254e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:53 - Epoch: [50][130/352]	Time  0.158 ( 0.160)	Data  0.002 ( 0.004)	Loss 7.2832e-02 (8.7923e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:55 - Epoch: [50][140/352]	Time  0.170 ( 0.161)	Data  0.002 ( 0.004)	Loss 5.1359e-02 (8.8627e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:56 - Epoch: [50][150/352]	Time  0.168 ( 0.161)	Data  0.002 ( 0.004)	Loss 4.4455e-02 (8.8031e-02)	Acc@1  99.22 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:36:58 - Epoch: [50][160/352]	Time  0.164 ( 0.162)	Data  0.002 ( 0.004)	Loss 4.4699e-02 (8.7446e-02)	Acc@1 100.00 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:37:00 - Epoch: [50][170/352]	Time  0.169 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.9842e-02 (8.6961e-02)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:01 - Epoch: [50][180/352]	Time  0.173 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.5131e-01 (8.7296e-02)	Acc@1  93.75 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:03 - Epoch: [50][190/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.0862e-02 (8.6350e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:05 - Epoch: [50][200/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.9992e-02 (8.5456e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:06 - Epoch: [50][210/352]	Time  0.170 ( 0.163)	Data  0.003 ( 0.003)	Loss 7.8823e-02 (8.5299e-02)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:08 - Epoch: [50][220/352]	Time  0.168 ( 0.163)	Data  0.002 ( 0.003)	Loss 5.1912e-02 (8.5684e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:10 - Epoch: [50][230/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 3.7471e-02 (8.6294e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:11 - Epoch: [50][240/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 9.7897e-02 (8.5933e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:13 - Epoch: [50][250/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 5.3868e-02 (8.5430e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:15 - Epoch: [50][260/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.1262e-01 (8.5570e-02)	Acc@1  95.31 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:16 - Epoch: [50][270/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 6.2401e-02 (8.5735e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:18 - Epoch: [50][280/352]	Time  0.170 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.7268e-02 (8.5308e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:20 - Epoch: [50][290/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 9.7412e-02 (8.5057e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:21 - Epoch: [50][300/352]	Time  0.170 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.4407e-01 (8.4904e-02)	Acc@1  95.31 ( 97.08)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:23 - Epoch: [50][310/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.6485e-02 (8.5339e-02)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:25 - Epoch: [50][320/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.3381e-02 (8.5603e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:37:26 - Epoch: [50][330/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.9536e-02 (8.5397e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:37:28 - Epoch: [50][340/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.0064e-01 (8.5219e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:37:30 - Epoch: [50][350/352]	Time  0.170 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.2998e-02 (8.5050e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:37:31 - Test: [ 0/20]	Time  0.382 ( 0.382)	Loss 3.2737e-01 (3.2737e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:37:32 - Test: [10/20]	Time  0.109 ( 0.126)	Loss 3.9926e-01 (3.8507e-01)	Acc@1  87.50 ( 88.35)	Acc@5  99.61 ( 99.57)
07-Mar-22 03:37:33 -  * Acc@1 89.080 Acc@5 99.540
07-Mar-22 03:37:33 - Best acc at epoch 50: 89.63999938964844
07-Mar-22 03:37:33 - Epoch: [51][  0/352]	Time  0.372 ( 0.372)	Data  0.226 ( 0.226)	Loss 8.5849e-02 (8.5849e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 03:37:35 - Epoch: [51][ 10/352]	Time  0.171 ( 0.188)	Data  0.002 ( 0.023)	Loss 9.1209e-02 (7.8290e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 (100.00)
07-Mar-22 03:37:36 - Epoch: [51][ 20/352]	Time  0.170 ( 0.179)	Data  0.002 ( 0.013)	Loss 1.3643e-01 (8.8131e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 (100.00)
07-Mar-22 03:37:38 - Epoch: [51][ 30/352]	Time  0.171 ( 0.176)	Data  0.002 ( 0.010)	Loss 9.5149e-02 (8.5674e-02)	Acc@1  95.31 ( 96.90)	Acc@5 100.00 (100.00)
07-Mar-22 03:37:40 - Epoch: [51][ 40/352]	Time  0.181 ( 0.172)	Data  0.003 ( 0.008)	Loss 6.8075e-02 (8.6501e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:37:42 - Epoch: [51][ 50/352]	Time  0.179 ( 0.174)	Data  0.003 ( 0.007)	Loss 5.1369e-02 (8.8370e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:37:43 - Epoch: [51][ 60/352]	Time  0.180 ( 0.175)	Data  0.002 ( 0.006)	Loss 1.2522e-01 (8.8336e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:37:45 - Epoch: [51][ 70/352]	Time  0.181 ( 0.176)	Data  0.003 ( 0.006)	Loss 3.6994e-02 (8.6447e-02)	Acc@1  99.22 ( 97.01)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:37:47 - Epoch: [51][ 80/352]	Time  0.184 ( 0.176)	Data  0.002 ( 0.005)	Loss 8.2455e-02 (8.5848e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:37:49 - Epoch: [51][ 90/352]	Time  0.177 ( 0.177)	Data  0.002 ( 0.005)	Loss 1.3415e-01 (8.6448e-02)	Acc@1  92.97 ( 96.99)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:37:51 - Epoch: [51][100/352]	Time  0.181 ( 0.177)	Data  0.002 ( 0.005)	Loss 1.0851e-01 (8.7557e-02)	Acc@1  93.75 ( 96.91)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:37:52 - Epoch: [51][110/352]	Time  0.182 ( 0.177)	Data  0.003 ( 0.004)	Loss 2.4266e-02 (8.8015e-02)	Acc@1 100.00 ( 96.86)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:37:54 - Epoch: [51][120/352]	Time  0.180 ( 0.178)	Data  0.002 ( 0.004)	Loss 7.1888e-02 (9.0466e-02)	Acc@1  97.66 ( 96.77)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:37:56 - Epoch: [51][130/352]	Time  0.180 ( 0.178)	Data  0.002 ( 0.004)	Loss 7.8606e-02 (9.0734e-02)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:37:58 - Epoch: [51][140/352]	Time  0.176 ( 0.178)	Data  0.002 ( 0.004)	Loss 1.1523e-01 (9.0778e-02)	Acc@1  94.53 ( 96.74)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:38:00 - Epoch: [51][150/352]	Time  0.179 ( 0.178)	Data  0.002 ( 0.004)	Loss 1.5234e-01 (9.1338e-02)	Acc@1  95.31 ( 96.73)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:38:01 - Epoch: [51][160/352]	Time  0.176 ( 0.178)	Data  0.002 ( 0.004)	Loss 9.3385e-02 (9.1154e-02)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:38:03 - Epoch: [51][170/352]	Time  0.178 ( 0.178)	Data  0.002 ( 0.004)	Loss 8.3493e-02 (9.0509e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:38:05 - Epoch: [51][180/352]	Time  0.178 ( 0.178)	Data  0.002 ( 0.004)	Loss 6.2419e-02 (9.0115e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:38:07 - Epoch: [51][190/352]	Time  0.183 ( 0.178)	Data  0.002 ( 0.004)	Loss 3.3061e-02 (8.9793e-02)	Acc@1 100.00 ( 96.81)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:38:09 - Epoch: [51][200/352]	Time  0.178 ( 0.178)	Data  0.002 ( 0.003)	Loss 1.3501e-01 (8.8955e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:38:10 - Epoch: [51][210/352]	Time  0.180 ( 0.178)	Data  0.002 ( 0.003)	Loss 8.4176e-02 (8.8531e-02)	Acc@1  95.31 ( 96.87)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:38:12 - Epoch: [51][220/352]	Time  0.177 ( 0.178)	Data  0.002 ( 0.003)	Loss 8.3141e-02 (8.8393e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:38:14 - Epoch: [51][230/352]	Time  0.179 ( 0.178)	Data  0.002 ( 0.003)	Loss 7.1936e-02 (8.8084e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:38:16 - Epoch: [51][240/352]	Time  0.178 ( 0.178)	Data  0.002 ( 0.003)	Loss 5.9300e-02 (8.7683e-02)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:38:17 - Epoch: [51][250/352]	Time  0.177 ( 0.178)	Data  0.002 ( 0.003)	Loss 8.1113e-02 (8.8606e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:38:19 - Epoch: [51][260/352]	Time  0.179 ( 0.178)	Data  0.003 ( 0.003)	Loss 3.5569e-02 (8.7866e-02)	Acc@1  99.22 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:38:21 - Epoch: [51][270/352]	Time  0.181 ( 0.178)	Data  0.002 ( 0.003)	Loss 1.4699e-01 (8.8574e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:38:23 - Epoch: [51][280/352]	Time  0.176 ( 0.178)	Data  0.002 ( 0.003)	Loss 2.8978e-02 (8.8084e-02)	Acc@1  99.22 ( 96.89)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:38:25 - Epoch: [51][290/352]	Time  0.181 ( 0.178)	Data  0.002 ( 0.003)	Loss 5.8852e-02 (8.7596e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:38:26 - Epoch: [51][300/352]	Time  0.181 ( 0.178)	Data  0.002 ( 0.003)	Loss 7.9276e-02 (8.7235e-02)	Acc@1  99.22 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:38:28 - Epoch: [51][310/352]	Time  0.182 ( 0.178)	Data  0.002 ( 0.003)	Loss 1.0124e-01 (8.7128e-02)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:38:30 - Epoch: [51][320/352]	Time  0.176 ( 0.178)	Data  0.003 ( 0.003)	Loss 1.1986e-01 (8.7331e-02)	Acc@1  95.31 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:38:32 - Epoch: [51][330/352]	Time  0.181 ( 0.178)	Data  0.002 ( 0.003)	Loss 1.0394e-01 (8.7452e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:38:34 - Epoch: [51][340/352]	Time  0.175 ( 0.178)	Data  0.002 ( 0.003)	Loss 6.1656e-02 (8.7313e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:38:35 - Epoch: [51][350/352]	Time  0.177 ( 0.178)	Data  0.002 ( 0.003)	Loss 6.4211e-02 (8.7405e-02)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:38:36 - Test: [ 0/20]	Time  0.361 ( 0.361)	Loss 3.7225e-01 (3.7225e-01)	Acc@1  89.45 ( 89.45)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:38:37 - Test: [10/20]	Time  0.098 ( 0.126)	Loss 3.5750e-01 (3.8065e-01)	Acc@1  88.67 ( 89.35)	Acc@5  99.22 ( 99.57)
07-Mar-22 03:38:38 -  * Acc@1 89.500 Acc@5 99.560
07-Mar-22 03:38:38 - Best acc at epoch 51: 89.63999938964844
07-Mar-22 03:38:38 - Epoch: [52][  0/352]	Time  0.399 ( 0.399)	Data  0.254 ( 0.254)	Loss 7.9182e-02 (7.9182e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:38:40 - Epoch: [52][ 10/352]	Time  0.165 ( 0.183)	Data  0.003 ( 0.025)	Loss 4.0957e-02 (8.3682e-02)	Acc@1  99.22 ( 97.16)	Acc@5 100.00 (100.00)
07-Mar-22 03:38:41 - Epoch: [52][ 20/352]	Time  0.141 ( 0.163)	Data  0.002 ( 0.014)	Loss 8.5568e-02 (9.0434e-02)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 (100.00)
07-Mar-22 03:38:43 - Epoch: [52][ 30/352]	Time  0.140 ( 0.156)	Data  0.001 ( 0.010)	Loss 8.8709e-02 (8.4292e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 (100.00)
07-Mar-22 03:38:44 - Epoch: [52][ 40/352]	Time  0.152 ( 0.157)	Data  0.002 ( 0.008)	Loss 1.0388e-01 (8.4241e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 (100.00)
07-Mar-22 03:38:46 - Epoch: [52][ 50/352]	Time  0.166 ( 0.156)	Data  0.002 ( 0.007)	Loss 1.1232e-01 (8.6806e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 03:38:48 - Epoch: [52][ 60/352]	Time  0.166 ( 0.158)	Data  0.002 ( 0.006)	Loss 6.0070e-02 (8.7238e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 (100.00)
07-Mar-22 03:38:49 - Epoch: [52][ 70/352]	Time  0.169 ( 0.159)	Data  0.002 ( 0.005)	Loss 1.3322e-01 (9.0396e-02)	Acc@1  94.53 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:38:51 - Epoch: [52][ 80/352]	Time  0.167 ( 0.160)	Data  0.002 ( 0.005)	Loss 7.7292e-02 (9.1528e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 (100.00)
07-Mar-22 03:38:53 - Epoch: [52][ 90/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.005)	Loss 8.4940e-02 (9.0381e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 (100.00)
07-Mar-22 03:38:54 - Epoch: [52][100/352]	Time  0.167 ( 0.161)	Data  0.002 ( 0.004)	Loss 1.1676e-01 (9.0256e-02)	Acc@1  93.75 ( 96.84)	Acc@5 100.00 (100.00)
07-Mar-22 03:38:56 - Epoch: [52][110/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.004)	Loss 5.8241e-02 (8.9512e-02)	Acc@1  96.88 ( 96.81)	Acc@5 100.00 (100.00)
07-Mar-22 03:38:58 - Epoch: [52][120/352]	Time  0.146 ( 0.161)	Data  0.002 ( 0.004)	Loss 1.2638e-01 (8.8127e-02)	Acc@1  94.53 ( 96.87)	Acc@5 100.00 (100.00)
07-Mar-22 03:38:59 - Epoch: [52][130/352]	Time  0.165 ( 0.162)	Data  0.002 ( 0.004)	Loss 8.6060e-02 (8.8475e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:01 - Epoch: [52][140/352]	Time  0.175 ( 0.162)	Data  0.002 ( 0.004)	Loss 8.3569e-02 (8.7867e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:03 - Epoch: [52][150/352]	Time  0.186 ( 0.163)	Data  0.002 ( 0.004)	Loss 1.2436e-01 (8.8737e-02)	Acc@1  94.53 ( 96.82)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:04 - Epoch: [52][160/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.3104e-02 (8.8207e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:06 - Epoch: [52][170/352]	Time  0.165 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.0767e-02 (8.7618e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:08 - Epoch: [52][180/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.2422e-01 (8.7544e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:09 - Epoch: [52][190/352]	Time  0.141 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.7004e-02 (8.6899e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:11 - Epoch: [52][200/352]	Time  0.170 ( 0.163)	Data  0.002 ( 0.003)	Loss 5.1278e-02 (8.6404e-02)	Acc@1  99.22 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:12 - Epoch: [52][210/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.2924e-01 (8.5863e-02)	Acc@1  96.88 ( 96.97)	Acc@5  99.22 ( 99.99)
07-Mar-22 03:39:14 - Epoch: [52][220/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.6598e-02 (8.6009e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:16 - Epoch: [52][230/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 2.8097e-02 (8.5439e-02)	Acc@1 100.00 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:17 - Epoch: [52][240/352]	Time  0.165 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.1420e-01 (8.5456e-02)	Acc@1  95.31 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:19 - Epoch: [52][250/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 9.5745e-02 (8.5879e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:21 - Epoch: [52][260/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.3556e-01 (8.5812e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:22 - Epoch: [52][270/352]	Time  0.168 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0219e-01 (8.5925e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:24 - Epoch: [52][280/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 6.6915e-02 (8.5877e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:26 - Epoch: [52][290/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.2545e-02 (8.5606e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:27 - Epoch: [52][300/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.3692e-01 (8.5898e-02)	Acc@1  94.53 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:29 - Epoch: [52][310/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.1338e-02 (8.5731e-02)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:31 - Epoch: [52][320/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.8520e-02 (8.5715e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:32 - Epoch: [52][330/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 3.3927e-02 (8.4836e-02)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:34 - Epoch: [52][340/352]	Time  0.159 ( 0.164)	Data  0.002 ( 0.003)	Loss 6.7106e-02 (8.4634e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:36 - Epoch: [52][350/352]	Time  0.165 ( 0.164)	Data  0.002 ( 0.003)	Loss 6.6488e-02 (8.4467e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:39:36 - Test: [ 0/20]	Time  0.374 ( 0.374)	Loss 3.1982e-01 (3.1982e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:39:37 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 3.6726e-01 (3.7816e-01)	Acc@1  90.62 ( 89.52)	Acc@5  98.83 ( 99.43)
07-Mar-22 03:39:38 -  * Acc@1 89.440 Acc@5 99.480
07-Mar-22 03:39:38 - Best acc at epoch 52: 89.63999938964844
07-Mar-22 03:39:39 - Epoch: [53][  0/352]	Time  0.408 ( 0.408)	Data  0.241 ( 0.241)	Loss 1.0963e-01 (1.0963e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:39:41 - Epoch: [53][ 10/352]	Time  0.169 ( 0.190)	Data  0.002 ( 0.024)	Loss 4.6011e-02 (7.6619e-02)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 (100.00)
07-Mar-22 03:39:42 - Epoch: [53][ 20/352]	Time  0.167 ( 0.180)	Data  0.002 ( 0.014)	Loss 1.0454e-01 (7.5206e-02)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 (100.00)
07-Mar-22 03:39:44 - Epoch: [53][ 30/352]	Time  0.142 ( 0.171)	Data  0.002 ( 0.010)	Loss 8.0799e-02 (7.9925e-02)	Acc@1  98.44 ( 97.18)	Acc@5 100.00 (100.00)
07-Mar-22 03:39:45 - Epoch: [53][ 40/352]	Time  0.141 ( 0.164)	Data  0.002 ( 0.008)	Loss 1.1393e-01 (8.7131e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 (100.00)
07-Mar-22 03:39:47 - Epoch: [53][ 50/352]	Time  0.141 ( 0.159)	Data  0.002 ( 0.007)	Loss 6.7359e-02 (8.6889e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 (100.00)
07-Mar-22 03:39:48 - Epoch: [53][ 60/352]	Time  0.164 ( 0.161)	Data  0.002 ( 0.006)	Loss 6.0466e-02 (8.7001e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 (100.00)
07-Mar-22 03:39:50 - Epoch: [53][ 70/352]	Time  0.169 ( 0.162)	Data  0.002 ( 0.005)	Loss 3.6081e-02 (8.4545e-02)	Acc@1  99.22 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:39:52 - Epoch: [53][ 80/352]	Time  0.168 ( 0.162)	Data  0.002 ( 0.005)	Loss 5.0121e-02 (8.3855e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:39:53 - Epoch: [53][ 90/352]	Time  0.170 ( 0.163)	Data  0.002 ( 0.005)	Loss 8.3018e-02 (8.3678e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 (100.00)
07-Mar-22 03:39:55 - Epoch: [53][100/352]	Time  0.164 ( 0.163)	Data  0.002 ( 0.004)	Loss 6.2586e-02 (8.3882e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:39:57 - Epoch: [53][110/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.004)	Loss 6.1333e-02 (8.2996e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 (100.00)
07-Mar-22 03:39:58 - Epoch: [53][120/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.004)	Loss 4.5836e-02 (8.2758e-02)	Acc@1  99.22 ( 96.98)	Acc@5 100.00 (100.00)
07-Mar-22 03:40:00 - Epoch: [53][130/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.004)	Loss 6.5392e-02 (8.3569e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 (100.00)
07-Mar-22 03:40:02 - Epoch: [53][140/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.004)	Loss 7.9125e-02 (8.4122e-02)	Acc@1  96.09 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:03 - Epoch: [53][150/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.004)	Loss 8.4792e-02 (8.3051e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:05 - Epoch: [53][160/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.004)	Loss 4.8644e-02 (8.2128e-02)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 (100.00)
07-Mar-22 03:40:07 - Epoch: [53][170/352]	Time  0.169 ( 0.165)	Data  0.001 ( 0.003)	Loss 8.8099e-02 (8.1996e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:08 - Epoch: [53][180/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.0209e-01 (8.2440e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:10 - Epoch: [53][190/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 8.6559e-02 (8.2417e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:12 - Epoch: [53][200/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.1397e-01 (8.2348e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:13 - Epoch: [53][210/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.1292e-01 (8.2596e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:15 - Epoch: [53][220/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.4773e-02 (8.2704e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:17 - Epoch: [53][230/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.4024e-01 (8.2513e-02)	Acc@1  93.75 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:18 - Epoch: [53][240/352]	Time  0.172 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.7217e-01 (8.3078e-02)	Acc@1  93.75 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:40:20 - Epoch: [53][250/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0455e-01 (8.3292e-02)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:40:22 - Epoch: [53][260/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.7459e-02 (8.3441e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:23 - Epoch: [53][270/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.9488e-02 (8.3155e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:25 - Epoch: [53][280/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 9.2599e-02 (8.3014e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:27 - Epoch: [53][290/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.1804e-02 (8.3157e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:28 - Epoch: [53][300/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1745e-01 (8.3941e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:30 - Epoch: [53][310/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 4.8851e-02 (8.4222e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:32 - Epoch: [53][320/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 2.9183e-02 (8.3657e-02)	Acc@1 100.00 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:33 - Epoch: [53][330/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.7319e-02 (8.3350e-02)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:35 - Epoch: [53][340/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.4923e-01 (8.3661e-02)	Acc@1  94.53 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:37 - Epoch: [53][350/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.5793e-02 (8.3153e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:37 - Test: [ 0/20]	Time  0.367 ( 0.367)	Loss 3.3531e-01 (3.3531e-01)	Acc@1  89.45 ( 89.45)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:40:39 - Test: [10/20]	Time  0.098 ( 0.126)	Loss 3.6906e-01 (3.8647e-01)	Acc@1  89.84 ( 89.42)	Acc@5  99.61 ( 99.43)
07-Mar-22 03:40:39 -  * Acc@1 89.540 Acc@5 99.440
07-Mar-22 03:40:39 - Best acc at epoch 53: 89.63999938964844
07-Mar-22 03:40:40 - Epoch: [54][  0/352]	Time  0.400 ( 0.400)	Data  0.247 ( 0.247)	Loss 6.0480e-02 (6.0480e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 03:40:41 - Epoch: [54][ 10/352]	Time  0.149 ( 0.175)	Data  0.003 ( 0.024)	Loss 1.1824e-01 (9.7788e-02)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:40:43 - Epoch: [54][ 20/352]	Time  0.143 ( 0.162)	Data  0.002 ( 0.014)	Loss 4.7383e-02 (8.9303e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:40:44 - Epoch: [54][ 30/352]	Time  0.143 ( 0.157)	Data  0.002 ( 0.010)	Loss 9.1997e-02 (8.8871e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:40:46 - Epoch: [54][ 40/352]	Time  0.164 ( 0.155)	Data  0.002 ( 0.008)	Loss 1.1587e-01 (8.9600e-02)	Acc@1  96.09 ( 97.24)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:40:47 - Epoch: [54][ 50/352]	Time  0.151 ( 0.155)	Data  0.002 ( 0.007)	Loss 6.5895e-02 (8.8876e-02)	Acc@1  96.88 ( 97.17)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:40:49 - Epoch: [54][ 60/352]	Time  0.142 ( 0.154)	Data  0.002 ( 0.006)	Loss 6.2006e-02 (8.8999e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:50 - Epoch: [54][ 70/352]	Time  0.145 ( 0.153)	Data  0.002 ( 0.005)	Loss 5.4222e-02 (8.8761e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:52 - Epoch: [54][ 80/352]	Time  0.150 ( 0.152)	Data  0.002 ( 0.005)	Loss 7.9052e-02 (8.8103e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:54 - Epoch: [54][ 90/352]	Time  0.171 ( 0.154)	Data  0.002 ( 0.005)	Loss 1.1032e-01 (8.7839e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:55 - Epoch: [54][100/352]	Time  0.141 ( 0.154)	Data  0.002 ( 0.004)	Loss 7.6035e-02 (8.7313e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:57 - Epoch: [54][110/352]	Time  0.149 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.1604e-02 (8.8358e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:58 - Epoch: [54][120/352]	Time  0.144 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.2177e-01 (8.9561e-02)	Acc@1  95.31 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:40:59 - Epoch: [54][130/352]	Time  0.141 ( 0.152)	Data  0.002 ( 0.004)	Loss 1.0345e-01 (8.9273e-02)	Acc@1  95.31 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:01 - Epoch: [54][140/352]	Time  0.149 ( 0.151)	Data  0.002 ( 0.004)	Loss 3.9098e-02 (8.8786e-02)	Acc@1  99.22 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:02 - Epoch: [54][150/352]	Time  0.142 ( 0.151)	Data  0.002 ( 0.004)	Loss 1.4835e-01 (8.9581e-02)	Acc@1  94.53 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:04 - Epoch: [54][160/352]	Time  0.143 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.1044e-01 (8.9775e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:05 - Epoch: [54][170/352]	Time  0.143 ( 0.150)	Data  0.002 ( 0.003)	Loss 5.8070e-02 (8.9531e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:07 - Epoch: [54][180/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.1250e-01 (8.9228e-02)	Acc@1  94.53 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:08 - Epoch: [54][190/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.7332e-02 (8.8630e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:10 - Epoch: [54][200/352]	Time  0.141 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.3750e-01 (8.9243e-02)	Acc@1  93.75 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:11 - Epoch: [54][210/352]	Time  0.141 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.1511e-02 (8.8998e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:13 - Epoch: [54][220/352]	Time  0.146 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.8593e-02 (8.8179e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:14 - Epoch: [54][230/352]	Time  0.141 ( 0.149)	Data  0.002 ( 0.003)	Loss 4.7239e-02 (8.7800e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:16 - Epoch: [54][240/352]	Time  0.155 ( 0.149)	Data  0.002 ( 0.003)	Loss 5.9205e-02 (8.7951e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:17 - Epoch: [54][250/352]	Time  0.151 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1877e-01 (8.7775e-02)	Acc@1  95.31 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:18 - Epoch: [54][260/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1244e-01 (8.7587e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:20 - Epoch: [54][270/352]	Time  0.167 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.8660e-02 (8.7504e-02)	Acc@1  95.31 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:22 - Epoch: [54][280/352]	Time  0.142 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.7983e-02 (8.7280e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:23 - Epoch: [54][290/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.003)	Loss 7.0846e-02 (8.7091e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:24 - Epoch: [54][300/352]	Time  0.145 ( 0.149)	Data  0.002 ( 0.003)	Loss 3.0106e-02 (8.6782e-02)	Acc@1  99.22 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:26 - Epoch: [54][310/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.003)	Loss 8.1317e-02 (8.6214e-02)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:27 - Epoch: [54][320/352]	Time  0.146 ( 0.149)	Data  0.002 ( 0.003)	Loss 6.3127e-02 (8.6537e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:29 - Epoch: [54][330/352]	Time  0.141 ( 0.149)	Data  0.002 ( 0.003)	Loss 6.3167e-02 (8.6353e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:30 - Epoch: [54][340/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.003)	Loss 4.7739e-02 (8.6586e-02)	Acc@1  99.22 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:32 - Epoch: [54][350/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.003)	Loss 4.5744e-02 (8.6349e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:32 - Test: [ 0/20]	Time  0.374 ( 0.374)	Loss 3.5958e-01 (3.5958e-01)	Acc@1  89.45 ( 89.45)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:41:33 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.7500e-01 (3.7851e-01)	Acc@1  89.84 ( 88.96)	Acc@5  98.83 ( 99.47)
07-Mar-22 03:41:34 -  * Acc@1 89.240 Acc@5 99.440
07-Mar-22 03:41:34 - Best acc at epoch 54: 89.63999938964844
07-Mar-22 03:41:35 - Epoch: [55][  0/352]	Time  0.395 ( 0.395)	Data  0.231 ( 0.231)	Loss 1.2917e-01 (1.2917e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:41:36 - Epoch: [55][ 10/352]	Time  0.170 ( 0.184)	Data  0.002 ( 0.023)	Loss 3.8366e-02 (8.1557e-02)	Acc@1  99.22 ( 97.37)	Acc@5 100.00 (100.00)
07-Mar-22 03:41:38 - Epoch: [55][ 20/352]	Time  0.169 ( 0.176)	Data  0.003 ( 0.013)	Loss 4.2093e-02 (7.8293e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 (100.00)
07-Mar-22 03:41:40 - Epoch: [55][ 30/352]	Time  0.172 ( 0.174)	Data  0.002 ( 0.010)	Loss 1.0543e-01 (8.5296e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:41:42 - Epoch: [55][ 40/352]	Time  0.168 ( 0.173)	Data  0.003 ( 0.008)	Loss 7.5585e-02 (8.6205e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:41:43 - Epoch: [55][ 50/352]	Time  0.166 ( 0.172)	Data  0.002 ( 0.007)	Loss 7.6778e-02 (8.4856e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:41:45 - Epoch: [55][ 60/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.006)	Loss 9.4611e-02 (8.4181e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:41:47 - Epoch: [55][ 70/352]	Time  0.165 ( 0.171)	Data  0.002 ( 0.006)	Loss 6.7858e-02 (8.2706e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:41:48 - Epoch: [55][ 80/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.005)	Loss 8.5549e-02 (8.3128e-02)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:41:50 - Epoch: [55][ 90/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.005)	Loss 9.4247e-02 (8.1868e-02)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:41:52 - Epoch: [55][100/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.005)	Loss 2.2856e-02 (7.9769e-02)	Acc@1 100.00 ( 97.28)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:41:53 - Epoch: [55][110/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.004)	Loss 8.4004e-02 (8.0711e-02)	Acc@1  97.66 ( 97.27)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:55 - Epoch: [55][120/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.004)	Loss 5.9343e-02 (8.1171e-02)	Acc@1  99.22 ( 97.26)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:41:57 - Epoch: [55][130/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.0940e-01 (8.0882e-02)	Acc@1  96.09 ( 97.24)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:41:58 - Epoch: [55][140/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.004)	Loss 8.7129e-02 (8.2195e-02)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:42:00 - Epoch: [55][150/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 4.8901e-02 (8.2962e-02)	Acc@1  99.22 ( 97.13)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:42:02 - Epoch: [55][160/352]	Time  0.141 ( 0.169)	Data  0.002 ( 0.004)	Loss 9.3005e-02 (8.3376e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:03 - Epoch: [55][170/352]	Time  0.141 ( 0.168)	Data  0.002 ( 0.004)	Loss 3.8733e-02 (8.2885e-02)	Acc@1 100.00 ( 97.17)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:05 - Epoch: [55][180/352]	Time  0.169 ( 0.167)	Data  0.003 ( 0.003)	Loss 1.5141e-01 (8.4651e-02)	Acc@1  94.53 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:42:06 - Epoch: [55][190/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.0923e-02 (8.4218e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:42:08 - Epoch: [55][200/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0659e-01 (8.4237e-02)	Acc@1  95.31 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:42:10 - Epoch: [55][210/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.7114e-02 (8.4517e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:11 - Epoch: [55][220/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3699e-01 (8.4520e-02)	Acc@1  95.31 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:13 - Epoch: [55][230/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.3133e-02 (8.4315e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:15 - Epoch: [55][240/352]	Time  0.172 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.1994e-02 (8.3963e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:16 - Epoch: [55][250/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.7265e-02 (8.3589e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:18 - Epoch: [55][260/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.3401e-02 (8.3514e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:20 - Epoch: [55][270/352]	Time  0.144 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.1210e-02 (8.3203e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:21 - Epoch: [55][280/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.5959e-02 (8.2907e-02)	Acc@1  99.22 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:23 - Epoch: [55][290/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.9623e-02 (8.2965e-02)	Acc@1  98.44 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:25 - Epoch: [55][300/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.3326e-02 (8.2757e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:26 - Epoch: [55][310/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.6019e-02 (8.2518e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:28 - Epoch: [55][320/352]	Time  0.171 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1635e-01 (8.3322e-02)	Acc@1  95.31 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:30 - Epoch: [55][330/352]	Time  0.142 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.1406e-02 (8.3310e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:31 - Epoch: [55][340/352]	Time  0.171 ( 0.166)	Data  0.003 ( 0.003)	Loss 1.2002e-01 (8.3391e-02)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:33 - Epoch: [55][350/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.3395e-01 (8.3563e-02)	Acc@1  95.31 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:34 - Test: [ 0/20]	Time  0.371 ( 0.371)	Loss 3.5614e-01 (3.5614e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:42:35 - Test: [10/20]	Time  0.098 ( 0.128)	Loss 4.0222e-01 (3.7908e-01)	Acc@1  87.89 ( 89.49)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:42:36 -  * Acc@1 89.400 Acc@5 99.440
07-Mar-22 03:42:36 - Best acc at epoch 55: 89.63999938964844
07-Mar-22 03:42:36 - Epoch: [56][  0/352]	Time  0.413 ( 0.413)	Data  0.245 ( 0.245)	Loss 9.6196e-02 (9.6196e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:42:38 - Epoch: [56][ 10/352]	Time  0.167 ( 0.187)	Data  0.002 ( 0.024)	Loss 9.3693e-02 (1.0394e-01)	Acc@1  96.88 ( 96.16)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:42:39 - Epoch: [56][ 20/352]	Time  0.168 ( 0.179)	Data  0.002 ( 0.014)	Loss 6.3883e-02 (9.2848e-02)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:42:41 - Epoch: [56][ 30/352]	Time  0.168 ( 0.175)	Data  0.001 ( 0.010)	Loss 1.2724e-01 (9.0532e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:42:43 - Epoch: [56][ 40/352]	Time  0.168 ( 0.173)	Data  0.002 ( 0.008)	Loss 7.7971e-02 (9.0270e-02)	Acc@1  97.66 ( 96.68)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:42:44 - Epoch: [56][ 50/352]	Time  0.167 ( 0.172)	Data  0.002 ( 0.007)	Loss 6.6955e-02 (8.5531e-02)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:42:46 - Epoch: [56][ 60/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.006)	Loss 1.3191e-01 (8.5374e-02)	Acc@1  94.53 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:48 - Epoch: [56][ 70/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.006)	Loss 8.0695e-02 (8.5585e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:42:49 - Epoch: [56][ 80/352]	Time  0.170 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.1122e-01 (8.4384e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:42:51 - Epoch: [56][ 90/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.005)	Loss 9.1774e-02 (8.4285e-02)	Acc@1  94.53 ( 96.89)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:42:53 - Epoch: [56][100/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.005)	Loss 8.3277e-02 (8.4559e-02)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:42:54 - Epoch: [56][110/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 8.0242e-02 (8.4926e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:42:56 - Epoch: [56][120/352]	Time  0.168 ( 0.169)	Data  0.001 ( 0.004)	Loss 4.9877e-02 (8.4849e-02)	Acc@1  99.22 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:42:58 - Epoch: [56][130/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.4518e-01 (8.4787e-02)	Acc@1  94.53 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:42:59 - Epoch: [56][140/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 9.4457e-02 (8.5281e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:01 - Epoch: [56][150/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.9919e-02 (8.4341e-02)	Acc@1  98.44 ( 96.97)	Acc@5  99.22 ( 99.97)
07-Mar-22 03:43:03 - Epoch: [56][160/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.4349e-02 (8.3539e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:04 - Epoch: [56][170/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.7365e-02 (8.3458e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:06 - Epoch: [56][180/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.4241e-02 (8.3938e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:08 - Epoch: [56][190/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.1094e-02 (8.3951e-02)	Acc@1  99.22 ( 97.05)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:09 - Epoch: [56][200/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1535e-01 (8.4959e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:11 - Epoch: [56][210/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0290e-01 (8.5347e-02)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:13 - Epoch: [56][220/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.2212e-02 (8.6094e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:14 - Epoch: [56][230/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.6468e-02 (8.5665e-02)	Acc@1 100.00 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:16 - Epoch: [56][240/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.3049e-01 (8.5877e-02)	Acc@1  93.75 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:18 - Epoch: [56][250/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.9820e-02 (8.5486e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:19 - Epoch: [56][260/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.0366e-02 (8.5110e-02)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:21 - Epoch: [56][270/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.4041e-02 (8.4729e-02)	Acc@1  99.22 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:23 - Epoch: [56][280/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.2667e-02 (8.4421e-02)	Acc@1  95.31 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:24 - Epoch: [56][290/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.3240e-02 (8.5145e-02)	Acc@1 100.00 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:26 - Epoch: [56][300/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.5727e-02 (8.5154e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:28 - Epoch: [56][310/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.6204e-02 (8.4887e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:29 - Epoch: [56][320/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2444e-01 (8.4633e-02)	Acc@1  94.53 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:31 - Epoch: [56][330/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.8948e-02 (8.4355e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:33 - Epoch: [56][340/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0538e-01 (8.4204e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:34 - Epoch: [56][350/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.6226e-02 (8.4412e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:43:35 - Test: [ 0/20]	Time  0.377 ( 0.377)	Loss 3.2559e-01 (3.2559e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:43:36 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 3.6855e-01 (3.9161e-01)	Acc@1  88.67 ( 88.99)	Acc@5  99.61 ( 99.57)
07-Mar-22 03:43:37 -  * Acc@1 89.320 Acc@5 99.480
07-Mar-22 03:43:37 - Best acc at epoch 56: 89.63999938964844
07-Mar-22 03:43:38 - Epoch: [57][  0/352]	Time  0.363 ( 0.363)	Data  0.228 ( 0.228)	Loss 8.4239e-02 (8.4239e-02)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:39 - Epoch: [57][ 10/352]	Time  0.143 ( 0.161)	Data  0.002 ( 0.022)	Loss 7.9371e-02 (8.6930e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:40 - Epoch: [57][ 20/352]	Time  0.143 ( 0.152)	Data  0.001 ( 0.012)	Loss 8.9972e-02 (8.4403e-02)	Acc@1  96.09 ( 97.17)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:42 - Epoch: [57][ 30/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.009)	Loss 4.8206e-02 (8.0182e-02)	Acc@1  99.22 ( 97.20)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:43 - Epoch: [57][ 40/352]	Time  0.143 ( 0.151)	Data  0.001 ( 0.007)	Loss 8.6664e-02 (8.2455e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:45 - Epoch: [57][ 50/352]	Time  0.165 ( 0.151)	Data  0.002 ( 0.006)	Loss 1.1864e-01 (8.3400e-02)	Acc@1  94.53 ( 97.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:46 - Epoch: [57][ 60/352]	Time  0.142 ( 0.152)	Data  0.001 ( 0.005)	Loss 3.6889e-02 (8.0619e-02)	Acc@1  99.22 ( 97.18)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:48 - Epoch: [57][ 70/352]	Time  0.153 ( 0.151)	Data  0.002 ( 0.005)	Loss 6.2295e-02 (7.9894e-02)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:49 - Epoch: [57][ 80/352]	Time  0.153 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0604e-01 (8.0341e-02)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:51 - Epoch: [57][ 90/352]	Time  0.143 ( 0.150)	Data  0.001 ( 0.004)	Loss 1.0770e-01 (8.2732e-02)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:52 - Epoch: [57][100/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.004)	Loss 8.6804e-02 (8.1740e-02)	Acc@1  96.09 ( 97.18)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:54 - Epoch: [57][110/352]	Time  0.163 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.1220e-01 (8.2069e-02)	Acc@1  95.31 ( 97.16)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:55 - Epoch: [57][120/352]	Time  0.165 ( 0.150)	Data  0.002 ( 0.004)	Loss 1.0990e-01 (8.1099e-02)	Acc@1  96.09 ( 97.18)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:57 - Epoch: [57][130/352]	Time  0.161 ( 0.150)	Data  0.002 ( 0.003)	Loss 5.7702e-02 (8.1076e-02)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 (100.00)
07-Mar-22 03:43:58 - Epoch: [57][140/352]	Time  0.150 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.6688e-02 (8.1112e-02)	Acc@1  95.31 ( 97.15)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:00 - Epoch: [57][150/352]	Time  0.151 ( 0.150)	Data  0.002 ( 0.003)	Loss 6.5663e-02 (8.1152e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:01 - Epoch: [57][160/352]	Time  0.142 ( 0.150)	Data  0.001 ( 0.003)	Loss 9.6075e-02 (8.2301e-02)	Acc@1  96.09 ( 97.11)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:03 - Epoch: [57][170/352]	Time  0.145 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.5899e-02 (8.3195e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:04 - Epoch: [57][180/352]	Time  0.166 ( 0.150)	Data  0.002 ( 0.003)	Loss 7.6123e-02 (8.3509e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:06 - Epoch: [57][190/352]	Time  0.161 ( 0.150)	Data  0.002 ( 0.003)	Loss 9.5959e-02 (8.3908e-02)	Acc@1  95.31 ( 97.02)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:07 - Epoch: [57][200/352]	Time  0.169 ( 0.150)	Data  0.002 ( 0.003)	Loss 6.0327e-02 (8.3282e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:09 - Epoch: [57][210/352]	Time  0.144 ( 0.150)	Data  0.002 ( 0.003)	Loss 1.0044e-01 (8.3582e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:10 - Epoch: [57][220/352]	Time  0.170 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.0827e-01 (8.3383e-02)	Acc@1  95.31 ( 97.06)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:12 - Epoch: [57][230/352]	Time  0.142 ( 0.151)	Data  0.001 ( 0.003)	Loss 6.7141e-02 (8.4132e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:14 - Epoch: [57][240/352]	Time  0.147 ( 0.151)	Data  0.002 ( 0.003)	Loss 1.3125e-01 (8.4491e-02)	Acc@1  94.53 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:15 - Epoch: [57][250/352]	Time  0.168 ( 0.152)	Data  0.002 ( 0.003)	Loss 1.1217e-01 (8.4527e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:17 - Epoch: [57][260/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.0323e-02 (8.4436e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:18 - Epoch: [57][270/352]	Time  0.166 ( 0.152)	Data  0.002 ( 0.003)	Loss 8.0623e-02 (8.4489e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:20 - Epoch: [57][280/352]	Time  0.143 ( 0.152)	Data  0.001 ( 0.003)	Loss 4.1530e-02 (8.4879e-02)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:21 - Epoch: [57][290/352]	Time  0.143 ( 0.152)	Data  0.001 ( 0.003)	Loss 5.4688e-02 (8.4945e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:23 - Epoch: [57][300/352]	Time  0.149 ( 0.152)	Data  0.002 ( 0.002)	Loss 7.0229e-02 (8.4964e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:24 - Epoch: [57][310/352]	Time  0.170 ( 0.152)	Data  0.002 ( 0.002)	Loss 5.8732e-02 (8.5037e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:26 - Epoch: [57][320/352]	Time  0.152 ( 0.152)	Data  0.002 ( 0.002)	Loss 5.5321e-02 (8.4426e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:28 - Epoch: [57][330/352]	Time  0.167 ( 0.153)	Data  0.002 ( 0.002)	Loss 1.6587e-01 (8.5271e-02)	Acc@1  93.75 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:29 - Epoch: [57][340/352]	Time  0.143 ( 0.153)	Data  0.001 ( 0.002)	Loss 6.2025e-02 (8.5730e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:31 - Epoch: [57][350/352]	Time  0.144 ( 0.153)	Data  0.001 ( 0.002)	Loss 8.3140e-02 (8.5487e-02)	Acc@1  95.31 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:31 - Test: [ 0/20]	Time  0.387 ( 0.387)	Loss 2.9739e-01 (2.9739e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:44:32 - Test: [10/20]	Time  0.098 ( 0.128)	Loss 3.6507e-01 (3.7187e-01)	Acc@1  89.84 ( 89.03)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:44:33 -  * Acc@1 89.440 Acc@5 99.500
07-Mar-22 03:44:33 - Best acc at epoch 57: 89.63999938964844
07-Mar-22 03:44:34 - Epoch: [58][  0/352]	Time  0.376 ( 0.376)	Data  0.234 ( 0.234)	Loss 7.6439e-02 (7.6439e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:35 - Epoch: [58][ 10/352]	Time  0.166 ( 0.186)	Data  0.002 ( 0.023)	Loss 4.2739e-02 (8.3057e-02)	Acc@1  99.22 ( 97.23)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:37 - Epoch: [58][ 20/352]	Time  0.168 ( 0.177)	Data  0.002 ( 0.013)	Loss 7.3198e-02 (8.1489e-02)	Acc@1  96.09 ( 97.32)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:39 - Epoch: [58][ 30/352]	Time  0.167 ( 0.174)	Data  0.002 ( 0.010)	Loss 3.7896e-02 (7.9933e-02)	Acc@1  99.22 ( 97.25)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:40 - Epoch: [58][ 40/352]	Time  0.166 ( 0.172)	Data  0.002 ( 0.008)	Loss 6.3300e-02 (8.0221e-02)	Acc@1  97.66 ( 97.39)	Acc@5 100.00 (100.00)
07-Mar-22 03:44:42 - Epoch: [58][ 50/352]	Time  0.164 ( 0.171)	Data  0.002 ( 0.007)	Loss 4.8800e-02 (7.8119e-02)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:44:44 - Epoch: [58][ 60/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.006)	Loss 6.8467e-02 (7.6635e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:45 - Epoch: [58][ 70/352]	Time  0.164 ( 0.170)	Data  0.002 ( 0.005)	Loss 3.8327e-02 (7.7525e-02)	Acc@1  99.22 ( 97.40)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:47 - Epoch: [58][ 80/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.1448e-01 (7.7041e-02)	Acc@1  94.53 ( 97.39)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:49 - Epoch: [58][ 90/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.005)	Loss 7.7076e-02 (8.0421e-02)	Acc@1  96.88 ( 97.26)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:50 - Epoch: [58][100/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 9.8311e-02 (8.1643e-02)	Acc@1  96.09 ( 97.24)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:52 - Epoch: [58][110/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.4753e-02 (8.2783e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:54 - Epoch: [58][120/352]	Time  0.172 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.8015e-02 (8.2767e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:55 - Epoch: [58][130/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.0831e-01 (8.3697e-02)	Acc@1  95.31 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:57 - Epoch: [58][140/352]	Time  0.141 ( 0.167)	Data  0.002 ( 0.004)	Loss 6.0998e-02 (8.4275e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:44:58 - Epoch: [58][150/352]	Time  0.140 ( 0.165)	Data  0.001 ( 0.004)	Loss 8.7111e-02 (8.3994e-02)	Acc@1  96.09 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:00 - Epoch: [58][160/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.3510e-02 (8.3860e-02)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:02 - Epoch: [58][170/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.4555e-01 (8.3707e-02)	Acc@1  95.31 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:03 - Epoch: [58][180/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.0206e-02 (8.3496e-02)	Acc@1  99.22 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:05 - Epoch: [58][190/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.3772e-02 (8.3721e-02)	Acc@1  96.09 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:07 - Epoch: [58][200/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.1938e-02 (8.3752e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:08 - Epoch: [58][210/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 4.3192e-02 (8.3683e-02)	Acc@1  99.22 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:10 - Epoch: [58][220/352]	Time  0.171 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0202e-01 (8.4327e-02)	Acc@1  96.09 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:12 - Epoch: [58][230/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.1193e-01 (8.3890e-02)	Acc@1  96.09 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:13 - Epoch: [58][240/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 4.8441e-02 (8.3442e-02)	Acc@1  99.22 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:15 - Epoch: [58][250/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.2792e-01 (8.3540e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:17 - Epoch: [58][260/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.0375e-02 (8.3129e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:18 - Epoch: [58][270/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.0980e-02 (8.3063e-02)	Acc@1  98.44 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:20 - Epoch: [58][280/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.3043e-02 (8.3494e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:22 - Epoch: [58][290/352]	Time  0.171 ( 0.166)	Data  0.002 ( 0.003)	Loss 4.5785e-02 (8.2694e-02)	Acc@1  98.44 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:23 - Epoch: [58][300/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.6907e-02 (8.2816e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:25 - Epoch: [58][310/352]	Time  0.141 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.1356e-01 (8.2568e-02)	Acc@1  96.09 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:26 - Epoch: [58][320/352]	Time  0.141 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.1029e-01 (8.2632e-02)	Acc@1  96.09 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:28 - Epoch: [58][330/352]	Time  0.141 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.0509e-02 (8.2827e-02)	Acc@1  99.22 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:29 - Epoch: [58][340/352]	Time  0.141 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.3198e-02 (8.2734e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:30 - Epoch: [58][350/352]	Time  0.141 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1106e-01 (8.2506e-02)	Acc@1  96.09 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:31 - Test: [ 0/20]	Time  0.382 ( 0.382)	Loss 3.4496e-01 (3.4496e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:45:32 - Test: [10/20]	Time  0.098 ( 0.127)	Loss 3.8954e-01 (3.7899e-01)	Acc@1  88.28 ( 89.38)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:45:33 -  * Acc@1 89.500 Acc@5 99.480
07-Mar-22 03:45:33 - Best acc at epoch 58: 89.63999938964844
07-Mar-22 03:45:34 - Epoch: [59][  0/352]	Time  0.399 ( 0.399)	Data  0.253 ( 0.253)	Loss 8.9337e-02 (8.9337e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:45:35 - Epoch: [59][ 10/352]	Time  0.141 ( 0.180)	Data  0.002 ( 0.025)	Loss 8.1735e-02 (7.2868e-02)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 (100.00)
07-Mar-22 03:45:37 - Epoch: [59][ 20/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.014)	Loss 1.4429e-01 (8.9885e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 (100.00)
07-Mar-22 03:45:38 - Epoch: [59][ 30/352]	Time  0.151 ( 0.161)	Data  0.002 ( 0.010)	Loss 1.3975e-01 (8.9882e-02)	Acc@1  93.75 ( 97.13)	Acc@5 100.00 (100.00)
07-Mar-22 03:45:40 - Epoch: [59][ 40/352]	Time  0.143 ( 0.157)	Data  0.002 ( 0.008)	Loss 7.5173e-02 (8.7235e-02)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 (100.00)
07-Mar-22 03:45:41 - Epoch: [59][ 50/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.007)	Loss 6.4957e-02 (8.6719e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 (100.00)
07-Mar-22 03:45:43 - Epoch: [59][ 60/352]	Time  0.165 ( 0.160)	Data  0.002 ( 0.006)	Loss 7.6450e-02 (8.8190e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:45:45 - Epoch: [59][ 70/352]	Time  0.165 ( 0.160)	Data  0.002 ( 0.005)	Loss 3.6298e-02 (8.6069e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 (100.00)
07-Mar-22 03:45:46 - Epoch: [59][ 80/352]	Time  0.168 ( 0.161)	Data  0.002 ( 0.005)	Loss 6.3736e-02 (8.7323e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:48 - Epoch: [59][ 90/352]	Time  0.170 ( 0.162)	Data  0.002 ( 0.005)	Loss 9.5387e-02 (8.6948e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:50 - Epoch: [59][100/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.004)	Loss 1.1209e-01 (8.7183e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:51 - Epoch: [59][110/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.004)	Loss 1.0500e-01 (8.8604e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:53 - Epoch: [59][120/352]	Time  0.157 ( 0.162)	Data  0.002 ( 0.004)	Loss 1.2435e-01 (8.8207e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:54 - Epoch: [59][130/352]	Time  0.143 ( 0.161)	Data  0.002 ( 0.004)	Loss 1.5783e-01 (8.7200e-02)	Acc@1  93.75 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:45:56 - Epoch: [59][140/352]	Time  0.154 ( 0.161)	Data  0.002 ( 0.004)	Loss 1.5519e-01 (8.7678e-02)	Acc@1  96.09 ( 97.01)	Acc@5  99.22 ( 99.98)
07-Mar-22 03:45:57 - Epoch: [59][150/352]	Time  0.148 ( 0.160)	Data  0.002 ( 0.004)	Loss 3.1816e-02 (8.7052e-02)	Acc@1  99.22 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:45:59 - Epoch: [59][160/352]	Time  0.161 ( 0.160)	Data  0.002 ( 0.004)	Loss 6.1484e-02 (8.6688e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:01 - Epoch: [59][170/352]	Time  0.200 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.3305e-01 (8.6111e-02)	Acc@1  94.53 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:02 - Epoch: [59][180/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.003)	Loss 6.6588e-02 (8.4916e-02)	Acc@1  99.22 ( 97.05)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:04 - Epoch: [59][190/352]	Time  0.165 ( 0.161)	Data  0.002 ( 0.003)	Loss 7.1924e-02 (8.4732e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:06 - Epoch: [59][200/352]	Time  0.164 ( 0.161)	Data  0.002 ( 0.003)	Loss 8.6056e-02 (8.4098e-02)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:07 - Epoch: [59][210/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 5.3525e-02 (8.4292e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:09 - Epoch: [59][220/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.0644e-02 (8.4838e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:11 - Epoch: [59][230/352]	Time  0.169 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.5927e-01 (8.5724e-02)	Acc@1  93.75 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:12 - Epoch: [59][240/352]	Time  0.168 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.5360e-01 (8.5574e-02)	Acc@1  92.97 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:14 - Epoch: [59][250/352]	Time  0.164 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.2380e-02 (8.5199e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:16 - Epoch: [59][260/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.0833e-01 (8.5331e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:17 - Epoch: [59][270/352]	Time  0.164 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.3372e-02 (8.4989e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:19 - Epoch: [59][280/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.6582e-02 (8.4400e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:21 - Epoch: [59][290/352]	Time  0.170 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.1367e-02 (8.4463e-02)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:22 - Epoch: [59][300/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.8716e-02 (8.4687e-02)	Acc@1  95.31 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:24 - Epoch: [59][310/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.8225e-02 (8.4420e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:26 - Epoch: [59][320/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.7161e-02 (8.4993e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:27 - Epoch: [59][330/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 5.8593e-02 (8.5272e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:29 - Epoch: [59][340/352]	Time  0.166 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0701e-01 (8.5226e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:31 - Epoch: [59][350/352]	Time  0.165 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.3300e-01 (8.5076e-02)	Acc@1  94.53 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:31 - Test: [ 0/20]	Time  0.371 ( 0.371)	Loss 3.4384e-01 (3.4384e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:46:32 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 3.6783e-01 (3.7897e-01)	Acc@1  89.06 ( 89.20)	Acc@5  99.22 ( 99.43)
07-Mar-22 03:46:33 -  * Acc@1 89.460 Acc@5 99.520
07-Mar-22 03:46:33 - Best acc at epoch 59: 89.63999938964844
07-Mar-22 03:46:34 - Epoch: [60][  0/352]	Time  0.412 ( 0.412)	Data  0.257 ( 0.257)	Loss 1.0046e-01 (1.0046e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 03:46:35 - Epoch: [60][ 10/352]	Time  0.169 ( 0.190)	Data  0.002 ( 0.025)	Loss 8.4458e-02 (7.5850e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:46:37 - Epoch: [60][ 20/352]	Time  0.168 ( 0.179)	Data  0.002 ( 0.014)	Loss 7.9929e-02 (7.7176e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 (100.00)
07-Mar-22 03:46:39 - Epoch: [60][ 30/352]	Time  0.168 ( 0.176)	Data  0.002 ( 0.010)	Loss 1.2088e-01 (8.3393e-02)	Acc@1  94.53 ( 97.15)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:46:40 - Epoch: [60][ 40/352]	Time  0.166 ( 0.174)	Data  0.002 ( 0.008)	Loss 8.3878e-02 (8.7521e-02)	Acc@1  95.31 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:42 - Epoch: [60][ 50/352]	Time  0.169 ( 0.172)	Data  0.002 ( 0.007)	Loss 2.5229e-02 (8.4331e-02)	Acc@1 100.00 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:44 - Epoch: [60][ 60/352]	Time  0.170 ( 0.172)	Data  0.002 ( 0.006)	Loss 9.1108e-02 (8.4417e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:45 - Epoch: [60][ 70/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.006)	Loss 5.9212e-02 (8.5267e-02)	Acc@1  99.22 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:47 - Epoch: [60][ 80/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.005)	Loss 8.8346e-02 (8.5127e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:49 - Epoch: [60][ 90/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.005)	Loss 9.9612e-02 (8.4001e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:50 - Epoch: [60][100/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.005)	Loss 4.2537e-02 (8.4890e-02)	Acc@1  99.22 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:46:52 - Epoch: [60][110/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.8659e-02 (8.5528e-02)	Acc@1  99.22 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:54 - Epoch: [60][120/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 9.2567e-02 (8.6300e-02)	Acc@1  95.31 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:55 - Epoch: [60][130/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.3743e-01 (8.6372e-02)	Acc@1  95.31 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:57 - Epoch: [60][140/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.1739e-01 (8.7123e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:46:59 - Epoch: [60][150/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.0188e-01 (8.7665e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:00 - Epoch: [60][160/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.1859e-01 (8.8299e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:02 - Epoch: [60][170/352]	Time  0.163 ( 0.168)	Data  0.002 ( 0.004)	Loss 4.4494e-02 (8.7624e-02)	Acc@1  99.22 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:04 - Epoch: [60][180/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.0073e-02 (8.8317e-02)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:05 - Epoch: [60][190/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.9424e-02 (8.8226e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:07 - Epoch: [60][200/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.9070e-02 (8.8608e-02)	Acc@1 100.00 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:09 - Epoch: [60][210/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.7792e-02 (8.7628e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:10 - Epoch: [60][220/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.0216e-02 (8.6898e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:12 - Epoch: [60][230/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1308e-01 (8.6865e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:14 - Epoch: [60][240/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.6358e-02 (8.6018e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:15 - Epoch: [60][250/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2509e-01 (8.6037e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:47:17 - Epoch: [60][260/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.1464e-02 (8.6791e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:19 - Epoch: [60][270/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1003e-01 (8.6851e-02)	Acc@1  93.75 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:20 - Epoch: [60][280/352]	Time  0.152 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0777e-01 (8.6277e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:22 - Epoch: [60][290/352]	Time  0.156 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.5667e-02 (8.6180e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:24 - Epoch: [60][300/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.5259e-02 (8.5418e-02)	Acc@1 100.00 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:25 - Epoch: [60][310/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0353e-01 (8.5538e-02)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:27 - Epoch: [60][320/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.1607e-02 (8.5623e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:29 - Epoch: [60][330/352]	Time  0.167 ( 0.167)	Data  0.001 ( 0.003)	Loss 6.7901e-02 (8.5359e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:30 - Epoch: [60][340/352]	Time  0.171 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.1034e-02 (8.5423e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:32 - Epoch: [60][350/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2228e-01 (8.5467e-02)	Acc@1  93.75 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:33 - Test: [ 0/20]	Time  0.385 ( 0.385)	Loss 3.3686e-01 (3.3686e-01)	Acc@1  88.67 ( 88.67)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:47:34 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 3.9041e-01 (3.7315e-01)	Acc@1  88.67 ( 89.20)	Acc@5  99.61 ( 99.64)
07-Mar-22 03:47:35 -  * Acc@1 89.480 Acc@5 99.480
07-Mar-22 03:47:35 - Best acc at epoch 60: 89.63999938964844
07-Mar-22 03:47:35 - Epoch: [61][  0/352]	Time  0.388 ( 0.388)	Data  0.247 ( 0.247)	Loss 2.9410e-02 (2.9410e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
07-Mar-22 03:47:37 - Epoch: [61][ 10/352]	Time  0.172 ( 0.195)	Data  0.004 ( 0.025)	Loss 1.0699e-01 (8.8793e-02)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 (100.00)
07-Mar-22 03:47:38 - Epoch: [61][ 20/352]	Time  0.165 ( 0.183)	Data  0.002 ( 0.014)	Loss 3.5911e-02 (8.8323e-02)	Acc@1  99.22 ( 96.54)	Acc@5 100.00 (100.00)
07-Mar-22 03:47:40 - Epoch: [61][ 30/352]	Time  0.166 ( 0.178)	Data  0.002 ( 0.010)	Loss 5.4419e-02 (8.8693e-02)	Acc@1  98.44 ( 96.62)	Acc@5 100.00 (100.00)
07-Mar-22 03:47:42 - Epoch: [61][ 40/352]	Time  0.164 ( 0.175)	Data  0.002 ( 0.008)	Loss 9.1602e-02 (8.4802e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 (100.00)
07-Mar-22 03:47:43 - Epoch: [61][ 50/352]	Time  0.168 ( 0.174)	Data  0.002 ( 0.007)	Loss 4.8253e-02 (8.4147e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 (100.00)
07-Mar-22 03:47:45 - Epoch: [61][ 60/352]	Time  0.166 ( 0.173)	Data  0.003 ( 0.006)	Loss 6.9372e-02 (8.6436e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:47 - Epoch: [61][ 70/352]	Time  0.165 ( 0.172)	Data  0.002 ( 0.006)	Loss 5.1532e-02 (8.9049e-02)	Acc@1  99.22 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:47:48 - Epoch: [61][ 80/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.005)	Loss 8.2521e-02 (8.8007e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:47:50 - Epoch: [61][ 90/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.4747e-01 (8.9261e-02)	Acc@1  92.97 ( 96.74)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:47:52 - Epoch: [61][100/352]	Time  0.150 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.8884e-01 (9.0137e-02)	Acc@1  93.75 ( 96.75)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:47:53 - Epoch: [61][110/352]	Time  0.142 ( 0.167)	Data  0.002 ( 0.004)	Loss 5.2516e-02 (9.1242e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:55 - Epoch: [61][120/352]	Time  0.158 ( 0.166)	Data  0.002 ( 0.004)	Loss 7.7297e-02 (9.1090e-02)	Acc@1  97.66 ( 96.68)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:47:56 - Epoch: [61][130/352]	Time  0.156 ( 0.165)	Data  0.002 ( 0.004)	Loss 1.0192e-01 (9.0235e-02)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:47:58 - Epoch: [61][140/352]	Time  0.141 ( 0.164)	Data  0.002 ( 0.004)	Loss 1.1721e-01 (8.9787e-02)	Acc@1  95.31 ( 96.76)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:47:59 - Epoch: [61][150/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.004)	Loss 4.6771e-02 (8.9648e-02)	Acc@1  99.22 ( 96.80)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:01 - Epoch: [61][160/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.004)	Loss 1.0321e-01 (8.9814e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:03 - Epoch: [61][170/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.004)	Loss 8.1664e-02 (8.9805e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:04 - Epoch: [61][180/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.8537e-02 (8.9926e-02)	Acc@1  95.31 ( 96.81)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:06 - Epoch: [61][190/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.0448e-02 (8.9588e-02)	Acc@1  99.22 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:08 - Epoch: [61][200/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.6034e-02 (8.9000e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:09 - Epoch: [61][210/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.8082e-02 (8.8424e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:11 - Epoch: [61][220/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.8298e-02 (8.7980e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:13 - Epoch: [61][230/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 8.0019e-02 (8.7816e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:14 - Epoch: [61][240/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.6764e-02 (8.7386e-02)	Acc@1  99.22 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:16 - Epoch: [61][250/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.9365e-02 (8.7513e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:18 - Epoch: [61][260/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.5919e-01 (8.7913e-02)	Acc@1  95.31 ( 96.88)	Acc@5  99.22 ( 99.98)
07-Mar-22 03:48:19 - Epoch: [61][270/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.9604e-02 (8.7887e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:21 - Epoch: [61][280/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.0930e-01 (8.7923e-02)	Acc@1  95.31 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:23 - Epoch: [61][290/352]	Time  0.144 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.9820e-02 (8.7683e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:24 - Epoch: [61][300/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 9.5581e-02 (8.7368e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:26 - Epoch: [61][310/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.1140e-01 (8.7383e-02)	Acc@1  95.31 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:27 - Epoch: [61][320/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 9.2964e-02 (8.7134e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:29 - Epoch: [61][330/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.8161e-02 (8.7108e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:31 - Epoch: [61][340/352]	Time  0.170 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.1446e-01 (8.6622e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:32 - Epoch: [61][350/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 3.6857e-02 (8.6555e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:33 - Test: [ 0/20]	Time  0.381 ( 0.381)	Loss 3.1902e-01 (3.1902e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:48:34 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 3.7378e-01 (3.7950e-01)	Acc@1  89.06 ( 89.56)	Acc@5  99.61 ( 99.43)
07-Mar-22 03:48:35 -  * Acc@1 89.660 Acc@5 99.480
07-Mar-22 03:48:35 - Best acc at epoch 61: 89.65999603271484
07-Mar-22 03:48:36 - Epoch: [62][  0/352]	Time  0.430 ( 0.430)	Data  0.240 ( 0.240)	Loss 7.8226e-02 (7.8226e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:48:37 - Epoch: [62][ 10/352]	Time  0.141 ( 0.174)	Data  0.002 ( 0.024)	Loss 5.7299e-02 (8.3411e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:48:39 - Epoch: [62][ 20/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.013)	Loss 7.4695e-02 (7.9693e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 03:48:41 - Epoch: [62][ 30/352]	Time  0.163 ( 0.170)	Data  0.002 ( 0.010)	Loss 1.4897e-01 (8.4128e-02)	Acc@1  95.31 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:48:42 - Epoch: [62][ 40/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.008)	Loss 1.6813e-01 (8.6818e-02)	Acc@1  92.97 ( 96.82)	Acc@5 100.00 (100.00)
07-Mar-22 03:48:44 - Epoch: [62][ 50/352]	Time  0.152 ( 0.166)	Data  0.002 ( 0.007)	Loss 1.0194e-01 (8.7921e-02)	Acc@1  96.09 ( 96.74)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:48:45 - Epoch: [62][ 60/352]	Time  0.146 ( 0.163)	Data  0.002 ( 0.006)	Loss 8.6483e-02 (8.6630e-02)	Acc@1  97.66 ( 96.77)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:48:47 - Epoch: [62][ 70/352]	Time  0.153 ( 0.162)	Data  0.002 ( 0.005)	Loss 3.4174e-02 (8.5349e-02)	Acc@1  99.22 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:48:48 - Epoch: [62][ 80/352]	Time  0.143 ( 0.160)	Data  0.002 ( 0.005)	Loss 8.2691e-02 (8.4839e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:48:50 - Epoch: [62][ 90/352]	Time  0.142 ( 0.159)	Data  0.001 ( 0.004)	Loss 5.9761e-02 (8.4247e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:48:51 - Epoch: [62][100/352]	Time  0.140 ( 0.158)	Data  0.001 ( 0.004)	Loss 5.4621e-02 (8.4497e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:48:53 - Epoch: [62][110/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.004)	Loss 8.8519e-02 (8.4206e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:48:55 - Epoch: [62][120/352]	Time  0.167 ( 0.159)	Data  0.002 ( 0.004)	Loss 6.8997e-02 (8.4106e-02)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:48:56 - Epoch: [62][130/352]	Time  0.150 ( 0.159)	Data  0.002 ( 0.004)	Loss 1.3299e-01 (8.4704e-02)	Acc@1  95.31 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:48:58 - Epoch: [62][140/352]	Time  0.150 ( 0.159)	Data  0.002 ( 0.004)	Loss 5.4670e-02 (8.4679e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:48:59 - Epoch: [62][150/352]	Time  0.165 ( 0.159)	Data  0.002 ( 0.004)	Loss 6.6207e-02 (8.4842e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:49:01 - Epoch: [62][160/352]	Time  0.163 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.1297e-01 (8.5293e-02)	Acc@1  94.53 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:03 - Epoch: [62][170/352]	Time  0.156 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.0164e-01 (8.5340e-02)	Acc@1  94.53 ( 96.91)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:04 - Epoch: [62][180/352]	Time  0.151 ( 0.159)	Data  0.001 ( 0.003)	Loss 4.2068e-02 (8.5523e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:06 - Epoch: [62][190/352]	Time  0.149 ( 0.158)	Data  0.002 ( 0.003)	Loss 3.2430e-02 (8.5522e-02)	Acc@1  99.22 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:07 - Epoch: [62][200/352]	Time  0.146 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.1139e-01 (8.6708e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:09 - Epoch: [62][210/352]	Time  0.142 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.4767e-01 (8.7380e-02)	Acc@1  92.19 ( 96.83)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:10 - Epoch: [62][220/352]	Time  0.156 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.3440e-02 (8.7569e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:12 - Epoch: [62][230/352]	Time  0.156 ( 0.157)	Data  0.002 ( 0.003)	Loss 3.5473e-02 (8.7002e-02)	Acc@1 100.00 ( 96.87)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:13 - Epoch: [62][240/352]	Time  0.148 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.5461e-02 (8.6534e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:15 - Epoch: [62][250/352]	Time  0.143 ( 0.158)	Data  0.001 ( 0.003)	Loss 3.4438e-02 (8.6983e-02)	Acc@1  99.22 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:49:16 - Epoch: [62][260/352]	Time  0.152 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.6534e-02 (8.7450e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:49:18 - Epoch: [62][270/352]	Time  0.155 ( 0.157)	Data  0.002 ( 0.003)	Loss 5.2351e-02 (8.7934e-02)	Acc@1  98.44 ( 96.83)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:49:19 - Epoch: [62][280/352]	Time  0.141 ( 0.157)	Data  0.002 ( 0.003)	Loss 9.5228e-02 (8.8168e-02)	Acc@1  96.88 ( 96.83)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:49:21 - Epoch: [62][290/352]	Time  0.154 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.8348e-02 (8.7949e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:49:22 - Epoch: [62][300/352]	Time  0.146 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.1520e-01 (8.8351e-02)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:49:24 - Epoch: [62][310/352]	Time  0.185 ( 0.156)	Data  0.002 ( 0.003)	Loss 6.6943e-02 (8.8391e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:49:26 - Epoch: [62][320/352]	Time  0.165 ( 0.157)	Data  0.002 ( 0.003)	Loss 6.4490e-02 (8.8514e-02)	Acc@1  96.88 ( 96.81)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:49:27 - Epoch: [62][330/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.1175e-01 (8.9212e-02)	Acc@1  95.31 ( 96.78)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:49:29 - Epoch: [62][340/352]	Time  0.166 ( 0.157)	Data  0.002 ( 0.003)	Loss 5.9667e-02 (8.8790e-02)	Acc@1  98.44 ( 96.80)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:49:31 - Epoch: [62][350/352]	Time  0.166 ( 0.158)	Data  0.002 ( 0.003)	Loss 6.1794e-02 (8.8428e-02)	Acc@1  98.44 ( 96.82)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:49:31 - Test: [ 0/20]	Time  0.383 ( 0.383)	Loss 3.3944e-01 (3.3944e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:49:32 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 4.2233e-01 (3.8269e-01)	Acc@1  88.67 ( 89.20)	Acc@5  99.61 ( 99.54)
07-Mar-22 03:49:33 -  * Acc@1 89.520 Acc@5 99.540
07-Mar-22 03:49:33 - Best acc at epoch 62: 89.65999603271484
07-Mar-22 03:49:34 - Epoch: [63][  0/352]	Time  0.425 ( 0.425)	Data  0.273 ( 0.273)	Loss 7.8258e-02 (7.8258e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:35 - Epoch: [63][ 10/352]	Time  0.149 ( 0.189)	Data  0.001 ( 0.027)	Loss 5.5091e-02 (7.8124e-02)	Acc@1  99.22 ( 97.23)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:37 - Epoch: [63][ 20/352]	Time  0.168 ( 0.179)	Data  0.003 ( 0.015)	Loss 8.9925e-02 (8.3298e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:39 - Epoch: [63][ 30/352]	Time  0.166 ( 0.175)	Data  0.002 ( 0.011)	Loss 4.1740e-02 (7.9081e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:40 - Epoch: [63][ 40/352]	Time  0.167 ( 0.173)	Data  0.002 ( 0.009)	Loss 7.2576e-02 (7.8375e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:42 - Epoch: [63][ 50/352]	Time  0.167 ( 0.172)	Data  0.002 ( 0.007)	Loss 6.7727e-02 (7.5144e-02)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:44 - Epoch: [63][ 60/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.006)	Loss 6.6023e-02 (7.9403e-02)	Acc@1  97.66 ( 97.34)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:45 - Epoch: [63][ 70/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.006)	Loss 5.5866e-02 (7.7937e-02)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:47 - Epoch: [63][ 80/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.005)	Loss 3.8017e-02 (7.8951e-02)	Acc@1 100.00 ( 97.38)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:49 - Epoch: [63][ 90/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.005)	Loss 7.9699e-02 (8.1064e-02)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:50 - Epoch: [63][100/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.005)	Loss 4.7405e-02 (8.0268e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:52 - Epoch: [63][110/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.1222e-02 (8.0154e-02)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:54 - Epoch: [63][120/352]	Time  0.162 ( 0.169)	Data  0.003 ( 0.004)	Loss 6.3308e-02 (8.0427e-02)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:55 - Epoch: [63][130/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.9914e-02 (7.9875e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:57 - Epoch: [63][140/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.9745e-01 (8.1072e-02)	Acc@1  94.53 ( 97.21)	Acc@5 100.00 (100.00)
07-Mar-22 03:49:59 - Epoch: [63][150/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.4746e-02 (8.0426e-02)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:00 - Epoch: [63][160/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.2787e-02 (8.1583e-02)	Acc@1  99.22 ( 97.20)	Acc@5 100.00 (100.00)
07-Mar-22 03:50:02 - Epoch: [63][170/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.0619e-01 (8.1791e-02)	Acc@1  96.88 ( 97.17)	Acc@5 100.00 (100.00)
07-Mar-22 03:50:04 - Epoch: [63][180/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.7535e-02 (8.1499e-02)	Acc@1  99.22 ( 97.16)	Acc@5 100.00 (100.00)
07-Mar-22 03:50:05 - Epoch: [63][190/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.3922e-02 (8.1161e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 (100.00)
07-Mar-22 03:50:07 - Epoch: [63][200/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4342e-01 (8.2626e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:09 - Epoch: [63][210/352]	Time  0.163 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.9920e-02 (8.3048e-02)	Acc@1  99.22 ( 97.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:10 - Epoch: [63][220/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.9072e-02 (8.3615e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:12 - Epoch: [63][230/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.8414e-02 (8.3676e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:14 - Epoch: [63][240/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.1410e-02 (8.3773e-02)	Acc@1  99.22 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:15 - Epoch: [63][250/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2233e-01 (8.3988e-02)	Acc@1  95.31 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:17 - Epoch: [63][260/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.9058e-02 (8.4108e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:19 - Epoch: [63][270/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.2077e-02 (8.4124e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:20 - Epoch: [63][280/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0342e-01 (8.4733e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:22 - Epoch: [63][290/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.1812e-02 (8.4844e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:24 - Epoch: [63][300/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.3401e-01 (8.4570e-02)	Acc@1  94.53 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:25 - Epoch: [63][310/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.7668e-02 (8.4196e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:27 - Epoch: [63][320/352]	Time  0.163 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.5354e-02 (8.4156e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:29 - Epoch: [63][330/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.7641e-02 (8.4061e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:31 - Epoch: [63][340/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1942e-01 (8.3926e-02)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:32 - Epoch: [63][350/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.3834e-02 (8.3386e-02)	Acc@1 100.00 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:33 - Test: [ 0/20]	Time  0.365 ( 0.365)	Loss 3.1984e-01 (3.1984e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
07-Mar-22 03:50:34 - Test: [10/20]	Time  0.098 ( 0.126)	Loss 3.5481e-01 (3.7499e-01)	Acc@1  90.23 ( 88.99)	Acc@5  99.22 ( 99.72)
07-Mar-22 03:50:35 -  * Acc@1 89.060 Acc@5 99.600
07-Mar-22 03:50:35 - Best acc at epoch 63: 89.65999603271484
07-Mar-22 03:50:35 - Epoch: [64][  0/352]	Time  0.389 ( 0.389)	Data  0.239 ( 0.239)	Loss 1.0668e-01 (1.0668e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 03:50:37 - Epoch: [64][ 10/352]	Time  0.167 ( 0.182)	Data  0.002 ( 0.024)	Loss 4.6255e-02 (9.3597e-02)	Acc@1  98.44 ( 96.66)	Acc@5 100.00 (100.00)
07-Mar-22 03:50:39 - Epoch: [64][ 20/352]	Time  0.165 ( 0.174)	Data  0.002 ( 0.013)	Loss 7.7694e-02 (8.1532e-02)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 (100.00)
07-Mar-22 03:50:40 - Epoch: [64][ 30/352]	Time  0.164 ( 0.169)	Data  0.003 ( 0.010)	Loss 3.4496e-02 (7.9594e-02)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 (100.00)
07-Mar-22 03:50:42 - Epoch: [64][ 40/352]	Time  0.203 ( 0.170)	Data  0.003 ( 0.008)	Loss 1.0896e-01 (8.2127e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 (100.00)
07-Mar-22 03:50:44 - Epoch: [64][ 50/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.007)	Loss 8.2703e-02 (8.3746e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 (100.00)
07-Mar-22 03:50:45 - Epoch: [64][ 60/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.006)	Loss 1.0765e-01 (8.5030e-02)	Acc@1  96.09 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:47 - Epoch: [64][ 70/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.006)	Loss 6.5784e-02 (8.8180e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:49 - Epoch: [64][ 80/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.005)	Loss 6.0296e-02 (8.7086e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:50 - Epoch: [64][ 90/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.005)	Loss 8.7248e-02 (8.6009e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:52 - Epoch: [64][100/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.005)	Loss 8.1674e-02 (8.8313e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:54 - Epoch: [64][110/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.2312e-02 (8.7469e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:55 - Epoch: [64][120/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.9141e-02 (8.5804e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:57 - Epoch: [64][130/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.2187e-02 (8.6348e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:50:59 - Epoch: [64][140/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.2077e-02 (8.6297e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:00 - Epoch: [64][150/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.6680e-02 (8.5337e-02)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:02 - Epoch: [64][160/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.0554e-02 (8.5192e-02)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:04 - Epoch: [64][170/352]	Time  0.167 ( 0.168)	Data  0.001 ( 0.004)	Loss 8.6443e-02 (8.5892e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:05 - Epoch: [64][180/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0999e-01 (8.5071e-02)	Acc@1  93.75 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:07 - Epoch: [64][190/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.2690e-02 (8.4780e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:09 - Epoch: [64][200/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2581e-01 (8.4353e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:10 - Epoch: [64][210/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0304e-01 (8.4617e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:12 - Epoch: [64][220/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.3437e-02 (8.4513e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:14 - Epoch: [64][230/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.8464e-02 (8.4476e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:15 - Epoch: [64][240/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.3487e-02 (8.4140e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:17 - Epoch: [64][250/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2542e-01 (8.4088e-02)	Acc@1  94.53 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:19 - Epoch: [64][260/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0439e-01 (8.4052e-02)	Acc@1  94.53 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:20 - Epoch: [64][270/352]	Time  0.181 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.2348e-01 (8.4390e-02)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:22 - Epoch: [64][280/352]	Time  0.173 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.5571e-01 (8.4362e-02)	Acc@1  95.31 ( 96.94)	Acc@5  99.22 ( 99.99)
07-Mar-22 03:51:24 - Epoch: [64][290/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.7092e-02 (8.4316e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:25 - Epoch: [64][300/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0470e-01 (8.4616e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:27 - Epoch: [64][310/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.0374e-02 (8.4408e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:29 - Epoch: [64][320/352]	Time  0.163 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.5321e-02 (8.4698e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:30 - Epoch: [64][330/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.2827e-02 (8.4097e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:32 - Epoch: [64][340/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.7752e-02 (8.3846e-02)	Acc@1 100.00 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:34 - Epoch: [64][350/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.5751e-02 (8.3667e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:34 - Test: [ 0/20]	Time  0.380 ( 0.380)	Loss 3.5982e-01 (3.5982e-01)	Acc@1  88.67 ( 88.67)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:51:35 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.6577e-01 (3.7302e-01)	Acc@1  89.45 ( 88.99)	Acc@5  99.22 ( 99.57)
07-Mar-22 03:51:36 -  * Acc@1 89.220 Acc@5 99.540
07-Mar-22 03:51:36 - Best acc at epoch 64: 89.65999603271484
07-Mar-22 03:51:37 - Epoch: [65][  0/352]	Time  0.413 ( 0.413)	Data  0.250 ( 0.250)	Loss 6.3097e-02 (6.3097e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 03:51:38 - Epoch: [65][ 10/352]	Time  0.173 ( 0.185)	Data  0.002 ( 0.024)	Loss 7.3698e-02 (8.6499e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:51:40 - Epoch: [65][ 20/352]	Time  0.167 ( 0.176)	Data  0.002 ( 0.014)	Loss 9.5904e-02 (9.1987e-02)	Acc@1  95.31 ( 96.84)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:51:42 - Epoch: [65][ 30/352]	Time  0.167 ( 0.173)	Data  0.002 ( 0.010)	Loss 3.6148e-02 (8.9078e-02)	Acc@1  99.22 ( 96.90)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:51:43 - Epoch: [65][ 40/352]	Time  0.167 ( 0.172)	Data  0.002 ( 0.008)	Loss 5.7413e-02 (9.1903e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:51:45 - Epoch: [65][ 50/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.007)	Loss 5.0300e-02 (8.8038e-02)	Acc@1  99.22 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:51:47 - Epoch: [65][ 60/352]	Time  0.172 ( 0.170)	Data  0.002 ( 0.006)	Loss 6.6205e-02 (8.8104e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:49 - Epoch: [65][ 70/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.006)	Loss 4.5982e-02 (8.7266e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:50 - Epoch: [65][ 80/352]	Time  0.172 ( 0.169)	Data  0.002 ( 0.005)	Loss 7.1169e-02 (8.4855e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:52 - Epoch: [65][ 90/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.005)	Loss 8.5545e-02 (8.3737e-02)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:54 - Epoch: [65][100/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.005)	Loss 7.1165e-02 (8.3865e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:51:55 - Epoch: [65][110/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.2408e-02 (8.2773e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:51:57 - Epoch: [65][120/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.7789e-02 (8.2701e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:51:59 - Epoch: [65][130/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.1182e-01 (8.2859e-02)	Acc@1  95.31 ( 97.17)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:52:00 - Epoch: [65][140/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.0316e-02 (8.1785e-02)	Acc@1  96.09 ( 97.22)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:52:02 - Epoch: [65][150/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.6620e-02 (8.1381e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:52:04 - Epoch: [65][160/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2281e-01 (8.1804e-02)	Acc@1  95.31 ( 97.25)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:52:05 - Epoch: [65][170/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.1945e-01 (8.3322e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:52:07 - Epoch: [65][180/352]	Time  0.184 ( 0.169)	Data  0.003 ( 0.004)	Loss 5.7765e-02 (8.3312e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:52:09 - Epoch: [65][190/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.3768e-01 (8.3773e-02)	Acc@1  92.97 ( 97.15)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:52:10 - Epoch: [65][200/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.003)	Loss 6.1075e-02 (8.3671e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:52:12 - Epoch: [65][210/352]	Time  0.170 ( 0.169)	Data  0.003 ( 0.003)	Loss 8.8481e-02 (8.3995e-02)	Acc@1  95.31 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:14 - Epoch: [65][220/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 9.2961e-02 (8.3993e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:15 - Epoch: [65][230/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0811e-01 (8.4601e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:17 - Epoch: [65][240/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 7.0433e-02 (8.4957e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:19 - Epoch: [65][250/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 7.7480e-02 (8.5084e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:20 - Epoch: [65][260/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 6.3055e-02 (8.4454e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:22 - Epoch: [65][270/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0249e-01 (8.4274e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:24 - Epoch: [65][280/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.003)	Loss 5.2608e-02 (8.4256e-02)	Acc@1  99.22 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:25 - Epoch: [65][290/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 4.6084e-02 (8.4885e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:27 - Epoch: [65][300/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.9790e-02 (8.4739e-02)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:29 - Epoch: [65][310/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.2069e-02 (8.5168e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:31 - Epoch: [65][320/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.2134e-02 (8.4823e-02)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:32 - Epoch: [65][330/352]	Time  0.163 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.7844e-02 (8.4701e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:34 - Epoch: [65][340/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.3674e-01 (8.5064e-02)	Acc@1  95.31 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:35 - Epoch: [65][350/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.0947e-02 (8.5172e-02)	Acc@1 100.00 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:52:36 - Test: [ 0/20]	Time  0.385 ( 0.385)	Loss 3.2795e-01 (3.2795e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:52:37 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.1629e-01 (3.7332e-01)	Acc@1  89.84 ( 89.35)	Acc@5  99.61 ( 99.57)
07-Mar-22 03:52:38 -  * Acc@1 89.700 Acc@5 99.480
07-Mar-22 03:52:38 - Best acc at epoch 65: 89.69999694824219
07-Mar-22 03:52:39 - Epoch: [66][  0/352]	Time  0.404 ( 0.404)	Data  0.248 ( 0.248)	Loss 9.9394e-02 (9.9394e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:52:40 - Epoch: [66][ 10/352]	Time  0.166 ( 0.187)	Data  0.002 ( 0.024)	Loss 8.5936e-02 (1.0215e-01)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 (100.00)
07-Mar-22 03:52:42 - Epoch: [66][ 20/352]	Time  0.169 ( 0.178)	Data  0.002 ( 0.014)	Loss 7.3859e-02 (9.4713e-02)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 (100.00)
07-Mar-22 03:52:43 - Epoch: [66][ 30/352]	Time  0.142 ( 0.171)	Data  0.001 ( 0.010)	Loss 3.6730e-02 (9.1579e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:52:45 - Epoch: [66][ 40/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.008)	Loss 8.9016e-02 (8.9188e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 (100.00)
07-Mar-22 03:52:47 - Epoch: [66][ 50/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.007)	Loss 8.5169e-02 (8.6129e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 03:52:48 - Epoch: [66][ 60/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.006)	Loss 3.2171e-02 (8.4271e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 (100.00)
07-Mar-22 03:52:50 - Epoch: [66][ 70/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.006)	Loss 3.9144e-02 (8.5391e-02)	Acc@1  99.22 ( 96.94)	Acc@5 100.00 (100.00)
07-Mar-22 03:52:52 - Epoch: [66][ 80/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.0133e-01 (8.6663e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:52:53 - Epoch: [66][ 90/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.005)	Loss 5.6889e-02 (8.5725e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:52:55 - Epoch: [66][100/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.005)	Loss 9.6811e-02 (8.6128e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:52:57 - Epoch: [66][110/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.9217e-02 (8.5778e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 (100.00)
07-Mar-22 03:52:58 - Epoch: [66][120/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.7069e-02 (8.5679e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:00 - Epoch: [66][130/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.6706e-02 (8.5291e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:02 - Epoch: [66][140/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.2416e-02 (8.4578e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:03 - Epoch: [66][150/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.2220e-01 (8.4279e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:05 - Epoch: [66][160/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.004)	Loss 5.5325e-02 (8.4827e-02)	Acc@1  99.22 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:07 - Epoch: [66][170/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.004)	Loss 9.7569e-02 (8.4991e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:08 - Epoch: [66][180/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.4994e-02 (8.4826e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:10 - Epoch: [66][190/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.9377e-02 (8.4973e-02)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:12 - Epoch: [66][200/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.1183e-02 (8.4704e-02)	Acc@1  99.22 ( 97.02)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:13 - Epoch: [66][210/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0663e-01 (8.4830e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:15 - Epoch: [66][220/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.4733e-02 (8.4445e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:17 - Epoch: [66][230/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2592e-01 (8.4355e-02)	Acc@1  95.31 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:18 - Epoch: [66][240/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.5604e-02 (8.4744e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:53:20 - Epoch: [66][250/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.6141e-02 (8.5826e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:53:22 - Epoch: [66][260/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.2610e-02 (8.6095e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:53:23 - Epoch: [66][270/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.4824e-02 (8.5801e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:53:25 - Epoch: [66][280/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.4515e-02 (8.6131e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:53:27 - Epoch: [66][290/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.6612e-02 (8.6012e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:53:28 - Epoch: [66][300/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.2324e-02 (8.6554e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:53:30 - Epoch: [66][310/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.7877e-02 (8.6863e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:53:32 - Epoch: [66][320/352]	Time  0.162 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.1382e-02 (8.6353e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:53:33 - Epoch: [66][330/352]	Time  0.141 ( 0.167)	Data  0.001 ( 0.003)	Loss 1.6064e-01 (8.6839e-02)	Acc@1  92.97 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:53:35 - Epoch: [66][340/352]	Time  0.161 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.5876e-02 (8.6684e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:53:37 - Epoch: [66][350/352]	Time  0.158 ( 0.166)	Data  0.002 ( 0.003)	Loss 2.8491e-02 (8.6485e-02)	Acc@1 100.00 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:53:37 - Test: [ 0/20]	Time  0.451 ( 0.451)	Loss 3.5128e-01 (3.5128e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:53:38 - Test: [10/20]	Time  0.098 ( 0.137)	Loss 3.3669e-01 (3.7582e-01)	Acc@1  90.62 ( 89.38)	Acc@5  99.22 ( 99.47)
07-Mar-22 03:53:39 -  * Acc@1 89.560 Acc@5 99.380
07-Mar-22 03:53:39 - Best acc at epoch 66: 89.69999694824219
07-Mar-22 03:53:40 - Epoch: [67][  0/352]	Time  0.385 ( 0.385)	Data  0.232 ( 0.232)	Loss 1.0454e-01 (1.0454e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:41 - Epoch: [67][ 10/352]	Time  0.166 ( 0.179)	Data  0.002 ( 0.023)	Loss 9.8770e-02 (8.0146e-02)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:43 - Epoch: [67][ 20/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.013)	Loss 8.5302e-02 (7.9972e-02)	Acc@1  95.31 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 03:53:45 - Epoch: [67][ 30/352]	Time  0.141 ( 0.168)	Data  0.002 ( 0.009)	Loss 1.2943e-01 (8.8761e-02)	Acc@1  94.53 ( 96.85)	Acc@5  99.22 ( 99.95)
07-Mar-22 03:53:46 - Epoch: [67][ 40/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.008)	Loss 1.0346e-01 (8.6477e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:53:48 - Epoch: [67][ 50/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.006)	Loss 2.2929e-02 (8.3394e-02)	Acc@1 100.00 ( 97.04)	Acc@5 100.00 ( 99.94)
07-Mar-22 03:53:50 - Epoch: [67][ 60/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.006)	Loss 5.9768e-02 (8.3354e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.94)
07-Mar-22 03:53:51 - Epoch: [67][ 70/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.005)	Loss 1.2563e-01 (8.4808e-02)	Acc@1  93.75 ( 96.88)	Acc@5 100.00 ( 99.94)
07-Mar-22 03:53:53 - Epoch: [67][ 80/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.005)	Loss 8.3433e-02 (8.5653e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.95)
07-Mar-22 03:53:55 - Epoch: [67][ 90/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.005)	Loss 1.8447e-01 (8.4633e-02)	Acc@1  92.97 ( 96.93)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:53:56 - Epoch: [67][100/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.004)	Loss 9.5903e-02 (8.5652e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:53:58 - Epoch: [67][110/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.004)	Loss 9.5734e-02 (8.3784e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:54:00 - Epoch: [67][120/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.0704e-01 (8.4046e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:54:01 - Epoch: [67][130/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.4116e-01 (8.4751e-02)	Acc@1  95.31 ( 96.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:54:03 - Epoch: [67][140/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.3413e-01 (8.5538e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:54:05 - Epoch: [67][150/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.004)	Loss 4.7869e-02 (8.5467e-02)	Acc@1  99.22 ( 96.95)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:54:06 - Epoch: [67][160/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.3879e-02 (8.5034e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:08 - Epoch: [67][170/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.2317e-02 (8.4889e-02)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:10 - Epoch: [67][180/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.3450e-02 (8.4795e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:11 - Epoch: [67][190/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.3997e-02 (8.4159e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:13 - Epoch: [67][200/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0031e-01 (8.4584e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:15 - Epoch: [67][210/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0503e-01 (8.4600e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:16 - Epoch: [67][220/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 4.9438e-02 (8.4208e-02)	Acc@1  99.22 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:18 - Epoch: [67][230/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0463e-01 (8.3991e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:20 - Epoch: [67][240/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 9.0981e-02 (8.3606e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:21 - Epoch: [67][250/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 9.1784e-02 (8.3541e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:23 - Epoch: [67][260/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 3.2902e-02 (8.3777e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:25 - Epoch: [67][270/352]	Time  0.190 ( 0.167)	Data  0.002 ( 0.003)	Loss 3.7532e-02 (8.4244e-02)	Acc@1  99.22 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:27 - Epoch: [67][280/352]	Time  0.188 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.7543e-02 (8.4530e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:28 - Epoch: [67][290/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1653e-01 (8.4416e-02)	Acc@1  94.53 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:30 - Epoch: [67][300/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.6861e-01 (8.4490e-02)	Acc@1  93.75 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:32 - Epoch: [67][310/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.7125e-02 (8.4040e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:33 - Epoch: [67][320/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.2525e-02 (8.4382e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:35 - Epoch: [67][330/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.4417e-02 (8.3815e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:37 - Epoch: [67][340/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0708e-01 (8.3758e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:38 - Epoch: [67][350/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.7179e-02 (8.3259e-02)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:39 - Test: [ 0/20]	Time  0.388 ( 0.388)	Loss 3.1363e-01 (3.1363e-01)	Acc@1  91.80 ( 91.80)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:54:40 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.3485e-01 (3.7080e-01)	Acc@1  89.84 ( 89.60)	Acc@5  99.61 ( 99.72)
07-Mar-22 03:54:41 -  * Acc@1 89.540 Acc@5 99.640
07-Mar-22 03:54:41 - Best acc at epoch 67: 89.69999694824219
07-Mar-22 03:54:41 - Epoch: [68][  0/352]	Time  0.382 ( 0.382)	Data  0.227 ( 0.227)	Loss 1.0009e-01 (1.0009e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:54:43 - Epoch: [68][ 10/352]	Time  0.166 ( 0.182)	Data  0.002 ( 0.022)	Loss 8.0124e-02 (8.8962e-02)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 (100.00)
07-Mar-22 03:54:45 - Epoch: [68][ 20/352]	Time  0.167 ( 0.175)	Data  0.002 ( 0.013)	Loss 4.4312e-02 (8.9052e-02)	Acc@1  98.44 ( 96.65)	Acc@5 100.00 (100.00)
07-Mar-22 03:54:46 - Epoch: [68][ 30/352]	Time  0.165 ( 0.172)	Data  0.002 ( 0.009)	Loss 1.1204e-01 (8.5215e-02)	Acc@1  94.53 ( 96.82)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:54:48 - Epoch: [68][ 40/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.007)	Loss 1.0621e-01 (8.6833e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:50 - Epoch: [68][ 50/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.006)	Loss 9.1214e-02 (8.5752e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:54:51 - Epoch: [68][ 60/352]	Time  0.164 ( 0.169)	Data  0.002 ( 0.006)	Loss 5.8791e-02 (8.2973e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:54:53 - Epoch: [68][ 70/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.3117e-01 (8.6955e-02)	Acc@1  93.75 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:54:55 - Epoch: [68][ 80/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.005)	Loss 5.0892e-02 (8.5077e-02)	Acc@1  99.22 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:54:56 - Epoch: [68][ 90/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.2011e-02 (8.5216e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:54:58 - Epoch: [68][100/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 3.5976e-02 (8.5637e-02)	Acc@1  99.22 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:55:00 - Epoch: [68][110/352]	Time  0.168 ( 0.168)	Data  0.001 ( 0.004)	Loss 6.5333e-02 (8.5422e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:55:01 - Epoch: [68][120/352]	Time  0.163 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.2194e-02 (8.3949e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:55:03 - Epoch: [68][130/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.1582e-01 (8.4271e-02)	Acc@1  94.53 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:55:05 - Epoch: [68][140/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2843e-01 (8.3996e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:55:06 - Epoch: [68][150/352]	Time  0.152 ( 0.167)	Data  0.001 ( 0.003)	Loss 8.5721e-02 (8.5381e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:55:08 - Epoch: [68][160/352]	Time  0.158 ( 0.166)	Data  0.002 ( 0.003)	Loss 3.9820e-02 (8.4146e-02)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 (100.00)
07-Mar-22 03:55:09 - Epoch: [68][170/352]	Time  0.162 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.5354e-01 (8.5989e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 (100.00)
07-Mar-22 03:55:11 - Epoch: [68][180/352]	Time  0.154 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.0052e-02 (8.6526e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:55:13 - Epoch: [68][190/352]	Time  0.160 ( 0.165)	Data  0.002 ( 0.003)	Loss 9.1512e-02 (8.6299e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:55:14 - Epoch: [68][200/352]	Time  0.143 ( 0.164)	Data  0.002 ( 0.003)	Loss 5.5301e-02 (8.6812e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:16 - Epoch: [68][210/352]	Time  0.161 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.5188e-02 (8.7779e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:17 - Epoch: [68][220/352]	Time  0.153 ( 0.163)	Data  0.002 ( 0.003)	Loss 5.9411e-02 (8.7787e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:19 - Epoch: [68][230/352]	Time  0.152 ( 0.162)	Data  0.001 ( 0.003)	Loss 6.7349e-02 (8.7173e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:20 - Epoch: [68][240/352]	Time  0.155 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.7809e-02 (8.6938e-02)	Acc@1  99.22 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:22 - Epoch: [68][250/352]	Time  0.160 ( 0.161)	Data  0.002 ( 0.003)	Loss 7.3064e-02 (8.6644e-02)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:23 - Epoch: [68][260/352]	Time  0.142 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.4179e-01 (8.6737e-02)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:24 - Epoch: [68][270/352]	Time  0.157 ( 0.160)	Data  0.002 ( 0.003)	Loss 6.4895e-02 (8.6462e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:55:26 - Epoch: [68][280/352]	Time  0.142 ( 0.160)	Data  0.002 ( 0.003)	Loss 5.0024e-02 (8.6246e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:55:27 - Epoch: [68][290/352]	Time  0.152 ( 0.160)	Data  0.002 ( 0.003)	Loss 9.0246e-02 (8.6283e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:29 - Epoch: [68][300/352]	Time  0.143 ( 0.159)	Data  0.002 ( 0.003)	Loss 8.7147e-02 (8.6399e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:30 - Epoch: [68][310/352]	Time  0.164 ( 0.159)	Data  0.002 ( 0.003)	Loss 6.9863e-02 (8.6723e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:32 - Epoch: [68][320/352]	Time  0.148 ( 0.159)	Data  0.002 ( 0.003)	Loss 3.5162e-02 (8.6708e-02)	Acc@1 100.00 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:33 - Epoch: [68][330/352]	Time  0.155 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.0002e-01 (8.6401e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:35 - Epoch: [68][340/352]	Time  0.141 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.0486e-01 (8.6192e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:36 - Epoch: [68][350/352]	Time  0.159 ( 0.158)	Data  0.002 ( 0.002)	Loss 5.9915e-02 (8.6431e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:37 - Test: [ 0/20]	Time  0.377 ( 0.377)	Loss 3.6337e-01 (3.6337e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:55:38 - Test: [10/20]	Time  0.099 ( 0.124)	Loss 4.4066e-01 (3.8916e-01)	Acc@1  87.89 ( 89.03)	Acc@5  99.22 ( 99.54)
07-Mar-22 03:55:39 -  * Acc@1 89.200 Acc@5 99.520
07-Mar-22 03:55:39 - Best acc at epoch 68: 89.69999694824219
07-Mar-22 03:55:39 - Epoch: [69][  0/352]	Time  0.423 ( 0.423)	Data  0.236 ( 0.236)	Loss 9.2126e-02 (9.2126e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 03:55:41 - Epoch: [69][ 10/352]	Time  0.166 ( 0.189)	Data  0.002 ( 0.023)	Loss 5.9858e-02 (7.3384e-02)	Acc@1  98.44 ( 97.37)	Acc@5 100.00 (100.00)
07-Mar-22 03:55:43 - Epoch: [69][ 20/352]	Time  0.165 ( 0.178)	Data  0.002 ( 0.013)	Loss 1.1409e-01 (8.5918e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:55:44 - Epoch: [69][ 30/352]	Time  0.168 ( 0.174)	Data  0.002 ( 0.010)	Loss 8.3926e-02 (8.4094e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:55:46 - Epoch: [69][ 40/352]	Time  0.165 ( 0.172)	Data  0.002 ( 0.008)	Loss 1.0261e-01 (7.9367e-02)	Acc@1  96.09 ( 97.22)	Acc@5 100.00 (100.00)
07-Mar-22 03:55:47 - Epoch: [69][ 50/352]	Time  0.143 ( 0.167)	Data  0.002 ( 0.007)	Loss 7.8932e-02 (8.0845e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 (100.00)
07-Mar-22 03:55:49 - Epoch: [69][ 60/352]	Time  0.141 ( 0.163)	Data  0.001 ( 0.006)	Loss 1.1145e-01 (8.0492e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 (100.00)
07-Mar-22 03:55:50 - Epoch: [69][ 70/352]	Time  0.143 ( 0.160)	Data  0.002 ( 0.005)	Loss 1.6516e-01 (8.2602e-02)	Acc@1  94.53 ( 97.22)	Acc@5  99.22 ( 99.98)
07-Mar-22 03:55:52 - Epoch: [69][ 80/352]	Time  0.143 ( 0.158)	Data  0.001 ( 0.005)	Loss 1.3148e-01 (8.3193e-02)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:53 - Epoch: [69][ 90/352]	Time  0.142 ( 0.156)	Data  0.002 ( 0.004)	Loss 7.6675e-02 (8.1550e-02)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:55 - Epoch: [69][100/352]	Time  0.164 ( 0.156)	Data  0.002 ( 0.004)	Loss 9.5385e-02 (8.1024e-02)	Acc@1  97.66 ( 97.25)	Acc@5  99.22 ( 99.98)
07-Mar-22 03:55:56 - Epoch: [69][110/352]	Time  0.164 ( 0.157)	Data  0.002 ( 0.004)	Loss 5.7347e-02 (8.1477e-02)	Acc@1  97.66 ( 97.21)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:55:58 - Epoch: [69][120/352]	Time  0.165 ( 0.158)	Data  0.002 ( 0.004)	Loss 4.6154e-02 (7.9668e-02)	Acc@1  99.22 ( 97.28)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:00 - Epoch: [69][130/352]	Time  0.164 ( 0.159)	Data  0.002 ( 0.004)	Loss 4.5891e-02 (8.0089e-02)	Acc@1 100.00 ( 97.27)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:01 - Epoch: [69][140/352]	Time  0.166 ( 0.159)	Data  0.002 ( 0.004)	Loss 7.1869e-02 (8.0279e-02)	Acc@1  98.44 ( 97.27)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:03 - Epoch: [69][150/352]	Time  0.168 ( 0.160)	Data  0.002 ( 0.003)	Loss 7.5215e-02 (7.9983e-02)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:05 - Epoch: [69][160/352]	Time  0.168 ( 0.160)	Data  0.002 ( 0.003)	Loss 9.9477e-02 (7.9932e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:06 - Epoch: [69][170/352]	Time  0.168 ( 0.161)	Data  0.002 ( 0.003)	Loss 4.6633e-02 (8.0368e-02)	Acc@1  98.44 ( 97.26)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:08 - Epoch: [69][180/352]	Time  0.169 ( 0.161)	Data  0.002 ( 0.003)	Loss 5.6481e-02 (8.0095e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:10 - Epoch: [69][190/352]	Time  0.169 ( 0.161)	Data  0.001 ( 0.003)	Loss 6.7210e-02 (7.9692e-02)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:11 - Epoch: [69][200/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.0727e-01 (7.9727e-02)	Acc@1  96.09 ( 97.27)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:13 - Epoch: [69][210/352]	Time  0.170 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.5856e-02 (7.9355e-02)	Acc@1  96.88 ( 97.26)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:15 - Epoch: [69][220/352]	Time  0.168 ( 0.162)	Data  0.002 ( 0.003)	Loss 4.3608e-02 (7.9182e-02)	Acc@1  99.22 ( 97.29)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:17 - Epoch: [69][230/352]	Time  0.168 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.4834e-02 (7.9863e-02)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:18 - Epoch: [69][240/352]	Time  0.168 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.3595e-01 (8.0443e-02)	Acc@1  94.53 ( 97.25)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:20 - Epoch: [69][250/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.7630e-02 (8.0992e-02)	Acc@1  96.88 ( 97.22)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:22 - Epoch: [69][260/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.7859e-02 (8.1934e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:56:23 - Epoch: [69][270/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.1038e-02 (8.1700e-02)	Acc@1  96.09 ( 97.18)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:56:25 - Epoch: [69][280/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.003)	Loss 5.7211e-02 (8.1257e-02)	Acc@1  99.22 ( 97.21)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:56:27 - Epoch: [69][290/352]	Time  0.164 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.1461e-02 (8.0882e-02)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:56:28 - Epoch: [69][300/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.2399e-01 (8.0922e-02)	Acc@1  94.53 ( 97.22)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:30 - Epoch: [69][310/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.4713e-01 (8.1519e-02)	Acc@1  93.75 ( 97.20)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:32 - Epoch: [69][320/352]	Time  0.168 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.3901e-02 (8.1939e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:56:33 - Epoch: [69][330/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.1422e-01 (8.1899e-02)	Acc@1  96.09 ( 97.17)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:56:35 - Epoch: [69][340/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.2787e-02 (8.1865e-02)	Acc@1  95.31 ( 97.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:56:37 - Epoch: [69][350/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 9.5815e-02 (8.2064e-02)	Acc@1  96.09 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:56:37 - Test: [ 0/20]	Time  0.366 ( 0.366)	Loss 3.0430e-01 (3.0430e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:56:38 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.4653e-01 (3.7031e-01)	Acc@1  90.23 ( 89.99)	Acc@5  99.22 ( 99.47)
07-Mar-22 03:56:39 -  * Acc@1 89.780 Acc@5 99.480
07-Mar-22 03:56:39 - Best acc at epoch 69: 89.77999877929688
07-Mar-22 03:56:40 - Epoch: [70][  0/352]	Time  0.422 ( 0.422)	Data  0.253 ( 0.253)	Loss 4.9874e-02 (4.9874e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
07-Mar-22 03:56:41 - Epoch: [70][ 10/352]	Time  0.165 ( 0.190)	Data  0.002 ( 0.025)	Loss 1.0766e-01 (8.5094e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 (100.00)
07-Mar-22 03:56:43 - Epoch: [70][ 20/352]	Time  0.168 ( 0.180)	Data  0.002 ( 0.014)	Loss 4.9360e-02 (8.4476e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:56:45 - Epoch: [70][ 30/352]	Time  0.170 ( 0.176)	Data  0.002 ( 0.010)	Loss 6.1303e-02 (8.2445e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:56:46 - Epoch: [70][ 40/352]	Time  0.168 ( 0.174)	Data  0.002 ( 0.008)	Loss 3.8972e-02 (8.2452e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:48 - Epoch: [70][ 50/352]	Time  0.167 ( 0.173)	Data  0.002 ( 0.007)	Loss 5.0084e-02 (8.0609e-02)	Acc@1  98.44 ( 97.18)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:56:50 - Epoch: [70][ 60/352]	Time  0.169 ( 0.172)	Data  0.002 ( 0.007)	Loss 5.4552e-02 (8.1093e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:56:51 - Epoch: [70][ 70/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.006)	Loss 5.1105e-02 (8.0867e-02)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:56:53 - Epoch: [70][ 80/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.005)	Loss 5.6870e-02 (8.2779e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:56:55 - Epoch: [70][ 90/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.005)	Loss 5.2157e-02 (8.3975e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:56:56 - Epoch: [70][100/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.1747e-01 (8.4560e-02)	Acc@1  94.53 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:56:58 - Epoch: [70][110/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.0311e-01 (8.2562e-02)	Acc@1  95.31 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:00 - Epoch: [70][120/352]	Time  0.172 ( 0.170)	Data  0.003 ( 0.004)	Loss 1.0155e-01 (8.2331e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:01 - Epoch: [70][130/352]	Time  0.172 ( 0.170)	Data  0.003 ( 0.004)	Loss 9.4538e-02 (8.3061e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:03 - Epoch: [70][140/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.004)	Loss 9.2917e-02 (8.3241e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:05 - Epoch: [70][150/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.0893e-01 (8.2352e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:06 - Epoch: [70][160/352]	Time  0.172 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.4772e-01 (8.2488e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 (100.00)
07-Mar-22 03:57:08 - Epoch: [70][170/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.2791e-02 (8.1950e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:10 - Epoch: [70][180/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.2747e-01 (8.2341e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:11 - Epoch: [70][190/352]	Time  0.169 ( 0.169)	Data  0.003 ( 0.004)	Loss 5.2946e-02 (8.3368e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:13 - Epoch: [70][200/352]	Time  0.168 ( 0.169)	Data  0.003 ( 0.004)	Loss 3.9515e-02 (8.3144e-02)	Acc@1  99.22 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:15 - Epoch: [70][210/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 8.1766e-02 (8.2508e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:17 - Epoch: [70][220/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.004)	Loss 4.8903e-02 (8.2538e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:18 - Epoch: [70][230/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0345e-01 (8.2738e-02)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:20 - Epoch: [70][240/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0658e-01 (8.2817e-02)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:22 - Epoch: [70][250/352]	Time  0.172 ( 0.169)	Data  0.002 ( 0.003)	Loss 5.4955e-02 (8.2494e-02)	Acc@1  99.22 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:23 - Epoch: [70][260/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 9.5158e-02 (8.2547e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:25 - Epoch: [70][270/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 7.7729e-02 (8.3656e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:27 - Epoch: [70][280/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 9.0225e-02 (8.3332e-02)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:28 - Epoch: [70][290/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 9.8815e-02 (8.3784e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:30 - Epoch: [70][300/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 6.3236e-02 (8.3791e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:32 - Epoch: [70][310/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 6.1757e-02 (8.3751e-02)	Acc@1  98.44 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:33 - Epoch: [70][320/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.6798e-01 (8.4149e-02)	Acc@1  94.53 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:35 - Epoch: [70][330/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 9.7200e-02 (8.4644e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:37 - Epoch: [70][340/352]	Time  0.143 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.3194e-02 (8.4435e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:38 - Epoch: [70][350/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.3097e-01 (8.4694e-02)	Acc@1  94.53 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:39 - Test: [ 0/20]	Time  0.359 ( 0.359)	Loss 3.3400e-01 (3.3400e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:57:40 - Test: [10/20]	Time  0.098 ( 0.122)	Loss 3.4460e-01 (3.7669e-01)	Acc@1  90.23 ( 89.10)	Acc@5  99.22 ( 99.43)
07-Mar-22 03:57:41 -  * Acc@1 89.100 Acc@5 99.360
07-Mar-22 03:57:41 - Best acc at epoch 70: 89.77999877929688
07-Mar-22 03:57:41 - Epoch: [71][  0/352]	Time  0.398 ( 0.398)	Data  0.242 ( 0.242)	Loss 5.9150e-02 (5.9150e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 03:57:43 - Epoch: [71][ 10/352]	Time  0.163 ( 0.185)	Data  0.002 ( 0.024)	Loss 1.5478e-01 (8.0825e-02)	Acc@1  93.75 ( 97.30)	Acc@5 100.00 (100.00)
07-Mar-22 03:57:45 - Epoch: [71][ 20/352]	Time  0.142 ( 0.172)	Data  0.002 ( 0.013)	Loss 9.6159e-02 (8.6565e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:57:46 - Epoch: [71][ 30/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.010)	Loss 7.5813e-02 (8.5525e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 03:57:48 - Epoch: [71][ 40/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.008)	Loss 9.9944e-02 (8.5166e-02)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 (100.00)
07-Mar-22 03:57:49 - Epoch: [71][ 50/352]	Time  0.169 ( 0.167)	Data  0.003 ( 0.007)	Loss 8.8621e-02 (8.3229e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 (100.00)
07-Mar-22 03:57:51 - Epoch: [71][ 60/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.006)	Loss 7.9613e-02 (8.1961e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 (100.00)
07-Mar-22 03:57:53 - Epoch: [71][ 70/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.006)	Loss 3.3060e-02 (8.3034e-02)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:55 - Epoch: [71][ 80/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.005)	Loss 6.1431e-02 (8.4043e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:56 - Epoch: [71][ 90/352]	Time  0.144 ( 0.166)	Data  0.002 ( 0.005)	Loss 6.7857e-02 (8.4525e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:58 - Epoch: [71][100/352]	Time  0.154 ( 0.165)	Data  0.002 ( 0.005)	Loss 1.3774e-01 (8.4686e-02)	Acc@1  92.97 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:57:59 - Epoch: [71][110/352]	Time  0.143 ( 0.163)	Data  0.002 ( 0.004)	Loss 1.1817e-01 (8.5786e-02)	Acc@1  95.31 ( 96.96)	Acc@5  99.22 ( 99.99)
07-Mar-22 03:58:01 - Epoch: [71][120/352]	Time  0.148 ( 0.162)	Data  0.002 ( 0.004)	Loss 1.8137e-01 (8.5461e-02)	Acc@1  91.41 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:02 - Epoch: [71][130/352]	Time  0.171 ( 0.161)	Data  0.002 ( 0.004)	Loss 1.3427e-01 (8.6015e-02)	Acc@1  95.31 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:04 - Epoch: [71][140/352]	Time  0.141 ( 0.161)	Data  0.002 ( 0.004)	Loss 1.0807e-01 (8.5565e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:05 - Epoch: [71][150/352]	Time  0.141 ( 0.159)	Data  0.002 ( 0.004)	Loss 7.5874e-02 (8.4144e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:06 - Epoch: [71][160/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.004)	Loss 4.0105e-02 (8.3542e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:08 - Epoch: [71][170/352]	Time  0.142 ( 0.157)	Data  0.002 ( 0.003)	Loss 3.1033e-02 (8.3798e-02)	Acc@1  99.22 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:09 - Epoch: [71][180/352]	Time  0.144 ( 0.157)	Data  0.002 ( 0.003)	Loss 9.1738e-02 (8.3402e-02)	Acc@1  96.09 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:11 - Epoch: [71][190/352]	Time  0.143 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.1425e-01 (8.4192e-02)	Acc@1  95.31 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:12 - Epoch: [71][200/352]	Time  0.169 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.0143e-02 (8.4788e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:14 - Epoch: [71][210/352]	Time  0.166 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.0278e-01 (8.4460e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:15 - Epoch: [71][220/352]	Time  0.144 ( 0.155)	Data  0.001 ( 0.003)	Loss 7.1819e-02 (8.4145e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:17 - Epoch: [71][230/352]	Time  0.143 ( 0.155)	Data  0.002 ( 0.003)	Loss 8.2673e-02 (8.4759e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:58:18 - Epoch: [71][240/352]	Time  0.143 ( 0.154)	Data  0.002 ( 0.003)	Loss 1.1186e-01 (8.4598e-02)	Acc@1  95.31 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:58:20 - Epoch: [71][250/352]	Time  0.143 ( 0.154)	Data  0.001 ( 0.003)	Loss 8.7482e-02 (8.5006e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:58:21 - Epoch: [71][260/352]	Time  0.141 ( 0.154)	Data  0.001 ( 0.003)	Loss 1.0965e-01 (8.4893e-02)	Acc@1  93.75 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:22 - Epoch: [71][270/352]	Time  0.144 ( 0.153)	Data  0.002 ( 0.003)	Loss 6.4642e-02 (8.4920e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:24 - Epoch: [71][280/352]	Time  0.147 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.0358e-02 (8.4985e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:25 - Epoch: [71][290/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.003)	Loss 6.1480e-02 (8.5309e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:27 - Epoch: [71][300/352]	Time  0.153 ( 0.153)	Data  0.002 ( 0.003)	Loss 4.5028e-02 (8.5000e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:29 - Epoch: [71][310/352]	Time  0.159 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0628e-01 (8.5003e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:30 - Epoch: [71][320/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.2250e-01 (8.5770e-02)	Acc@1  93.75 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:32 - Epoch: [71][330/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.003)	Loss 7.5285e-02 (8.5581e-02)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:33 - Epoch: [71][340/352]	Time  0.151 ( 0.153)	Data  0.002 ( 0.003)	Loss 1.0003e-01 (8.6281e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:35 - Epoch: [71][350/352]	Time  0.152 ( 0.153)	Data  0.001 ( 0.003)	Loss 1.2320e-01 (8.6122e-02)	Acc@1  94.53 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:35 - Test: [ 0/20]	Time  0.375 ( 0.375)	Loss 3.7210e-01 (3.7210e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.61 ( 99.61)
07-Mar-22 03:58:36 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 3.6241e-01 (3.7895e-01)	Acc@1  89.06 ( 89.03)	Acc@5  99.22 ( 99.50)
07-Mar-22 03:58:37 -  * Acc@1 89.400 Acc@5 99.520
07-Mar-22 03:58:37 - Best acc at epoch 71: 89.77999877929688
07-Mar-22 03:58:38 - Epoch: [72][  0/352]	Time  0.383 ( 0.383)	Data  0.236 ( 0.236)	Loss 2.8907e-02 (2.8907e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
07-Mar-22 03:58:39 - Epoch: [72][ 10/352]	Time  0.143 ( 0.184)	Data  0.002 ( 0.023)	Loss 5.8668e-02 (7.7520e-02)	Acc@1  98.44 ( 97.51)	Acc@5 100.00 ( 99.93)
07-Mar-22 03:58:41 - Epoch: [72][ 20/352]	Time  0.143 ( 0.165)	Data  0.002 ( 0.013)	Loss 7.8561e-02 (8.2796e-02)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:58:42 - Epoch: [72][ 30/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.010)	Loss 9.0805e-02 (9.2171e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:58:44 - Epoch: [72][ 40/352]	Time  0.143 ( 0.160)	Data  0.002 ( 0.008)	Loss 9.6429e-02 (9.0577e-02)	Acc@1  95.31 ( 96.78)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:58:45 - Epoch: [72][ 50/352]	Time  0.142 ( 0.156)	Data  0.002 ( 0.007)	Loss 7.5281e-02 (8.7690e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:58:47 - Epoch: [72][ 60/352]	Time  0.148 ( 0.154)	Data  0.002 ( 0.006)	Loss 3.7761e-02 (8.9518e-02)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:48 - Epoch: [72][ 70/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.005)	Loss 9.5717e-02 (8.8145e-02)	Acc@1  96.09 ( 96.82)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:50 - Epoch: [72][ 80/352]	Time  0.167 ( 0.152)	Data  0.002 ( 0.005)	Loss 7.3096e-02 (8.6109e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:51 - Epoch: [72][ 90/352]	Time  0.165 ( 0.153)	Data  0.002 ( 0.004)	Loss 7.8991e-02 (8.5933e-02)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:53 - Epoch: [72][100/352]	Time  0.165 ( 0.155)	Data  0.002 ( 0.004)	Loss 3.1971e-02 (8.4297e-02)	Acc@1 100.00 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:55 - Epoch: [72][110/352]	Time  0.168 ( 0.156)	Data  0.002 ( 0.004)	Loss 1.4272e-01 (8.5747e-02)	Acc@1  92.97 ( 96.83)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:56 - Epoch: [72][120/352]	Time  0.165 ( 0.157)	Data  0.002 ( 0.004)	Loss 1.0620e-01 (8.5523e-02)	Acc@1  94.53 ( 96.83)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:58:58 - Epoch: [72][130/352]	Time  0.166 ( 0.158)	Data  0.002 ( 0.004)	Loss 4.1918e-02 (8.4552e-02)	Acc@1  98.44 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:59:00 - Epoch: [72][140/352]	Time  0.169 ( 0.158)	Data  0.002 ( 0.004)	Loss 5.2051e-02 (8.3903e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:59:01 - Epoch: [72][150/352]	Time  0.159 ( 0.159)	Data  0.002 ( 0.004)	Loss 6.7478e-02 (8.3118e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:59:03 - Epoch: [72][160/352]	Time  0.169 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.2267e-01 (8.3453e-02)	Acc@1  95.31 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:05 - Epoch: [72][170/352]	Time  0.169 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.3353e-01 (8.3937e-02)	Acc@1  95.31 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:06 - Epoch: [72][180/352]	Time  0.168 ( 0.160)	Data  0.002 ( 0.003)	Loss 8.7584e-02 (8.4131e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:08 - Epoch: [72][190/352]	Time  0.171 ( 0.161)	Data  0.002 ( 0.003)	Loss 5.9639e-02 (8.4615e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:10 - Epoch: [72][200/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.2307e-01 (8.4854e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:11 - Epoch: [72][210/352]	Time  0.167 ( 0.161)	Data  0.003 ( 0.003)	Loss 7.6213e-02 (8.4914e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:13 - Epoch: [72][220/352]	Time  0.168 ( 0.162)	Data  0.002 ( 0.003)	Loss 5.0400e-02 (8.5430e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:15 - Epoch: [72][230/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.0773e-01 (8.5897e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:59:16 - Epoch: [72][240/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.0112e-01 (8.5846e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:59:18 - Epoch: [72][250/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.2223e-01 (8.5512e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:59:20 - Epoch: [72][260/352]	Time  0.168 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.0701e-01 (8.5204e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:59:21 - Epoch: [72][270/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.5183e-02 (8.5200e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:59:23 - Epoch: [72][280/352]	Time  0.168 ( 0.163)	Data  0.003 ( 0.003)	Loss 9.8825e-02 (8.5629e-02)	Acc@1  94.53 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:59:25 - Epoch: [72][290/352]	Time  0.165 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.1575e-02 (8.5980e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:59:26 - Epoch: [72][300/352]	Time  0.172 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.7572e-02 (8.6042e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:59:28 - Epoch: [72][310/352]	Time  0.164 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.0073e-02 (8.5706e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 03:59:30 - Epoch: [72][320/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 9.1263e-02 (8.5367e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:31 - Epoch: [72][330/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 4.9299e-02 (8.4940e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:33 - Epoch: [72][340/352]	Time  0.170 ( 0.163)	Data  0.002 ( 0.003)	Loss 9.3359e-02 (8.4746e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:35 - Epoch: [72][350/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.6199e-02 (8.4843e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:35 - Test: [ 0/20]	Time  0.397 ( 0.397)	Loss 3.2229e-01 (3.2229e-01)	Acc@1  89.45 ( 89.45)	Acc@5  99.22 ( 99.22)
07-Mar-22 03:59:36 - Test: [10/20]	Time  0.099 ( 0.129)	Loss 3.6443e-01 (3.6909e-01)	Acc@1  89.45 ( 89.24)	Acc@5  99.22 ( 99.57)
07-Mar-22 03:59:37 -  * Acc@1 89.460 Acc@5 99.540
07-Mar-22 03:59:37 - Best acc at epoch 72: 89.77999877929688
07-Mar-22 03:59:38 - Epoch: [73][  0/352]	Time  0.380 ( 0.380)	Data  0.231 ( 0.231)	Loss 1.0850e-01 (1.0850e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:40 - Epoch: [73][ 10/352]	Time  0.159 ( 0.183)	Data  0.002 ( 0.023)	Loss 1.1628e-01 (8.3134e-02)	Acc@1  94.53 ( 96.66)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:41 - Epoch: [73][ 20/352]	Time  0.157 ( 0.171)	Data  0.002 ( 0.013)	Loss 1.6332e-01 (8.2934e-02)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:43 - Epoch: [73][ 30/352]	Time  0.150 ( 0.166)	Data  0.002 ( 0.009)	Loss 1.0470e-01 (8.3746e-02)	Acc@1  95.31 ( 96.82)	Acc@5 100.00 (100.00)
07-Mar-22 03:59:44 - Epoch: [73][ 40/352]	Time  0.142 ( 0.161)	Data  0.002 ( 0.008)	Loss 5.3143e-02 (8.3244e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 03:59:46 - Epoch: [73][ 50/352]	Time  0.142 ( 0.161)	Data  0.002 ( 0.006)	Loss 9.7175e-02 (8.6736e-02)	Acc@1  96.09 ( 96.77)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:59:47 - Epoch: [73][ 60/352]	Time  0.155 ( 0.160)	Data  0.002 ( 0.006)	Loss 1.2837e-01 (8.6493e-02)	Acc@1  95.31 ( 96.82)	Acc@5  99.22 ( 99.96)
07-Mar-22 03:59:49 - Epoch: [73][ 70/352]	Time  0.165 ( 0.160)	Data  0.002 ( 0.005)	Loss 9.3747e-02 (8.8183e-02)	Acc@1  96.09 ( 96.76)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:59:50 - Epoch: [73][ 80/352]	Time  0.163 ( 0.161)	Data  0.002 ( 0.005)	Loss 1.7710e-01 (9.1717e-02)	Acc@1  93.75 ( 96.69)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:59:52 - Epoch: [73][ 90/352]	Time  0.149 ( 0.160)	Data  0.002 ( 0.005)	Loss 1.1175e-01 (9.1527e-02)	Acc@1  96.09 ( 96.68)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:59:54 - Epoch: [73][100/352]	Time  0.169 ( 0.159)	Data  0.002 ( 0.004)	Loss 5.0778e-02 (9.1616e-02)	Acc@1  98.44 ( 96.69)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:59:55 - Epoch: [73][110/352]	Time  0.162 ( 0.159)	Data  0.002 ( 0.004)	Loss 2.1132e-02 (9.0519e-02)	Acc@1 100.00 ( 96.73)	Acc@5 100.00 ( 99.96)
07-Mar-22 03:59:57 - Epoch: [73][120/352]	Time  0.167 ( 0.160)	Data  0.002 ( 0.004)	Loss 1.4614e-01 (9.0094e-02)	Acc@1  95.31 ( 96.75)	Acc@5 100.00 ( 99.97)
07-Mar-22 03:59:59 - Epoch: [73][130/352]	Time  0.168 ( 0.161)	Data  0.002 ( 0.004)	Loss 4.8215e-02 (8.9713e-02)	Acc@1  98.44 ( 96.76)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:00:00 - Epoch: [73][140/352]	Time  0.168 ( 0.161)	Data  0.002 ( 0.004)	Loss 9.3697e-02 (8.8742e-02)	Acc@1  96.09 ( 96.76)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:00:02 - Epoch: [73][150/352]	Time  0.172 ( 0.162)	Data  0.002 ( 0.004)	Loss 1.0229e-01 (8.9115e-02)	Acc@1  96.09 ( 96.76)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:00:04 - Epoch: [73][160/352]	Time  0.168 ( 0.162)	Data  0.002 ( 0.004)	Loss 7.4188e-02 (8.8502e-02)	Acc@1  96.09 ( 96.78)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:00:05 - Epoch: [73][170/352]	Time  0.165 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.5530e-02 (8.7958e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:00:07 - Epoch: [73][180/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.0440e-02 (8.7557e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:00:09 - Epoch: [73][190/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.0399e-01 (8.7618e-02)	Acc@1  96.88 ( 96.81)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:10 - Epoch: [73][200/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.1597e-01 (8.7500e-02)	Acc@1  96.88 ( 96.81)	Acc@5  99.22 ( 99.97)
07-Mar-22 04:00:12 - Epoch: [73][210/352]	Time  0.171 ( 0.163)	Data  0.002 ( 0.003)	Loss 2.9807e-02 (8.6922e-02)	Acc@1  99.22 ( 96.83)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:00:14 - Epoch: [73][220/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.5961e-02 (8.6508e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:15 - Epoch: [73][230/352]	Time  0.171 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.1144e-02 (8.6073e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:17 - Epoch: [73][240/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 6.7462e-02 (8.5810e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:19 - Epoch: [73][250/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.1067e-01 (8.5474e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:20 - Epoch: [73][260/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.5589e-02 (8.5384e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:22 - Epoch: [73][270/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.0480e-01 (8.5257e-02)	Acc@1  95.31 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:24 - Epoch: [73][280/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.0209e-02 (8.5267e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:25 - Epoch: [73][290/352]	Time  0.171 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.2350e-02 (8.4143e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:27 - Epoch: [73][300/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.2243e-02 (8.4088e-02)	Acc@1  99.22 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:29 - Epoch: [73][310/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.9453e-02 (8.3674e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:30 - Epoch: [73][320/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.2820e-02 (8.3448e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:32 - Epoch: [73][330/352]	Time  0.170 ( 0.165)	Data  0.003 ( 0.003)	Loss 7.9725e-02 (8.3927e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:34 - Epoch: [73][340/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.6881e-01 (8.3722e-02)	Acc@1  92.97 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:35 - Epoch: [73][350/352]	Time  0.171 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.4397e-01 (8.4119e-02)	Acc@1  92.97 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:00:36 - Test: [ 0/20]	Time  0.390 ( 0.390)	Loss 3.2165e-01 (3.2165e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:00:37 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.4769e-01 (3.8576e-01)	Acc@1  88.67 ( 88.85)	Acc@5  99.22 ( 99.47)
07-Mar-22 04:00:38 -  * Acc@1 89.200 Acc@5 99.480
07-Mar-22 04:00:38 - Best acc at epoch 73: 89.77999877929688
07-Mar-22 04:00:39 - Epoch: [74][  0/352]	Time  0.394 ( 0.394)	Data  0.234 ( 0.234)	Loss 1.4300e-01 (1.4300e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
07-Mar-22 04:00:40 - Epoch: [74][ 10/352]	Time  0.167 ( 0.182)	Data  0.002 ( 0.023)	Loss 4.5981e-02 (9.5015e-02)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 (100.00)
07-Mar-22 04:00:42 - Epoch: [74][ 20/352]	Time  0.155 ( 0.170)	Data  0.002 ( 0.013)	Loss 9.5055e-02 (9.6393e-02)	Acc@1  96.09 ( 96.69)	Acc@5 100.00 (100.00)
07-Mar-22 04:00:43 - Epoch: [74][ 30/352]	Time  0.144 ( 0.164)	Data  0.001 ( 0.009)	Loss 1.2085e-01 (9.6082e-02)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 (100.00)
07-Mar-22 04:00:45 - Epoch: [74][ 40/352]	Time  0.151 ( 0.160)	Data  0.002 ( 0.008)	Loss 1.0007e-01 (9.0796e-02)	Acc@1  96.88 ( 96.78)	Acc@5 100.00 (100.00)
07-Mar-22 04:00:46 - Epoch: [74][ 50/352]	Time  0.165 ( 0.161)	Data  0.002 ( 0.006)	Loss 4.8236e-02 (8.8759e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 (100.00)
07-Mar-22 04:00:48 - Epoch: [74][ 60/352]	Time  0.147 ( 0.159)	Data  0.002 ( 0.006)	Loss 4.2634e-02 (8.9092e-02)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:00:49 - Epoch: [74][ 70/352]	Time  0.161 ( 0.158)	Data  0.002 ( 0.005)	Loss 4.8749e-02 (8.5721e-02)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:00:51 - Epoch: [74][ 80/352]	Time  0.160 ( 0.158)	Data  0.002 ( 0.005)	Loss 7.6417e-02 (8.4635e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:00:53 - Epoch: [74][ 90/352]	Time  0.161 ( 0.158)	Data  0.002 ( 0.004)	Loss 6.2345e-02 (8.4030e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:00:54 - Epoch: [74][100/352]	Time  0.156 ( 0.158)	Data  0.002 ( 0.004)	Loss 7.4263e-02 (8.3140e-02)	Acc@1  98.44 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:00:56 - Epoch: [74][110/352]	Time  0.156 ( 0.158)	Data  0.002 ( 0.004)	Loss 8.3748e-02 (8.2236e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:00:57 - Epoch: [74][120/352]	Time  0.166 ( 0.159)	Data  0.002 ( 0.004)	Loss 9.6081e-02 (8.3511e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:00:59 - Epoch: [74][130/352]	Time  0.160 ( 0.159)	Data  0.002 ( 0.004)	Loss 1.2303e-01 (8.4326e-02)	Acc@1  95.31 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:01:01 - Epoch: [74][140/352]	Time  0.165 ( 0.159)	Data  0.002 ( 0.004)	Loss 1.0492e-01 (8.4920e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:01:02 - Epoch: [74][150/352]	Time  0.154 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.0067e-01 (8.5294e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:01:04 - Epoch: [74][160/352]	Time  0.166 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.2965e-01 (8.5552e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:01:06 - Epoch: [74][170/352]	Time  0.168 ( 0.160)	Data  0.002 ( 0.003)	Loss 1.1572e-01 (8.6688e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:01:07 - Epoch: [74][180/352]	Time  0.170 ( 0.160)	Data  0.002 ( 0.003)	Loss 6.8405e-02 (8.7612e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:09 - Epoch: [74][190/352]	Time  0.166 ( 0.161)	Data  0.002 ( 0.003)	Loss 9.9216e-02 (8.7492e-02)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:11 - Epoch: [74][200/352]	Time  0.169 ( 0.161)	Data  0.002 ( 0.003)	Loss 4.5918e-02 (8.7127e-02)	Acc@1  99.22 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:12 - Epoch: [74][210/352]	Time  0.190 ( 0.162)	Data  0.002 ( 0.003)	Loss 6.1581e-02 (8.7106e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:01:14 - Epoch: [74][220/352]	Time  0.170 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.0135e-01 (8.6922e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:16 - Epoch: [74][230/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.003)	Loss 8.8211e-02 (8.6527e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:18 - Epoch: [74][240/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.0920e-01 (8.6430e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:19 - Epoch: [74][250/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 5.5542e-02 (8.6317e-02)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:21 - Epoch: [74][260/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.3802e-01 (8.6284e-02)	Acc@1  93.75 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:01:23 - Epoch: [74][270/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.7081e-02 (8.6536e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:01:24 - Epoch: [74][280/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 4.3662e-02 (8.6659e-02)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:01:26 - Epoch: [74][290/352]	Time  0.171 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.3001e-02 (8.6505e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:28 - Epoch: [74][300/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 9.0790e-02 (8.6643e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:29 - Epoch: [74][310/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 5.8480e-02 (8.7069e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:31 - Epoch: [74][320/352]	Time  0.165 ( 0.164)	Data  0.002 ( 0.003)	Loss 6.2601e-02 (8.6952e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:32 - Epoch: [74][330/352]	Time  0.169 ( 0.164)	Data  0.002 ( 0.003)	Loss 4.9653e-02 (8.6406e-02)	Acc@1 100.00 ( 97.05)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:34 - Epoch: [74][340/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.0152e-02 (8.6579e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:36 - Epoch: [74][350/352]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0883e-01 (8.7092e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:36 - Test: [ 0/20]	Time  0.394 ( 0.394)	Loss 3.2694e-01 (3.2694e-01)	Acc@1  91.02 ( 91.02)	Acc@5  99.61 ( 99.61)
07-Mar-22 04:01:37 - Test: [10/20]	Time  0.098 ( 0.126)	Loss 4.0447e-01 (3.9183e-01)	Acc@1  87.89 ( 89.03)	Acc@5  99.22 ( 99.47)
07-Mar-22 04:01:38 -  * Acc@1 89.380 Acc@5 99.460
07-Mar-22 04:01:38 - Best acc at epoch 74: 89.77999877929688
07-Mar-22 04:01:39 - Epoch: [75][  0/352]	Time  0.412 ( 0.412)	Data  0.252 ( 0.252)	Loss 8.7001e-02 (8.7001e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
07-Mar-22 04:01:40 - Epoch: [75][ 10/352]	Time  0.170 ( 0.179)	Data  0.002 ( 0.024)	Loss 1.1599e-01 (9.1164e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 04:01:42 - Epoch: [75][ 20/352]	Time  0.168 ( 0.173)	Data  0.002 ( 0.014)	Loss 1.1515e-01 (8.4915e-02)	Acc@1  96.09 ( 97.17)	Acc@5 100.00 (100.00)
07-Mar-22 04:01:44 - Epoch: [75][ 30/352]	Time  0.163 ( 0.171)	Data  0.002 ( 0.010)	Loss 3.7576e-02 (7.7639e-02)	Acc@1  99.22 ( 97.40)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:01:45 - Epoch: [75][ 40/352]	Time  0.164 ( 0.170)	Data  0.002 ( 0.008)	Loss 6.2305e-02 (7.5560e-02)	Acc@1  98.44 ( 97.47)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:47 - Epoch: [75][ 50/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.007)	Loss 5.8380e-02 (7.9623e-02)	Acc@1  97.66 ( 97.27)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:49 - Epoch: [75][ 60/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.006)	Loss 6.1478e-02 (8.0075e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:01:50 - Epoch: [75][ 70/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.006)	Loss 5.5545e-02 (8.0288e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:01:52 - Epoch: [75][ 80/352]	Time  0.161 ( 0.168)	Data  0.002 ( 0.005)	Loss 6.4193e-02 (7.9736e-02)	Acc@1  96.88 ( 97.23)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:01:54 - Epoch: [75][ 90/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.005)	Loss 3.9827e-02 (8.0724e-02)	Acc@1  99.22 ( 97.21)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:01:55 - Epoch: [75][100/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.005)	Loss 3.4171e-02 (8.0958e-02)	Acc@1 100.00 ( 97.22)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:57 - Epoch: [75][110/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.0725e-01 (8.0278e-02)	Acc@1  96.09 ( 97.22)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:01:59 - Epoch: [75][120/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2409e-01 (8.1986e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:02:00 - Epoch: [75][130/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.2699e-02 (8.2730e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:02:02 - Epoch: [75][140/352]	Time  0.154 ( 0.167)	Data  0.002 ( 0.004)	Loss 9.7959e-02 (8.3998e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:02:03 - Epoch: [75][150/352]	Time  0.165 ( 0.166)	Data  0.001 ( 0.004)	Loss 9.4553e-02 (8.4774e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:02:05 - Epoch: [75][160/352]	Time  0.160 ( 0.166)	Data  0.002 ( 0.004)	Loss 5.5082e-02 (8.4547e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:02:07 - Epoch: [75][170/352]	Time  0.176 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.9812e-02 (8.3973e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:02:08 - Epoch: [75][180/352]	Time  0.155 ( 0.166)	Data  0.002 ( 0.003)	Loss 4.3499e-02 (8.3335e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:02:10 - Epoch: [75][190/352]	Time  0.177 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.1782e-02 (8.2844e-02)	Acc@1  96.88 ( 97.17)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:02:12 - Epoch: [75][200/352]	Time  0.152 ( 0.166)	Data  0.001 ( 0.003)	Loss 1.1224e-01 (8.3179e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:02:13 - Epoch: [75][210/352]	Time  0.178 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.3315e-02 (8.2970e-02)	Acc@1  97.66 ( 97.16)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:02:15 - Epoch: [75][220/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0303e-01 (8.3337e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:02:17 - Epoch: [75][230/352]	Time  0.161 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.6244e-02 (8.3351e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:02:18 - Epoch: [75][240/352]	Time  0.159 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.3689e-02 (8.3291e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:02:20 - Epoch: [75][250/352]	Time  0.178 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.3276e-02 (8.3493e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:02:22 - Epoch: [75][260/352]	Time  0.180 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0192e-01 (8.3908e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:02:23 - Epoch: [75][270/352]	Time  0.178 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.7183e-01 (8.4814e-02)	Acc@1  92.19 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:02:25 - Epoch: [75][280/352]	Time  0.180 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.2818e-02 (8.4794e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:02:27 - Epoch: [75][290/352]	Time  0.178 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.2376e-02 (8.4671e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:02:29 - Epoch: [75][300/352]	Time  0.178 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.7540e-02 (8.4411e-02)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:02:31 - Epoch: [75][310/352]	Time  0.177 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1820e-01 (8.4468e-02)	Acc@1  94.53 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:02:32 - Epoch: [75][320/352]	Time  0.178 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.5786e-02 (8.3741e-02)	Acc@1  99.22 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:02:34 - Epoch: [75][330/352]	Time  0.184 ( 0.169)	Data  0.002 ( 0.003)	Loss 6.0523e-02 (8.3348e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:02:36 - Epoch: [75][340/352]	Time  0.175 ( 0.169)	Data  0.002 ( 0.003)	Loss 4.7263e-02 (8.3287e-02)	Acc@1  99.22 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:02:38 - Epoch: [75][350/352]	Time  0.180 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0538e-01 (8.3382e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:02:39 - Test: [ 0/20]	Time  0.364 ( 0.364)	Loss 3.2740e-01 (3.2740e-01)	Acc@1  89.45 ( 89.45)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:02:40 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 3.5289e-01 (3.8301e-01)	Acc@1  89.84 ( 88.96)	Acc@5  99.22 ( 99.50)
07-Mar-22 04:02:40 -  * Acc@1 89.400 Acc@5 99.400
07-Mar-22 04:02:40 - Best acc at epoch 75: 89.77999877929688
07-Mar-22 04:02:41 - Epoch: [76][  0/352]	Time  0.395 ( 0.395)	Data  0.236 ( 0.236)	Loss 1.0359e-01 (1.0359e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 04:02:43 - Epoch: [76][ 10/352]	Time  0.160 ( 0.184)	Data  0.002 ( 0.023)	Loss 5.6760e-02 (7.6138e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 (100.00)
07-Mar-22 04:02:44 - Epoch: [76][ 20/352]	Time  0.163 ( 0.172)	Data  0.002 ( 0.013)	Loss 6.4933e-02 (7.6198e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 (100.00)
07-Mar-22 04:02:46 - Epoch: [76][ 30/352]	Time  0.152 ( 0.169)	Data  0.002 ( 0.009)	Loss 4.2049e-02 (7.1288e-02)	Acc@1  98.44 ( 97.18)	Acc@5 100.00 (100.00)
07-Mar-22 04:02:47 - Epoch: [76][ 40/352]	Time  0.158 ( 0.165)	Data  0.002 ( 0.008)	Loss 7.2995e-02 (7.6678e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 (100.00)
07-Mar-22 04:02:49 - Epoch: [76][ 50/352]	Time  0.147 ( 0.162)	Data  0.002 ( 0.006)	Loss 1.3177e-01 (7.9775e-02)	Acc@1  94.53 ( 96.98)	Acc@5 100.00 (100.00)
07-Mar-22 04:02:50 - Epoch: [76][ 60/352]	Time  0.168 ( 0.161)	Data  0.002 ( 0.006)	Loss 1.0340e-01 (8.0827e-02)	Acc@1  94.53 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 04:02:52 - Epoch: [76][ 70/352]	Time  0.162 ( 0.161)	Data  0.002 ( 0.005)	Loss 5.9975e-02 (8.0046e-02)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 (100.00)
07-Mar-22 04:02:54 - Epoch: [76][ 80/352]	Time  0.168 ( 0.161)	Data  0.002 ( 0.005)	Loss 9.9708e-02 (7.8274e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 (100.00)
07-Mar-22 04:02:55 - Epoch: [76][ 90/352]	Time  0.167 ( 0.162)	Data  0.002 ( 0.004)	Loss 4.4049e-02 (8.1608e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 (100.00)
07-Mar-22 04:02:57 - Epoch: [76][100/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.004)	Loss 1.3878e-01 (8.2522e-02)	Acc@1  94.53 ( 96.96)	Acc@5 100.00 (100.00)
07-Mar-22 04:02:59 - Epoch: [76][110/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.004)	Loss 8.8429e-02 (8.2430e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:00 - Epoch: [76][120/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.004)	Loss 1.1339e-01 (8.2022e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:02 - Epoch: [76][130/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.004)	Loss 6.8529e-02 (8.1803e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:04 - Epoch: [76][140/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.004)	Loss 6.8489e-02 (8.1574e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:05 - Epoch: [76][150/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.004)	Loss 3.6584e-02 (8.1441e-02)	Acc@1  99.22 ( 97.05)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:07 - Epoch: [76][160/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 9.2413e-02 (8.1392e-02)	Acc@1  95.31 ( 97.06)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:09 - Epoch: [76][170/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 4.8261e-02 (8.0859e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:10 - Epoch: [76][180/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.2093e-02 (8.0658e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:12 - Epoch: [76][190/352]	Time  0.155 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.3408e-02 (8.0933e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:13 - Epoch: [76][200/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.3757e-01 (8.2000e-02)	Acc@1  94.53 ( 97.04)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:15 - Epoch: [76][210/352]	Time  0.156 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.9515e-02 (8.2423e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:17 - Epoch: [76][220/352]	Time  0.160 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.9154e-02 (8.1673e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:18 - Epoch: [76][230/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 9.4290e-02 (8.1998e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:20 - Epoch: [76][240/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.1170e-02 (8.2880e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:22 - Epoch: [76][250/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.5163e-02 (8.3110e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:23 - Epoch: [76][260/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 4.7314e-02 (8.2353e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:25 - Epoch: [76][270/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.8805e-01 (8.2682e-02)	Acc@1  92.19 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:27 - Epoch: [76][280/352]	Time  0.164 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.3804e-01 (8.3038e-02)	Acc@1  97.66 ( 97.00)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:03:28 - Epoch: [76][290/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.1397e-01 (8.3404e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:30 - Epoch: [76][300/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 3.7994e-02 (8.3521e-02)	Acc@1  99.22 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:32 - Epoch: [76][310/352]	Time  0.166 ( 0.164)	Data  0.002 ( 0.003)	Loss 6.3256e-02 (8.3468e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:33 - Epoch: [76][320/352]	Time  0.165 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.1153e-02 (8.3438e-02)	Acc@1  99.22 ( 96.98)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:03:35 - Epoch: [76][330/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 4.1361e-02 (8.3640e-02)	Acc@1  99.22 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:37 - Epoch: [76][340/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.003)	Loss 9.2137e-02 (8.3736e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:38 - Epoch: [76][350/352]	Time  0.154 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.5137e-02 (8.3867e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:39 - Test: [ 0/20]	Time  0.334 ( 0.334)	Loss 3.3503e-01 (3.3503e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.61 ( 99.61)
07-Mar-22 04:03:40 - Test: [10/20]	Time  0.100 ( 0.121)	Loss 3.7253e-01 (3.7949e-01)	Acc@1  88.67 ( 89.13)	Acc@5  99.22 ( 99.68)
07-Mar-22 04:03:41 -  * Acc@1 89.540 Acc@5 99.580
07-Mar-22 04:03:41 - Best acc at epoch 76: 89.77999877929688
07-Mar-22 04:03:41 - Epoch: [77][  0/352]	Time  0.394 ( 0.394)	Data  0.238 ( 0.238)	Loss 1.3990e-01 (1.3990e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:43 - Epoch: [77][ 10/352]	Time  0.173 ( 0.186)	Data  0.002 ( 0.024)	Loss 5.7530e-02 (8.2193e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:45 - Epoch: [77][ 20/352]	Time  0.165 ( 0.176)	Data  0.002 ( 0.013)	Loss 4.2475e-02 (8.2991e-02)	Acc@1  99.22 ( 96.76)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:46 - Epoch: [77][ 30/352]	Time  0.170 ( 0.174)	Data  0.002 ( 0.010)	Loss 7.3207e-02 (8.3848e-02)	Acc@1  97.66 ( 96.77)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:48 - Epoch: [77][ 40/352]	Time  0.167 ( 0.173)	Data  0.002 ( 0.008)	Loss 1.2920e-01 (8.4475e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 04:03:50 - Epoch: [77][ 50/352]	Time  0.172 ( 0.172)	Data  0.002 ( 0.007)	Loss 7.9064e-02 (8.3582e-02)	Acc@1  95.31 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:03:51 - Epoch: [77][ 60/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.006)	Loss 1.0751e-01 (8.6171e-02)	Acc@1  95.31 ( 96.81)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:53 - Epoch: [77][ 70/352]	Time  0.153 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.0040e-01 (8.5465e-02)	Acc@1  96.09 ( 96.82)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:54 - Epoch: [77][ 80/352]	Time  0.159 ( 0.167)	Data  0.002 ( 0.005)	Loss 6.4602e-02 (8.3619e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:56 - Epoch: [77][ 90/352]	Time  0.160 ( 0.167)	Data  0.002 ( 0.005)	Loss 3.8957e-02 (8.3771e-02)	Acc@1 100.00 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:58 - Epoch: [77][100/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.2388e-01 (8.5101e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:03:59 - Epoch: [77][110/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 5.1681e-02 (8.4279e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:01 - Epoch: [77][120/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.004)	Loss 7.8779e-02 (8.5293e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:03 - Epoch: [77][130/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 6.5870e-02 (8.4809e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:04 - Epoch: [77][140/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.004)	Loss 8.4250e-02 (8.4725e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:06 - Epoch: [77][150/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 7.6852e-02 (8.4168e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:08 - Epoch: [77][160/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 8.7015e-02 (8.3694e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:09 - Epoch: [77][170/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.4498e-02 (8.3383e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:11 - Epoch: [77][180/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.8580e-02 (8.3588e-02)	Acc@1  95.31 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:13 - Epoch: [77][190/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.9936e-02 (8.3931e-02)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:14 - Epoch: [77][200/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.8647e-02 (8.2469e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:16 - Epoch: [77][210/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.0779e-02 (8.1918e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:18 - Epoch: [77][220/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.2966e-02 (8.1869e-02)	Acc@1  99.22 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:19 - Epoch: [77][230/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.9823e-02 (8.1873e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:21 - Epoch: [77][240/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.2627e-02 (8.1732e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:23 - Epoch: [77][250/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.4085e-02 (8.1644e-02)	Acc@1  98.44 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:24 - Epoch: [77][260/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0981e-01 (8.1932e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:26 - Epoch: [77][270/352]	Time  0.166 ( 0.167)	Data  0.001 ( 0.003)	Loss 5.1850e-02 (8.1990e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:28 - Epoch: [77][280/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.6901e-01 (8.2474e-02)	Acc@1  92.19 ( 97.09)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:29 - Epoch: [77][290/352]	Time  0.166 ( 0.167)	Data  0.001 ( 0.003)	Loss 1.3572e-01 (8.2254e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:31 - Epoch: [77][300/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.4183e-02 (8.2411e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:33 - Epoch: [77][310/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.6998e-02 (8.2619e-02)	Acc@1  98.44 ( 97.08)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:34 - Epoch: [77][320/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 3.3281e-02 (8.2985e-02)	Acc@1  99.22 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:36 - Epoch: [77][330/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.9865e-02 (8.3439e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:38 - Epoch: [77][340/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.1849e-02 (8.3521e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:39 - Epoch: [77][350/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0546e-01 (8.3068e-02)	Acc@1  94.53 ( 97.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:40 - Test: [ 0/20]	Time  0.363 ( 0.363)	Loss 3.4549e-01 (3.4549e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.61 ( 99.61)
07-Mar-22 04:04:41 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 3.8040e-01 (3.8717e-01)	Acc@1  89.45 ( 88.88)	Acc@5  99.22 ( 99.36)
07-Mar-22 04:04:42 -  * Acc@1 89.180 Acc@5 99.400
07-Mar-22 04:04:42 - Best acc at epoch 77: 89.77999877929688
07-Mar-22 04:04:42 - Epoch: [78][  0/352]	Time  0.413 ( 0.413)	Data  0.246 ( 0.246)	Loss 1.0293e-01 (1.0293e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 04:04:44 - Epoch: [78][ 10/352]	Time  0.163 ( 0.186)	Data  0.002 ( 0.024)	Loss 1.0053e-01 (9.4475e-02)	Acc@1  94.53 ( 96.66)	Acc@5 100.00 (100.00)
07-Mar-22 04:04:46 - Epoch: [78][ 20/352]	Time  0.178 ( 0.179)	Data  0.003 ( 0.014)	Loss 8.7986e-02 (9.2930e-02)	Acc@1  96.09 ( 96.76)	Acc@5 100.00 (100.00)
07-Mar-22 04:04:47 - Epoch: [78][ 30/352]	Time  0.167 ( 0.176)	Data  0.002 ( 0.010)	Loss 9.8974e-02 (8.9065e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:04:49 - Epoch: [78][ 40/352]	Time  0.169 ( 0.173)	Data  0.002 ( 0.008)	Loss 9.6073e-02 (8.8400e-02)	Acc@1  95.31 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:51 - Epoch: [78][ 50/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.007)	Loss 4.3030e-02 (8.5848e-02)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:52 - Epoch: [78][ 60/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.006)	Loss 7.1950e-02 (8.5946e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:54 - Epoch: [78][ 70/352]	Time  0.170 ( 0.171)	Data  0.002 ( 0.006)	Loss 7.7862e-02 (8.5697e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:56 - Epoch: [78][ 80/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.005)	Loss 5.5853e-02 (8.5834e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:04:57 - Epoch: [78][ 90/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.005)	Loss 6.0814e-02 (8.6234e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:04:59 - Epoch: [78][100/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.8015e-01 (8.7244e-02)	Acc@1  93.75 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:05:01 - Epoch: [78][110/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.7189e-02 (8.7206e-02)	Acc@1  99.22 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:05:03 - Epoch: [78][120/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.004)	Loss 3.7218e-02 (8.7403e-02)	Acc@1 100.00 ( 96.91)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:04 - Epoch: [78][130/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 8.7978e-02 (8.6447e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:06 - Epoch: [78][140/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.4032e-02 (8.5756e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:08 - Epoch: [78][150/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.1663e-02 (8.5774e-02)	Acc@1  99.22 ( 96.98)	Acc@5 100.00 ( 99.96)
07-Mar-22 04:05:09 - Epoch: [78][160/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2187e-01 (8.6044e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:11 - Epoch: [78][170/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 2.8956e-02 (8.5127e-02)	Acc@1 100.00 ( 96.99)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:12 - Epoch: [78][180/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.3436e-02 (8.4143e-02)	Acc@1  99.22 ( 97.05)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:14 - Epoch: [78][190/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.7544e-02 (8.3394e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:16 - Epoch: [78][200/352]	Time  0.144 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.1080e-02 (8.4204e-02)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:17 - Epoch: [78][210/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.0011e-02 (8.3669e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:19 - Epoch: [78][220/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.9992e-02 (8.4761e-02)	Acc@1  94.53 ( 97.01)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:21 - Epoch: [78][230/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2166e-01 (8.5286e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:22 - Epoch: [78][240/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.7318e-02 (8.5068e-02)	Acc@1  94.53 ( 96.99)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:24 - Epoch: [78][250/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.9275e-02 (8.5842e-02)	Acc@1  99.22 ( 96.96)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:26 - Epoch: [78][260/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0041e-01 (8.5866e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:27 - Epoch: [78][270/352]	Time  0.166 ( 0.167)	Data  0.003 ( 0.003)	Loss 6.5581e-02 (8.5735e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:29 - Epoch: [78][280/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.3772e-02 (8.5798e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:31 - Epoch: [78][290/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1313e-01 (8.5815e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:32 - Epoch: [78][300/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.5527e-02 (8.6257e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:34 - Epoch: [78][310/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.2672e-02 (8.6703e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:36 - Epoch: [78][320/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.0913e-02 (8.6674e-02)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:37 - Epoch: [78][330/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.6169e-02 (8.6544e-02)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:39 - Epoch: [78][340/352]	Time  0.142 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.2533e-02 (8.6263e-02)	Acc@1  99.22 ( 96.92)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:41 - Epoch: [78][350/352]	Time  0.152 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.9030e-02 (8.5919e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:05:41 - Test: [ 0/20]	Time  0.378 ( 0.378)	Loss 2.9353e-01 (2.9353e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:05:42 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 4.3110e-01 (3.7975e-01)	Acc@1  88.67 ( 89.13)	Acc@5  98.44 ( 99.40)
07-Mar-22 04:05:43 -  * Acc@1 89.400 Acc@5 99.440
07-Mar-22 04:05:43 - Best acc at epoch 78: 89.77999877929688
07-Mar-22 04:05:44 - Epoch: [79][  0/352]	Time  0.378 ( 0.378)	Data  0.232 ( 0.232)	Loss 9.5959e-02 (9.5959e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 04:05:45 - Epoch: [79][ 10/352]	Time  0.141 ( 0.182)	Data  0.002 ( 0.023)	Loss 4.1532e-02 (7.5931e-02)	Acc@1  99.22 ( 97.51)	Acc@5 100.00 (100.00)
07-Mar-22 04:05:47 - Epoch: [79][ 20/352]	Time  0.170 ( 0.174)	Data  0.002 ( 0.013)	Loss 1.3778e-01 (7.6970e-02)	Acc@1  95.31 ( 97.21)	Acc@5 100.00 (100.00)
07-Mar-22 04:05:49 - Epoch: [79][ 30/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.010)	Loss 3.6418e-02 (7.9559e-02)	Acc@1  99.22 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 04:05:50 - Epoch: [79][ 40/352]	Time  0.173 ( 0.172)	Data  0.003 ( 0.008)	Loss 1.0653e-01 (8.2474e-02)	Acc@1  95.31 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 04:05:52 - Epoch: [79][ 50/352]	Time  0.164 ( 0.172)	Data  0.002 ( 0.007)	Loss 7.0393e-02 (7.9174e-02)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 (100.00)
07-Mar-22 04:05:54 - Epoch: [79][ 60/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.006)	Loss 4.9225e-02 (7.9442e-02)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 (100.00)
07-Mar-22 04:05:55 - Epoch: [79][ 70/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.006)	Loss 8.2965e-02 (7.8987e-02)	Acc@1  98.44 ( 97.32)	Acc@5 100.00 (100.00)
07-Mar-22 04:05:57 - Epoch: [79][ 80/352]	Time  0.164 ( 0.170)	Data  0.002 ( 0.005)	Loss 5.6374e-02 (8.0139e-02)	Acc@1  98.44 ( 97.26)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:05:59 - Epoch: [79][ 90/352]	Time  0.141 ( 0.169)	Data  0.002 ( 0.005)	Loss 8.9967e-02 (7.9526e-02)	Acc@1  96.09 ( 97.25)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:00 - Epoch: [79][100/352]	Time  0.152 ( 0.167)	Data  0.002 ( 0.005)	Loss 7.6052e-02 (7.9994e-02)	Acc@1  96.88 ( 97.23)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:02 - Epoch: [79][110/352]	Time  0.143 ( 0.165)	Data  0.002 ( 0.004)	Loss 8.7355e-02 (8.0554e-02)	Acc@1  96.09 ( 97.21)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:03 - Epoch: [79][120/352]	Time  0.159 ( 0.165)	Data  0.002 ( 0.004)	Loss 7.4413e-02 (8.0184e-02)	Acc@1  96.09 ( 97.21)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:05 - Epoch: [79][130/352]	Time  0.154 ( 0.164)	Data  0.002 ( 0.004)	Loss 8.2878e-02 (8.0706e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:06 - Epoch: [79][140/352]	Time  0.151 ( 0.164)	Data  0.002 ( 0.004)	Loss 1.2063e-01 (8.2753e-02)	Acc@1  95.31 ( 97.14)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:08 - Epoch: [79][150/352]	Time  0.153 ( 0.163)	Data  0.002 ( 0.004)	Loss 3.2117e-02 (8.2890e-02)	Acc@1 100.00 ( 97.16)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:10 - Epoch: [79][160/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.004)	Loss 5.2589e-02 (8.2190e-02)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:11 - Epoch: [79][170/352]	Time  0.169 ( 0.164)	Data  0.002 ( 0.004)	Loss 1.1707e-01 (8.2735e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:13 - Epoch: [79][180/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.004)	Loss 9.1327e-02 (8.2779e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:15 - Epoch: [79][190/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 9.8472e-02 (8.3022e-02)	Acc@1  96.09 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:16 - Epoch: [79][200/352]	Time  0.163 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.7448e-02 (8.3639e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:18 - Epoch: [79][210/352]	Time  0.169 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.4656e-02 (8.3400e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:20 - Epoch: [79][220/352]	Time  0.169 ( 0.164)	Data  0.002 ( 0.003)	Loss 6.1329e-02 (8.3155e-02)	Acc@1  98.44 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:21 - Epoch: [79][230/352]	Time  0.169 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.5558e-02 (8.2672e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:23 - Epoch: [79][240/352]	Time  0.154 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.0038e-01 (8.2534e-02)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:24 - Epoch: [79][250/352]	Time  0.147 ( 0.164)	Data  0.002 ( 0.003)	Loss 4.8239e-02 (8.2488e-02)	Acc@1  98.44 ( 97.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:26 - Epoch: [79][260/352]	Time  0.142 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.3448e-01 (8.2734e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:27 - Epoch: [79][270/352]	Time  0.150 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.2931e-02 (8.3006e-02)	Acc@1  96.09 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:29 - Epoch: [79][280/352]	Time  0.158 ( 0.163)	Data  0.002 ( 0.003)	Loss 4.5376e-02 (8.2778e-02)	Acc@1  99.22 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:31 - Epoch: [79][290/352]	Time  0.153 ( 0.162)	Data  0.002 ( 0.003)	Loss 4.0144e-02 (8.3105e-02)	Acc@1  99.22 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:32 - Epoch: [79][300/352]	Time  0.160 ( 0.162)	Data  0.002 ( 0.003)	Loss 1.1086e-01 (8.2595e-02)	Acc@1  94.53 ( 97.14)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:34 - Epoch: [79][310/352]	Time  0.162 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.5322e-02 (8.2337e-02)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:35 - Epoch: [79][320/352]	Time  0.161 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.5361e-02 (8.2514e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:37 - Epoch: [79][330/352]	Time  0.154 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.5541e-02 (8.2987e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:38 - Epoch: [79][340/352]	Time  0.153 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.9496e-02 (8.2865e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:40 - Epoch: [79][350/352]	Time  0.153 ( 0.162)	Data  0.002 ( 0.003)	Loss 6.6744e-02 (8.2917e-02)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:41 - Test: [ 0/20]	Time  0.351 ( 0.351)	Loss 3.1950e-01 (3.1950e-01)	Acc@1  91.02 ( 91.02)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:06:42 - Test: [10/20]	Time  0.102 ( 0.127)	Loss 3.6439e-01 (3.8351e-01)	Acc@1  90.23 ( 89.03)	Acc@5  99.22 ( 99.54)
07-Mar-22 04:06:43 -  * Acc@1 89.340 Acc@5 99.460
07-Mar-22 04:06:43 - Best acc at epoch 79: 89.77999877929688
07-Mar-22 04:06:43 - Epoch: [80][  0/352]	Time  0.407 ( 0.407)	Data  0.264 ( 0.264)	Loss 5.7570e-02 (5.7570e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 04:06:45 - Epoch: [80][ 10/352]	Time  0.141 ( 0.176)	Data  0.001 ( 0.026)	Loss 9.7469e-02 (9.1346e-02)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 (100.00)
07-Mar-22 04:06:46 - Epoch: [80][ 20/352]	Time  0.148 ( 0.162)	Data  0.002 ( 0.014)	Loss 6.5355e-02 (8.1589e-02)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 (100.00)
07-Mar-22 04:06:48 - Epoch: [80][ 30/352]	Time  0.155 ( 0.160)	Data  0.002 ( 0.010)	Loss 8.6748e-02 (8.2799e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 04:06:49 - Epoch: [80][ 40/352]	Time  0.157 ( 0.159)	Data  0.003 ( 0.008)	Loss 1.0124e-01 (8.4170e-02)	Acc@1  94.53 ( 96.91)	Acc@5 100.00 (100.00)
07-Mar-22 04:06:51 - Epoch: [80][ 50/352]	Time  0.166 ( 0.162)	Data  0.003 ( 0.007)	Loss 1.5128e-01 (8.5984e-02)	Acc@1  93.75 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:06:53 - Epoch: [80][ 60/352]	Time  0.175 ( 0.163)	Data  0.002 ( 0.006)	Loss 8.2310e-02 (8.6161e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:54 - Epoch: [80][ 70/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.006)	Loss 1.2345e-01 (8.6485e-02)	Acc@1  94.53 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:06:56 - Epoch: [80][ 80/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.005)	Loss 7.9133e-02 (8.8832e-02)	Acc@1  96.88 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:06:58 - Epoch: [80][ 90/352]	Time  0.169 ( 0.165)	Data  0.003 ( 0.005)	Loss 8.4917e-02 (8.7251e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:07:00 - Epoch: [80][100/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.005)	Loss 7.5279e-02 (8.5179e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:07:01 - Epoch: [80][110/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.005)	Loss 6.2388e-02 (8.5094e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:03 - Epoch: [80][120/352]	Time  0.170 ( 0.166)	Data  0.003 ( 0.004)	Loss 8.8382e-02 (8.4111e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:05 - Epoch: [80][130/352]	Time  0.169 ( 0.166)	Data  0.003 ( 0.004)	Loss 7.8948e-02 (8.4342e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:06 - Epoch: [80][140/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.004)	Loss 6.3499e-02 (8.4056e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:08 - Epoch: [80][150/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.004)	Loss 9.5413e-02 (8.5048e-02)	Acc@1  94.53 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:10 - Epoch: [80][160/352]	Time  0.170 ( 0.166)	Data  0.002 ( 0.004)	Loss 4.1330e-02 (8.4438e-02)	Acc@1 100.00 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:11 - Epoch: [80][170/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.004)	Loss 7.3660e-02 (8.4385e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:13 - Epoch: [80][180/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.004)	Loss 4.8611e-02 (8.4273e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:15 - Epoch: [80][190/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.3898e-01 (8.4235e-02)	Acc@1  93.75 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:16 - Epoch: [80][200/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 8.5389e-02 (8.4698e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:18 - Epoch: [80][210/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 5.5991e-02 (8.4211e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:20 - Epoch: [80][220/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0529e-01 (8.3889e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:21 - Epoch: [80][230/352]	Time  0.167 ( 0.167)	Data  0.003 ( 0.003)	Loss 4.0374e-02 (8.2935e-02)	Acc@1  99.22 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:23 - Epoch: [80][240/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.7404e-02 (8.2890e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:25 - Epoch: [80][250/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.3291e-02 (8.3199e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:26 - Epoch: [80][260/352]	Time  0.169 ( 0.167)	Data  0.003 ( 0.003)	Loss 4.0658e-02 (8.3323e-02)	Acc@1  99.22 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:28 - Epoch: [80][270/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.0346e-02 (8.2860e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:30 - Epoch: [80][280/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.9846e-02 (8.3045e-02)	Acc@1  96.88 ( 97.03)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:07:31 - Epoch: [80][290/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3184e-01 (8.3009e-02)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:33 - Epoch: [80][300/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.8814e-02 (8.2567e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:35 - Epoch: [80][310/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.4818e-02 (8.2958e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:37 - Epoch: [80][320/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.5006e-02 (8.2999e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:38 - Epoch: [80][330/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.0648e-02 (8.3117e-02)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:40 - Epoch: [80][340/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.2713e-02 (8.3261e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:42 - Epoch: [80][350/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.6655e-02 (8.3490e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:07:42 - Test: [ 0/20]	Time  0.378 ( 0.378)	Loss 2.6750e-01 (2.6750e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.61 ( 99.61)
07-Mar-22 04:07:43 - Test: [10/20]	Time  0.098 ( 0.128)	Loss 3.9764e-01 (3.7157e-01)	Acc@1  87.89 ( 89.31)	Acc@5  99.22 ( 99.57)
07-Mar-22 04:07:44 -  * Acc@1 89.580 Acc@5 99.580
07-Mar-22 04:07:44 - Best acc at epoch 80: 89.77999877929688
07-Mar-22 04:07:45 - Epoch: [81][  0/352]	Time  0.422 ( 0.422)	Data  0.251 ( 0.251)	Loss 9.5993e-02 (9.5993e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 04:07:46 - Epoch: [81][ 10/352]	Time  0.168 ( 0.186)	Data  0.002 ( 0.025)	Loss 1.2526e-01 (9.8799e-02)	Acc@1  94.53 ( 96.66)	Acc@5  99.22 ( 99.93)
07-Mar-22 04:07:48 - Epoch: [81][ 20/352]	Time  0.166 ( 0.178)	Data  0.002 ( 0.014)	Loss 4.6450e-02 (8.5522e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.93)
07-Mar-22 04:07:50 - Epoch: [81][ 30/352]	Time  0.169 ( 0.174)	Data  0.002 ( 0.010)	Loss 5.0859e-02 (8.1174e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.95)
07-Mar-22 04:07:51 - Epoch: [81][ 40/352]	Time  0.167 ( 0.172)	Data  0.002 ( 0.008)	Loss 4.9698e-02 (8.3268e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.96)
07-Mar-22 04:07:53 - Epoch: [81][ 50/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.007)	Loss 5.8224e-02 (8.0699e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:07:55 - Epoch: [81][ 60/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.006)	Loss 6.4445e-02 (7.9041e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:07:56 - Epoch: [81][ 70/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.006)	Loss 6.3013e-02 (8.0669e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:07:58 - Epoch: [81][ 80/352]	Time  0.164 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.1468e-01 (8.0759e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:00 - Epoch: [81][ 90/352]	Time  0.170 ( 0.170)	Data  0.002 ( 0.005)	Loss 8.2605e-02 (8.2786e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:01 - Epoch: [81][100/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.1374e-01 (8.3422e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:03 - Epoch: [81][110/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.5788e-02 (8.2792e-02)	Acc@1  99.22 ( 97.18)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:05 - Epoch: [81][120/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.0979e-01 (8.3439e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:06 - Epoch: [81][130/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.3722e-01 (8.4147e-02)	Acc@1  93.75 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:08 - Epoch: [81][140/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.7163e-02 (8.4560e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:10 - Epoch: [81][150/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.0153e-02 (8.4385e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:11 - Epoch: [81][160/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.6985e-02 (8.4199e-02)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:13 - Epoch: [81][170/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.0960e-01 (8.4251e-02)	Acc@1  95.31 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:15 - Epoch: [81][180/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.1617e-02 (8.3760e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:16 - Epoch: [81][190/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1694e-01 (8.5078e-02)	Acc@1  96.09 ( 97.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:18 - Epoch: [81][200/352]	Time  0.172 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1478e-01 (8.4329e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:20 - Epoch: [81][210/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.7476e-02 (8.4371e-02)	Acc@1  99.22 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:21 - Epoch: [81][220/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.0541e-02 (8.4803e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:23 - Epoch: [81][230/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0424e-01 (8.4529e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:25 - Epoch: [81][240/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0311e-01 (8.3833e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:26 - Epoch: [81][250/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.9025e-02 (8.3963e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:28 - Epoch: [81][260/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.6336e-02 (8.4065e-02)	Acc@1  99.22 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:30 - Epoch: [81][270/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.3185e-02 (8.4216e-02)	Acc@1  99.22 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:31 - Epoch: [81][280/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.8665e-02 (8.4039e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:33 - Epoch: [81][290/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.5474e-02 (8.3812e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:35 - Epoch: [81][300/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.4558e-02 (8.3612e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:36 - Epoch: [81][310/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.4630e-02 (8.3698e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:38 - Epoch: [81][320/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.2238e-02 (8.3324e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:40 - Epoch: [81][330/352]	Time  0.172 ( 0.168)	Data  0.002 ( 0.003)	Loss 2.3302e-02 (8.3246e-02)	Acc@1 100.00 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:41 - Epoch: [81][340/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0121e-01 (8.3274e-02)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:43 - Epoch: [81][350/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.2329e-02 (8.2896e-02)	Acc@1  96.09 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:44 - Test: [ 0/20]	Time  0.370 ( 0.370)	Loss 3.1819e-01 (3.1819e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.61 ( 99.61)
07-Mar-22 04:08:45 - Test: [10/20]	Time  0.098 ( 0.123)	Loss 3.6936e-01 (3.7937e-01)	Acc@1  87.89 ( 89.52)	Acc@5  99.22 ( 99.61)
07-Mar-22 04:08:46 -  * Acc@1 89.740 Acc@5 99.580
07-Mar-22 04:08:46 - Best acc at epoch 81: 89.77999877929688
07-Mar-22 04:08:46 - Epoch: [82][  0/352]	Time  0.401 ( 0.401)	Data  0.243 ( 0.243)	Loss 7.6643e-02 (7.6643e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 04:08:48 - Epoch: [82][ 10/352]	Time  0.141 ( 0.175)	Data  0.001 ( 0.024)	Loss 7.3014e-02 (8.5313e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 04:08:49 - Epoch: [82][ 20/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.013)	Loss 9.3972e-02 (7.8225e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 (100.00)
07-Mar-22 04:08:51 - Epoch: [82][ 30/352]	Time  0.172 ( 0.166)	Data  0.002 ( 0.010)	Loss 9.7985e-02 (8.1443e-02)	Acc@1  96.88 ( 97.23)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:08:53 - Epoch: [82][ 40/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.008)	Loss 4.4583e-02 (8.4833e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:54 - Epoch: [82][ 50/352]	Time  0.172 ( 0.167)	Data  0.002 ( 0.007)	Loss 1.5147e-01 (8.5481e-02)	Acc@1  93.75 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:56 - Epoch: [82][ 60/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.006)	Loss 1.1721e-01 (8.7664e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:08:58 - Epoch: [82][ 70/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.006)	Loss 1.3046e-01 (8.9833e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:08:59 - Epoch: [82][ 80/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.005)	Loss 1.2566e-01 (9.0021e-02)	Acc@1  93.75 ( 96.75)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:01 - Epoch: [82][ 90/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.005)	Loss 6.9073e-02 (8.9269e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:03 - Epoch: [82][100/352]	Time  0.141 ( 0.167)	Data  0.002 ( 0.005)	Loss 7.3828e-02 (8.7595e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:04 - Epoch: [82][110/352]	Time  0.163 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.7969e-01 (8.7975e-02)	Acc@1  92.97 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:06 - Epoch: [82][120/352]	Time  0.159 ( 0.166)	Data  0.002 ( 0.004)	Loss 5.8160e-02 (8.6784e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:07 - Epoch: [82][130/352]	Time  0.163 ( 0.165)	Data  0.002 ( 0.004)	Loss 8.4980e-02 (8.7240e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:09 - Epoch: [82][140/352]	Time  0.156 ( 0.165)	Data  0.002 ( 0.004)	Loss 1.0016e-01 (8.7000e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:09:11 - Epoch: [82][150/352]	Time  0.158 ( 0.164)	Data  0.002 ( 0.004)	Loss 1.1860e-01 (8.6969e-02)	Acc@1  94.53 ( 96.83)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:09:12 - Epoch: [82][160/352]	Time  0.155 ( 0.164)	Data  0.002 ( 0.004)	Loss 6.7716e-02 (8.7221e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:09:14 - Epoch: [82][170/352]	Time  0.154 ( 0.163)	Data  0.002 ( 0.004)	Loss 6.0783e-02 (8.6229e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:09:15 - Epoch: [82][180/352]	Time  0.141 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.5786e-01 (8.6050e-02)	Acc@1  95.31 ( 96.95)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:09:17 - Epoch: [82][190/352]	Time  0.152 ( 0.162)	Data  0.002 ( 0.003)	Loss 5.5187e-02 (8.5941e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:09:18 - Epoch: [82][200/352]	Time  0.143 ( 0.161)	Data  0.002 ( 0.003)	Loss 1.0771e-01 (8.6388e-02)	Acc@1  94.53 ( 96.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:09:20 - Epoch: [82][210/352]	Time  0.142 ( 0.161)	Data  0.002 ( 0.003)	Loss 5.0099e-02 (8.5169e-02)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:09:21 - Epoch: [82][220/352]	Time  0.143 ( 0.160)	Data  0.002 ( 0.003)	Loss 6.1079e-02 (8.4923e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:23 - Epoch: [82][230/352]	Time  0.158 ( 0.160)	Data  0.002 ( 0.003)	Loss 4.4563e-02 (8.4743e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:24 - Epoch: [82][240/352]	Time  0.151 ( 0.159)	Data  0.002 ( 0.003)	Loss 7.6830e-02 (8.5075e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:26 - Epoch: [82][250/352]	Time  0.155 ( 0.159)	Data  0.002 ( 0.003)	Loss 8.5320e-02 (8.5644e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:27 - Epoch: [82][260/352]	Time  0.150 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.0911e-01 (8.6280e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:29 - Epoch: [82][270/352]	Time  0.156 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.9325e-02 (8.5693e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:30 - Epoch: [82][280/352]	Time  0.152 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.1165e-01 (8.5997e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:32 - Epoch: [82][290/352]	Time  0.141 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.2408e-02 (8.5815e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:33 - Epoch: [82][300/352]	Time  0.159 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.6678e-02 (8.5780e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:35 - Epoch: [82][310/352]	Time  0.142 ( 0.158)	Data  0.001 ( 0.003)	Loss 7.7498e-02 (8.5734e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:36 - Epoch: [82][320/352]	Time  0.145 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.4733e-01 (8.6157e-02)	Acc@1  94.53 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:38 - Epoch: [82][330/352]	Time  0.143 ( 0.157)	Data  0.002 ( 0.003)	Loss 6.5209e-02 (8.5929e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:39 - Epoch: [82][340/352]	Time  0.145 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.9325e-02 (8.5804e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:41 - Epoch: [82][350/352]	Time  0.153 ( 0.157)	Data  0.002 ( 0.003)	Loss 7.7598e-02 (8.5847e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:41 - Test: [ 0/20]	Time  0.385 ( 0.385)	Loss 3.0811e-01 (3.0811e-01)	Acc@1  91.41 ( 91.41)	Acc@5  98.83 ( 98.83)
07-Mar-22 04:09:42 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.5616e-01 (3.8826e-01)	Acc@1  90.23 ( 89.35)	Acc@5  99.22 ( 99.47)
07-Mar-22 04:09:43 -  * Acc@1 89.480 Acc@5 99.460
07-Mar-22 04:09:43 - Best acc at epoch 82: 89.77999877929688
07-Mar-22 04:09:44 - Epoch: [83][  0/352]	Time  0.387 ( 0.387)	Data  0.242 ( 0.242)	Loss 1.5276e-01 (1.5276e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 04:09:45 - Epoch: [83][ 10/352]	Time  0.166 ( 0.183)	Data  0.002 ( 0.024)	Loss 3.6981e-02 (8.1286e-02)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 (100.00)
07-Mar-22 04:09:47 - Epoch: [83][ 20/352]	Time  0.166 ( 0.175)	Data  0.002 ( 0.014)	Loss 1.3087e-01 (8.3112e-02)	Acc@1  93.75 ( 96.73)	Acc@5 100.00 (100.00)
07-Mar-22 04:09:49 - Epoch: [83][ 30/352]	Time  0.167 ( 0.173)	Data  0.002 ( 0.010)	Loss 1.4526e-01 (9.0927e-02)	Acc@1  95.31 ( 96.70)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:09:50 - Epoch: [83][ 40/352]	Time  0.170 ( 0.171)	Data  0.002 ( 0.008)	Loss 1.1899e-01 (9.0836e-02)	Acc@1  94.53 ( 96.67)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:52 - Epoch: [83][ 50/352]	Time  0.177 ( 0.172)	Data  0.002 ( 0.007)	Loss 8.3831e-02 (8.9353e-02)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:54 - Epoch: [83][ 60/352]	Time  0.178 ( 0.173)	Data  0.002 ( 0.006)	Loss 7.3038e-02 (9.0219e-02)	Acc@1  96.88 ( 96.68)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:09:56 - Epoch: [83][ 70/352]	Time  0.179 ( 0.174)	Data  0.002 ( 0.006)	Loss 5.6863e-02 (8.8952e-02)	Acc@1  96.88 ( 96.75)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:58 - Epoch: [83][ 80/352]	Time  0.177 ( 0.174)	Data  0.002 ( 0.005)	Loss 5.7608e-02 (8.8477e-02)	Acc@1  97.66 ( 96.79)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:09:59 - Epoch: [83][ 90/352]	Time  0.179 ( 0.174)	Data  0.002 ( 0.005)	Loss 3.6125e-02 (8.7938e-02)	Acc@1  99.22 ( 96.81)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:10:01 - Epoch: [83][100/352]	Time  0.176 ( 0.175)	Data  0.002 ( 0.005)	Loss 9.9711e-02 (8.6408e-02)	Acc@1  95.31 ( 96.82)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:10:03 - Epoch: [83][110/352]	Time  0.177 ( 0.175)	Data  0.002 ( 0.004)	Loss 1.8306e-01 (8.6336e-02)	Acc@1  93.75 ( 96.79)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:05 - Epoch: [83][120/352]	Time  0.176 ( 0.175)	Data  0.002 ( 0.004)	Loss 9.1699e-02 (8.5338e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:06 - Epoch: [83][130/352]	Time  0.181 ( 0.176)	Data  0.002 ( 0.004)	Loss 4.2723e-02 (8.6022e-02)	Acc@1  99.22 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:08 - Epoch: [83][140/352]	Time  0.176 ( 0.176)	Data  0.002 ( 0.004)	Loss 1.1625e-01 (8.7406e-02)	Acc@1  95.31 ( 96.83)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:10 - Epoch: [83][150/352]	Time  0.177 ( 0.176)	Data  0.002 ( 0.004)	Loss 4.7469e-02 (8.7270e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:12 - Epoch: [83][160/352]	Time  0.176 ( 0.176)	Data  0.002 ( 0.004)	Loss 8.4132e-02 (8.7413e-02)	Acc@1  96.88 ( 96.83)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:14 - Epoch: [83][170/352]	Time  0.180 ( 0.176)	Data  0.002 ( 0.004)	Loss 6.8885e-02 (8.7163e-02)	Acc@1  96.88 ( 96.83)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:15 - Epoch: [83][180/352]	Time  0.180 ( 0.176)	Data  0.002 ( 0.003)	Loss 5.1613e-02 (8.6776e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:17 - Epoch: [83][190/352]	Time  0.182 ( 0.176)	Data  0.002 ( 0.003)	Loss 7.5206e-02 (8.6394e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:19 - Epoch: [83][200/352]	Time  0.177 ( 0.176)	Data  0.002 ( 0.003)	Loss 1.0538e-01 (8.6497e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:21 - Epoch: [83][210/352]	Time  0.168 ( 0.177)	Data  0.002 ( 0.003)	Loss 5.7694e-02 (8.5570e-02)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:22 - Epoch: [83][220/352]	Time  0.178 ( 0.176)	Data  0.002 ( 0.003)	Loss 4.4348e-02 (8.5476e-02)	Acc@1 100.00 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:24 - Epoch: [83][230/352]	Time  0.182 ( 0.176)	Data  0.002 ( 0.003)	Loss 5.9787e-02 (8.5487e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:26 - Epoch: [83][240/352]	Time  0.183 ( 0.177)	Data  0.002 ( 0.003)	Loss 2.9338e-02 (8.5712e-02)	Acc@1  99.22 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:28 - Epoch: [83][250/352]	Time  0.178 ( 0.177)	Data  0.002 ( 0.003)	Loss 5.4614e-02 (8.5667e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:30 - Epoch: [83][260/352]	Time  0.178 ( 0.177)	Data  0.002 ( 0.003)	Loss 9.4954e-02 (8.5660e-02)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:31 - Epoch: [83][270/352]	Time  0.180 ( 0.177)	Data  0.002 ( 0.003)	Loss 8.4903e-02 (8.5616e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:33 - Epoch: [83][280/352]	Time  0.179 ( 0.177)	Data  0.002 ( 0.003)	Loss 8.9042e-02 (8.6175e-02)	Acc@1  95.31 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:35 - Epoch: [83][290/352]	Time  0.178 ( 0.177)	Data  0.002 ( 0.003)	Loss 6.6098e-02 (8.5610e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:37 - Epoch: [83][300/352]	Time  0.177 ( 0.177)	Data  0.002 ( 0.003)	Loss 2.5319e-02 (8.5416e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:38 - Epoch: [83][310/352]	Time  0.181 ( 0.177)	Data  0.002 ( 0.003)	Loss 9.3711e-02 (8.5754e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:40 - Epoch: [83][320/352]	Time  0.181 ( 0.177)	Data  0.002 ( 0.003)	Loss 9.9666e-02 (8.5782e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:42 - Epoch: [83][330/352]	Time  0.181 ( 0.177)	Data  0.002 ( 0.003)	Loss 5.2224e-02 (8.5882e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:44 - Epoch: [83][340/352]	Time  0.178 ( 0.177)	Data  0.002 ( 0.003)	Loss 8.7229e-02 (8.6177e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:46 - Epoch: [83][350/352]	Time  0.181 ( 0.177)	Data  0.002 ( 0.003)	Loss 5.0803e-02 (8.5900e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:10:46 - Test: [ 0/20]	Time  0.402 ( 0.402)	Loss 3.5406e-01 (3.5406e-01)	Acc@1  89.45 ( 89.45)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:10:47 - Test: [10/20]	Time  0.101 ( 0.131)	Loss 3.6926e-01 (3.7482e-01)	Acc@1  89.84 ( 89.42)	Acc@5  99.22 ( 99.47)
07-Mar-22 04:10:48 -  * Acc@1 89.540 Acc@5 99.460
07-Mar-22 04:10:48 - Best acc at epoch 83: 89.77999877929688
07-Mar-22 04:10:49 - Epoch: [84][  0/352]	Time  0.398 ( 0.398)	Data  0.241 ( 0.241)	Loss 8.9541e-02 (8.9541e-02)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 04:10:51 - Epoch: [84][ 10/352]	Time  0.178 ( 0.200)	Data  0.002 ( 0.024)	Loss 1.0085e-01 (7.4054e-02)	Acc@1  96.88 ( 97.37)	Acc@5 100.00 (100.00)
07-Mar-22 04:10:52 - Epoch: [84][ 20/352]	Time  0.181 ( 0.191)	Data  0.002 ( 0.014)	Loss 1.3753e-01 (7.3026e-02)	Acc@1  94.53 ( 97.62)	Acc@5 100.00 (100.00)
07-Mar-22 04:10:54 - Epoch: [84][ 30/352]	Time  0.182 ( 0.187)	Data  0.003 ( 0.010)	Loss 5.6942e-02 (8.1136e-02)	Acc@1  98.44 ( 97.18)	Acc@5 100.00 (100.00)
07-Mar-22 04:10:56 - Epoch: [84][ 40/352]	Time  0.180 ( 0.186)	Data  0.003 ( 0.008)	Loss 1.1736e-01 (7.9677e-02)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 (100.00)
07-Mar-22 04:10:58 - Epoch: [84][ 50/352]	Time  0.181 ( 0.185)	Data  0.002 ( 0.007)	Loss 6.6741e-02 (8.1651e-02)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 (100.00)
07-Mar-22 04:11:00 - Epoch: [84][ 60/352]	Time  0.181 ( 0.184)	Data  0.002 ( 0.006)	Loss 1.2093e-01 (8.2652e-02)	Acc@1  95.31 ( 97.03)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:11:02 - Epoch: [84][ 70/352]	Time  0.185 ( 0.184)	Data  0.002 ( 0.006)	Loss 6.7707e-02 (8.2978e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:03 - Epoch: [84][ 80/352]	Time  0.184 ( 0.183)	Data  0.003 ( 0.005)	Loss 1.0330e-01 (8.3130e-02)	Acc@1  95.31 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:05 - Epoch: [84][ 90/352]	Time  0.180 ( 0.183)	Data  0.003 ( 0.005)	Loss 9.8781e-02 (8.2151e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:07 - Epoch: [84][100/352]	Time  0.181 ( 0.183)	Data  0.003 ( 0.005)	Loss 4.4274e-02 (8.1963e-02)	Acc@1  99.22 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:09 - Epoch: [84][110/352]	Time  0.181 ( 0.183)	Data  0.002 ( 0.005)	Loss 9.5711e-02 (8.3914e-02)	Acc@1  95.31 ( 96.98)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:11:11 - Epoch: [84][120/352]	Time  0.185 ( 0.182)	Data  0.002 ( 0.004)	Loss 7.6693e-02 (8.4786e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:12 - Epoch: [84][130/352]	Time  0.180 ( 0.182)	Data  0.002 ( 0.004)	Loss 1.4491e-01 (8.4878e-02)	Acc@1  92.97 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:14 - Epoch: [84][140/352]	Time  0.178 ( 0.182)	Data  0.002 ( 0.004)	Loss 7.0704e-02 (8.5135e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:11:16 - Epoch: [84][150/352]	Time  0.182 ( 0.182)	Data  0.002 ( 0.004)	Loss 1.3006e-01 (8.5077e-02)	Acc@1  94.53 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:11:18 - Epoch: [84][160/352]	Time  0.182 ( 0.182)	Data  0.002 ( 0.004)	Loss 1.4475e-01 (8.4755e-02)	Acc@1  95.31 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:20 - Epoch: [84][170/352]	Time  0.179 ( 0.182)	Data  0.002 ( 0.004)	Loss 3.1526e-02 (8.5164e-02)	Acc@1 100.00 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:21 - Epoch: [84][180/352]	Time  0.185 ( 0.182)	Data  0.002 ( 0.004)	Loss 8.1461e-02 (8.5149e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:23 - Epoch: [84][190/352]	Time  0.178 ( 0.182)	Data  0.002 ( 0.004)	Loss 6.3609e-02 (8.4685e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:25 - Epoch: [84][200/352]	Time  0.181 ( 0.182)	Data  0.002 ( 0.004)	Loss 4.7139e-02 (8.3731e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:27 - Epoch: [84][210/352]	Time  0.182 ( 0.181)	Data  0.002 ( 0.004)	Loss 3.4532e-02 (8.4293e-02)	Acc@1  99.22 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:29 - Epoch: [84][220/352]	Time  0.177 ( 0.181)	Data  0.002 ( 0.003)	Loss 8.5353e-02 (8.4161e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:30 - Epoch: [84][230/352]	Time  0.180 ( 0.181)	Data  0.003 ( 0.003)	Loss 8.7752e-02 (8.3304e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:32 - Epoch: [84][240/352]	Time  0.175 ( 0.181)	Data  0.002 ( 0.003)	Loss 1.5050e-01 (8.3616e-02)	Acc@1  95.31 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:34 - Epoch: [84][250/352]	Time  0.181 ( 0.181)	Data  0.002 ( 0.003)	Loss 6.0022e-02 (8.3455e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:36 - Epoch: [84][260/352]	Time  0.181 ( 0.181)	Data  0.002 ( 0.003)	Loss 1.0914e-01 (8.3724e-02)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:38 - Epoch: [84][270/352]	Time  0.181 ( 0.181)	Data  0.002 ( 0.003)	Loss 8.1406e-02 (8.4127e-02)	Acc@1  95.31 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:39 - Epoch: [84][280/352]	Time  0.179 ( 0.181)	Data  0.002 ( 0.003)	Loss 7.9042e-02 (8.3315e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:41 - Epoch: [84][290/352]	Time  0.180 ( 0.181)	Data  0.002 ( 0.003)	Loss 5.3955e-02 (8.2895e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:43 - Epoch: [84][300/352]	Time  0.184 ( 0.181)	Data  0.003 ( 0.003)	Loss 1.0836e-01 (8.2539e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:45 - Epoch: [84][310/352]	Time  0.179 ( 0.181)	Data  0.003 ( 0.003)	Loss 1.1385e-01 (8.2694e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:47 - Epoch: [84][320/352]	Time  0.180 ( 0.181)	Data  0.002 ( 0.003)	Loss 5.8334e-02 (8.2300e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:48 - Epoch: [84][330/352]	Time  0.180 ( 0.181)	Data  0.002 ( 0.003)	Loss 1.0197e-01 (8.2271e-02)	Acc@1  96.09 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:50 - Epoch: [84][340/352]	Time  0.179 ( 0.181)	Data  0.002 ( 0.003)	Loss 2.2725e-02 (8.2218e-02)	Acc@1  99.22 ( 97.14)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:11:52 - Epoch: [84][350/352]	Time  0.183 ( 0.181)	Data  0.002 ( 0.003)	Loss 8.4793e-02 (8.2703e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:11:53 - Test: [ 0/20]	Time  0.370 ( 0.370)	Loss 3.2641e-01 (3.2641e-01)	Acc@1  91.02 ( 91.02)	Acc@5  99.61 ( 99.61)
07-Mar-22 04:11:54 - Test: [10/20]	Time  0.104 ( 0.124)	Loss 4.1281e-01 (3.7844e-01)	Acc@1  88.28 ( 88.99)	Acc@5  99.22 ( 99.47)
07-Mar-22 04:11:55 -  * Acc@1 89.060 Acc@5 99.500
07-Mar-22 04:11:55 - Best acc at epoch 84: 89.77999877929688
07-Mar-22 04:11:55 - Epoch: [85][  0/352]	Time  0.399 ( 0.399)	Data  0.241 ( 0.241)	Loss 5.9590e-02 (5.9590e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 04:11:57 - Epoch: [85][ 10/352]	Time  0.167 ( 0.200)	Data  0.002 ( 0.024)	Loss 1.0961e-01 (8.3354e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 04:11:58 - Epoch: [85][ 20/352]	Time  0.161 ( 0.182)	Data  0.002 ( 0.014)	Loss 5.9722e-02 (8.0193e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 04:12:00 - Epoch: [85][ 30/352]	Time  0.168 ( 0.177)	Data  0.002 ( 0.010)	Loss 4.5649e-02 (7.9975e-02)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 (100.00)
07-Mar-22 04:12:02 - Epoch: [85][ 40/352]	Time  0.165 ( 0.175)	Data  0.002 ( 0.008)	Loss 1.0094e-01 (8.0488e-02)	Acc@1  97.66 ( 97.22)	Acc@5 100.00 (100.00)
07-Mar-22 04:12:04 - Epoch: [85][ 50/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.007)	Loss 1.1648e-01 (8.4482e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 04:12:05 - Epoch: [85][ 60/352]	Time  0.166 ( 0.172)	Data  0.002 ( 0.006)	Loss 5.5877e-02 (8.5392e-02)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 (100.00)
07-Mar-22 04:12:07 - Epoch: [85][ 70/352]	Time  0.170 ( 0.172)	Data  0.002 ( 0.006)	Loss 5.6911e-02 (8.6846e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 (100.00)
07-Mar-22 04:12:09 - Epoch: [85][ 80/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.005)	Loss 8.7801e-02 (8.7299e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 (100.00)
07-Mar-22 04:12:10 - Epoch: [85][ 90/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.005)	Loss 9.8652e-02 (8.9381e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:12 - Epoch: [85][100/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.005)	Loss 5.9989e-02 (8.8105e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:14 - Epoch: [85][110/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.004)	Loss 6.8118e-02 (8.9223e-02)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:15 - Epoch: [85][120/352]	Time  0.157 ( 0.169)	Data  0.002 ( 0.004)	Loss 8.1010e-02 (8.8314e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:17 - Epoch: [85][130/352]	Time  0.158 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.6700e-02 (8.8498e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:18 - Epoch: [85][140/352]	Time  0.146 ( 0.167)	Data  0.002 ( 0.004)	Loss 8.9937e-02 (8.7838e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:20 - Epoch: [85][150/352]	Time  0.157 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.0105e-01 (8.7628e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:21 - Epoch: [85][160/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.004)	Loss 7.0000e-02 (8.6962e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:23 - Epoch: [85][170/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.5269e-01 (8.7448e-02)	Acc@1  94.53 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:25 - Epoch: [85][180/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.2530e-01 (8.6655e-02)	Acc@1  94.53 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:26 - Epoch: [85][190/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0806e-01 (8.6818e-02)	Acc@1  95.31 ( 96.89)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:28 - Epoch: [85][200/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.2740e-02 (8.7043e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:30 - Epoch: [85][210/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.2475e-01 (8.7611e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:31 - Epoch: [85][220/352]	Time  0.170 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.2451e-01 (8.8545e-02)	Acc@1  95.31 ( 96.81)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:33 - Epoch: [85][230/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.1560e-02 (8.8339e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:35 - Epoch: [85][240/352]	Time  0.163 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.4359e-02 (8.7733e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:36 - Epoch: [85][250/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.3881e-02 (8.8137e-02)	Acc@1  99.22 ( 96.83)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:38 - Epoch: [85][260/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 9.1681e-02 (8.7651e-02)	Acc@1  95.31 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:39 - Epoch: [85][270/352]	Time  0.141 ( 0.165)	Data  0.001 ( 0.003)	Loss 6.7583e-02 (8.7752e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:41 - Epoch: [85][280/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 8.8704e-02 (8.7929e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:43 - Epoch: [85][290/352]	Time  0.168 ( 0.165)	Data  0.002 ( 0.003)	Loss 4.9421e-02 (8.7320e-02)	Acc@1  99.22 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:44 - Epoch: [85][300/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 3.2712e-02 (8.7434e-02)	Acc@1 100.00 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:46 - Epoch: [85][310/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.9173e-02 (8.7908e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:48 - Epoch: [85][320/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.0849e-01 (8.8166e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:49 - Epoch: [85][330/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.1285e-02 (8.8170e-02)	Acc@1  98.44 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:51 - Epoch: [85][340/352]	Time  0.154 ( 0.165)	Data  0.002 ( 0.003)	Loss 7.9073e-02 (8.7633e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:53 - Epoch: [85][350/352]	Time  0.144 ( 0.165)	Data  0.001 ( 0.003)	Loss 5.7922e-02 (8.7517e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:12:53 - Test: [ 0/20]	Time  0.385 ( 0.385)	Loss 3.1859e-01 (3.1859e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:12:54 - Test: [10/20]	Time  0.119 ( 0.140)	Loss 3.9212e-01 (3.8411e-01)	Acc@1  88.67 ( 89.13)	Acc@5  99.22 ( 99.54)
07-Mar-22 04:12:55 -  * Acc@1 89.640 Acc@5 99.480
07-Mar-22 04:12:55 - Best acc at epoch 85: 89.77999877929688
07-Mar-22 04:12:56 - Epoch: [86][  0/352]	Time  0.407 ( 0.407)	Data  0.249 ( 0.249)	Loss 9.4929e-02 (9.4929e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 04:12:57 - Epoch: [86][ 10/352]	Time  0.165 ( 0.186)	Data  0.002 ( 0.024)	Loss 9.8057e-02 (9.0959e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.93)
07-Mar-22 04:12:59 - Epoch: [86][ 20/352]	Time  0.170 ( 0.177)	Data  0.002 ( 0.014)	Loss 6.4568e-02 (8.9474e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.96)
07-Mar-22 04:13:01 - Epoch: [86][ 30/352]	Time  0.168 ( 0.174)	Data  0.002 ( 0.010)	Loss 1.3230e-01 (9.6640e-02)	Acc@1  94.53 ( 96.65)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:13:02 - Epoch: [86][ 40/352]	Time  0.165 ( 0.172)	Data  0.002 ( 0.008)	Loss 4.5876e-02 (8.9266e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:13:04 - Epoch: [86][ 50/352]	Time  0.165 ( 0.171)	Data  0.002 ( 0.007)	Loss 3.3649e-02 (8.5892e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:13:06 - Epoch: [86][ 60/352]	Time  0.171 ( 0.171)	Data  0.002 ( 0.006)	Loss 7.3907e-02 (8.4653e-02)	Acc@1  98.44 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:07 - Epoch: [86][ 70/352]	Time  0.165 ( 0.170)	Data  0.002 ( 0.006)	Loss 3.4923e-02 (8.5065e-02)	Acc@1  99.22 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:09 - Epoch: [86][ 80/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.005)	Loss 5.3308e-02 (8.4879e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:13:11 - Epoch: [86][ 90/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.005)	Loss 7.5469e-02 (8.4485e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:13:12 - Epoch: [86][100/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.005)	Loss 9.3141e-02 (8.4867e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:13:14 - Epoch: [86][110/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.7541e-02 (8.3178e-02)	Acc@1  97.66 ( 97.21)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:16 - Epoch: [86][120/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 4.0003e-02 (8.2869e-02)	Acc@1  99.22 ( 97.24)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:18 - Epoch: [86][130/352]	Time  0.173 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.1521e-01 (8.3477e-02)	Acc@1  93.75 ( 97.16)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:19 - Epoch: [86][140/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 9.6711e-02 (8.3493e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:21 - Epoch: [86][150/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.5322e-02 (8.3246e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:23 - Epoch: [86][160/352]	Time  0.175 ( 0.169)	Data  0.002 ( 0.004)	Loss 4.1784e-02 (8.3074e-02)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:24 - Epoch: [86][170/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.0316e-02 (8.2851e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:26 - Epoch: [86][180/352]	Time  0.159 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.6267e-02 (8.2713e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:27 - Epoch: [86][190/352]	Time  0.146 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.7795e-02 (8.2025e-02)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:29 - Epoch: [86][200/352]	Time  0.161 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0828e-01 (8.2091e-02)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:30 - Epoch: [86][210/352]	Time  0.147 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0129e-01 (8.2046e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:32 - Epoch: [86][220/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.5883e-02 (8.2212e-02)	Acc@1  98.44 ( 97.18)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:34 - Epoch: [86][230/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.003)	Loss 3.8288e-02 (8.2255e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:35 - Epoch: [86][240/352]	Time  0.167 ( 0.165)	Data  0.002 ( 0.003)	Loss 5.4421e-02 (8.2568e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:37 - Epoch: [86][250/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.0285e-01 (8.3036e-02)	Acc@1  95.31 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:39 - Epoch: [86][260/352]	Time  0.171 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.6130e-01 (8.3109e-02)	Acc@1  94.53 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:40 - Epoch: [86][270/352]	Time  0.169 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.1196e-01 (8.3226e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:42 - Epoch: [86][280/352]	Time  0.164 ( 0.165)	Data  0.002 ( 0.003)	Loss 4.7481e-02 (8.3123e-02)	Acc@1  98.44 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:44 - Epoch: [86][290/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.4814e-02 (8.2652e-02)	Acc@1 100.00 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:45 - Epoch: [86][300/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.003)	Loss 9.0078e-02 (8.2842e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:47 - Epoch: [86][310/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.7828e-02 (8.2975e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:49 - Epoch: [86][320/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0112e-01 (8.2736e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:50 - Epoch: [86][330/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.3730e-02 (8.2822e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:52 - Epoch: [86][340/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.2455e-02 (8.2597e-02)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:54 - Epoch: [86][350/352]	Time  0.169 ( 0.166)	Data  0.001 ( 0.003)	Loss 1.0166e-01 (8.2093e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:13:54 - Test: [ 0/20]	Time  0.373 ( 0.373)	Loss 2.9557e-01 (2.9557e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:13:55 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 4.0507e-01 (3.7882e-01)	Acc@1  88.67 ( 89.31)	Acc@5  99.22 ( 99.33)
07-Mar-22 04:13:56 -  * Acc@1 89.400 Acc@5 99.400
07-Mar-22 04:13:56 - Best acc at epoch 86: 89.77999877929688
07-Mar-22 04:13:57 - Epoch: [87][  0/352]	Time  0.398 ( 0.398)	Data  0.240 ( 0.240)	Loss 9.4857e-02 (9.4857e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 04:13:58 - Epoch: [87][ 10/352]	Time  0.168 ( 0.190)	Data  0.002 ( 0.024)	Loss 1.5081e-01 (9.5262e-02)	Acc@1  95.31 ( 97.16)	Acc@5 100.00 (100.00)
07-Mar-22 04:14:00 - Epoch: [87][ 20/352]	Time  0.167 ( 0.180)	Data  0.002 ( 0.014)	Loss 7.3348e-02 (9.2534e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 04:14:02 - Epoch: [87][ 30/352]	Time  0.167 ( 0.176)	Data  0.002 ( 0.010)	Loss 5.3772e-02 (9.1148e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 (100.00)
07-Mar-22 04:14:03 - Epoch: [87][ 40/352]	Time  0.167 ( 0.174)	Data  0.002 ( 0.008)	Loss 9.2750e-02 (9.2782e-02)	Acc@1  96.09 ( 96.74)	Acc@5 100.00 (100.00)
07-Mar-22 04:14:05 - Epoch: [87][ 50/352]	Time  0.166 ( 0.175)	Data  0.002 ( 0.007)	Loss 5.4773e-02 (8.9674e-02)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:07 - Epoch: [87][ 60/352]	Time  0.167 ( 0.173)	Data  0.002 ( 0.006)	Loss 7.5453e-02 (8.7318e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:14:09 - Epoch: [87][ 70/352]	Time  0.168 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.2068e-01 (8.9161e-02)	Acc@1  95.31 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:14:10 - Epoch: [87][ 80/352]	Time  0.167 ( 0.172)	Data  0.002 ( 0.005)	Loss 7.0379e-02 (8.9680e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:12 - Epoch: [87][ 90/352]	Time  0.170 ( 0.171)	Data  0.002 ( 0.005)	Loss 8.5106e-02 (8.7842e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:14 - Epoch: [87][100/352]	Time  0.170 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.0919e-01 (8.7455e-02)	Acc@1  95.31 ( 96.93)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:14:15 - Epoch: [87][110/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.1130e-01 (8.6876e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:14:17 - Epoch: [87][120/352]	Time  0.170 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.4160e-01 (8.8049e-02)	Acc@1  93.75 ( 96.87)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:14:19 - Epoch: [87][130/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.1503e-01 (8.9154e-02)	Acc@1  95.31 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:20 - Epoch: [87][140/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.004)	Loss 8.3636e-02 (8.9010e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:22 - Epoch: [87][150/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.004)	Loss 4.9796e-02 (8.8636e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:24 - Epoch: [87][160/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.0499e-01 (8.8927e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:25 - Epoch: [87][170/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.004)	Loss 1.2422e-01 (8.8615e-02)	Acc@1  94.53 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:27 - Epoch: [87][180/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.9194e-02 (8.8272e-02)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:29 - Epoch: [87][190/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.5979e-02 (8.7600e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:30 - Epoch: [87][200/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0959e-01 (8.6933e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:32 - Epoch: [87][210/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0631e-01 (8.6162e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:34 - Epoch: [87][220/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 5.8025e-02 (8.6105e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:35 - Epoch: [87][230/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.2174e-01 (8.5732e-02)	Acc@1  93.75 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:37 - Epoch: [87][240/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.6393e-01 (8.6088e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:39 - Epoch: [87][250/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 5.9256e-02 (8.6093e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:40 - Epoch: [87][260/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.2457e-01 (8.6422e-02)	Acc@1  95.31 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:14:42 - Epoch: [87][270/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0866e-01 (8.6338e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:14:44 - Epoch: [87][280/352]	Time  0.164 ( 0.169)	Data  0.002 ( 0.003)	Loss 8.5542e-02 (8.6302e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:14:45 - Epoch: [87][290/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.4810e-01 (8.6519e-02)	Acc@1  94.53 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:14:47 - Epoch: [87][300/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.1097e-01 (8.7039e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:14:49 - Epoch: [87][310/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 7.1281e-02 (8.6760e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:14:50 - Epoch: [87][320/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.3260e-01 (8.7380e-02)	Acc@1  94.53 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:14:52 - Epoch: [87][330/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.2817e-01 (8.7194e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:14:54 - Epoch: [87][340/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.2724e-01 (8.6961e-02)	Acc@1  94.53 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:55 - Epoch: [87][350/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0561e-01 (8.7035e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:14:56 - Test: [ 0/20]	Time  0.386 ( 0.386)	Loss 3.0369e-01 (3.0369e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.61 ( 99.61)
07-Mar-22 04:14:57 - Test: [10/20]	Time  0.116 ( 0.135)	Loss 3.4711e-01 (3.7393e-01)	Acc@1  89.45 ( 89.49)	Acc@5  99.22 ( 99.50)
07-Mar-22 04:14:58 -  * Acc@1 89.340 Acc@5 99.440
07-Mar-22 04:14:58 - Best acc at epoch 87: 89.77999877929688
07-Mar-22 04:14:59 - Epoch: [88][  0/352]	Time  0.399 ( 0.399)	Data  0.235 ( 0.235)	Loss 1.0041e-01 (1.0041e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 04:15:00 - Epoch: [88][ 10/352]	Time  0.167 ( 0.188)	Data  0.002 ( 0.023)	Loss 8.4237e-02 (9.3632e-02)	Acc@1  96.88 ( 96.59)	Acc@5 100.00 (100.00)
07-Mar-22 04:15:02 - Epoch: [88][ 20/352]	Time  0.166 ( 0.177)	Data  0.002 ( 0.013)	Loss 8.6793e-02 (8.7912e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 (100.00)
07-Mar-22 04:15:03 - Epoch: [88][ 30/352]	Time  0.142 ( 0.169)	Data  0.001 ( 0.009)	Loss 8.2339e-02 (8.5602e-02)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 (100.00)
07-Mar-22 04:15:05 - Epoch: [88][ 40/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.008)	Loss 1.2506e-01 (9.0971e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 04:15:07 - Epoch: [88][ 50/352]	Time  0.147 ( 0.166)	Data  0.002 ( 0.007)	Loss 9.0922e-02 (8.8193e-02)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 (100.00)
07-Mar-22 04:15:08 - Epoch: [88][ 60/352]	Time  0.155 ( 0.164)	Data  0.002 ( 0.006)	Loss 1.2254e-01 (8.8656e-02)	Acc@1  96.09 ( 96.86)	Acc@5 100.00 (100.00)
07-Mar-22 04:15:10 - Epoch: [88][ 70/352]	Time  0.153 ( 0.163)	Data  0.002 ( 0.005)	Loss 4.4635e-02 (8.8698e-02)	Acc@1  99.22 ( 96.84)	Acc@5 100.00 (100.00)
07-Mar-22 04:15:11 - Epoch: [88][ 80/352]	Time  0.155 ( 0.162)	Data  0.002 ( 0.005)	Loss 9.4471e-02 (8.7901e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 04:15:13 - Epoch: [88][ 90/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.004)	Loss 9.5747e-02 (8.8278e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 (100.00)
07-Mar-22 04:15:15 - Epoch: [88][100/352]	Time  0.158 ( 0.162)	Data  0.002 ( 0.004)	Loss 1.1465e-01 (8.6749e-02)	Acc@1  95.31 ( 96.89)	Acc@5 100.00 (100.00)
07-Mar-22 04:15:16 - Epoch: [88][110/352]	Time  0.147 ( 0.161)	Data  0.002 ( 0.004)	Loss 3.1087e-02 (8.5665e-02)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 (100.00)
07-Mar-22 04:15:18 - Epoch: [88][120/352]	Time  0.140 ( 0.160)	Data  0.001 ( 0.004)	Loss 1.0479e-01 (8.6806e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:19 - Epoch: [88][130/352]	Time  0.141 ( 0.159)	Data  0.001 ( 0.004)	Loss 1.0130e-01 (8.7694e-02)	Acc@1  95.31 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:20 - Epoch: [88][140/352]	Time  0.149 ( 0.158)	Data  0.002 ( 0.004)	Loss 8.1302e-02 (8.7725e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:22 - Epoch: [88][150/352]	Time  0.164 ( 0.158)	Data  0.002 ( 0.003)	Loss 6.3442e-02 (8.6346e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:24 - Epoch: [88][160/352]	Time  0.170 ( 0.159)	Data  0.002 ( 0.003)	Loss 3.6810e-02 (8.6407e-02)	Acc@1 100.00 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:25 - Epoch: [88][170/352]	Time  0.143 ( 0.158)	Data  0.001 ( 0.003)	Loss 8.7750e-02 (8.6309e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:27 - Epoch: [88][180/352]	Time  0.149 ( 0.158)	Data  0.002 ( 0.003)	Loss 9.9284e-02 (8.5584e-02)	Acc@1  95.31 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:28 - Epoch: [88][190/352]	Time  0.142 ( 0.157)	Data  0.001 ( 0.003)	Loss 1.1206e-01 (8.5615e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:30 - Epoch: [88][200/352]	Time  0.142 ( 0.157)	Data  0.002 ( 0.003)	Loss 9.3418e-02 (8.5724e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:31 - Epoch: [88][210/352]	Time  0.155 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.1710e-01 (8.4790e-02)	Acc@1  95.31 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:33 - Epoch: [88][220/352]	Time  0.144 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.0500e-01 (8.5071e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:34 - Epoch: [88][230/352]	Time  0.146 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.1699e-02 (8.5063e-02)	Acc@1  95.31 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:36 - Epoch: [88][240/352]	Time  0.166 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.3506e-02 (8.5499e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:38 - Epoch: [88][250/352]	Time  0.169 ( 0.157)	Data  0.002 ( 0.003)	Loss 1.0315e-01 (8.5798e-02)	Acc@1  95.31 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:39 - Epoch: [88][260/352]	Time  0.167 ( 0.157)	Data  0.002 ( 0.003)	Loss 6.6456e-02 (8.5746e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:41 - Epoch: [88][270/352]	Time  0.168 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.0911e-01 (8.5418e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:43 - Epoch: [88][280/352]	Time  0.158 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.3072e-01 (8.5656e-02)	Acc@1  94.53 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:44 - Epoch: [88][290/352]	Time  0.160 ( 0.158)	Data  0.002 ( 0.003)	Loss 8.0961e-02 (8.5825e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:46 - Epoch: [88][300/352]	Time  0.143 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.2465e-01 (8.5963e-02)	Acc@1  94.53 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:47 - Epoch: [88][310/352]	Time  0.157 ( 0.158)	Data  0.002 ( 0.003)	Loss 6.7094e-02 (8.5694e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:49 - Epoch: [88][320/352]	Time  0.158 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.3298e-01 (8.5753e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:51 - Epoch: [88][330/352]	Time  0.159 ( 0.158)	Data  0.002 ( 0.003)	Loss 4.2815e-02 (8.5000e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:52 - Epoch: [88][340/352]	Time  0.158 ( 0.158)	Data  0.001 ( 0.003)	Loss 1.3612e-01 (8.5030e-02)	Acc@1  95.31 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:54 - Epoch: [88][350/352]	Time  0.166 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.1347e-01 (8.5366e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:15:55 - Test: [ 0/20]	Time  0.385 ( 0.385)	Loss 3.1440e-01 (3.1440e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.61 ( 99.61)
07-Mar-22 04:15:56 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.9761e-01 (3.7965e-01)	Acc@1  88.67 ( 89.13)	Acc@5  99.22 ( 99.50)
07-Mar-22 04:15:56 -  * Acc@1 89.240 Acc@5 99.580
07-Mar-22 04:15:56 - Best acc at epoch 88: 89.77999877929688
07-Mar-22 04:15:57 - Epoch: [89][  0/352]	Time  0.413 ( 0.413)	Data  0.247 ( 0.247)	Loss 2.7644e-02 (2.7644e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
07-Mar-22 04:15:59 - Epoch: [89][ 10/352]	Time  0.169 ( 0.191)	Data  0.002 ( 0.024)	Loss 9.4830e-02 (1.0320e-01)	Acc@1  97.66 ( 96.73)	Acc@5 100.00 (100.00)
07-Mar-22 04:16:00 - Epoch: [89][ 20/352]	Time  0.173 ( 0.179)	Data  0.002 ( 0.014)	Loss 1.0082e-01 (9.0526e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 (100.00)
07-Mar-22 04:16:02 - Epoch: [89][ 30/352]	Time  0.167 ( 0.175)	Data  0.002 ( 0.010)	Loss 9.8127e-02 (9.4263e-02)	Acc@1  95.31 ( 96.57)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:16:04 - Epoch: [89][ 40/352]	Time  0.171 ( 0.173)	Data  0.002 ( 0.008)	Loss 9.5894e-02 (9.0058e-02)	Acc@1  95.31 ( 96.68)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:16:05 - Epoch: [89][ 50/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.007)	Loss 1.1824e-01 (9.0503e-02)	Acc@1  96.09 ( 96.71)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:16:07 - Epoch: [89][ 60/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.006)	Loss 8.4259e-02 (8.9703e-02)	Acc@1  97.66 ( 96.76)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:09 - Epoch: [89][ 70/352]	Time  0.173 ( 0.170)	Data  0.002 ( 0.006)	Loss 1.1579e-01 (8.9575e-02)	Acc@1  96.09 ( 96.82)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:10 - Epoch: [89][ 80/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.005)	Loss 3.7818e-02 (8.9397e-02)	Acc@1  99.22 ( 96.80)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:12 - Epoch: [89][ 90/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.005)	Loss 7.2453e-02 (8.7049e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:14 - Epoch: [89][100/352]	Time  0.160 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.0953e-01 (8.6301e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:15 - Epoch: [89][110/352]	Time  0.160 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.2191e-02 (8.5484e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:17 - Epoch: [89][120/352]	Time  0.159 ( 0.167)	Data  0.002 ( 0.004)	Loss 3.5484e-02 (8.4395e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:18 - Epoch: [89][130/352]	Time  0.150 ( 0.166)	Data  0.001 ( 0.004)	Loss 7.6377e-02 (8.4836e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:20 - Epoch: [89][140/352]	Time  0.152 ( 0.165)	Data  0.002 ( 0.004)	Loss 7.6193e-02 (8.4989e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:21 - Epoch: [89][150/352]	Time  0.159 ( 0.165)	Data  0.002 ( 0.004)	Loss 9.2794e-02 (8.5818e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:23 - Epoch: [89][160/352]	Time  0.159 ( 0.164)	Data  0.002 ( 0.004)	Loss 1.3319e-01 (8.5466e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:25 - Epoch: [89][170/352]	Time  0.158 ( 0.164)	Data  0.002 ( 0.003)	Loss 4.9971e-02 (8.4795e-02)	Acc@1  99.22 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:26 - Epoch: [89][180/352]	Time  0.154 ( 0.163)	Data  0.002 ( 0.003)	Loss 4.0939e-02 (8.4778e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:28 - Epoch: [89][190/352]	Time  0.162 ( 0.163)	Data  0.002 ( 0.003)	Loss 6.1096e-02 (8.5001e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:29 - Epoch: [89][200/352]	Time  0.142 ( 0.162)	Data  0.002 ( 0.003)	Loss 5.7430e-02 (8.4997e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:31 - Epoch: [89][210/352]	Time  0.141 ( 0.161)	Data  0.001 ( 0.003)	Loss 1.0576e-01 (8.5098e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:32 - Epoch: [89][220/352]	Time  0.141 ( 0.160)	Data  0.002 ( 0.003)	Loss 5.1413e-02 (8.4679e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:33 - Epoch: [89][230/352]	Time  0.141 ( 0.159)	Data  0.001 ( 0.003)	Loss 6.1484e-02 (8.3890e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:35 - Epoch: [89][240/352]	Time  0.141 ( 0.159)	Data  0.002 ( 0.003)	Loss 6.0214e-02 (8.3538e-02)	Acc@1  99.22 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:36 - Epoch: [89][250/352]	Time  0.141 ( 0.158)	Data  0.001 ( 0.003)	Loss 4.0499e-02 (8.3472e-02)	Acc@1  99.22 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:38 - Epoch: [89][260/352]	Time  0.141 ( 0.157)	Data  0.002 ( 0.003)	Loss 8.4133e-02 (8.3206e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:39 - Epoch: [89][270/352]	Time  0.141 ( 0.157)	Data  0.001 ( 0.003)	Loss 3.7675e-02 (8.3951e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:40 - Epoch: [89][280/352]	Time  0.141 ( 0.156)	Data  0.001 ( 0.003)	Loss 9.0446e-02 (8.3981e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:42 - Epoch: [89][290/352]	Time  0.141 ( 0.156)	Data  0.002 ( 0.003)	Loss 6.1751e-02 (8.3543e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:43 - Epoch: [89][300/352]	Time  0.167 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.8311e-02 (8.3847e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:45 - Epoch: [89][310/352]	Time  0.167 ( 0.156)	Data  0.002 ( 0.003)	Loss 6.9995e-02 (8.3440e-02)	Acc@1  99.22 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:47 - Epoch: [89][320/352]	Time  0.168 ( 0.156)	Data  0.002 ( 0.003)	Loss 5.7483e-02 (8.3508e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:48 - Epoch: [89][330/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.003)	Loss 7.4466e-02 (8.3671e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:50 - Epoch: [89][340/352]	Time  0.166 ( 0.157)	Data  0.002 ( 0.003)	Loss 6.5510e-02 (8.3722e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:52 - Epoch: [89][350/352]	Time  0.168 ( 0.157)	Data  0.002 ( 0.003)	Loss 6.1933e-02 (8.3634e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:16:52 - Test: [ 0/20]	Time  0.374 ( 0.374)	Loss 3.1418e-01 (3.1418e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:16:53 - Test: [10/20]	Time  0.099 ( 0.125)	Loss 4.0657e-01 (3.8085e-01)	Acc@1  87.89 ( 89.56)	Acc@5  99.22 ( 99.57)
07-Mar-22 04:16:54 -  * Acc@1 89.480 Acc@5 99.560
07-Mar-22 04:16:54 - Best acc at epoch 89: 89.77999877929688
07-Mar-22 04:16:55 - Epoch: [90][  0/352]	Time  0.382 ( 0.382)	Data  0.238 ( 0.238)	Loss 6.7091e-02 (6.7091e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
07-Mar-22 04:16:56 - Epoch: [90][ 10/352]	Time  0.164 ( 0.179)	Data  0.002 ( 0.023)	Loss 1.0779e-01 (8.5711e-02)	Acc@1  95.31 ( 96.80)	Acc@5 100.00 (100.00)
07-Mar-22 04:16:58 - Epoch: [90][ 20/352]	Time  0.164 ( 0.173)	Data  0.002 ( 0.013)	Loss 8.8586e-02 (8.0726e-02)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 (100.00)
07-Mar-22 04:17:00 - Epoch: [90][ 30/352]	Time  0.169 ( 0.171)	Data  0.002 ( 0.010)	Loss 1.3111e-01 (8.5277e-02)	Acc@1  95.31 ( 96.95)	Acc@5 100.00 (100.00)
07-Mar-22 04:17:01 - Epoch: [90][ 40/352]	Time  0.165 ( 0.171)	Data  0.002 ( 0.008)	Loss 1.1722e-01 (8.4218e-02)	Acc@1  95.31 ( 96.93)	Acc@5 100.00 (100.00)
07-Mar-22 04:17:03 - Epoch: [90][ 50/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.007)	Loss 8.7397e-02 (8.1610e-02)	Acc@1  96.09 ( 97.06)	Acc@5 100.00 (100.00)
07-Mar-22 04:17:05 - Epoch: [90][ 60/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.006)	Loss 1.2768e-01 (8.3878e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 (100.00)
07-Mar-22 04:17:06 - Epoch: [90][ 70/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.005)	Loss 7.9186e-02 (8.5178e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 (100.00)
07-Mar-22 04:17:08 - Epoch: [90][ 80/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.005)	Loss 7.9537e-02 (8.4851e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 (100.00)
07-Mar-22 04:17:10 - Epoch: [90][ 90/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.1313e-01 (8.4099e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 (100.00)
07-Mar-22 04:17:11 - Epoch: [90][100/352]	Time  0.163 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.0366e-01 (8.5761e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:13 - Epoch: [90][110/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 3.8907e-02 (8.5390e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:15 - Epoch: [90][120/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2401e-01 (8.5939e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:17 - Epoch: [90][130/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.5176e-02 (8.4989e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:18 - Epoch: [90][140/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.7754e-02 (8.6248e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:17:20 - Epoch: [90][150/352]	Time  0.167 ( 0.168)	Data  0.003 ( 0.004)	Loss 7.9703e-02 (8.6758e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:17:22 - Epoch: [90][160/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.9551e-02 (8.6375e-02)	Acc@1  94.53 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:23 - Epoch: [90][170/352]	Time  0.167 ( 0.168)	Data  0.001 ( 0.003)	Loss 9.1940e-02 (8.5418e-02)	Acc@1  98.44 ( 96.88)	Acc@5  99.22 ( 99.98)
07-Mar-22 04:17:25 - Epoch: [90][180/352]	Time  0.141 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.1644e-02 (8.4859e-02)	Acc@1  99.22 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:17:26 - Epoch: [90][190/352]	Time  0.144 ( 0.166)	Data  0.002 ( 0.003)	Loss 9.9855e-02 (8.5393e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:17:28 - Epoch: [90][200/352]	Time  0.143 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.4997e-01 (8.5790e-02)	Acc@1  93.75 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:17:29 - Epoch: [90][210/352]	Time  0.142 ( 0.163)	Data  0.002 ( 0.003)	Loss 9.6638e-02 (8.5927e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:17:30 - Epoch: [90][220/352]	Time  0.143 ( 0.162)	Data  0.002 ( 0.003)	Loss 6.1735e-02 (8.4783e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:17:32 - Epoch: [90][230/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 6.1016e-02 (8.4816e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:17:34 - Epoch: [90][240/352]	Time  0.164 ( 0.162)	Data  0.002 ( 0.003)	Loss 8.1504e-02 (8.4922e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:17:35 - Epoch: [90][250/352]	Time  0.166 ( 0.162)	Data  0.002 ( 0.003)	Loss 7.0718e-02 (8.5192e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:17:37 - Epoch: [90][260/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 1.1240e-01 (8.5358e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:39 - Epoch: [90][270/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.6957e-02 (8.5493e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:40 - Epoch: [90][280/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 5.6580e-02 (8.5492e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:42 - Epoch: [90][290/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 9.6018e-02 (8.5697e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:44 - Epoch: [90][300/352]	Time  0.166 ( 0.163)	Data  0.002 ( 0.003)	Loss 7.9168e-02 (8.5085e-02)	Acc@1  96.09 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:45 - Epoch: [90][310/352]	Time  0.167 ( 0.163)	Data  0.002 ( 0.003)	Loss 3.5272e-02 (8.5362e-02)	Acc@1  99.22 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:47 - Epoch: [90][320/352]	Time  0.169 ( 0.163)	Data  0.002 ( 0.003)	Loss 9.8641e-02 (8.5291e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:49 - Epoch: [90][330/352]	Time  0.167 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.1153e-01 (8.5106e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:50 - Epoch: [90][340/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 1.2487e-01 (8.4764e-02)	Acc@1  96.88 ( 96.95)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:17:52 - Epoch: [90][350/352]	Time  0.168 ( 0.164)	Data  0.002 ( 0.003)	Loss 2.2778e-02 (8.4458e-02)	Acc@1 100.00 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:17:53 - Test: [ 0/20]	Time  0.406 ( 0.406)	Loss 3.4909e-01 (3.4909e-01)	Acc@1  88.67 ( 88.67)	Acc@5  99.61 ( 99.61)
07-Mar-22 04:17:54 - Test: [10/20]	Time  0.098 ( 0.127)	Loss 3.9890e-01 (3.7446e-01)	Acc@1  89.06 ( 89.17)	Acc@5  99.22 ( 99.47)
07-Mar-22 04:17:55 -  * Acc@1 89.200 Acc@5 99.440
07-Mar-22 04:17:55 - Best acc at epoch 90: 89.77999877929688
07-Mar-22 04:17:55 - Epoch: [91][  0/352]	Time  0.411 ( 0.411)	Data  0.240 ( 0.240)	Loss 8.1227e-02 (8.1227e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 04:17:57 - Epoch: [91][ 10/352]	Time  0.165 ( 0.190)	Data  0.002 ( 0.024)	Loss 9.1372e-02 (7.2634e-02)	Acc@1  94.53 ( 97.30)	Acc@5 100.00 (100.00)
07-Mar-22 04:17:58 - Epoch: [91][ 20/352]	Time  0.169 ( 0.180)	Data  0.002 ( 0.013)	Loss 8.6566e-02 (8.4476e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 (100.00)
07-Mar-22 04:18:00 - Epoch: [91][ 30/352]	Time  0.165 ( 0.176)	Data  0.002 ( 0.010)	Loss 8.1309e-02 (8.4756e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.95)
07-Mar-22 04:18:02 - Epoch: [91][ 40/352]	Time  0.166 ( 0.173)	Data  0.002 ( 0.008)	Loss 8.9290e-02 (8.3375e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.96)
07-Mar-22 04:18:03 - Epoch: [91][ 50/352]	Time  0.162 ( 0.170)	Data  0.002 ( 0.007)	Loss 1.1232e-01 (8.3313e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:18:05 - Epoch: [91][ 60/352]	Time  0.159 ( 0.168)	Data  0.002 ( 0.006)	Loss 7.8692e-02 (8.3336e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.96)
07-Mar-22 04:18:06 - Epoch: [91][ 70/352]	Time  0.162 ( 0.166)	Data  0.002 ( 0.005)	Loss 7.3619e-02 (8.1869e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:18:08 - Epoch: [91][ 80/352]	Time  0.142 ( 0.164)	Data  0.001 ( 0.005)	Loss 1.3730e-01 (8.2372e-02)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:18:09 - Epoch: [91][ 90/352]	Time  0.147 ( 0.162)	Data  0.002 ( 0.005)	Loss 8.1814e-02 (8.3142e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:18:11 - Epoch: [91][100/352]	Time  0.152 ( 0.161)	Data  0.002 ( 0.004)	Loss 3.9879e-02 (8.1656e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:12 - Epoch: [91][110/352]	Time  0.152 ( 0.159)	Data  0.001 ( 0.004)	Loss 6.5096e-02 (8.3061e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:14 - Epoch: [91][120/352]	Time  0.162 ( 0.159)	Data  0.002 ( 0.004)	Loss 9.0380e-02 (8.4259e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:15 - Epoch: [91][130/352]	Time  0.165 ( 0.159)	Data  0.002 ( 0.004)	Loss 6.8042e-02 (8.3345e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:17 - Epoch: [91][140/352]	Time  0.157 ( 0.159)	Data  0.001 ( 0.004)	Loss 8.3854e-02 (8.3317e-02)	Acc@1  96.09 ( 97.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:19 - Epoch: [91][150/352]	Time  0.144 ( 0.159)	Data  0.001 ( 0.003)	Loss 2.1800e-01 (8.4179e-02)	Acc@1  92.19 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:20 - Epoch: [91][160/352]	Time  0.165 ( 0.158)	Data  0.001 ( 0.003)	Loss 1.2025e-01 (8.4038e-02)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:22 - Epoch: [91][170/352]	Time  0.167 ( 0.159)	Data  0.002 ( 0.003)	Loss 9.4329e-02 (8.3156e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:23 - Epoch: [91][180/352]	Time  0.160 ( 0.159)	Data  0.002 ( 0.003)	Loss 1.5870e-01 (8.3574e-02)	Acc@1  93.75 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:25 - Epoch: [91][190/352]	Time  0.165 ( 0.159)	Data  0.002 ( 0.003)	Loss 9.7369e-02 (8.4103e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:27 - Epoch: [91][200/352]	Time  0.143 ( 0.159)	Data  0.002 ( 0.003)	Loss 9.2734e-02 (8.4280e-02)	Acc@1  96.09 ( 97.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:28 - Epoch: [91][210/352]	Time  0.152 ( 0.158)	Data  0.002 ( 0.003)	Loss 6.5679e-02 (8.4182e-02)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:18:30 - Epoch: [91][220/352]	Time  0.144 ( 0.158)	Data  0.002 ( 0.003)	Loss 1.2633e-01 (8.4265e-02)	Acc@1  92.97 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:18:31 - Epoch: [91][230/352]	Time  0.157 ( 0.157)	Data  0.002 ( 0.003)	Loss 9.7807e-02 (8.4674e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:33 - Epoch: [91][240/352]	Time  0.159 ( 0.157)	Data  0.002 ( 0.003)	Loss 6.9762e-02 (8.4623e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:34 - Epoch: [91][250/352]	Time  0.167 ( 0.157)	Data  0.002 ( 0.003)	Loss 4.4181e-02 (8.4457e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:18:36 - Epoch: [91][260/352]	Time  0.143 ( 0.157)	Data  0.001 ( 0.003)	Loss 1.1487e-01 (8.4862e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:18:37 - Epoch: [91][270/352]	Time  0.150 ( 0.157)	Data  0.001 ( 0.003)	Loss 7.4861e-02 (8.4086e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:18:39 - Epoch: [91][280/352]	Time  0.148 ( 0.156)	Data  0.002 ( 0.003)	Loss 1.1900e-01 (8.4449e-02)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:18:40 - Epoch: [91][290/352]	Time  0.157 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.9374e-02 (8.4784e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:18:42 - Epoch: [91][300/352]	Time  0.142 ( 0.156)	Data  0.001 ( 0.003)	Loss 6.1973e-02 (8.4608e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:18:43 - Epoch: [91][310/352]	Time  0.162 ( 0.156)	Data  0.002 ( 0.003)	Loss 5.4536e-02 (8.5140e-02)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:18:45 - Epoch: [91][320/352]	Time  0.161 ( 0.156)	Data  0.002 ( 0.003)	Loss 8.5887e-02 (8.5128e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:18:46 - Epoch: [91][330/352]	Time  0.162 ( 0.156)	Data  0.002 ( 0.003)	Loss 6.4794e-02 (8.4568e-02)	Acc@1  96.88 ( 97.02)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:18:48 - Epoch: [91][340/352]	Time  0.153 ( 0.156)	Data  0.002 ( 0.003)	Loss 9.6647e-02 (8.5099e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:18:49 - Epoch: [91][350/352]	Time  0.159 ( 0.156)	Data  0.002 ( 0.003)	Loss 3.7768e-02 (8.4898e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:18:50 - Test: [ 0/20]	Time  0.390 ( 0.390)	Loss 3.3430e-01 (3.3430e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.61 ( 99.61)
07-Mar-22 04:18:51 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 4.6822e-01 (3.8034e-01)	Acc@1  85.94 ( 88.96)	Acc@5  99.22 ( 99.40)
07-Mar-22 04:18:52 -  * Acc@1 89.280 Acc@5 99.500
07-Mar-22 04:18:52 - Best acc at epoch 91: 89.77999877929688
07-Mar-22 04:18:53 - Epoch: [92][  0/352]	Time  0.430 ( 0.430)	Data  0.248 ( 0.248)	Loss 1.0175e-01 (1.0175e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 04:18:54 - Epoch: [92][ 10/352]	Time  0.161 ( 0.188)	Data  0.002 ( 0.024)	Loss 4.7710e-02 (9.2688e-02)	Acc@1  98.44 ( 96.66)	Acc@5 100.00 (100.00)
07-Mar-22 04:18:56 - Epoch: [92][ 20/352]	Time  0.168 ( 0.178)	Data  0.002 ( 0.014)	Loss 3.8184e-02 (8.6351e-02)	Acc@1  99.22 ( 97.10)	Acc@5 100.00 (100.00)
07-Mar-22 04:18:58 - Epoch: [92][ 30/352]	Time  0.167 ( 0.175)	Data  0.002 ( 0.010)	Loss 9.8210e-02 (8.5962e-02)	Acc@1  95.31 ( 96.98)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:18:59 - Epoch: [92][ 40/352]	Time  0.166 ( 0.172)	Data  0.002 ( 0.008)	Loss 7.5735e-02 (8.6132e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:19:01 - Epoch: [92][ 50/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.007)	Loss 7.6233e-02 (8.4626e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:19:03 - Epoch: [92][ 60/352]	Time  0.170 ( 0.171)	Data  0.002 ( 0.006)	Loss 7.5472e-02 (8.3447e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:04 - Epoch: [92][ 70/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.006)	Loss 6.9676e-02 (8.2622e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:19:06 - Epoch: [92][ 80/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.005)	Loss 6.4792e-02 (8.5353e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:08 - Epoch: [92][ 90/352]	Time  0.166 ( 0.171)	Data  0.002 ( 0.005)	Loss 4.3528e-02 (8.5836e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:09 - Epoch: [92][100/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.005)	Loss 9.1592e-02 (8.5523e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:11 - Epoch: [92][110/352]	Time  0.165 ( 0.170)	Data  0.002 ( 0.004)	Loss 5.2649e-02 (8.3868e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:13 - Epoch: [92][120/352]	Time  0.169 ( 0.170)	Data  0.002 ( 0.004)	Loss 6.0518e-02 (8.2573e-02)	Acc@1  99.22 ( 97.09)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:14 - Epoch: [92][130/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.004)	Loss 9.4928e-02 (8.2398e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:16 - Epoch: [92][140/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.004)	Loss 9.1605e-02 (8.1492e-02)	Acc@1  96.09 ( 97.15)	Acc@5  99.22 ( 99.97)
07-Mar-22 04:19:18 - Epoch: [92][150/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.7674e-02 (8.1282e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:19 - Epoch: [92][160/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.5626e-02 (8.1085e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:21 - Epoch: [92][170/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.7299e-02 (8.1472e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:23 - Epoch: [92][180/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.1212e-01 (8.1831e-02)	Acc@1  95.31 ( 97.13)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:24 - Epoch: [92][190/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.003)	Loss 9.2095e-02 (8.2416e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:26 - Epoch: [92][200/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 8.0243e-02 (8.2892e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:28 - Epoch: [92][210/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.8007e-02 (8.2920e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:29 - Epoch: [92][220/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.5203e-02 (8.3192e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:19:31 - Epoch: [92][230/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0893e-01 (8.3158e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:19:33 - Epoch: [92][240/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.1212e-01 (8.3190e-02)	Acc@1  93.75 ( 97.09)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:19:34 - Epoch: [92][250/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.0466e-02 (8.2134e-02)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:19:36 - Epoch: [92][260/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.3216e-02 (8.2295e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:38 - Epoch: [92][270/352]	Time  0.141 ( 0.168)	Data  0.001 ( 0.003)	Loss 7.9076e-02 (8.1970e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:39 - Epoch: [92][280/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.4504e-02 (8.2453e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:41 - Epoch: [92][290/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.4367e-01 (8.2560e-02)	Acc@1  95.31 ( 97.12)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:43 - Epoch: [92][300/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0388e-01 (8.2672e-02)	Acc@1  94.53 ( 97.10)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:44 - Epoch: [92][310/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.6281e-01 (8.2884e-02)	Acc@1  94.53 ( 97.10)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:19:46 - Epoch: [92][320/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.2675e-02 (8.3139e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:19:48 - Epoch: [92][330/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.8547e-02 (8.2991e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:19:49 - Epoch: [92][340/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.5579e-02 (8.2879e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:19:51 - Epoch: [92][350/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.3107e-02 (8.2690e-02)	Acc@1  99.22 ( 97.09)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:19:52 - Test: [ 0/20]	Time  0.388 ( 0.388)	Loss 3.4162e-01 (3.4162e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:19:53 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.6059e-01 (3.8652e-01)	Acc@1  90.23 ( 89.24)	Acc@5  98.83 ( 99.33)
07-Mar-22 04:19:54 -  * Acc@1 89.580 Acc@5 99.300
07-Mar-22 04:19:54 - Best acc at epoch 92: 89.77999877929688
07-Mar-22 04:19:54 - Epoch: [93][  0/352]	Time  0.380 ( 0.380)	Data  0.233 ( 0.233)	Loss 7.9196e-02 (7.9196e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 04:19:56 - Epoch: [93][ 10/352]	Time  0.141 ( 0.167)	Data  0.001 ( 0.023)	Loss 3.8890e-02 (7.7791e-02)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 04:19:57 - Epoch: [93][ 20/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.013)	Loss 9.8881e-02 (7.5212e-02)	Acc@1  96.09 ( 97.66)	Acc@5 100.00 (100.00)
07-Mar-22 04:19:59 - Epoch: [93][ 30/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.009)	Loss 4.8410e-02 (7.6247e-02)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 (100.00)
07-Mar-22 04:20:01 - Epoch: [93][ 40/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.008)	Loss 1.0086e-01 (7.7710e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:02 - Epoch: [93][ 50/352]	Time  0.168 ( 0.167)	Data  0.003 ( 0.006)	Loss 7.6293e-02 (7.6530e-02)	Acc@1  99.22 ( 97.69)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:04 - Epoch: [93][ 60/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.006)	Loss 1.0959e-01 (7.8367e-02)	Acc@1  96.09 ( 97.58)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:06 - Epoch: [93][ 70/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.005)	Loss 1.5863e-01 (7.9524e-02)	Acc@1  96.09 ( 97.47)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:07 - Epoch: [93][ 80/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.005)	Loss 6.6523e-02 (7.9414e-02)	Acc@1  98.44 ( 97.37)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:09 - Epoch: [93][ 90/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.005)	Loss 9.3366e-02 (8.0641e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:11 - Epoch: [93][100/352]	Time  0.172 ( 0.167)	Data  0.003 ( 0.004)	Loss 1.1344e-01 (8.3028e-02)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:12 - Epoch: [93][110/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.004)	Loss 8.5282e-02 (8.2967e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:14 - Epoch: [93][120/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.004)	Loss 6.8834e-02 (8.3749e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:16 - Epoch: [93][130/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.004)	Loss 6.3345e-02 (8.4335e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:17 - Epoch: [93][140/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.004)	Loss 9.1828e-02 (8.3719e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:19 - Epoch: [93][150/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.004)	Loss 1.4656e-01 (8.4954e-02)	Acc@1  92.97 ( 97.01)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:21 - Epoch: [93][160/352]	Time  0.171 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1471e-01 (8.6044e-02)	Acc@1  96.09 ( 96.94)	Acc@5  99.22 ( 99.98)
07-Mar-22 04:20:22 - Epoch: [93][170/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.0570e-02 (8.5342e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:24 - Epoch: [93][180/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.1879e-02 (8.5782e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:26 - Epoch: [93][190/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.8228e-02 (8.5979e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:27 - Epoch: [93][200/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.9594e-02 (8.6258e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:29 - Epoch: [93][210/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2546e-01 (8.6836e-02)	Acc@1  94.53 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:31 - Epoch: [93][220/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0949e-01 (8.7271e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:32 - Epoch: [93][230/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.8107e-02 (8.7076e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:34 - Epoch: [93][240/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.0998e-02 (8.6449e-02)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:36 - Epoch: [93][250/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.9422e-02 (8.6728e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:37 - Epoch: [93][260/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3308e-01 (8.6567e-02)	Acc@1  96.09 ( 96.93)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:20:39 - Epoch: [93][270/352]	Time  0.143 ( 0.166)	Data  0.001 ( 0.003)	Loss 4.5022e-02 (8.5978e-02)	Acc@1  99.22 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:40 - Epoch: [93][280/352]	Time  0.150 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.3509e-02 (8.5879e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:20:42 - Epoch: [93][290/352]	Time  0.166 ( 0.165)	Data  0.002 ( 0.003)	Loss 8.3217e-02 (8.5837e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:43 - Epoch: [93][300/352]	Time  0.163 ( 0.165)	Data  0.002 ( 0.003)	Loss 6.9274e-02 (8.5318e-02)	Acc@1  99.22 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:45 - Epoch: [93][310/352]	Time  0.163 ( 0.165)	Data  0.002 ( 0.003)	Loss 4.8374e-02 (8.5115e-02)	Acc@1  99.22 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:47 - Epoch: [93][320/352]	Time  0.153 ( 0.165)	Data  0.002 ( 0.003)	Loss 1.3451e-01 (8.4970e-02)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:48 - Epoch: [93][330/352]	Time  0.148 ( 0.165)	Data  0.002 ( 0.003)	Loss 9.8887e-02 (8.4997e-02)	Acc@1  96.09 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:50 - Epoch: [93][340/352]	Time  0.159 ( 0.164)	Data  0.002 ( 0.003)	Loss 9.5724e-02 (8.4999e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:51 - Epoch: [93][350/352]	Time  0.170 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.7796e-02 (8.4831e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:20:52 - Test: [ 0/20]	Time  0.390 ( 0.390)	Loss 3.3657e-01 (3.3657e-01)	Acc@1  87.89 ( 87.89)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:20:53 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 4.1466e-01 (3.9352e-01)	Acc@1  87.89 ( 88.78)	Acc@5  99.22 ( 99.47)
07-Mar-22 04:20:54 -  * Acc@1 89.440 Acc@5 99.520
07-Mar-22 04:20:54 - Best acc at epoch 93: 89.77999877929688
07-Mar-22 04:20:55 - Epoch: [94][  0/352]	Time  0.421 ( 0.421)	Data  0.252 ( 0.252)	Loss 1.3655e-01 (1.3655e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 04:20:56 - Epoch: [94][ 10/352]	Time  0.166 ( 0.190)	Data  0.002 ( 0.025)	Loss 7.6014e-02 (8.3474e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 (100.00)
07-Mar-22 04:20:58 - Epoch: [94][ 20/352]	Time  0.171 ( 0.180)	Data  0.002 ( 0.014)	Loss 5.9119e-02 (8.7730e-02)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 (100.00)
07-Mar-22 04:21:00 - Epoch: [94][ 30/352]	Time  0.172 ( 0.176)	Data  0.002 ( 0.010)	Loss 9.1893e-02 (8.7350e-02)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 (100.00)
07-Mar-22 04:21:01 - Epoch: [94][ 40/352]	Time  0.167 ( 0.174)	Data  0.002 ( 0.008)	Loss 1.0256e-01 (8.3741e-02)	Acc@1  94.53 ( 97.31)	Acc@5 100.00 (100.00)
07-Mar-22 04:21:03 - Epoch: [94][ 50/352]	Time  0.167 ( 0.173)	Data  0.002 ( 0.007)	Loss 6.0439e-02 (8.4435e-02)	Acc@1  96.88 ( 97.20)	Acc@5 100.00 (100.00)
07-Mar-22 04:21:04 - Epoch: [94][ 60/352]	Time  0.141 ( 0.169)	Data  0.002 ( 0.006)	Loss 1.4353e-01 (8.6269e-02)	Acc@1  93.75 ( 97.09)	Acc@5 100.00 (100.00)
07-Mar-22 04:21:06 - Epoch: [94][ 70/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.006)	Loss 1.3667e-01 (8.7807e-02)	Acc@1  96.88 ( 97.01)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:21:08 - Epoch: [94][ 80/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.005)	Loss 7.8618e-02 (8.5490e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:10 - Epoch: [94][ 90/352]	Time  0.164 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.1827e-01 (8.5183e-02)	Acc@1  95.31 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:11 - Epoch: [94][100/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.005)	Loss 4.7412e-02 (8.3643e-02)	Acc@1  99.22 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:13 - Epoch: [94][110/352]	Time  0.173 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.3111e-01 (8.4381e-02)	Acc@1  93.75 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:15 - Epoch: [94][120/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.004)	Loss 8.6231e-02 (8.5472e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:16 - Epoch: [94][130/352]	Time  0.152 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.9466e-02 (8.5731e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:18 - Epoch: [94][140/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.2106e-01 (8.6160e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:19 - Epoch: [94][150/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.4734e-02 (8.6919e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:21 - Epoch: [94][160/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.6837e-02 (8.6995e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:23 - Epoch: [94][170/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.3709e-01 (8.6687e-02)	Acc@1  93.75 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:24 - Epoch: [94][180/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.3995e-02 (8.6725e-02)	Acc@1  95.31 ( 96.91)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:26 - Epoch: [94][190/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.8770e-02 (8.6544e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:28 - Epoch: [94][200/352]	Time  0.141 ( 0.167)	Data  0.001 ( 0.003)	Loss 5.4392e-02 (8.5945e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:29 - Epoch: [94][210/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.9500e-02 (8.6402e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:31 - Epoch: [94][220/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.1345e-02 (8.6268e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:33 - Epoch: [94][230/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.9834e-02 (8.6469e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:34 - Epoch: [94][240/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.9115e-02 (8.6173e-02)	Acc@1  96.88 ( 96.95)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:21:36 - Epoch: [94][250/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.4618e-01 (8.6317e-02)	Acc@1  94.53 ( 96.96)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:38 - Epoch: [94][260/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3316e-01 (8.6034e-02)	Acc@1  95.31 ( 96.98)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:39 - Epoch: [94][270/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.9079e-02 (8.5793e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:41 - Epoch: [94][280/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.4845e-01 (8.5542e-02)	Acc@1  95.31 ( 97.00)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:43 - Epoch: [94][290/352]	Time  0.170 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.3162e-01 (8.5105e-02)	Acc@1  95.31 ( 97.01)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:44 - Epoch: [94][300/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.0659e-02 (8.4600e-02)	Acc@1  99.22 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:46 - Epoch: [94][310/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 3.6789e-02 (8.4738e-02)	Acc@1 100.00 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:48 - Epoch: [94][320/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.2457e-02 (8.4401e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:49 - Epoch: [94][330/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.9607e-02 (8.4036e-02)	Acc@1  99.22 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:51 - Epoch: [94][340/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0315e-01 (8.4148e-02)	Acc@1  95.31 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:53 - Epoch: [94][350/352]	Time  0.164 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.9552e-02 (8.3812e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:21:53 - Test: [ 0/20]	Time  0.392 ( 0.392)	Loss 2.8873e-01 (2.8873e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:21:54 - Test: [10/20]	Time  0.098 ( 0.125)	Loss 3.9099e-01 (3.7092e-01)	Acc@1  89.06 ( 89.49)	Acc@5  99.22 ( 99.54)
07-Mar-22 04:21:55 -  * Acc@1 89.160 Acc@5 99.480
07-Mar-22 04:21:55 - Best acc at epoch 94: 89.77999877929688
07-Mar-22 04:21:56 - Epoch: [95][  0/352]	Time  0.382 ( 0.382)	Data  0.236 ( 0.236)	Loss 9.3125e-02 (9.3125e-02)	Acc@1  96.09 ( 96.09)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:21:57 - Epoch: [95][ 10/352]	Time  0.154 ( 0.172)	Data  0.002 ( 0.023)	Loss 1.3271e-01 (8.4641e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 ( 99.93)
07-Mar-22 04:21:59 - Epoch: [95][ 20/352]	Time  0.143 ( 0.159)	Data  0.002 ( 0.013)	Loss 1.0045e-01 (8.4247e-02)	Acc@1  95.31 ( 97.14)	Acc@5 100.00 ( 99.96)
07-Mar-22 04:22:00 - Epoch: [95][ 30/352]	Time  0.141 ( 0.154)	Data  0.002 ( 0.009)	Loss 1.0240e-01 (8.6374e-02)	Acc@1  96.09 ( 97.05)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:22:02 - Epoch: [95][ 40/352]	Time  0.143 ( 0.153)	Data  0.002 ( 0.007)	Loss 1.1569e-01 (8.4344e-02)	Acc@1  95.31 ( 97.05)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:22:03 - Epoch: [95][ 50/352]	Time  0.143 ( 0.151)	Data  0.002 ( 0.006)	Loss 1.0280e-01 (8.5573e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:22:04 - Epoch: [95][ 60/352]	Time  0.143 ( 0.150)	Data  0.002 ( 0.006)	Loss 1.1711e-01 (8.6368e-02)	Acc@1  95.31 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:06 - Epoch: [95][ 70/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.005)	Loss 5.6972e-02 (8.6245e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:07 - Epoch: [95][ 80/352]	Time  0.142 ( 0.148)	Data  0.002 ( 0.005)	Loss 7.0439e-02 (8.6272e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:09 - Epoch: [95][ 90/352]	Time  0.144 ( 0.147)	Data  0.002 ( 0.004)	Loss 6.5968e-02 (8.3843e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:10 - Epoch: [95][100/352]	Time  0.142 ( 0.147)	Data  0.002 ( 0.004)	Loss 1.2254e-01 (8.4192e-02)	Acc@1  93.75 ( 97.17)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:12 - Epoch: [95][110/352]	Time  0.143 ( 0.148)	Data  0.002 ( 0.004)	Loss 4.3100e-02 (8.4533e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:13 - Epoch: [95][120/352]	Time  0.143 ( 0.147)	Data  0.002 ( 0.004)	Loss 3.4488e-02 (8.3262e-02)	Acc@1  99.22 ( 97.16)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:15 - Epoch: [95][130/352]	Time  0.162 ( 0.147)	Data  0.002 ( 0.004)	Loss 5.7336e-02 (8.3248e-02)	Acc@1  96.09 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:16 - Epoch: [95][140/352]	Time  0.164 ( 0.148)	Data  0.002 ( 0.003)	Loss 7.5097e-02 (8.2685e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:18 - Epoch: [95][150/352]	Time  0.150 ( 0.148)	Data  0.002 ( 0.003)	Loss 4.7506e-02 (8.3036e-02)	Acc@1  99.22 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:19 - Epoch: [95][160/352]	Time  0.168 ( 0.148)	Data  0.002 ( 0.003)	Loss 6.0538e-02 (8.2291e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 (100.00)
07-Mar-22 04:22:21 - Epoch: [95][170/352]	Time  0.143 ( 0.149)	Data  0.002 ( 0.003)	Loss 2.4817e-02 (8.2181e-02)	Acc@1  99.22 ( 97.14)	Acc@5 100.00 (100.00)
07-Mar-22 04:22:22 - Epoch: [95][180/352]	Time  0.144 ( 0.149)	Data  0.002 ( 0.003)	Loss 4.9855e-02 (8.1207e-02)	Acc@1 100.00 ( 97.20)	Acc@5 100.00 (100.00)
07-Mar-22 04:22:24 - Epoch: [95][190/352]	Time  0.157 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.1109e-01 (8.1629e-02)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 (100.00)
07-Mar-22 04:22:25 - Epoch: [95][200/352]	Time  0.144 ( 0.148)	Data  0.002 ( 0.003)	Loss 5.8533e-02 (8.1311e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 (100.00)
07-Mar-22 04:22:27 - Epoch: [95][210/352]	Time  0.142 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.5398e-01 (8.1786e-02)	Acc@1  96.09 ( 97.16)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:22:28 - Epoch: [95][220/352]	Time  0.141 ( 0.148)	Data  0.002 ( 0.003)	Loss 1.7162e-01 (8.2339e-02)	Acc@1  94.53 ( 97.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:29 - Epoch: [95][230/352]	Time  0.141 ( 0.148)	Data  0.002 ( 0.003)	Loss 6.6349e-02 (8.2772e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:31 - Epoch: [95][240/352]	Time  0.140 ( 0.147)	Data  0.002 ( 0.003)	Loss 8.6568e-02 (8.2424e-02)	Acc@1  96.09 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:32 - Epoch: [95][250/352]	Time  0.141 ( 0.147)	Data  0.002 ( 0.003)	Loss 5.8763e-02 (8.2419e-02)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:34 - Epoch: [95][260/352]	Time  0.141 ( 0.147)	Data  0.002 ( 0.003)	Loss 7.2572e-02 (8.3108e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:35 - Epoch: [95][270/352]	Time  0.168 ( 0.147)	Data  0.002 ( 0.003)	Loss 7.6422e-02 (8.3262e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:37 - Epoch: [95][280/352]	Time  0.167 ( 0.147)	Data  0.002 ( 0.003)	Loss 1.1646e-01 (8.2909e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:38 - Epoch: [95][290/352]	Time  0.171 ( 0.148)	Data  0.002 ( 0.003)	Loss 4.3590e-02 (8.2712e-02)	Acc@1  99.22 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:40 - Epoch: [95][300/352]	Time  0.167 ( 0.148)	Data  0.002 ( 0.003)	Loss 9.5654e-02 (8.3021e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:42 - Epoch: [95][310/352]	Time  0.169 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.8793e-01 (8.3450e-02)	Acc@1  92.19 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:43 - Epoch: [95][320/352]	Time  0.166 ( 0.149)	Data  0.002 ( 0.003)	Loss 1.1361e-01 (8.3673e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:45 - Epoch: [95][330/352]	Time  0.168 ( 0.150)	Data  0.002 ( 0.003)	Loss 8.1047e-02 (8.3358e-02)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:47 - Epoch: [95][340/352]	Time  0.167 ( 0.151)	Data  0.002 ( 0.003)	Loss 4.6754e-02 (8.3493e-02)	Acc@1  98.44 ( 97.11)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:48 - Epoch: [95][350/352]	Time  0.169 ( 0.151)	Data  0.002 ( 0.003)	Loss 7.9928e-02 (8.3475e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:22:49 - Test: [ 0/20]	Time  0.355 ( 0.355)	Loss 3.0548e-01 (3.0548e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.61 ( 99.61)
07-Mar-22 04:22:50 - Test: [10/20]	Time  0.098 ( 0.122)	Loss 3.8107e-01 (3.7806e-01)	Acc@1  89.45 ( 89.28)	Acc@5  99.61 ( 99.33)
07-Mar-22 04:22:51 -  * Acc@1 89.480 Acc@5 99.340
07-Mar-22 04:22:51 - Best acc at epoch 95: 89.77999877929688
07-Mar-22 04:22:51 - Epoch: [96][  0/352]	Time  0.406 ( 0.406)	Data  0.258 ( 0.258)	Loss 1.0143e-01 (1.0143e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 04:22:53 - Epoch: [96][ 10/352]	Time  0.171 ( 0.188)	Data  0.002 ( 0.025)	Loss 1.1957e-01 (1.0211e-01)	Acc@1  96.88 ( 96.66)	Acc@5 100.00 (100.00)
07-Mar-22 04:22:55 - Epoch: [96][ 20/352]	Time  0.170 ( 0.179)	Data  0.002 ( 0.014)	Loss 4.3168e-02 (8.9104e-02)	Acc@1  99.22 ( 97.17)	Acc@5 100.00 (100.00)
07-Mar-22 04:22:56 - Epoch: [96][ 30/352]	Time  0.171 ( 0.175)	Data  0.002 ( 0.010)	Loss 7.0896e-02 (8.6990e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 (100.00)
07-Mar-22 04:22:58 - Epoch: [96][ 40/352]	Time  0.169 ( 0.173)	Data  0.002 ( 0.008)	Loss 1.0707e-01 (8.6455e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 (100.00)
07-Mar-22 04:23:00 - Epoch: [96][ 50/352]	Time  0.164 ( 0.173)	Data  0.002 ( 0.007)	Loss 7.8548e-02 (8.4378e-02)	Acc@1  98.44 ( 97.15)	Acc@5 100.00 (100.00)
07-Mar-22 04:23:01 - Epoch: [96][ 60/352]	Time  0.168 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.2509e-01 (8.5479e-02)	Acc@1  98.44 ( 97.14)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:23:03 - Epoch: [96][ 70/352]	Time  0.165 ( 0.171)	Data  0.002 ( 0.006)	Loss 4.1599e-02 (8.4497e-02)	Acc@1  99.22 ( 97.16)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:05 - Epoch: [96][ 80/352]	Time  0.166 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.3049e-01 (8.3820e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:06 - Epoch: [96][ 90/352]	Time  0.173 ( 0.170)	Data  0.003 ( 0.005)	Loss 4.7068e-02 (8.3399e-02)	Acc@1 100.00 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:08 - Epoch: [96][100/352]	Time  0.168 ( 0.170)	Data  0.002 ( 0.005)	Loss 4.0598e-02 (8.2201e-02)	Acc@1  99.22 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:10 - Epoch: [96][110/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.2185e-02 (8.2468e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:11 - Epoch: [96][120/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.5204e-01 (8.3423e-02)	Acc@1  96.09 ( 97.08)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:23:13 - Epoch: [96][130/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 6.9162e-02 (8.3177e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:15 - Epoch: [96][140/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 4.0290e-02 (8.3288e-02)	Acc@1  99.22 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:16 - Epoch: [96][150/352]	Time  0.164 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.7503e-02 (8.3739e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:18 - Epoch: [96][160/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.5848e-01 (8.4227e-02)	Acc@1  93.75 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:20 - Epoch: [96][170/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 5.4210e-02 (8.3863e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:21 - Epoch: [96][180/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 9.4409e-02 (8.4029e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:23 - Epoch: [96][190/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.0568e-01 (8.4058e-02)	Acc@1  94.53 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:25 - Epoch: [96][200/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.9995e-02 (8.3612e-02)	Acc@1  96.09 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:26 - Epoch: [96][210/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.0927e-02 (8.3562e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:28 - Epoch: [96][220/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.1902e-02 (8.3909e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:30 - Epoch: [96][230/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.3923e-02 (8.3600e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:31 - Epoch: [96][240/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.7980e-02 (8.3500e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:33 - Epoch: [96][250/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.3312e-02 (8.3256e-02)	Acc@1  99.22 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:35 - Epoch: [96][260/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.7071e-02 (8.3529e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:36 - Epoch: [96][270/352]	Time  0.146 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.0389e-02 (8.3911e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:38 - Epoch: [96][280/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3702e-01 (8.4842e-02)	Acc@1  96.09 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:40 - Epoch: [96][290/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.3853e-02 (8.4577e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:41 - Epoch: [96][300/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.7959e-01 (8.5316e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:43 - Epoch: [96][310/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0023e-01 (8.5668e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:45 - Epoch: [96][320/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.9071e-02 (8.5352e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:46 - Epoch: [96][330/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.8780e-02 (8.5009e-02)	Acc@1  96.09 ( 97.05)	Acc@5  99.22 ( 99.99)
07-Mar-22 04:23:48 - Epoch: [96][340/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 7.8712e-02 (8.5360e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:50 - Epoch: [96][350/352]	Time  0.167 ( 0.167)	Data  0.001 ( 0.003)	Loss 6.2308e-02 (8.5436e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:23:50 - Test: [ 0/20]	Time  0.363 ( 0.363)	Loss 3.3244e-01 (3.3244e-01)	Acc@1  89.45 ( 89.45)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:23:51 - Test: [10/20]	Time  0.098 ( 0.132)	Loss 4.0084e-01 (3.8258e-01)	Acc@1  88.28 ( 89.24)	Acc@5  99.22 ( 99.40)
07-Mar-22 04:23:52 -  * Acc@1 89.400 Acc@5 99.500
07-Mar-22 04:23:52 - Best acc at epoch 96: 89.77999877929688
07-Mar-22 04:23:53 - Epoch: [97][  0/352]	Time  0.396 ( 0.396)	Data  0.250 ( 0.250)	Loss 8.3409e-02 (8.3409e-02)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
07-Mar-22 04:23:55 - Epoch: [97][ 10/352]	Time  0.168 ( 0.189)	Data  0.002 ( 0.024)	Loss 1.1560e-01 (8.3848e-02)	Acc@1  95.31 ( 96.73)	Acc@5 100.00 ( 99.93)
07-Mar-22 04:23:56 - Epoch: [97][ 20/352]	Time  0.167 ( 0.173)	Data  0.002 ( 0.014)	Loss 3.6368e-02 (8.2116e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.96)
07-Mar-22 04:23:58 - Epoch: [97][ 30/352]	Time  0.168 ( 0.171)	Data  0.003 ( 0.010)	Loss 1.5244e-01 (8.5091e-02)	Acc@1  93.75 ( 97.00)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:23:59 - Epoch: [97][ 40/352]	Time  0.165 ( 0.170)	Data  0.002 ( 0.008)	Loss 1.2457e-01 (8.2448e-02)	Acc@1  93.75 ( 97.05)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:01 - Epoch: [97][ 50/352]	Time  0.171 ( 0.170)	Data  0.002 ( 0.007)	Loss 6.7060e-02 (8.3322e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.95)
07-Mar-22 04:24:03 - Epoch: [97][ 60/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.006)	Loss 1.1670e-01 (8.5390e-02)	Acc@1  98.44 ( 97.05)	Acc@5  99.22 ( 99.95)
07-Mar-22 04:24:04 - Epoch: [97][ 70/352]	Time  0.143 ( 0.168)	Data  0.002 ( 0.006)	Loss 1.4956e-01 (8.5087e-02)	Acc@1  94.53 ( 97.07)	Acc@5 100.00 ( 99.96)
07-Mar-22 04:24:06 - Epoch: [97][ 80/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.005)	Loss 6.2722e-02 (8.5755e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.96)
07-Mar-22 04:24:08 - Epoch: [97][ 90/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.005)	Loss 5.8191e-02 (8.4790e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:24:09 - Epoch: [97][100/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.1476e-01 (8.6890e-02)	Acc@1  94.53 ( 96.94)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:24:11 - Epoch: [97][110/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 9.7605e-02 (8.6389e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:24:13 - Epoch: [97][120/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.004)	Loss 7.2226e-02 (8.5909e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:24:15 - Epoch: [97][130/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.004)	Loss 9.4919e-02 (8.5754e-02)	Acc@1  95.31 ( 97.02)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:16 - Epoch: [97][140/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.0381e-02 (8.5282e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:24:18 - Epoch: [97][150/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 1.0075e-01 (8.4862e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:24:20 - Epoch: [97][160/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.6349e-02 (8.6107e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:21 - Epoch: [97][170/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.0204e-02 (8.5665e-02)	Acc@1 100.00 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:23 - Epoch: [97][180/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.8517e-02 (8.5713e-02)	Acc@1  99.22 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:25 - Epoch: [97][190/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.6429e-02 (8.5366e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:26 - Epoch: [97][200/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.1781e-02 (8.4434e-02)	Acc@1  99.22 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:28 - Epoch: [97][210/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.2227e-02 (8.4303e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:29 - Epoch: [97][220/352]	Time  0.168 ( 0.168)	Data  0.001 ( 0.003)	Loss 1.4498e-01 (8.3981e-02)	Acc@1  95.31 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:31 - Epoch: [97][230/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.5168e-02 (8.3997e-02)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:33 - Epoch: [97][240/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.1667e-02 (8.3897e-02)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:34 - Epoch: [97][250/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.8562e-02 (8.4072e-02)	Acc@1  96.09 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:36 - Epoch: [97][260/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.6512e-02 (8.3781e-02)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:38 - Epoch: [97][270/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.8586e-02 (8.3749e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:40 - Epoch: [97][280/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.6497e-02 (8.4605e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:41 - Epoch: [97][290/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.003)	Loss 7.6970e-02 (8.4489e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:43 - Epoch: [97][300/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0422e-01 (8.4332e-02)	Acc@1  97.66 ( 97.10)	Acc@5  99.22 ( 99.98)
07-Mar-22 04:24:45 - Epoch: [97][310/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 3.5886e-02 (8.3965e-02)	Acc@1 100.00 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:46 - Epoch: [97][320/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.1485e-02 (8.3902e-02)	Acc@1  96.09 ( 97.13)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:48 - Epoch: [97][330/352]	Time  0.164 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0230e-01 (8.3724e-02)	Acc@1  97.66 ( 97.13)	Acc@5  99.22 ( 99.98)
07-Mar-22 04:24:50 - Epoch: [97][340/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.2416e-02 (8.3581e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:51 - Epoch: [97][350/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 6.7838e-02 (8.3457e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:24:52 - Test: [ 0/20]	Time  0.400 ( 0.400)	Loss 2.9966e-01 (2.9966e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:24:53 - Test: [10/20]	Time  0.098 ( 0.126)	Loss 3.8537e-01 (3.7277e-01)	Acc@1  87.11 ( 88.78)	Acc@5  99.22 ( 99.57)
07-Mar-22 04:24:54 -  * Acc@1 89.080 Acc@5 99.560
07-Mar-22 04:24:54 - Best acc at epoch 97: 89.77999877929688
07-Mar-22 04:24:54 - Epoch: [98][  0/352]	Time  0.390 ( 0.390)	Data  0.248 ( 0.248)	Loss 8.2537e-02 (8.2537e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 04:24:56 - Epoch: [98][ 10/352]	Time  0.165 ( 0.165)	Data  0.001 ( 0.024)	Loss 3.9607e-02 (8.7018e-02)	Acc@1  99.22 ( 96.88)	Acc@5 100.00 ( 99.93)
07-Mar-22 04:24:57 - Epoch: [98][ 20/352]	Time  0.165 ( 0.165)	Data  0.002 ( 0.014)	Loss 1.5958e-01 (8.2628e-02)	Acc@1  92.97 ( 97.17)	Acc@5 100.00 ( 99.96)
07-Mar-22 04:24:59 - Epoch: [98][ 30/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.010)	Loss 1.0335e-01 (8.2633e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:25:01 - Epoch: [98][ 40/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.008)	Loss 4.9799e-02 (8.4895e-02)	Acc@1  99.22 ( 97.07)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:02 - Epoch: [98][ 50/352]	Time  0.170 ( 0.166)	Data  0.002 ( 0.007)	Loss 1.1885e-01 (8.4783e-02)	Acc@1  93.75 ( 96.98)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:04 - Epoch: [98][ 60/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.006)	Loss 8.6981e-02 (8.7467e-02)	Acc@1  95.31 ( 96.84)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:25:06 - Epoch: [98][ 70/352]	Time  0.163 ( 0.166)	Data  0.002 ( 0.005)	Loss 1.0458e-01 (8.9217e-02)	Acc@1  96.09 ( 96.73)	Acc@5  99.22 ( 99.97)
07-Mar-22 04:25:07 - Epoch: [98][ 80/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.005)	Loss 4.4586e-02 (8.8127e-02)	Acc@1  99.22 ( 96.78)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:25:09 - Epoch: [98][ 90/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.005)	Loss 1.1400e-01 (8.9303e-02)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:25:11 - Epoch: [98][100/352]	Time  0.166 ( 0.166)	Data  0.001 ( 0.004)	Loss 1.8430e-01 (8.9540e-02)	Acc@1  95.31 ( 96.78)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:12 - Epoch: [98][110/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.004)	Loss 7.7092e-02 (9.0398e-02)	Acc@1  98.44 ( 96.76)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:14 - Epoch: [98][120/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.004)	Loss 9.1788e-02 (8.9172e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:25:16 - Epoch: [98][130/352]	Time  0.168 ( 0.166)	Data  0.002 ( 0.004)	Loss 5.4470e-02 (8.8268e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:17 - Epoch: [98][140/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.004)	Loss 6.7487e-02 (8.8449e-02)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:19 - Epoch: [98][150/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.004)	Loss 6.2203e-02 (8.8107e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:21 - Epoch: [98][160/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.3608e-01 (8.8723e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:22 - Epoch: [98][170/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.3962e-01 (8.8544e-02)	Acc@1  94.53 ( 96.87)	Acc@5  99.22 ( 99.98)
07-Mar-22 04:25:24 - Epoch: [98][180/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.6329e-02 (8.8641e-02)	Acc@1  96.09 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:26 - Epoch: [98][190/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.2499e-01 (8.8587e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:27 - Epoch: [98][200/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0834e-01 (8.8801e-02)	Acc@1  94.53 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:29 - Epoch: [98][210/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.7197e-02 (8.7816e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:31 - Epoch: [98][220/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.5163e-02 (8.7845e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:32 - Epoch: [98][230/352]	Time  0.170 ( 0.166)	Data  0.002 ( 0.003)	Loss 9.6208e-02 (8.8213e-02)	Acc@1  96.09 ( 96.86)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:34 - Epoch: [98][240/352]	Time  0.170 ( 0.166)	Data  0.002 ( 0.003)	Loss 5.5007e-02 (8.7646e-02)	Acc@1  99.22 ( 96.89)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:36 - Epoch: [98][250/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 9.0196e-02 (8.8203e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:25:37 - Epoch: [98][260/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 9.3306e-02 (8.7963e-02)	Acc@1  96.09 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:25:39 - Epoch: [98][270/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.8840e-02 (8.8222e-02)	Acc@1  99.22 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:25:41 - Epoch: [98][280/352]	Time  0.166 ( 0.166)	Data  0.002 ( 0.003)	Loss 7.1834e-02 (8.7700e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:25:42 - Epoch: [98][290/352]	Time  0.164 ( 0.166)	Data  0.002 ( 0.003)	Loss 3.6478e-02 (8.7702e-02)	Acc@1  99.22 ( 96.84)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:25:44 - Epoch: [98][300/352]	Time  0.163 ( 0.166)	Data  0.002 ( 0.003)	Loss 8.5766e-02 (8.7341e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:25:46 - Epoch: [98][310/352]	Time  0.170 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.4629e-01 (8.6992e-02)	Acc@1  94.53 ( 96.88)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:25:47 - Epoch: [98][320/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.4530e-01 (8.7047e-02)	Acc@1  94.53 ( 96.89)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:25:49 - Epoch: [98][330/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.003)	Loss 4.8056e-02 (8.7174e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:25:51 - Epoch: [98][340/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.003)	Loss 6.6691e-02 (8.7301e-02)	Acc@1  98.44 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:25:52 - Epoch: [98][350/352]	Time  0.165 ( 0.166)	Data  0.002 ( 0.003)	Loss 1.0262e-01 (8.7484e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:25:53 - Test: [ 0/20]	Time  0.393 ( 0.393)	Loss 3.0900e-01 (3.0900e-01)	Acc@1  91.02 ( 91.02)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:25:54 - Test: [10/20]	Time  0.098 ( 0.132)	Loss 3.5567e-01 (3.6994e-01)	Acc@1  90.23 ( 89.49)	Acc@5  99.61 ( 99.64)
07-Mar-22 04:25:55 -  * Acc@1 89.440 Acc@5 99.580
07-Mar-22 04:25:55 - Best acc at epoch 98: 89.77999877929688
07-Mar-22 04:25:56 - Epoch: [99][  0/352]	Time  0.386 ( 0.386)	Data  0.240 ( 0.240)	Loss 5.6109e-02 (5.6109e-02)	Acc@1  99.22 ( 99.22)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:25:57 - Epoch: [99][ 10/352]	Time  0.141 ( 0.183)	Data  0.001 ( 0.024)	Loss 6.7241e-02 (8.4965e-02)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 ( 99.93)
07-Mar-22 04:25:59 - Epoch: [99][ 20/352]	Time  0.168 ( 0.174)	Data  0.002 ( 0.013)	Loss 1.0149e-01 (8.6905e-02)	Acc@1  97.66 ( 97.40)	Acc@5 100.00 ( 99.96)
07-Mar-22 04:26:00 - Epoch: [99][ 30/352]	Time  0.167 ( 0.172)	Data  0.002 ( 0.010)	Loss 1.1003e-01 (8.6367e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.97)
07-Mar-22 04:26:02 - Epoch: [99][ 40/352]	Time  0.167 ( 0.171)	Data  0.002 ( 0.008)	Loss 6.3343e-02 (8.4714e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:26:04 - Epoch: [99][ 50/352]	Time  0.167 ( 0.170)	Data  0.002 ( 0.007)	Loss 1.0995e-01 (8.3437e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:26:05 - Epoch: [99][ 60/352]	Time  0.173 ( 0.170)	Data  0.002 ( 0.006)	Loss 1.0957e-01 (8.2782e-02)	Acc@1  96.09 ( 97.25)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:07 - Epoch: [99][ 70/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.005)	Loss 6.5040e-02 (8.2255e-02)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:09 - Epoch: [99][ 80/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.005)	Loss 6.8333e-02 (8.1676e-02)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:11 - Epoch: [99][ 90/352]	Time  0.165 ( 0.169)	Data  0.002 ( 0.005)	Loss 9.7965e-02 (8.2228e-02)	Acc@1  96.09 ( 97.12)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:26:12 - Epoch: [99][100/352]	Time  0.173 ( 0.169)	Data  0.003 ( 0.004)	Loss 9.0292e-02 (8.4095e-02)	Acc@1  98.44 ( 97.09)	Acc@5  99.22 ( 99.98)
07-Mar-22 04:26:14 - Epoch: [99][110/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 6.7759e-02 (8.2517e-02)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:26:16 - Epoch: [99][120/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.004)	Loss 9.2937e-02 (8.2569e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:26:17 - Epoch: [99][130/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 5.7759e-02 (8.3589e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:26:19 - Epoch: [99][140/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.3622e-02 (8.3055e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:26:20 - Epoch: [99][150/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.004)	Loss 3.8949e-02 (8.2126e-02)	Acc@1  99.22 ( 97.16)	Acc@5 100.00 ( 99.98)
07-Mar-22 04:26:22 - Epoch: [99][160/352]	Time  0.166 ( 0.168)	Data  0.002 ( 0.004)	Loss 7.6073e-02 (8.1761e-02)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:24 - Epoch: [99][170/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.004)	Loss 8.4465e-02 (8.1800e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:26 - Epoch: [99][180/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.003)	Loss 4.7642e-02 (8.1826e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:27 - Epoch: [99][190/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.3785e-02 (8.2119e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:29 - Epoch: [99][200/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 9.3796e-02 (8.2890e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:30 - Epoch: [99][210/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.6753e-01 (8.2796e-02)	Acc@1  94.53 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:32 - Epoch: [99][220/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.8214e-02 (8.3122e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:34 - Epoch: [99][230/352]	Time  0.167 ( 0.168)	Data  0.002 ( 0.003)	Loss 5.5408e-02 (8.2583e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:35 - Epoch: [99][240/352]	Time  0.165 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0808e-01 (8.2849e-02)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:37 - Epoch: [99][250/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.2847e-02 (8.2957e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:39 - Epoch: [99][260/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.6494e-02 (8.3392e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:41 - Epoch: [99][270/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 8.7030e-02 (8.3511e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:42 - Epoch: [99][280/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.6190e-02 (8.3077e-02)	Acc@1  98.44 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:44 - Epoch: [99][290/352]	Time  0.170 ( 0.167)	Data  0.002 ( 0.003)	Loss 2.2196e-02 (8.3624e-02)	Acc@1 100.00 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:46 - Epoch: [99][300/352]	Time  0.166 ( 0.167)	Data  0.002 ( 0.003)	Loss 6.2523e-02 (8.3701e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:47 - Epoch: [99][310/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 5.6638e-02 (8.3489e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:49 - Epoch: [99][320/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1070e-01 (8.3366e-02)	Acc@1  95.31 ( 97.08)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:51 - Epoch: [99][330/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0663e-01 (8.3603e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:52 - Epoch: [99][340/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1786e-01 (8.3765e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:54 - Epoch: [99][350/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 4.9805e-02 (8.3386e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.99)
07-Mar-22 04:26:54 - Test: [ 0/20]	Time  0.380 ( 0.380)	Loss 3.2300e-01 (3.2300e-01)	Acc@1  90.23 ( 90.23)	Acc@5  99.22 ( 99.22)
07-Mar-22 04:26:55 - Test: [10/20]	Time  0.098 ( 0.124)	Loss 3.9629e-01 (3.8525e-01)	Acc@1  89.06 ( 89.49)	Acc@5  99.22 ( 99.64)
07-Mar-22 04:26:56 -  * Acc@1 89.400 Acc@5 99.600
07-Mar-22 04:26:56 - Best acc at epoch 99: 89.77999877929688
07-Mar-22 04:32:24 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=64, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 04:32:24 - Use GPU: 0 for training
07-Mar-22 04:32:24 - => using pre-trained PyTorchCV model 'resnet20_unfold'
07-Mar-22 04:32:29 - match all modules defined in bit_config: True
07-Mar-22 04:32:29 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_conv_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
07-Mar-22 04:32:29 - Epoch: [0][  0/352]	Time  0.468 ( 0.468)	Data  0.236 ( 0.236)	Loss 6.5918e-01 (6.5918e-01)	Acc@1  83.59 ( 83.59)	Acc@5  95.31 ( 95.31)
07-Mar-22 04:32:31 - Epoch: [0][ 10/352]	Time  0.143 ( 0.189)	Data  0.002 ( 0.023)	Loss 5.0786e-01 (5.0387e-01)	Acc@1  85.94 ( 84.66)	Acc@5  98.44 ( 98.86)
07-Mar-22 04:32:32 - Epoch: [0][ 20/352]	Time  0.168 ( 0.178)	Data  0.002 ( 0.013)	Loss 3.0998e-01 (4.1607e-01)	Acc@1  91.41 ( 86.90)	Acc@5 100.00 ( 99.37)
07-Mar-22 04:32:34 - Epoch: [0][ 30/352]	Time  0.168 ( 0.174)	Data  0.002 ( 0.010)	Loss 1.8630e-01 (3.7148e-01)	Acc@1  93.75 ( 88.18)	Acc@5 100.00 ( 99.47)
07-Mar-22 04:32:36 - Epoch: [0][ 40/352]	Time  0.168 ( 0.171)	Data  0.002 ( 0.008)	Loss 1.7898e-01 (3.3049e-01)	Acc@1  95.31 ( 89.18)	Acc@5  99.22 ( 99.52)
07-Mar-22 04:32:37 - Epoch: [0][ 50/352]	Time  0.144 ( 0.169)	Data  0.002 ( 0.007)	Loss 1.9592e-01 (3.0624e-01)	Acc@1  92.97 ( 89.84)	Acc@5 100.00 ( 99.57)
07-Mar-22 04:32:39 - Epoch: [0][ 60/352]	Time  0.165 ( 0.168)	Data  0.002 ( 0.006)	Loss 2.6760e-01 (2.8889e-01)	Acc@1  92.97 ( 90.41)	Acc@5 100.00 ( 99.63)
07-Mar-22 04:32:40 - Epoch: [0][ 70/352]	Time  0.144 ( 0.167)	Data  0.002 ( 0.005)	Loss 2.4330e-01 (2.7465e-01)	Acc@1  89.84 ( 90.80)	Acc@5 100.00 ( 99.68)
07-Mar-22 04:32:42 - Epoch: [0][ 80/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.005)	Loss 1.4020e-01 (2.6400e-01)	Acc@1  93.75 ( 91.12)	Acc@5 100.00 ( 99.69)
07-Mar-22 04:32:44 - Epoch: [0][ 90/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.005)	Loss 1.5644e-01 (2.5053e-01)	Acc@1  92.97 ( 91.54)	Acc@5 100.00 ( 99.73)
07-Mar-22 04:32:45 - Epoch: [0][100/352]	Time  0.171 ( 0.166)	Data  0.002 ( 0.004)	Loss 2.2532e-01 (2.4381e-01)	Acc@1  92.97 ( 91.72)	Acc@5 100.00 ( 99.74)
07-Mar-22 04:32:47 - Epoch: [0][110/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.4127e-01 (2.3676e-01)	Acc@1  92.97 ( 91.90)	Acc@5 100.00 ( 99.75)
07-Mar-22 04:32:49 - Epoch: [0][120/352]	Time  0.144 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.9570e-01 (2.3038e-01)	Acc@1  92.19 ( 92.11)	Acc@5 100.00 ( 99.77)
07-Mar-22 04:32:50 - Epoch: [0][130/352]	Time  0.167 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.8379e-01 (2.2489e-01)	Acc@1  94.53 ( 92.26)	Acc@5 100.00 ( 99.79)
07-Mar-22 04:32:52 - Epoch: [0][140/352]	Time  0.169 ( 0.166)	Data  0.002 ( 0.004)	Loss 1.4338e-01 (2.1971e-01)	Acc@1  94.53 ( 92.42)	Acc@5 100.00 ( 99.80)
07-Mar-22 04:32:54 - Epoch: [0][150/352]	Time  0.171 ( 0.166)	Data  0.002 ( 0.004)	Loss 2.5146e-01 (2.1654e-01)	Acc@1  91.41 ( 92.55)	Acc@5 100.00 ( 99.80)
07-Mar-22 04:32:55 - Epoch: [0][160/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.0917e-01 (2.1156e-01)	Acc@1  97.66 ( 92.71)	Acc@5 100.00 ( 99.81)
07-Mar-22 04:32:57 - Epoch: [0][170/352]	Time  0.172 ( 0.167)	Data  0.002 ( 0.003)	Loss 2.4559e-01 (2.0780e-01)	Acc@1  89.84 ( 92.82)	Acc@5  99.22 ( 99.82)
07-Mar-22 04:32:59 - Epoch: [0][180/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.9227e-01 (2.0456e-01)	Acc@1  90.62 ( 92.94)	Acc@5 100.00 ( 99.83)
07-Mar-22 04:33:00 - Epoch: [0][190/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 9.2955e-02 (2.0115e-01)	Acc@1  97.66 ( 93.07)	Acc@5 100.00 ( 99.83)
07-Mar-22 04:33:02 - Epoch: [0][200/352]	Time  0.167 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1298e-01 (1.9984e-01)	Acc@1  96.09 ( 93.13)	Acc@5 100.00 ( 99.83)
07-Mar-22 04:33:04 - Epoch: [0][210/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.7357e-01 (1.9743e-01)	Acc@1  94.53 ( 93.22)	Acc@5 100.00 ( 99.84)
07-Mar-22 04:33:06 - Epoch: [0][220/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3612e-01 (1.9604e-01)	Acc@1  95.31 ( 93.28)	Acc@5 100.00 ( 99.84)
07-Mar-22 04:33:07 - Epoch: [0][230/352]	Time  0.168 ( 0.167)	Data  0.002 ( 0.003)	Loss 2.5133e-01 (1.9457e-01)	Acc@1  91.41 ( 93.31)	Acc@5 100.00 ( 99.85)
07-Mar-22 04:33:09 - Epoch: [0][240/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.3953e-01 (1.9241e-01)	Acc@1  95.31 ( 93.40)	Acc@5 100.00 ( 99.85)
07-Mar-22 04:33:11 - Epoch: [0][250/352]	Time  0.169 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.2224e-01 (1.8997e-01)	Acc@1  96.09 ( 93.46)	Acc@5 100.00 ( 99.85)
07-Mar-22 04:33:12 - Epoch: [0][260/352]	Time  0.191 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.3619e-01 (1.8973e-01)	Acc@1  96.88 ( 93.45)	Acc@5 100.00 ( 99.86)
07-Mar-22 04:33:14 - Epoch: [0][270/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.0118e-01 (1.8860e-01)	Acc@1  96.88 ( 93.50)	Acc@5 100.00 ( 99.86)
07-Mar-22 04:33:16 - Epoch: [0][280/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.7808e-01 (1.8785e-01)	Acc@1  96.09 ( 93.53)	Acc@5 100.00 ( 99.86)
07-Mar-22 04:33:17 - Epoch: [0][290/352]	Time  0.169 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.3528e-01 (1.8665e-01)	Acc@1  97.66 ( 93.59)	Acc@5 100.00 ( 99.87)
07-Mar-22 04:33:19 - Epoch: [0][300/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.8552e-01 (1.8561e-01)	Acc@1  94.53 ( 93.63)	Acc@5 100.00 ( 99.86)
07-Mar-22 04:33:21 - Epoch: [0][310/352]	Time  0.170 ( 0.168)	Data  0.003 ( 0.003)	Loss 5.3083e-02 (1.8404e-01)	Acc@1  97.66 ( 93.68)	Acc@5 100.00 ( 99.86)
07-Mar-22 04:33:23 - Epoch: [0][320/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.5358e-01 (1.8257e-01)	Acc@1  93.75 ( 93.74)	Acc@5 100.00 ( 99.87)
07-Mar-22 04:33:24 - Epoch: [0][330/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.5307e-01 (1.8181e-01)	Acc@1  95.31 ( 93.74)	Acc@5  99.22 ( 99.87)
07-Mar-22 04:33:26 - Epoch: [0][340/352]	Time  0.170 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.5401e-01 (1.8075e-01)	Acc@1  94.53 ( 93.78)	Acc@5 100.00 ( 99.87)
07-Mar-22 04:33:28 - Epoch: [0][350/352]	Time  0.168 ( 0.168)	Data  0.002 ( 0.003)	Loss 8.4569e-02 (1.7907e-01)	Acc@1  97.66 ( 93.84)	Acc@5 100.00 ( 99.87)
07-Mar-22 04:33:28 - Test: [ 0/20]	Time  0.395 ( 0.395)	Loss 4.0648e-01 (4.0648e-01)	Acc@1  86.72 ( 86.72)	Acc@5  98.44 ( 98.44)
07-Mar-22 04:33:29 - Test: [10/20]	Time  0.103 ( 0.129)	Loss 4.1281e-01 (4.5869e-01)	Acc@1  88.67 ( 86.51)	Acc@5  98.83 ( 99.29)
07-Mar-22 04:33:31 -  * Acc@1 87.140 Acc@5 99.200
07-Mar-22 04:33:31 - Best acc at epoch 0: 87.13999938964844
07-Mar-22 04:33:31 - Epoch: [1][  0/352]	Time  0.385 ( 0.385)	Data  0.241 ( 0.241)	Loss 1.0556e-01 (1.0556e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
07-Mar-22 04:33:33 - Epoch: [1][ 10/352]	Time  0.157 ( 0.178)	Data  0.002 ( 0.024)	Loss 2.2719e-01 (2.0055e-01)	Acc@1  94.53 ( 93.39)	Acc@5 100.00 ( 99.93)
07-Mar-22 04:33:34 - Epoch: [1][ 20/352]	Time  0.155 ( 0.168)	Data  0.002 ( 0.013)	Loss 1.4413e-01 (1.6627e-01)	Acc@1  94.53 ( 94.46)	Acc@5 100.00 ( 99.96)
07-Mar-22 04:33:36 - Epoch: [1][ 30/352]	Time  0.144 ( 0.164)	Data  0.001 ( 0.010)	Loss 1.6309e-01 (1.6188e-01)	Acc@1  96.09 ( 94.66)	Acc@5  99.22 ( 99.95)
07-Mar-22 04:33:38 - Epoch: [1][ 40/352]	Time  0.193 ( 0.170)	Data  0.002 ( 0.008)	Loss 1.3518e-01 (1.5880e-01)	Acc@1  96.09 ( 94.91)	Acc@5 100.00 ( 99.96)
07-Mar-22 04:33:39 - Epoch: [1][ 50/352]	Time  0.169 ( 0.170)	Data  0.001 ( 0.007)	Loss 7.0675e-02 (1.5404e-01)	Acc@1  98.44 ( 95.08)	Acc@5 100.00 ( 99.97)
07-Mar-22 05:01:00 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='alexnet', batch_size=64, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 05:01:00 - Use GPU: 0 for training
07-Mar-22 05:01:00 - => using pre-trained PyTorchCV model 'alexnet'
07-Mar-22 05:01:01 - Model file not found. Downloading to /root/.torch/models/alexnet-1664-2768cdb3.pth.
07-Mar-22 05:02:47 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='alexnet', batch_size=64, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 05:02:47 - Use GPU: 0 for training
07-Mar-22 05:02:47 - => using pre-trained PyTorchCV model 'alexnet'
07-Mar-22 05:08:42 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='alexnet', batch_size=64, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 05:08:42 - Use GPU: 0 for training
07-Mar-22 05:08:42 - => using pre-trained PyTorchCV model 'alexnet'
07-Mar-22 05:11:00 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='alexnet', batch_size=64, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 05:11:00 - Use GPU: 0 for training
07-Mar-22 05:11:00 - => using pre-trained PyTorchCV model 'alexnet'
07-Mar-22 05:19:08 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='alexnet', batch_size=64, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 05:19:08 - Use GPU: 0 for training
07-Mar-22 05:19:08 - => using pre-trained PyTorchCV model 'alexnet'
07-Mar-22 05:22:10 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='alexnet', batch_size=64, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 05:22:10 - Use GPU: 0 for training
07-Mar-22 05:22:10 - => using pre-trained PyTorchCV model 'alexnet'
07-Mar-22 05:29:04 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=64, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 05:29:04 - Use GPU: 0 for training
07-Mar-22 05:29:04 - => using pre-trained PyTorchCV model 'resnet20_unfold'
07-Mar-22 05:29:08 - match all modules defined in bit_config: True
07-Mar-22 05:29:08 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_conv_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
07-Mar-22 05:30:21 - Namespace(act_percentile=0, act_range_momentum=0.99, arch='resnet20_unfold', batch_size=64, bias_bit=32, channel_wise=True, checkpoint_iter=-1, data='cifar10', data_percentage=1, dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distill_alpha=0.95, distill_method='None', dnn_path='/workspace/pretrained_models/cifar10/resnet20/checkpoint.pth', epochs=100, evaluate=False, evaluate_times=-1, fix_BN=True, fix_BN_threshold=None, fixed_point_quantization=False, gpu=0, lr=0.001, momentum=0.9, multiprocessing_distributed=False, pretrained=True, print_freq=10, quant_mode='symmetric', quant_scheme='uniform4', rank=-1, resume='', resume_quantize=False, save_path='checkpoints/imagenet/test/', seed=None, start_epoch=0, teacher_arch='resnet101', temperature=6, transfer_param=True, weight_decay=0.0001, weight_percentile=0, workers=4, world_size=-1)
07-Mar-22 05:30:21 - Use GPU: 0 for training
07-Mar-22 05:30:21 - => using pre-trained PyTorchCV model 'resnet20_unfold'
07-Mar-22 05:30:25 - match all modules defined in bit_config: True
07-Mar-22 05:30:25 - Q_ResNet20_unfold(
  (quant_input): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_conv): (QuantConv2d(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  ), weight_bit=8, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
  (quant_init_block_conv_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_init_block_bn): (QuantBn(
    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
  ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
  (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (act): ReLU()
  (stage1.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage1.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage2.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit1): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=None, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_identity_conv): (QuantConv2d(
      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_identity_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_identity_bn): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit2): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (stage3.unit3): Q_ResBlockBn_unfold(
    (quant_act): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv1): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv1_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn1): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act1): QuantAct(activation_bit=4, full_precision_flag=False, quant_mode=asymmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_conv2): (QuantConv2d(
      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    ), weight_bit=4, bias_bit=32, groups=1, wt-channel-wise=True, wt-percentile=0, quant_mode=symmetric)
    (quant_conv2_act): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
    (quant_bn2): (QuantBn(
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)
    ), weight_bit=16, bias_bit=32, wt-percentile=0, quant_mode=symmetric, fix_BN=True)
    (quant_act_int32): QuantAct(activation_bit=16, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  )
  (final_pool): QuantAveragePool2d(
    (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)
  )
  (quant_act_output): QuantAct(activation_bit=8, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (quant_output): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
)
07-Mar-22 05:30:26 - Epoch: [0][  0/352]	Time  0.501 ( 0.501)	Data  0.288 ( 0.288)	Loss 7.8035e-01 (7.8035e-01)	Acc@1  77.34 ( 77.34)	Acc@5  97.66 ( 97.66)
07-Mar-22 05:30:27 - Epoch: [0][ 10/352]	Time  0.159 ( 0.185)	Data  0.002 ( 0.028)	Loss 3.9962e-01 (4.8257e-01)	Acc@1  87.50 ( 84.02)	Acc@5  97.66 ( 99.29)
07-Mar-22 05:30:29 - Epoch: [0][ 20/352]	Time  0.173 ( 0.177)	Data  0.003 ( 0.016)	Loss 1.6751e-01 (4.0569e-01)	Acc@1  93.75 ( 86.53)	Acc@5  99.22 ( 99.44)
07-Mar-22 05:30:31 - Epoch: [0][ 30/352]	Time  0.171 ( 0.175)	Data  0.002 ( 0.011)	Loss 2.9371e-01 (3.7067e-01)	Acc@1  89.84 ( 87.73)	Acc@5 100.00 ( 99.60)
07-Mar-22 05:30:32 - Epoch: [0][ 40/352]	Time  0.171 ( 0.174)	Data  0.002 ( 0.009)	Loss 1.6598e-01 (3.3791e-01)	Acc@1  92.97 ( 88.66)	Acc@5 100.00 ( 99.62)
07-Mar-22 05:30:34 - Epoch: [0][ 50/352]	Time  0.169 ( 0.174)	Data  0.002 ( 0.008)	Loss 1.4541e-01 (3.1504e-01)	Acc@1  95.31 ( 89.40)	Acc@5 100.00 ( 99.66)
07-Mar-22 05:30:36 - Epoch: [0][ 60/352]	Time  0.171 ( 0.173)	Data  0.002 ( 0.007)	Loss 1.0166e-01 (2.9409e-01)	Acc@1  96.09 ( 90.18)	Acc@5 100.00 ( 99.71)
07-Mar-22 05:30:37 - Epoch: [0][ 70/352]	Time  0.154 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.7998e-01 (2.8468e-01)	Acc@1  96.09 ( 90.55)	Acc@5  99.22 ( 99.70)
07-Mar-22 05:30:39 - Epoch: [0][ 80/352]	Time  0.150 ( 0.170)	Data  0.002 ( 0.006)	Loss 1.5342e-01 (2.7425e-01)	Acc@1  92.97 ( 90.84)	Acc@5 100.00 ( 99.74)
07-Mar-22 05:30:40 - Epoch: [0][ 90/352]	Time  0.147 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.7337e-01 (2.6357e-01)	Acc@1  92.19 ( 91.21)	Acc@5 100.00 ( 99.74)
07-Mar-22 05:30:42 - Epoch: [0][100/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.005)	Loss 2.0572e-01 (2.5732e-01)	Acc@1  93.75 ( 91.45)	Acc@5 100.00 ( 99.77)
07-Mar-22 05:30:44 - Epoch: [0][110/352]	Time  0.172 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.3910e-01 (2.4916e-01)	Acc@1  95.31 ( 91.67)	Acc@5 100.00 ( 99.79)
07-Mar-22 05:30:45 - Epoch: [0][120/352]	Time  0.171 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.7589e-01 (2.4439e-01)	Acc@1  92.97 ( 91.81)	Acc@5 100.00 ( 99.80)
07-Mar-22 05:30:47 - Epoch: [0][130/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.3140e-01 (2.4029e-01)	Acc@1  95.31 ( 91.99)	Acc@5 100.00 ( 99.80)
07-Mar-22 05:30:49 - Epoch: [0][140/352]	Time  0.174 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.4905e-01 (2.3415e-01)	Acc@1  95.31 ( 92.19)	Acc@5 100.00 ( 99.82)
07-Mar-22 05:30:51 - Epoch: [0][150/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.004)	Loss 9.5241e-02 (2.2849e-01)	Acc@1  96.88 ( 92.39)	Acc@5 100.00 ( 99.83)
07-Mar-22 05:30:52 - Epoch: [0][160/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.004)	Loss 2.1239e-01 (2.2443e-01)	Acc@1  89.84 ( 92.48)	Acc@5 100.00 ( 99.84)
07-Mar-22 05:30:54 - Epoch: [0][170/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.1994e-01 (2.2029e-01)	Acc@1  95.31 ( 92.63)	Acc@5 100.00 ( 99.84)
07-Mar-22 05:30:56 - Epoch: [0][180/352]	Time  0.172 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.4115e-01 (2.1725e-01)	Acc@1  94.53 ( 92.71)	Acc@5 100.00 ( 99.85)
07-Mar-22 05:30:57 - Epoch: [0][190/352]	Time  0.166 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.0260e-01 (2.1482e-01)	Acc@1  95.31 ( 92.78)	Acc@5 100.00 ( 99.85)
07-Mar-22 05:30:59 - Epoch: [0][200/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.004)	Loss 1.8627e-01 (2.1278e-01)	Acc@1  94.53 ( 92.88)	Acc@5 100.00 ( 99.86)
07-Mar-22 05:31:01 - Epoch: [0][210/352]	Time  0.169 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.2970e-01 (2.1046e-01)	Acc@1  94.53 ( 92.95)	Acc@5 100.00 ( 99.86)
07-Mar-22 05:31:02 - Epoch: [0][220/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.3178e-01 (2.0809e-01)	Acc@1  96.09 ( 93.03)	Acc@5 100.00 ( 99.87)
07-Mar-22 05:31:04 - Epoch: [0][230/352]	Time  0.170 ( 0.169)	Data  0.002 ( 0.003)	Loss 9.9814e-02 (2.0517e-01)	Acc@1  95.31 ( 93.09)	Acc@5 100.00 ( 99.87)
07-Mar-22 05:31:06 - Epoch: [0][240/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 2.1077e-01 (2.0275e-01)	Acc@1  92.19 ( 93.16)	Acc@5 100.00 ( 99.88)
07-Mar-22 05:31:07 - Epoch: [0][250/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.2195e-01 (2.0144e-01)	Acc@1  96.09 ( 93.19)	Acc@5 100.00 ( 99.88)
07-Mar-22 05:31:09 - Epoch: [0][260/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.7115e-01 (1.9925e-01)	Acc@1  92.97 ( 93.24)	Acc@5 100.00 ( 99.87)
07-Mar-22 05:31:11 - Epoch: [0][270/352]	Time  0.171 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.7264e-01 (1.9771e-01)	Acc@1  92.97 ( 93.27)	Acc@5 100.00 ( 99.88)
07-Mar-22 05:31:13 - Epoch: [0][280/352]	Time  0.167 ( 0.169)	Data  0.002 ( 0.003)	Loss 6.8752e-02 (1.9557e-01)	Acc@1  97.66 ( 93.35)	Acc@5 100.00 ( 99.88)
07-Mar-22 05:31:14 - Epoch: [0][290/352]	Time  0.168 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.5431e-01 (1.9477e-01)	Acc@1  93.75 ( 93.37)	Acc@5 100.00 ( 99.89)
07-Mar-22 05:31:16 - Epoch: [0][300/352]	Time  0.153 ( 0.169)	Data  0.002 ( 0.003)	Loss 1.7211e-01 (1.9322e-01)	Acc@1  95.31 ( 93.44)	Acc@5 100.00 ( 99.89)
07-Mar-22 05:31:17 - Epoch: [0][310/352]	Time  0.153 ( 0.168)	Data  0.002 ( 0.003)	Loss 1.3628e-01 (1.9236e-01)	Acc@1  95.31 ( 93.47)	Acc@5 100.00 ( 99.89)
07-Mar-22 05:31:19 - Epoch: [0][320/352]	Time  0.152 ( 0.167)	Data  0.001 ( 0.003)	Loss 8.0118e-02 (1.9120e-01)	Acc@1  97.66 ( 93.50)	Acc@5 100.00 ( 99.89)
07-Mar-22 05:31:20 - Epoch: [0][330/352]	Time  0.147 ( 0.167)	Data  0.002 ( 0.003)	Loss 1.1414e-01 (1.8940e-01)	Acc@1  94.53 ( 93.54)	Acc@5 100.00 ( 99.89)
07-Mar-22 05:31:22 - Epoch: [0][340/352]	Time  0.151 ( 0.167)	Data  0.001 ( 0.003)	Loss 6.6060e-02 (1.8721e-01)	Acc@1  98.44 ( 93.63)	Acc@5 100.00 ( 99.89)
07-Mar-22 05:31:23 - Epoch: [0][350/352]	Time  0.145 ( 0.166)	Data  0.001 ( 0.003)	Loss 2.4611e-01 (1.8636e-01)	Acc@1  93.75 ( 93.66)	Acc@5 100.00 ( 99.90)
07-Mar-22 05:31:24 - Test: [ 0/20]	Time  0.421 ( 0.421)	Loss 4.3624e-01 (4.3624e-01)	Acc@1  88.67 ( 88.67)	Acc@5  99.22 ( 99.22)
07-Mar-22 05:31:25 - Test: [10/20]	Time  0.099 ( 0.132)	Loss 4.1512e-01 (4.4176e-01)	Acc@1  89.45 ( 87.29)	Acc@5  98.83 ( 98.93)
07-Mar-22 05:31:26 -  * Acc@1 87.600 Acc@5 99.000
07-Mar-22 05:31:26 - Best acc at epoch 0: 87.5999984741211
07-Mar-22 05:31:27 - Epoch: [1][  0/352]	Time  0.389 ( 0.389)	Data  0.236 ( 0.236)	Loss 1.5779e-01 (1.5779e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
07-Mar-22 05:31:28 - Epoch: [1][ 10/352]	Time  0.130 ( 0.164)	Data  0.002 ( 0.023)	Loss 1.3849e-01 (1.4813e-01)	Acc@1  96.09 ( 95.03)	Acc@5 100.00 ( 99.93)
07-Mar-22 05:31:30 - Epoch: [1][ 20/352]	Time  0.177 ( 0.167)	Data  0.003 ( 0.013)	Loss 1.5150e-01 (1.3542e-01)	Acc@1  95.31 ( 95.35)	Acc@5 100.00 ( 99.93)
07-Mar-22 05:31:32 - Epoch: [1][ 30/352]	Time  0.173 ( 0.171)	Data  0.002 ( 0.010)	Loss 1.0799e-01 (1.4151e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.95)
